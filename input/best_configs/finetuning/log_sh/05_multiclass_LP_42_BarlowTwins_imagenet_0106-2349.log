IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-07 01:32:51,857	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-07 01:32:51,858	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-07 01:32:54,383	SUCC scripts.py:747 -- --------------------
2024-01-07 01:32:54,383	SUCC scripts.py:748 -- Ray runtime started.
2024-01-07 01:32:54,383	SUCC scripts.py:749 -- --------------------
2024-01-07 01:32:54,383	INFO scripts.py:751 -- Next steps
2024-01-07 01:32:54,383	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-07 01:32:54,383	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-07 01:32:54,383	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-07 01:32:54,383	INFO scripts.py:773 -- import ray
2024-01-07 01:32:54,384	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-07 01:32:54,384	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-07 01:32:54,384	INFO scripts.py:791 --   ray status
2024-01-07 01:32:54,384	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-07 01:32:54,384	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-07 01:32:54,384	INFO scripts.py:810 --   ray stop
2024-01-07 01:32:54,384	INFO scripts.py:891 -- --block
2024-01-07 01:32:54,384	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-07 01:32:54,384	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              3379008939304740468
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7f3e6b9070a0>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          BarlowTwins
task_name:           multiclass
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          5
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         random
seed:                42
dropout:             None
transfer_learning:   LP
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1

Initial imbalanced dataset:
Diff. classes --> [ 1 21 22 23 31 35 41 42 47 51]
Samples/class --> [5 5 5 5 5 5 5 5 5 5]

Creating the sample distribution plot...
Sample distribution computation in train dataset (s): 1.78
Resulting balanced dataloader:
Diff. classes     --> [0 1 2 3 4 5 6 7 8 9]
New samples/class --> [5 5 5 5 5 5 5 5 5 5]
Done!

Model resnet18 with pretrained weights using BarlowTwins SSL
Model loaded from snapshot_BarlowTwins_resnet18_bd=False_iw=random.pt
Model name:        BarlowTwins
Backbone name:     resnet18
Hidden layer dim.: 256
Output layer dim.: 128
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Linear probing adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-07 01:33:38,185	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-07 01:33:38,198	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-07 01:34:00,214	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
== Status ==
Current time: 2024-01-07 01:34:00 (running for 00:00:21.09)
Memory usage on this node: 13.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |
| train_66d79_00001 | PENDING  |                     | 0.001  |       0.99 |         0      |
| train_66d79_00002 | PENDING  |                     | 0.01   |       0.99 |         0      |
| train_66d79_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111467)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=111467)[0m Configuration completed!
[2m[36m(func pid=111467)[0m New optimizer parameters:
[2m[36m(func pid=111467)[0m SGD (
[2m[36m(func pid=111467)[0m Parameter Group 0
[2m[36m(func pid=111467)[0m     dampening: 0
[2m[36m(func pid=111467)[0m     differentiable: False
[2m[36m(func pid=111467)[0m     foreach: None
[2m[36m(func pid=111467)[0m     lr: 0.0001
[2m[36m(func pid=111467)[0m     maximize: False
[2m[36m(func pid=111467)[0m     momentum: 0.99
[2m[36m(func pid=111467)[0m     nesterov: False
[2m[36m(func pid=111467)[0m     weight_decay: 0
[2m[36m(func pid=111467)[0m )
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9755 | Steps: 2 | Val loss: 2.3168 | Batch size: 32 | lr: 0.0001 | Duration: 4.63s
[2m[36m(func pid=111467)[0m top1: 0.177705223880597
[2m[36m(func pid=111467)[0m top5: 0.534981343283582
[2m[36m(func pid=111467)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=111467)[0m f1_macro: 0.11560276244563591
[2m[36m(func pid=111467)[0m f1_weighted: 0.12404213592226407
[2m[36m(func pid=111467)[0m f1_per_class: [0.301, 0.349, 0.0, 0.091, 0.0, 0.227, 0.015, 0.0, 0.0, 0.172]
== Status ==
Current time: 2024-01-07 01:34:09 (running for 00:00:29.77)
Memory usage on this node: 14.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |
| train_66d79_00002 | PENDING  |                     | 0.01   |       0.99 |         0      |
| train_66d79_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111842)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=111842)[0m Configuration completed!
[2m[36m(func pid=111842)[0m New optimizer parameters:
[2m[36m(func pid=111842)[0m SGD (
[2m[36m(func pid=111842)[0m Parameter Group 0
[2m[36m(func pid=111842)[0m     dampening: 0
[2m[36m(func pid=111842)[0m     differentiable: False
[2m[36m(func pid=111842)[0m     foreach: None
[2m[36m(func pid=111842)[0m     lr: 0.001
[2m[36m(func pid=111842)[0m     maximize: False
[2m[36m(func pid=111842)[0m     momentum: 0.99
[2m[36m(func pid=111842)[0m     nesterov: False
[2m[36m(func pid=111842)[0m     weight_decay: 0
[2m[36m(func pid=111842)[0m )
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0080 | Steps: 2 | Val loss: 2.3212 | Batch size: 32 | lr: 0.001 | Duration: 4.11s
[2m[36m(func pid=111842)[0m top1: 0.17583955223880596
[2m[36m(func pid=111842)[0m top5: 0.5289179104477612
[2m[36m(func pid=111842)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=111842)[0m f1_macro: 0.11934964845338539
[2m[36m(func pid=111842)[0m f1_weighted: 0.12523259421149865
[2m[36m(func pid=111842)[0m f1_per_class: [0.329, 0.352, 0.0, 0.09, 0.0, 0.205, 0.024, 0.0, 0.0, 0.194]
== Status ==
Current time: 2024-01-07 01:34:17 (running for 00:00:38.10)
Memory usage on this node: 17.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |
| train_66d79_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=112258)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112258)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=112258)[0m Configuration completed!
[2m[36m(func pid=112258)[0m New optimizer parameters:
[2m[36m(func pid=112258)[0m SGD (
[2m[36m(func pid=112258)[0m Parameter Group 0
[2m[36m(func pid=112258)[0m     dampening: 0
[2m[36m(func pid=112258)[0m     differentiable: False
[2m[36m(func pid=112258)[0m     foreach: None
[2m[36m(func pid=112258)[0m     lr: 0.01
[2m[36m(func pid=112258)[0m     maximize: False
[2m[36m(func pid=112258)[0m     momentum: 0.99
[2m[36m(func pid=112258)[0m     nesterov: False
[2m[36m(func pid=112258)[0m     weight_decay: 0
[2m[36m(func pid=112258)[0m )
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9885 | Steps: 2 | Val loss: 2.3126 | Batch size: 32 | lr: 0.01 | Duration: 4.75s
[2m[36m(func pid=112258)[0m top1: 0.18097014925373134
[2m[36m(func pid=112258)[0m top5: 0.5317164179104478
[2m[36m(func pid=112258)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=112258)[0m f1_macro: 0.1266325129215387
[2m[36m(func pid=112258)[0m f1_weighted: 0.1309880817727031
[2m[36m(func pid=112258)[0m f1_per_class: [0.347, 0.351, 0.0, 0.105, 0.0, 0.195, 0.024, 0.039, 0.0, 0.205]
== Status ==
Current time: 2024-01-07 01:34:25 (running for 00:00:46.48)
Memory usage on this node: 20.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 01:34:33 (running for 00:00:54.55)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.975 |      0.116 |                    1 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |        |            |                      |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |        |            |                      |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |        |            |                      |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112678)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=112678)[0m Configuration completed!
[2m[36m(func pid=112678)[0m New optimizer parameters:
[2m[36m(func pid=112678)[0m SGD (
[2m[36m(func pid=112678)[0m Parameter Group 0
[2m[36m(func pid=112678)[0m     dampening: 0
[2m[36m(func pid=112678)[0m     differentiable: False
[2m[36m(func pid=112678)[0m     foreach: None
[2m[36m(func pid=112678)[0m     lr: 0.1
[2m[36m(func pid=112678)[0m     maximize: False
[2m[36m(func pid=112678)[0m     momentum: 0.99
[2m[36m(func pid=112678)[0m     nesterov: False
[2m[36m(func pid=112678)[0m     weight_decay: 0
[2m[36m(func pid=112678)[0m )
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9306 | Steps: 2 | Val loss: 2.3195 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9900 | Steps: 2 | Val loss: 2.3228 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8898 | Steps: 2 | Val loss: 2.2872 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0748 | Steps: 2 | Val loss: 2.2767 | Batch size: 32 | lr: 0.1 | Duration: 4.36s
== Status ==
Current time: 2024-01-07 01:34:38 (running for 00:00:59.57)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.975 |      0.116 |                    1 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  3.008 |      0.119 |                    1 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  2.989 |      0.127 |                    1 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |        |            |                      |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.18330223880597016
[2m[36m(func pid=111467)[0m top5: 0.5331156716417911
[2m[36m(func pid=111467)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=111467)[0m f1_macro: 0.11080061264305156
[2m[36m(func pid=111467)[0m f1_weighted: 0.12896034249157445
[2m[36m(func pid=111467)[0m f1_per_class: [0.225, 0.331, 0.0, 0.098, 0.011, 0.287, 0.015, 0.024, 0.0, 0.118]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m top1: 0.1865671641791045
[2m[36m(func pid=111842)[0m top5: 0.534981343283582
[2m[36m(func pid=111842)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=111842)[0m f1_macro: 0.12137461804761554
[2m[36m(func pid=111842)[0m f1_weighted: 0.13434192208541434
[2m[36m(func pid=111842)[0m f1_per_class: [0.321, 0.337, 0.0, 0.104, 0.01, 0.288, 0.015, 0.035, 0.0, 0.103]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112258)[0m top1: 0.2019589552238806
[2m[36m(func pid=112258)[0m top5: 0.5578358208955224
[2m[36m(func pid=112258)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=112258)[0m f1_macro: 0.14949698758911373
[2m[36m(func pid=112258)[0m f1_weighted: 0.1627039812186416
[2m[36m(func pid=112258)[0m f1_per_class: [0.257, 0.331, 0.067, 0.18, 0.0, 0.274, 0.024, 0.156, 0.0, 0.206]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.14272388059701493
[2m[36m(func pid=112678)[0m top5: 0.6035447761194029
[2m[36m(func pid=112678)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=112678)[0m f1_macro: 0.14174592415604306
[2m[36m(func pid=112678)[0m f1_weighted: 0.12150249074430097
[2m[36m(func pid=112678)[0m f1_per_class: [0.101, 0.251, 0.375, 0.169, 0.0, 0.008, 0.027, 0.254, 0.021, 0.212]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9749 | Steps: 2 | Val loss: 2.3306 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9480 | Steps: 2 | Val loss: 2.3218 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.6915 | Steps: 2 | Val loss: 2.2541 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.1107 | Steps: 2 | Val loss: 2.3881 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 01:34:44 (running for 00:01:05.10)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.975 |      0.099 |                    3 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  2.931 |      0.121 |                    2 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  2.89  |      0.149 |                    2 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  3.075 |      0.142 |                    1 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.17863805970149255
[2m[36m(func pid=111467)[0m top5: 0.519589552238806
[2m[36m(func pid=111467)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=111467)[0m f1_macro: 0.09911279092538391
[2m[36m(func pid=111467)[0m f1_weighted: 0.1266109611365931
[2m[36m(func pid=111467)[0m f1_per_class: [0.137, 0.31, 0.0, 0.108, 0.01, 0.309, 0.009, 0.021, 0.0, 0.087]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m top1: 0.18097014925373134
[2m[36m(func pid=111842)[0m top5: 0.5279850746268657
[2m[36m(func pid=111842)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=111842)[0m f1_macro: 0.12028332735390597
[2m[36m(func pid=111842)[0m f1_weighted: 0.13581973426195515
[2m[36m(func pid=111842)[0m f1_per_class: [0.306, 0.311, 0.0, 0.116, 0.01, 0.307, 0.018, 0.031, 0.0, 0.103]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112258)[0m top1: 0.21222014925373134
[2m[36m(func pid=112258)[0m top5: 0.5970149253731343
[2m[36m(func pid=112258)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=112258)[0m f1_macro: 0.19779087461716882
[2m[36m(func pid=112258)[0m f1_weighted: 0.1818250176400191
[2m[36m(func pid=112258)[0m f1_per_class: [0.244, 0.295, 0.259, 0.2, 0.065, 0.34, 0.033, 0.266, 0.045, 0.231]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.09468283582089553
[2m[36m(func pid=112678)[0m top5: 0.746268656716418
[2m[36m(func pid=112678)[0m f1_micro: 0.09468283582089553
[2m[36m(func pid=112678)[0m f1_macro: 0.18391304257386276
[2m[36m(func pid=112678)[0m f1_weighted: 0.09122717591920307
[2m[36m(func pid=112678)[0m f1_per_class: [0.058, 0.026, 0.69, 0.129, 0.074, 0.12, 0.019, 0.351, 0.044, 0.329]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9545 | Steps: 2 | Val loss: 2.3368 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9340 | Steps: 2 | Val loss: 2.3217 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.4127 | Steps: 2 | Val loss: 2.2090 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.2485 | Steps: 2 | Val loss: 2.2832 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 01:34:49 (running for 00:01:10.23)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.975 |      0.099 |                    3 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  2.934 |      0.126 |                    4 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  2.692 |      0.198 |                    3 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  2.111 |      0.184 |                    2 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.1791044776119403
[2m[36m(func pid=111842)[0m top5: 0.5242537313432836
[2m[36m(func pid=111842)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=111842)[0m f1_macro: 0.12559755256487942
[2m[36m(func pid=111842)[0m f1_weighted: 0.1384671761904004
[2m[36m(func pid=111842)[0m f1_per_class: [0.317, 0.308, 0.069, 0.118, 0.009, 0.324, 0.021, 0.028, 0.0, 0.062]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m top1: 0.17117537313432835
[2m[36m(func pid=111467)[0m top5: 0.5088619402985075
[2m[36m(func pid=111467)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=111467)[0m f1_macro: 0.09385670624959866
[2m[36m(func pid=111467)[0m f1_weighted: 0.12435001492030857
[2m[36m(func pid=111467)[0m f1_per_class: [0.113, 0.291, 0.0, 0.113, 0.01, 0.304, 0.012, 0.019, 0.0, 0.077]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.20755597014925373
[2m[36m(func pid=112258)[0m top5: 0.6539179104477612
[2m[36m(func pid=112258)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=112258)[0m f1_macro: 0.190422197647723
[2m[36m(func pid=112258)[0m f1_weighted: 0.18414192652778982
[2m[36m(func pid=112258)[0m f1_per_class: [0.206, 0.251, 0.113, 0.223, 0.128, 0.366, 0.027, 0.327, 0.045, 0.218]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.19542910447761194
[2m[36m(func pid=112678)[0m top5: 0.7821828358208955
[2m[36m(func pid=112678)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=112678)[0m f1_macro: 0.24984643938449244
[2m[36m(func pid=112678)[0m f1_weighted: 0.18764192074005218
[2m[36m(func pid=112678)[0m f1_per_class: [0.159, 0.128, 0.6, 0.241, 0.13, 0.364, 0.068, 0.382, 0.062, 0.364]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8826 | Steps: 2 | Val loss: 2.3156 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9708 | Steps: 2 | Val loss: 2.3398 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.1491 | Steps: 2 | Val loss: 2.1483 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.5782 | Steps: 2 | Val loss: 2.4928 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=111842)[0m top1: 0.17583955223880596
[2m[36m(func pid=111842)[0m top5: 0.5340485074626866
[2m[36m(func pid=111842)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=111842)[0m f1_macro: 0.1320151920231906
[2m[36m(func pid=111842)[0m f1_weighted: 0.14936504169598602
[2m[36m(func pid=111842)[0m f1_per_class: [0.308, 0.287, 0.05, 0.129, 0.009, 0.339, 0.045, 0.064, 0.015, 0.074]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 01:34:55 (running for 00:01:15.80)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.971 |      0.083 |                    5 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  2.883 |      0.132 |                    5 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  2.413 |      0.19  |                    4 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  1.249 |      0.25  |                    3 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.16511194029850745
[2m[36m(func pid=111467)[0m top5: 0.5046641791044776
[2m[36m(func pid=111467)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=111467)[0m f1_macro: 0.08332716815245642
[2m[36m(func pid=111467)[0m f1_weighted: 0.12191641523531405
[2m[36m(func pid=111467)[0m f1_per_class: [0.061, 0.273, 0.0, 0.117, 0.009, 0.308, 0.015, 0.018, 0.0, 0.033]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.21828358208955223
[2m[36m(func pid=112258)[0m top5: 0.7089552238805971
[2m[36m(func pid=112258)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=112258)[0m f1_macro: 0.1981058515649332
[2m[36m(func pid=112258)[0m f1_weighted: 0.19182751223088332
[2m[36m(func pid=112258)[0m f1_per_class: [0.201, 0.23, 0.109, 0.259, 0.118, 0.373, 0.024, 0.338, 0.047, 0.282]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.22154850746268656
[2m[36m(func pid=112678)[0m top5: 0.8101679104477612
[2m[36m(func pid=112678)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=112678)[0m f1_macro: 0.2811147821894301
[2m[36m(func pid=112678)[0m f1_weighted: 0.24232324213931233
[2m[36m(func pid=112678)[0m f1_per_class: [0.293, 0.179, 0.615, 0.222, 0.122, 0.392, 0.238, 0.282, 0.069, 0.4]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8047 | Steps: 2 | Val loss: 2.3047 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9662 | Steps: 2 | Val loss: 2.3474 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.8272 | Steps: 2 | Val loss: 2.0726 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5822 | Steps: 2 | Val loss: 2.6675 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=111842)[0m top1: 0.17537313432835822
[2m[36m(func pid=111842)[0m top5: 0.5503731343283582
[2m[36m(func pid=111842)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=111842)[0m f1_macro: 0.14487082079975253
[2m[36m(func pid=111842)[0m f1_weighted: 0.16227051920886812
[2m[36m(func pid=111842)[0m f1_per_class: [0.259, 0.266, 0.102, 0.147, 0.016, 0.345, 0.071, 0.121, 0.022, 0.1]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 01:35:00 (running for 00:01:21.18)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.971 |      0.083 |                    5 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  2.805 |      0.145 |                    6 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  1.827 |      0.205 |                    6 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.578 |      0.281 |                    4 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=112258)[0m top1: 0.23227611940298507
[2m[36m(func pid=112258)[0m top5: 0.7560634328358209
[2m[36m(func pid=112258)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=112258)[0m f1_macro: 0.20492626550806023
[2m[36m(func pid=112258)[0m f1_weighted: 0.22070575480374632
[2m[36m(func pid=112258)[0m f1_per_class: [0.176, 0.21, 0.113, 0.306, 0.135, 0.362, 0.096, 0.33, 0.045, 0.277]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111467)[0m top1: 0.15625
[2m[36m(func pid=111467)[0m top5: 0.4920708955223881
[2m[36m(func pid=111467)[0m f1_micro: 0.15625
[2m[36m(func pid=111467)[0m f1_macro: 0.08593142833424391
[2m[36m(func pid=111467)[0m f1_weighted: 0.12255606677999763
[2m[36m(func pid=111467)[0m f1_per_class: [0.099, 0.246, 0.0, 0.12, 0.009, 0.299, 0.029, 0.025, 0.0, 0.033]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112678)[0m top1: 0.2728544776119403
[2m[36m(func pid=112678)[0m top5: 0.8330223880597015
[2m[36m(func pid=112678)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=112678)[0m f1_macro: 0.2892586641110423
[2m[36m(func pid=112678)[0m f1_weighted: 0.30432332595134637
[2m[36m(func pid=112678)[0m f1_per_class: [0.224, 0.167, 0.667, 0.279, 0.099, 0.405, 0.417, 0.193, 0.076, 0.366]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7998 | Steps: 2 | Val loss: 2.2899 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.1937 | Steps: 2 | Val loss: 3.0423 | Batch size: 32 | lr: 0.1 | Duration: 2.58s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9659 | Steps: 2 | Val loss: 2.3514 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.4482 | Steps: 2 | Val loss: 2.0003 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=111842)[0m top1: 0.1730410447761194
[2m[36m(func pid=111842)[0m top5: 0.5732276119402985
[2m[36m(func pid=111842)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=111842)[0m f1_macro: 0.15572434922092387
[2m[36m(func pid=111842)[0m f1_weighted: 0.16751869979970058
[2m[36m(func pid=111842)[0m f1_per_class: [0.276, 0.24, 0.137, 0.156, 0.03, 0.356, 0.081, 0.161, 0.026, 0.094]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 01:35:05 (running for 00:01:26.19)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.966 |      0.086 |                    6 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  2.8   |      0.156 |                    7 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  1.827 |      0.205 |                    6 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.194 |      0.3   |                    6 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=112678)[0m top1: 0.31203358208955223
[2m[36m(func pid=112678)[0m top5: 0.8563432835820896
[2m[36m(func pid=112678)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=112678)[0m f1_macro: 0.300457258606703
[2m[36m(func pid=112678)[0m f1_weighted: 0.3404582123929696
[2m[36m(func pid=112678)[0m f1_per_class: [0.158, 0.186, 0.727, 0.347, 0.124, 0.373, 0.477, 0.188, 0.119, 0.306]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111467)[0m top1: 0.14972014925373134
[2m[36m(func pid=111467)[0m top5: 0.49486940298507465
[2m[36m(func pid=111467)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=111467)[0m f1_macro: 0.08350704942771632
[2m[36m(func pid=111467)[0m f1_weighted: 0.12408480727951109
[2m[36m(func pid=111467)[0m f1_per_class: [0.087, 0.217, 0.0, 0.117, 0.016, 0.301, 0.051, 0.047, 0.0, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.2635261194029851
[2m[36m(func pid=112258)[0m top5: 0.7924440298507462
[2m[36m(func pid=112258)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=112258)[0m f1_macro: 0.23541915459349552
[2m[36m(func pid=112258)[0m f1_weighted: 0.26862638766344105
[2m[36m(func pid=112258)[0m f1_per_class: [0.169, 0.205, 0.167, 0.356, 0.086, 0.379, 0.19, 0.352, 0.151, 0.299]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7090 | Steps: 2 | Val loss: 2.2700 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.1912 | Steps: 2 | Val loss: 3.4580 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9356 | Steps: 2 | Val loss: 2.3486 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.1671 | Steps: 2 | Val loss: 1.9351 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=111842)[0m top1: 0.1814365671641791
[2m[36m(func pid=111842)[0m top5: 0.6082089552238806
[2m[36m(func pid=111842)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=111842)[0m f1_macro: 0.16567864224764767
[2m[36m(func pid=111842)[0m f1_weighted: 0.1877960233713466
[2m[36m(func pid=111842)[0m f1_per_class: [0.294, 0.227, 0.165, 0.191, 0.028, 0.326, 0.127, 0.202, 0.024, 0.073]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m top1: 0.3185634328358209
[2m[36m(func pid=112678)[0m top5: 0.8796641791044776
[2m[36m(func pid=112678)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=112678)[0m f1_macro: 0.3069470384161922
[2m[36m(func pid=112678)[0m f1_weighted: 0.34450671064326804
[2m[36m(func pid=112678)[0m f1_per_class: [0.167, 0.212, 0.75, 0.385, 0.124, 0.354, 0.437, 0.227, 0.144, 0.269]
[2m[36m(func pid=112678)[0m 
== Status ==
Current time: 2024-01-07 01:35:10 (running for 00:01:31.65)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.936 |      0.088 |                    8 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  2.709 |      0.166 |                    8 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  1.448 |      0.235 |                    7 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.191 |      0.307 |                    7 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.15111940298507462
[2m[36m(func pid=111467)[0m top5: 0.5074626865671642
[2m[36m(func pid=111467)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=111467)[0m f1_macro: 0.08784206453671091
[2m[36m(func pid=111467)[0m f1_weighted: 0.13003129528023913
[2m[36m(func pid=111467)[0m f1_per_class: [0.077, 0.215, 0.0, 0.13, 0.037, 0.297, 0.058, 0.065, 0.0, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.29757462686567165
[2m[36m(func pid=112258)[0m top5: 0.8190298507462687
[2m[36m(func pid=112258)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=112258)[0m f1_macro: 0.2661944070037892
[2m[36m(func pid=112258)[0m f1_weighted: 0.31593134141564444
[2m[36m(func pid=112258)[0m f1_per_class: [0.174, 0.204, 0.27, 0.383, 0.101, 0.388, 0.313, 0.375, 0.154, 0.301]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6058 | Steps: 2 | Val loss: 2.2475 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.1031 | Steps: 2 | Val loss: 3.9078 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9465 | Steps: 2 | Val loss: 2.3462 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.0432 | Steps: 2 | Val loss: 1.9027 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=111842)[0m top1: 0.19169776119402984
[2m[36m(func pid=111842)[0m top5: 0.644589552238806
[2m[36m(func pid=111842)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=111842)[0m f1_macro: 0.1755465908104442
[2m[36m(func pid=111842)[0m f1_weighted: 0.20559650245798988
[2m[36m(func pid=111842)[0m f1_per_class: [0.298, 0.201, 0.159, 0.226, 0.03, 0.329, 0.166, 0.2, 0.023, 0.125]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m top1: 0.31156716417910446
[2m[36m(func pid=112678)[0m top5: 0.882929104477612
[2m[36m(func pid=112678)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=112678)[0m f1_macro: 0.30467457094521966
[2m[36m(func pid=112678)[0m f1_weighted: 0.32794840248424145
[2m[36m(func pid=112678)[0m f1_per_class: [0.19, 0.223, 0.75, 0.396, 0.113, 0.336, 0.362, 0.269, 0.158, 0.25]
[2m[36m(func pid=112678)[0m 
== Status ==
Current time: 2024-01-07 01:35:16 (running for 00:01:36.94)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.946 |      0.089 |                    9 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  2.606 |      0.176 |                    9 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  1.167 |      0.266 |                    8 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.103 |      0.305 |                    8 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.14738805970149255
[2m[36m(func pid=111467)[0m top5: 0.5027985074626866
[2m[36m(func pid=111467)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=111467)[0m f1_macro: 0.08940396411196527
[2m[36m(func pid=111467)[0m f1_weighted: 0.13112764590624343
[2m[36m(func pid=111467)[0m f1_per_class: [0.052, 0.193, 0.047, 0.14, 0.028, 0.299, 0.064, 0.072, 0.0, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.31902985074626866
[2m[36m(func pid=112258)[0m top5: 0.8418843283582089
[2m[36m(func pid=112258)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=112258)[0m f1_macro: 0.2855497783378695
[2m[36m(func pid=112258)[0m f1_weighted: 0.3458596198868559
[2m[36m(func pid=112258)[0m f1_per_class: [0.199, 0.223, 0.4, 0.396, 0.083, 0.388, 0.392, 0.371, 0.129, 0.275]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.5687 | Steps: 2 | Val loss: 2.2254 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.1461 | Steps: 2 | Val loss: 4.3218 | Batch size: 32 | lr: 0.1 | Duration: 2.54s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9739 | Steps: 2 | Val loss: 2.3385 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8785 | Steps: 2 | Val loss: 1.8759 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=111842)[0m top1: 0.20149253731343283
[2m[36m(func pid=111842)[0m top5: 0.6753731343283582
[2m[36m(func pid=111842)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=111842)[0m f1_macro: 0.19138873514834226
[2m[36m(func pid=111842)[0m f1_weighted: 0.21761664646404716
[2m[36m(func pid=111842)[0m f1_per_class: [0.338, 0.185, 0.121, 0.251, 0.033, 0.311, 0.184, 0.244, 0.022, 0.225]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m top1: 0.32322761194029853
[2m[36m(func pid=112678)[0m top5: 0.8852611940298507
[2m[36m(func pid=112678)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=112678)[0m f1_macro: 0.3137570886421751
[2m[36m(func pid=112678)[0m f1_weighted: 0.33265430102508003
[2m[36m(func pid=112678)[0m f1_per_class: [0.24, 0.264, 0.774, 0.422, 0.11, 0.344, 0.323, 0.267, 0.159, 0.234]
[2m[36m(func pid=112678)[0m 
== Status ==
Current time: 2024-01-07 01:35:21 (running for 00:01:42.14)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.974 |      0.092 |                   10 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  2.569 |      0.191 |                   10 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  1.043 |      0.286 |                    9 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.146 |      0.314 |                    9 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.14832089552238806
[2m[36m(func pid=111467)[0m top5: 0.5153917910447762
[2m[36m(func pid=111467)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=111467)[0m f1_macro: 0.09225247344537552
[2m[36m(func pid=111467)[0m f1_weighted: 0.13659738320670053
[2m[36m(func pid=111467)[0m f1_per_class: [0.051, 0.179, 0.044, 0.154, 0.02, 0.306, 0.071, 0.087, 0.011, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.3362873134328358
[2m[36m(func pid=112258)[0m top5: 0.8596082089552238
[2m[36m(func pid=112258)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=112258)[0m f1_macro: 0.3007778267999112
[2m[36m(func pid=112258)[0m f1_weighted: 0.36421631659218495
[2m[36m(func pid=112258)[0m f1_per_class: [0.232, 0.23, 0.522, 0.413, 0.08, 0.381, 0.442, 0.324, 0.116, 0.268]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.4260 | Steps: 2 | Val loss: 2.1992 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.0397 | Steps: 2 | Val loss: 4.8338 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9326 | Steps: 2 | Val loss: 2.3322 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=111842)[0m top1: 0.21641791044776118
[2m[36m(func pid=111842)[0m top5: 0.7056902985074627
[2m[36m(func pid=111842)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=111842)[0m f1_macro: 0.20122862367963026
[2m[36m(func pid=111842)[0m f1_weighted: 0.23600827321832804
[2m[36m(func pid=111842)[0m f1_per_class: [0.329, 0.162, 0.106, 0.286, 0.033, 0.332, 0.212, 0.269, 0.036, 0.247]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7042 | Steps: 2 | Val loss: 1.8924 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=112678)[0m top1: 0.32276119402985076
[2m[36m(func pid=112678)[0m top5: 0.8885261194029851
[2m[36m(func pid=112678)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=112678)[0m f1_macro: 0.3168110610932148
[2m[36m(func pid=112678)[0m f1_weighted: 0.3319504070064578
[2m[36m(func pid=112678)[0m f1_per_class: [0.273, 0.289, 0.8, 0.419, 0.098, 0.349, 0.307, 0.262, 0.165, 0.208]
[2m[36m(func pid=112678)[0m 
== Status ==
Current time: 2024-01-07 01:35:26 (running for 00:01:47.32)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.933 |      0.098 |                   11 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  2.426 |      0.201 |                   11 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.878 |      0.301 |                   10 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.04  |      0.317 |                   10 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.14972014925373134
[2m[36m(func pid=111467)[0m top5: 0.5219216417910447
[2m[36m(func pid=111467)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=111467)[0m f1_macro: 0.09793406158173792
[2m[36m(func pid=111467)[0m f1_weighted: 0.14069916124718324
[2m[36m(func pid=111467)[0m f1_per_class: [0.048, 0.169, 0.085, 0.161, 0.02, 0.304, 0.081, 0.1, 0.011, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.3451492537313433
[2m[36m(func pid=112258)[0m top5: 0.8689365671641791
[2m[36m(func pid=112258)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=112258)[0m f1_macro: 0.3212997497747688
[2m[36m(func pid=112258)[0m f1_weighted: 0.3726086484696893
[2m[36m(func pid=112258)[0m f1_per_class: [0.254, 0.241, 0.686, 0.436, 0.076, 0.374, 0.444, 0.29, 0.139, 0.273]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.3789 | Steps: 2 | Val loss: 2.1759 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.0903 | Steps: 2 | Val loss: 5.2880 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9172 | Steps: 2 | Val loss: 2.3253 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.6008 | Steps: 2 | Val loss: 1.9293 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=111842)[0m top1: 0.22994402985074627
[2m[36m(func pid=111842)[0m top5: 0.7243470149253731
[2m[36m(func pid=111842)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=111842)[0m f1_macro: 0.2098483074416752
[2m[36m(func pid=111842)[0m f1_weighted: 0.2511917854829235
[2m[36m(func pid=111842)[0m f1_per_class: [0.323, 0.156, 0.098, 0.312, 0.041, 0.322, 0.243, 0.269, 0.047, 0.286]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m top1: 0.32322761194029853
[2m[36m(func pid=112678)[0m top5: 0.8908582089552238
[2m[36m(func pid=112678)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=112678)[0m f1_macro: 0.319627822047028
[2m[36m(func pid=112678)[0m f1_weighted: 0.33099445717022447
[2m[36m(func pid=112678)[0m f1_per_class: [0.312, 0.305, 0.8, 0.418, 0.103, 0.349, 0.293, 0.245, 0.178, 0.191]
[2m[36m(func pid=112678)[0m 
== Status ==
Current time: 2024-01-07 01:35:32 (running for 00:01:52.78)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.917 |      0.102 |                   12 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  2.379 |      0.21  |                   12 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.704 |      0.321 |                   11 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.09  |      0.32  |                   11 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.14972014925373134
[2m[36m(func pid=111467)[0m top5: 0.5279850746268657
[2m[36m(func pid=111467)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=111467)[0m f1_macro: 0.1018078242863875
[2m[36m(func pid=111467)[0m f1_weighted: 0.14304015915258309
[2m[36m(func pid=111467)[0m f1_per_class: [0.046, 0.157, 0.118, 0.171, 0.025, 0.305, 0.085, 0.1, 0.011, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.34794776119402987
[2m[36m(func pid=112258)[0m top5: 0.875
[2m[36m(func pid=112258)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=112258)[0m f1_macro: 0.32959347438952324
[2m[36m(func pid=112258)[0m f1_weighted: 0.3730249894734042
[2m[36m(func pid=112258)[0m f1_per_class: [0.281, 0.239, 0.727, 0.438, 0.076, 0.39, 0.435, 0.286, 0.164, 0.26]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.3179 | Steps: 2 | Val loss: 2.1494 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.0340 | Steps: 2 | Val loss: 5.6798 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8826 | Steps: 2 | Val loss: 2.3177 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=111842)[0m top1: 0.2416044776119403
[2m[36m(func pid=111842)[0m top5: 0.7406716417910447
[2m[36m(func pid=111842)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=111842)[0m f1_macro: 0.21420037345571688
[2m[36m(func pid=111842)[0m f1_weighted: 0.2620120603159776
[2m[36m(func pid=111842)[0m f1_per_class: [0.337, 0.153, 0.096, 0.347, 0.04, 0.305, 0.252, 0.279, 0.054, 0.28]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.6666 | Steps: 2 | Val loss: 1.9446 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=112678)[0m top1: 0.32649253731343286
[2m[36m(func pid=112678)[0m top5: 0.8931902985074627
[2m[36m(func pid=112678)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=112678)[0m f1_macro: 0.3214365817340609
[2m[36m(func pid=112678)[0m f1_weighted: 0.3363381972860778
[2m[36m(func pid=112678)[0m f1_per_class: [0.33, 0.316, 0.8, 0.42, 0.099, 0.347, 0.307, 0.224, 0.187, 0.185]
[2m[36m(func pid=112678)[0m 
== Status ==
Current time: 2024-01-07 01:35:37 (running for 00:01:58.00)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.883 |      0.104 |                   13 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  2.318 |      0.214 |                   13 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.601 |      0.33  |                   12 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.034 |      0.321 |                   12 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.1515858208955224
[2m[36m(func pid=111467)[0m top5: 0.5373134328358209
[2m[36m(func pid=111467)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=111467)[0m f1_macro: 0.10361727354461014
[2m[36m(func pid=111467)[0m f1_weighted: 0.14698063878519582
[2m[36m(func pid=111467)[0m f1_per_class: [0.057, 0.162, 0.1, 0.173, 0.025, 0.304, 0.092, 0.113, 0.011, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.36007462686567165
[2m[36m(func pid=112258)[0m top5: 0.8903917910447762
[2m[36m(func pid=112258)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=112258)[0m f1_macro: 0.33629119288148523
[2m[36m(func pid=112258)[0m f1_weighted: 0.38124780325968005
[2m[36m(func pid=112258)[0m f1_per_class: [0.311, 0.263, 0.71, 0.456, 0.085, 0.401, 0.428, 0.258, 0.188, 0.264]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.2025 | Steps: 2 | Val loss: 2.1327 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.2188 | Steps: 2 | Val loss: 6.1370 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8783 | Steps: 2 | Val loss: 2.3129 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=111842)[0m top1: 0.24440298507462688
[2m[36m(func pid=111842)[0m top5: 0.7523320895522388
[2m[36m(func pid=111842)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=111842)[0m f1_macro: 0.2159366859225854
[2m[36m(func pid=111842)[0m f1_weighted: 0.2672562299938591
[2m[36m(func pid=111842)[0m f1_per_class: [0.306, 0.166, 0.089, 0.34, 0.047, 0.309, 0.267, 0.28, 0.07, 0.286]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m top1: 0.33115671641791045
[2m[36m(func pid=112678)[0m top5: 0.8969216417910447
[2m[36m(func pid=112678)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=112678)[0m f1_macro: 0.32156535184932483
[2m[36m(func pid=112678)[0m f1_weighted: 0.3467360565250206
[2m[36m(func pid=112678)[0m f1_per_class: [0.327, 0.316, 0.8, 0.417, 0.092, 0.346, 0.35, 0.206, 0.185, 0.176]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3338 | Steps: 2 | Val loss: 1.9982 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:35:42 (running for 00:02:03.21)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.878 |      0.108 |                   14 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  2.203 |      0.216 |                   14 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.667 |      0.336 |                   13 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.219 |      0.322 |                   13 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.15625
[2m[36m(func pid=111467)[0m top5: 0.5457089552238806
[2m[36m(func pid=111467)[0m f1_micro: 0.15625
[2m[36m(func pid=111467)[0m f1_macro: 0.10801629672473964
[2m[36m(func pid=111467)[0m f1_weighted: 0.15425866799219573
[2m[36m(func pid=111467)[0m f1_per_class: [0.065, 0.156, 0.095, 0.186, 0.025, 0.314, 0.1, 0.129, 0.01, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.363339552238806
[2m[36m(func pid=112258)[0m top5: 0.9006529850746269
[2m[36m(func pid=112258)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=112258)[0m f1_macro: 0.34140693817267576
[2m[36m(func pid=112258)[0m f1_weighted: 0.3822022226398385
[2m[36m(func pid=112258)[0m f1_per_class: [0.325, 0.286, 0.733, 0.449, 0.085, 0.421, 0.417, 0.247, 0.189, 0.262]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.0845 | Steps: 2 | Val loss: 2.1163 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.1025 | Steps: 2 | Val loss: 6.7144 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8836 | Steps: 2 | Val loss: 2.3046 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=111842)[0m top1: 0.2462686567164179
[2m[36m(func pid=111842)[0m top5: 0.7597947761194029
[2m[36m(func pid=111842)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=111842)[0m f1_macro: 0.22160191562224915
[2m[36m(func pid=111842)[0m f1_weighted: 0.2673156887056714
[2m[36m(func pid=111842)[0m f1_per_class: [0.296, 0.162, 0.095, 0.343, 0.056, 0.292, 0.27, 0.282, 0.081, 0.34]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m top1: 0.3381529850746269
[2m[36m(func pid=112678)[0m top5: 0.8917910447761194
[2m[36m(func pid=112678)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=112678)[0m f1_macro: 0.3222916115379852
[2m[36m(func pid=112678)[0m f1_weighted: 0.3611974995408468
[2m[36m(func pid=112678)[0m f1_per_class: [0.321, 0.319, 0.759, 0.407, 0.099, 0.341, 0.405, 0.22, 0.196, 0.155]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3138 | Steps: 2 | Val loss: 2.0929 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 01:35:47 (running for 00:02:08.46)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.884 |      0.11  |                   15 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  2.085 |      0.222 |                   15 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.334 |      0.341 |                   14 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.103 |      0.322 |                   14 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.15764925373134328
[2m[36m(func pid=111467)[0m top5: 0.558768656716418
[2m[36m(func pid=111467)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=111467)[0m f1_macro: 0.10972183059234511
[2m[36m(func pid=111467)[0m f1_weighted: 0.1568496009328563
[2m[36m(func pid=111467)[0m f1_per_class: [0.065, 0.161, 0.091, 0.184, 0.024, 0.32, 0.106, 0.116, 0.031, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.9698 | Steps: 2 | Val loss: 2.0955 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=112258)[0m top1: 0.3628731343283582
[2m[36m(func pid=112258)[0m top5: 0.9001865671641791
[2m[36m(func pid=112258)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=112258)[0m f1_macro: 0.3445769727194714
[2m[36m(func pid=112258)[0m f1_weighted: 0.38189211542981916
[2m[36m(func pid=112258)[0m f1_per_class: [0.343, 0.302, 0.759, 0.448, 0.086, 0.422, 0.405, 0.251, 0.189, 0.241]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0050 | Steps: 2 | Val loss: 7.3815 | Batch size: 32 | lr: 0.1 | Duration: 2.57s
[2m[36m(func pid=112678)[0m top1: 0.34281716417910446
[2m[36m(func pid=112678)[0m top5: 0.8824626865671642
[2m[36m(func pid=112678)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=112678)[0m f1_macro: 0.32328917549684444
[2m[36m(func pid=112678)[0m f1_weighted: 0.3704678707319119
[2m[36m(func pid=112678)[0m f1_per_class: [0.317, 0.312, 0.759, 0.388, 0.082, 0.353, 0.455, 0.21, 0.205, 0.151]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m top1: 0.25
[2m[36m(func pid=111842)[0m top5: 0.7649253731343284
[2m[36m(func pid=111842)[0m f1_micro: 0.25
[2m[36m(func pid=111842)[0m f1_macro: 0.22687544835182577
[2m[36m(func pid=111842)[0m f1_weighted: 0.2699061905515618
[2m[36m(func pid=111842)[0m f1_per_class: [0.282, 0.164, 0.108, 0.343, 0.059, 0.293, 0.272, 0.301, 0.089, 0.358]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8631 | Steps: 2 | Val loss: 2.2973 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3072 | Steps: 2 | Val loss: 2.1871 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:35:53 (running for 00:02:13.87)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.863 |      0.111 |                   16 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  1.97  |      0.227 |                   16 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.314 |      0.345 |                   15 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.005 |      0.323 |                   15 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.15951492537313433
[2m[36m(func pid=111467)[0m top5: 0.5746268656716418
[2m[36m(func pid=111467)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=111467)[0m f1_macro: 0.11054357574450757
[2m[36m(func pid=111467)[0m f1_weighted: 0.16248576485313365
[2m[36m(func pid=111467)[0m f1_per_class: [0.063, 0.146, 0.082, 0.21, 0.024, 0.321, 0.108, 0.121, 0.03, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0105 | Steps: 2 | Val loss: 8.0820 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.8945 | Steps: 2 | Val loss: 2.0739 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=112258)[0m top1: 0.3670708955223881
[2m[36m(func pid=112258)[0m top5: 0.9001865671641791
[2m[36m(func pid=112258)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=112258)[0m f1_macro: 0.35377420514319236
[2m[36m(func pid=112258)[0m f1_weighted: 0.3836266853679216
[2m[36m(func pid=112258)[0m f1_per_class: [0.376, 0.308, 0.786, 0.458, 0.094, 0.421, 0.391, 0.278, 0.19, 0.236]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.3353544776119403
[2m[36m(func pid=112678)[0m top5: 0.8759328358208955
[2m[36m(func pid=112678)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=112678)[0m f1_macro: 0.3175819339049498
[2m[36m(func pid=112678)[0m f1_weighted: 0.3624572476739512
[2m[36m(func pid=112678)[0m f1_per_class: [0.305, 0.307, 0.759, 0.364, 0.097, 0.367, 0.456, 0.177, 0.202, 0.141]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m top1: 0.26119402985074625
[2m[36m(func pid=111842)[0m top5: 0.7723880597014925
[2m[36m(func pid=111842)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=111842)[0m f1_macro: 0.23822648885705894
[2m[36m(func pid=111842)[0m f1_weighted: 0.28124704979781684
[2m[36m(func pid=111842)[0m f1_per_class: [0.274, 0.185, 0.123, 0.355, 0.062, 0.298, 0.281, 0.308, 0.1, 0.396]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8567 | Steps: 2 | Val loss: 2.2917 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.2632 | Steps: 2 | Val loss: 2.3159 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0725 | Steps: 2 | Val loss: 8.7451 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 01:35:58 (running for 00:02:19.05)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.857 |      0.113 |                   17 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  1.894 |      0.238 |                   17 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.307 |      0.354 |                   16 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.011 |      0.318 |                   16 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.1599813432835821
[2m[36m(func pid=111467)[0m top5: 0.5839552238805971
[2m[36m(func pid=111467)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=111467)[0m f1_macro: 0.11305207434876925
[2m[36m(func pid=111467)[0m f1_weighted: 0.16527197529746507
[2m[36m(func pid=111467)[0m f1_per_class: [0.063, 0.143, 0.088, 0.213, 0.03, 0.322, 0.114, 0.129, 0.029, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.8961 | Steps: 2 | Val loss: 2.0527 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=112258)[0m top1: 0.3614738805970149
[2m[36m(func pid=112258)[0m top5: 0.9053171641791045
[2m[36m(func pid=112258)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=112258)[0m f1_macro: 0.35329438920124623
[2m[36m(func pid=112258)[0m f1_weighted: 0.37694269437641215
[2m[36m(func pid=112258)[0m f1_per_class: [0.387, 0.323, 0.786, 0.439, 0.101, 0.418, 0.377, 0.29, 0.184, 0.229]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.33348880597014924
[2m[36m(func pid=112678)[0m top5: 0.8638059701492538
[2m[36m(func pid=112678)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=112678)[0m f1_macro: 0.3171679522279719
[2m[36m(func pid=112678)[0m f1_weighted: 0.3615926616344991
[2m[36m(func pid=112678)[0m f1_per_class: [0.3, 0.304, 0.759, 0.353, 0.09, 0.371, 0.462, 0.188, 0.204, 0.14]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8277 | Steps: 2 | Val loss: 2.2854 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=111842)[0m top1: 0.2691231343283582
[2m[36m(func pid=111842)[0m top5: 0.7803171641791045
[2m[36m(func pid=111842)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=111842)[0m f1_macro: 0.24404961568326033
[2m[36m(func pid=111842)[0m f1_weighted: 0.2898963927170429
[2m[36m(func pid=111842)[0m f1_per_class: [0.281, 0.205, 0.133, 0.361, 0.055, 0.301, 0.288, 0.318, 0.112, 0.387]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2439 | Steps: 2 | Val loss: 2.4401 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4885 | Steps: 2 | Val loss: 9.3692 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 01:36:03 (running for 00:02:24.21)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.828 |      0.117 |                   18 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  1.896 |      0.244 |                   18 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.263 |      0.353 |                   17 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.072 |      0.317 |                   17 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.16417910447761194
[2m[36m(func pid=111467)[0m top5: 0.5951492537313433
[2m[36m(func pid=111467)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=111467)[0m f1_macro: 0.11689433799336882
[2m[36m(func pid=111467)[0m f1_weighted: 0.1713616020994493
[2m[36m(func pid=111467)[0m f1_per_class: [0.063, 0.148, 0.087, 0.216, 0.031, 0.326, 0.125, 0.145, 0.029, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.7333 | Steps: 2 | Val loss: 2.0288 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=112258)[0m top1: 0.3605410447761194
[2m[36m(func pid=112258)[0m top5: 0.90625
[2m[36m(func pid=112258)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=112258)[0m f1_macro: 0.35369545590672213
[2m[36m(func pid=112258)[0m f1_weighted: 0.3763879837357416
[2m[36m(func pid=112258)[0m f1_per_class: [0.392, 0.332, 0.786, 0.439, 0.1, 0.406, 0.372, 0.295, 0.194, 0.221]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.32509328358208955
[2m[36m(func pid=112678)[0m top5: 0.8535447761194029
[2m[36m(func pid=112678)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=112678)[0m f1_macro: 0.3131752842348132
[2m[36m(func pid=112678)[0m f1_weighted: 0.3535057776211851
[2m[36m(func pid=112678)[0m f1_per_class: [0.29, 0.301, 0.759, 0.336, 0.1, 0.38, 0.451, 0.193, 0.188, 0.135]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.8110 | Steps: 2 | Val loss: 2.2780 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=111842)[0m top1: 0.27845149253731344
[2m[36m(func pid=111842)[0m top5: 0.7943097014925373
[2m[36m(func pid=111842)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=111842)[0m f1_macro: 0.24925236566277373
[2m[36m(func pid=111842)[0m f1_weighted: 0.2987148346700109
[2m[36m(func pid=111842)[0m f1_per_class: [0.288, 0.232, 0.171, 0.368, 0.054, 0.308, 0.294, 0.325, 0.092, 0.362]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.1792 | Steps: 2 | Val loss: 2.5600 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0419 | Steps: 2 | Val loss: 10.0397 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 01:36:08 (running for 00:02:29.49)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.811 |      0.121 |                   19 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  1.733 |      0.249 |                   19 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.244 |      0.354 |                   18 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.488 |      0.313 |                   18 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.16977611940298507
[2m[36m(func pid=111467)[0m top5: 0.6086753731343284
[2m[36m(func pid=111467)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=111467)[0m f1_macro: 0.12119045451846969
[2m[36m(func pid=111467)[0m f1_weighted: 0.17925065747750885
[2m[36m(func pid=111467)[0m f1_per_class: [0.075, 0.155, 0.079, 0.213, 0.032, 0.341, 0.143, 0.146, 0.029, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.7144 | Steps: 2 | Val loss: 2.0099 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=112258)[0m top1: 0.3568097014925373
[2m[36m(func pid=112258)[0m top5: 0.9034514925373134
[2m[36m(func pid=112258)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=112258)[0m f1_macro: 0.3510493458483301
[2m[36m(func pid=112258)[0m f1_weighted: 0.3739279155329118
[2m[36m(func pid=112258)[0m f1_per_class: [0.395, 0.326, 0.786, 0.44, 0.095, 0.403, 0.367, 0.292, 0.202, 0.204]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.31576492537313433
[2m[36m(func pid=112678)[0m top5: 0.847481343283582
[2m[36m(func pid=112678)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=112678)[0m f1_macro: 0.30531623389406426
[2m[36m(func pid=112678)[0m f1_weighted: 0.34621770249227035
[2m[36m(func pid=112678)[0m f1_per_class: [0.287, 0.287, 0.71, 0.33, 0.09, 0.368, 0.441, 0.215, 0.193, 0.133]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m top1: 0.279384328358209
[2m[36m(func pid=111842)[0m top5: 0.7980410447761194
[2m[36m(func pid=111842)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=111842)[0m f1_macro: 0.24832177452210943
[2m[36m(func pid=111842)[0m f1_weighted: 0.29935474992550726
[2m[36m(func pid=111842)[0m f1_per_class: [0.294, 0.239, 0.186, 0.36, 0.055, 0.309, 0.299, 0.322, 0.098, 0.319]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7583 | Steps: 2 | Val loss: 2.2717 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.1874 | Steps: 2 | Val loss: 2.6867 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0030 | Steps: 2 | Val loss: 10.7139 | Batch size: 32 | lr: 0.1 | Duration: 2.60s
== Status ==
Current time: 2024-01-07 01:36:13 (running for 00:02:34.70)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.758 |      0.124 |                   20 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  1.714 |      0.248 |                   20 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.179 |      0.351 |                   19 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.042 |      0.305 |                   19 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.17537313432835822
[2m[36m(func pid=111467)[0m top5: 0.6147388059701493
[2m[36m(func pid=111467)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=111467)[0m f1_macro: 0.12370048788490795
[2m[36m(func pid=111467)[0m f1_weighted: 0.18682212507213578
[2m[36m(func pid=111467)[0m f1_per_class: [0.074, 0.155, 0.073, 0.232, 0.033, 0.35, 0.148, 0.145, 0.028, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.6437 | Steps: 2 | Val loss: 1.9928 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=112258)[0m top1: 0.353544776119403
[2m[36m(func pid=112258)[0m top5: 0.9015858208955224
[2m[36m(func pid=112258)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=112258)[0m f1_macro: 0.3503736020650211
[2m[36m(func pid=112258)[0m f1_weighted: 0.3701018203079352
[2m[36m(func pid=112258)[0m f1_per_class: [0.383, 0.327, 0.786, 0.431, 0.095, 0.395, 0.363, 0.304, 0.21, 0.211]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.3069029850746269
[2m[36m(func pid=112678)[0m top5: 0.8390858208955224
[2m[36m(func pid=112678)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=112678)[0m f1_macro: 0.30386593025104874
[2m[36m(func pid=112678)[0m f1_weighted: 0.3383556375326761
[2m[36m(func pid=112678)[0m f1_per_class: [0.289, 0.278, 0.71, 0.328, 0.098, 0.366, 0.42, 0.228, 0.193, 0.13]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m top1: 0.28031716417910446
[2m[36m(func pid=111842)[0m top5: 0.8027052238805971
[2m[36m(func pid=111842)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=111842)[0m f1_macro: 0.25232113386127347
[2m[36m(func pid=111842)[0m f1_weighted: 0.30063731074055144
[2m[36m(func pid=111842)[0m f1_per_class: [0.273, 0.244, 0.216, 0.351, 0.055, 0.303, 0.306, 0.339, 0.14, 0.297]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7844 | Steps: 2 | Val loss: 2.2671 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.1152 | Steps: 2 | Val loss: 2.8370 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0004 | Steps: 2 | Val loss: 11.3938 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 01:36:19 (running for 00:02:39.90)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.784 |      0.131 |                   21 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  1.644 |      0.252 |                   21 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.187 |      0.35  |                   20 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.003 |      0.304 |                   20 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.17957089552238806
[2m[36m(func pid=111467)[0m top5: 0.6240671641791045
[2m[36m(func pid=111467)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=111467)[0m f1_macro: 0.13146922185882537
[2m[36m(func pid=111467)[0m f1_weighted: 0.19234083113149542
[2m[36m(func pid=111467)[0m f1_per_class: [0.086, 0.166, 0.113, 0.237, 0.033, 0.347, 0.153, 0.152, 0.027, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.4624 | Steps: 2 | Val loss: 1.9720 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=112258)[0m top1: 0.3423507462686567
[2m[36m(func pid=112258)[0m top5: 0.8959888059701493
[2m[36m(func pid=112258)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=112258)[0m f1_macro: 0.3438207709486186
[2m[36m(func pid=112258)[0m f1_weighted: 0.3601877417672094
[2m[36m(func pid=112258)[0m f1_per_class: [0.359, 0.324, 0.815, 0.418, 0.09, 0.38, 0.352, 0.303, 0.2, 0.197]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.3003731343283582
[2m[36m(func pid=112678)[0m top5: 0.8311567164179104
[2m[36m(func pid=112678)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=112678)[0m f1_macro: 0.301003472089435
[2m[36m(func pid=112678)[0m f1_weighted: 0.33231562240910545
[2m[36m(func pid=112678)[0m f1_per_class: [0.294, 0.271, 0.71, 0.321, 0.086, 0.358, 0.412, 0.24, 0.188, 0.131]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m top1: 0.283115671641791
[2m[36m(func pid=111842)[0m top5: 0.8106343283582089
[2m[36m(func pid=111842)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=111842)[0m f1_macro: 0.25787932595907315
[2m[36m(func pid=111842)[0m f1_weighted: 0.3037305460342603
[2m[36m(func pid=111842)[0m f1_per_class: [0.286, 0.247, 0.242, 0.349, 0.05, 0.327, 0.304, 0.347, 0.14, 0.288]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7897 | Steps: 2 | Val loss: 2.2644 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.1320 | Steps: 2 | Val loss: 2.9654 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.1912 | Steps: 2 | Val loss: 12.0608 | Batch size: 32 | lr: 0.1 | Duration: 2.49s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.3823 | Steps: 2 | Val loss: 1.9564 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 01:36:24 (running for 00:02:45.20)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.79  |      0.13  |                   22 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  1.462 |      0.258 |                   22 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.115 |      0.344 |                   21 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.301 |                   21 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.17723880597014927
[2m[36m(func pid=111467)[0m top5: 0.6296641791044776
[2m[36m(func pid=111467)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=111467)[0m f1_macro: 0.12981607322339941
[2m[36m(func pid=111467)[0m f1_weighted: 0.19328559779211243
[2m[36m(func pid=111467)[0m f1_per_class: [0.096, 0.164, 0.101, 0.244, 0.039, 0.329, 0.16, 0.138, 0.027, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.34375
[2m[36m(func pid=112258)[0m top5: 0.8936567164179104
[2m[36m(func pid=112258)[0m f1_micro: 0.34375
[2m[36m(func pid=112258)[0m f1_macro: 0.3461963312629442
[2m[36m(func pid=112258)[0m f1_weighted: 0.3626062334499993
[2m[36m(func pid=112258)[0m f1_per_class: [0.358, 0.329, 0.815, 0.413, 0.089, 0.381, 0.359, 0.31, 0.211, 0.196]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.2957089552238806
[2m[36m(func pid=112678)[0m top5: 0.8260261194029851
[2m[36m(func pid=112678)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=112678)[0m f1_macro: 0.3003048302791891
[2m[36m(func pid=112678)[0m f1_weighted: 0.32788066854454284
[2m[36m(func pid=112678)[0m f1_per_class: [0.291, 0.262, 0.71, 0.322, 0.086, 0.355, 0.4, 0.248, 0.193, 0.138]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m top1: 0.28824626865671643
[2m[36m(func pid=111842)[0m top5: 0.8171641791044776
[2m[36m(func pid=111842)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=111842)[0m f1_macro: 0.2625209868151359
[2m[36m(func pid=111842)[0m f1_weighted: 0.3080302909006468
[2m[36m(func pid=111842)[0m f1_per_class: [0.275, 0.259, 0.272, 0.361, 0.057, 0.331, 0.298, 0.356, 0.143, 0.274]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7899 | Steps: 2 | Val loss: 2.2584 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2648 | Steps: 2 | Val loss: 3.0639 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.1155 | Steps: 2 | Val loss: 12.5187 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.3338 | Steps: 2 | Val loss: 1.9456 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 01:36:29 (running for 00:02:50.43)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.79  |      0.134 |                   23 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  1.382 |      0.263 |                   23 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.132 |      0.346 |                   22 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.191 |      0.3   |                   22 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.18050373134328357
[2m[36m(func pid=111467)[0m top5: 0.6385261194029851
[2m[36m(func pid=111467)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=111467)[0m f1_macro: 0.13374525154086875
[2m[36m(func pid=111467)[0m f1_weighted: 0.1961753972107764
[2m[36m(func pid=111467)[0m f1_per_class: [0.101, 0.17, 0.106, 0.239, 0.039, 0.341, 0.163, 0.151, 0.027, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.34654850746268656
[2m[36m(func pid=112258)[0m top5: 0.8931902985074627
[2m[36m(func pid=112258)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=112258)[0m f1_macro: 0.34696422427688917
[2m[36m(func pid=112258)[0m f1_weighted: 0.3689792494399146
[2m[36m(func pid=112258)[0m f1_per_class: [0.35, 0.327, 0.815, 0.413, 0.088, 0.377, 0.385, 0.306, 0.219, 0.191]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.29197761194029853
[2m[36m(func pid=112678)[0m top5: 0.824160447761194
[2m[36m(func pid=112678)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=112678)[0m f1_macro: 0.3012824059393301
[2m[36m(func pid=112678)[0m f1_weighted: 0.3245593322513413
[2m[36m(func pid=112678)[0m f1_per_class: [0.301, 0.258, 0.71, 0.331, 0.09, 0.357, 0.379, 0.256, 0.194, 0.137]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m top1: 0.28638059701492535
[2m[36m(func pid=111842)[0m top5: 0.8250932835820896
[2m[36m(func pid=111842)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=111842)[0m f1_macro: 0.26425985102339944
[2m[36m(func pid=111842)[0m f1_weighted: 0.3051265251879303
[2m[36m(func pid=111842)[0m f1_per_class: [0.278, 0.261, 0.297, 0.352, 0.058, 0.329, 0.294, 0.361, 0.149, 0.264]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7601 | Steps: 2 | Val loss: 2.2519 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0839 | Steps: 2 | Val loss: 3.1874 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.1121 | Steps: 2 | Val loss: 13.0187 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.3094 | Steps: 2 | Val loss: 1.9384 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
== Status ==
Current time: 2024-01-07 01:36:35 (running for 00:02:55.72)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.76  |      0.136 |                   24 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  1.334 |      0.264 |                   24 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.265 |      0.347 |                   23 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.116 |      0.301 |                   23 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.1828358208955224
[2m[36m(func pid=111467)[0m top5: 0.6455223880597015
[2m[36m(func pid=111467)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=111467)[0m f1_macro: 0.13637817650799383
[2m[36m(func pid=111467)[0m f1_weighted: 0.19929126020800064
[2m[36m(func pid=111467)[0m f1_per_class: [0.1, 0.167, 0.102, 0.243, 0.034, 0.338, 0.167, 0.176, 0.037, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112678)[0m top1: 0.29151119402985076
[2m[36m(func pid=112678)[0m top5: 0.8134328358208955
[2m[36m(func pid=112678)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=112678)[0m f1_macro: 0.2982936024274926
[2m[36m(func pid=112678)[0m f1_weighted: 0.32556640667072223
[2m[36m(func pid=112678)[0m f1_per_class: [0.28, 0.244, 0.71, 0.362, 0.083, 0.355, 0.364, 0.264, 0.178, 0.144]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112258)[0m top1: 0.3474813432835821
[2m[36m(func pid=112258)[0m top5: 0.8917910447761194
[2m[36m(func pid=112258)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=112258)[0m f1_macro: 0.34397612520121607
[2m[36m(func pid=112258)[0m f1_weighted: 0.3734286137542178
[2m[36m(func pid=112258)[0m f1_per_class: [0.325, 0.321, 0.815, 0.414, 0.084, 0.377, 0.406, 0.301, 0.205, 0.194]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m top1: 0.2896455223880597
[2m[36m(func pid=111842)[0m top5: 0.8250932835820896
[2m[36m(func pid=111842)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=111842)[0m f1_macro: 0.27025110927200113
[2m[36m(func pid=111842)[0m f1_weighted: 0.3089215826083924
[2m[36m(func pid=111842)[0m f1_per_class: [0.268, 0.266, 0.319, 0.356, 0.06, 0.34, 0.291, 0.371, 0.186, 0.247]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.6989 | Steps: 2 | Val loss: 2.2453 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0221 | Steps: 2 | Val loss: 13.4119 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.1521 | Steps: 2 | Val loss: 3.2961 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.1335 | Steps: 2 | Val loss: 1.9254 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 01:36:40 (running for 00:03:01.06)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.699 |      0.142 |                   25 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  1.309 |      0.27  |                   25 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.084 |      0.344 |                   24 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.112 |      0.298 |                   24 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.18936567164179105
[2m[36m(func pid=111467)[0m top5: 0.6548507462686567
[2m[36m(func pid=111467)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=111467)[0m f1_macro: 0.1417414040968106
[2m[36m(func pid=111467)[0m f1_weighted: 0.20624792260150676
[2m[36m(func pid=111467)[0m f1_per_class: [0.12, 0.169, 0.11, 0.251, 0.036, 0.35, 0.178, 0.167, 0.037, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112678)[0m top1: 0.28824626865671643
[2m[36m(func pid=112678)[0m top5: 0.8157649253731343
[2m[36m(func pid=112678)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=112678)[0m f1_macro: 0.2974172554439154
[2m[36m(func pid=112678)[0m f1_weighted: 0.31938014473791243
[2m[36m(func pid=112678)[0m f1_per_class: [0.298, 0.231, 0.71, 0.384, 0.08, 0.354, 0.329, 0.262, 0.186, 0.139]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112258)[0m top1: 0.34654850746268656
[2m[36m(func pid=112258)[0m top5: 0.8885261194029851
[2m[36m(func pid=112258)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=112258)[0m f1_macro: 0.34291604920116303
[2m[36m(func pid=112258)[0m f1_weighted: 0.37419998017924355
[2m[36m(func pid=112258)[0m f1_per_class: [0.323, 0.318, 0.815, 0.401, 0.082, 0.379, 0.424, 0.292, 0.206, 0.191]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m top1: 0.2947761194029851
[2m[36m(func pid=111842)[0m top5: 0.8302238805970149
[2m[36m(func pid=111842)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=111842)[0m f1_macro: 0.2765983987038665
[2m[36m(func pid=111842)[0m f1_weighted: 0.3146413198273857
[2m[36m(func pid=111842)[0m f1_per_class: [0.278, 0.264, 0.344, 0.357, 0.061, 0.362, 0.3, 0.374, 0.186, 0.241]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0120 | Steps: 2 | Val loss: 13.7232 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.7245 | Steps: 2 | Val loss: 2.2381 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.1020 | Steps: 2 | Val loss: 3.4490 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.1574 | Steps: 2 | Val loss: 1.9148 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=112678)[0m top1: 0.28777985074626866
[2m[36m(func pid=112678)[0m top5: 0.8190298507462687
[2m[36m(func pid=112678)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=112678)[0m f1_macro: 0.30133371893617145
[2m[36m(func pid=112678)[0m f1_weighted: 0.31753267236412713
[2m[36m(func pid=112678)[0m f1_per_class: [0.32, 0.216, 0.733, 0.407, 0.075, 0.354, 0.306, 0.264, 0.197, 0.14]
[2m[36m(func pid=112678)[0m 
== Status ==
Current time: 2024-01-07 01:36:45 (running for 00:03:06.31)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.725 |      0.146 |                   26 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  1.134 |      0.277 |                   26 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.152 |      0.343 |                   25 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.012 |      0.301 |                   26 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.1958955223880597
[2m[36m(func pid=111467)[0m top5: 0.6627798507462687
[2m[36m(func pid=111467)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=111467)[0m f1_macro: 0.14606385298580588
[2m[36m(func pid=111467)[0m f1_weighted: 0.2167645726779283
[2m[36m(func pid=111467)[0m f1_per_class: [0.148, 0.161, 0.106, 0.27, 0.03, 0.357, 0.198, 0.154, 0.037, 0.0]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.35027985074626866
[2m[36m(func pid=112258)[0m top5: 0.8871268656716418
[2m[36m(func pid=112258)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=112258)[0m f1_macro: 0.34387798933354785
[2m[36m(func pid=112258)[0m f1_weighted: 0.3791103318902524
[2m[36m(func pid=112258)[0m f1_per_class: [0.321, 0.328, 0.815, 0.389, 0.077, 0.378, 0.447, 0.293, 0.195, 0.195]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m top1: 0.2980410447761194
[2m[36m(func pid=111842)[0m top5: 0.8372201492537313
[2m[36m(func pid=111842)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=111842)[0m f1_macro: 0.2810809083996558
[2m[36m(func pid=111842)[0m f1_weighted: 0.31862136889600906
[2m[36m(func pid=111842)[0m f1_per_class: [0.275, 0.261, 0.375, 0.352, 0.062, 0.39, 0.312, 0.357, 0.186, 0.241]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0004 | Steps: 2 | Val loss: 14.0149 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.6539 | Steps: 2 | Val loss: 2.2334 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2222 | Steps: 2 | Val loss: 3.5917 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.0285 | Steps: 2 | Val loss: 1.9119 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=112678)[0m top1: 0.2943097014925373
[2m[36m(func pid=112678)[0m top5: 0.8227611940298507
[2m[36m(func pid=112678)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=112678)[0m f1_macro: 0.3045579529418433
[2m[36m(func pid=112678)[0m f1_weighted: 0.32208400183248154
[2m[36m(func pid=112678)[0m f1_per_class: [0.316, 0.204, 0.759, 0.43, 0.077, 0.358, 0.306, 0.261, 0.197, 0.137]
[2m[36m(func pid=112678)[0m 
== Status ==
Current time: 2024-01-07 01:36:50 (running for 00:03:11.43)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.654 |      0.152 |                   27 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  1.157 |      0.281 |                   27 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.102 |      0.344 |                   26 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.305 |                   27 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.19869402985074627
[2m[36m(func pid=111467)[0m top5: 0.6683768656716418
[2m[36m(func pid=111467)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=111467)[0m f1_macro: 0.15247256122547792
[2m[36m(func pid=111467)[0m f1_weighted: 0.22126588405189637
[2m[36m(func pid=111467)[0m f1_per_class: [0.156, 0.152, 0.097, 0.278, 0.031, 0.352, 0.209, 0.161, 0.036, 0.052]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.35401119402985076
[2m[36m(func pid=112258)[0m top5: 0.8889925373134329
[2m[36m(func pid=112258)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=112258)[0m f1_macro: 0.3421265694470358
[2m[36m(func pid=112258)[0m f1_weighted: 0.38267753738302
[2m[36m(func pid=112258)[0m f1_per_class: [0.321, 0.331, 0.815, 0.383, 0.078, 0.384, 0.468, 0.254, 0.196, 0.191]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m top1: 0.300839552238806
[2m[36m(func pid=111842)[0m top5: 0.8404850746268657
[2m[36m(func pid=111842)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=111842)[0m f1_macro: 0.2837848476064047
[2m[36m(func pid=111842)[0m f1_weighted: 0.32240441951414894
[2m[36m(func pid=111842)[0m f1_per_class: [0.28, 0.261, 0.393, 0.357, 0.063, 0.403, 0.315, 0.354, 0.188, 0.224]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0180 | Steps: 2 | Val loss: 14.4709 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6652 | Steps: 2 | Val loss: 2.2265 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0497 | Steps: 2 | Val loss: 3.6729 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.9550 | Steps: 2 | Val loss: 1.9122 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=112678)[0m top1: 0.2971082089552239
[2m[36m(func pid=112678)[0m top5: 0.8246268656716418
[2m[36m(func pid=112678)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=112678)[0m f1_macro: 0.3067436637873245
[2m[36m(func pid=112678)[0m f1_weighted: 0.32146961093575893
[2m[36m(func pid=112678)[0m f1_per_class: [0.325, 0.21, 0.759, 0.434, 0.083, 0.366, 0.292, 0.271, 0.186, 0.141]
[2m[36m(func pid=112678)[0m 
== Status ==
Current time: 2024-01-07 01:36:56 (running for 00:03:16.73)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.665 |      0.157 |                   28 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  1.029 |      0.284 |                   28 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.222 |      0.342 |                   27 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.018 |      0.307 |                   28 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.20475746268656717
[2m[36m(func pid=111467)[0m top5: 0.6800373134328358
[2m[36m(func pid=111467)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=111467)[0m f1_macro: 0.15740320129479016
[2m[36m(func pid=111467)[0m f1_weighted: 0.22716777716999134
[2m[36m(func pid=111467)[0m f1_per_class: [0.163, 0.154, 0.1, 0.285, 0.032, 0.363, 0.212, 0.177, 0.037, 0.051]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.3568097014925373
[2m[36m(func pid=112258)[0m top5: 0.8861940298507462
[2m[36m(func pid=112258)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=112258)[0m f1_macro: 0.3425666700435686
[2m[36m(func pid=112258)[0m f1_weighted: 0.386952070345012
[2m[36m(func pid=112258)[0m f1_per_class: [0.328, 0.324, 0.815, 0.383, 0.078, 0.38, 0.489, 0.245, 0.2, 0.183]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3003731343283582
[2m[36m(func pid=111842)[0m top5: 0.8404850746268657
[2m[36m(func pid=111842)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=111842)[0m f1_macro: 0.28597529675471706
[2m[36m(func pid=111842)[0m f1_weighted: 0.32156995525349596
[2m[36m(func pid=111842)[0m f1_per_class: [0.272, 0.264, 0.431, 0.355, 0.065, 0.397, 0.315, 0.349, 0.189, 0.221]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0121 | Steps: 2 | Val loss: 14.8458 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6084 | Steps: 2 | Val loss: 2.2254 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.1336 | Steps: 2 | Val loss: 3.7998 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.8761 | Steps: 2 | Val loss: 1.9050 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=112678)[0m top1: 0.2980410447761194
[2m[36m(func pid=112678)[0m top5: 0.8297574626865671
[2m[36m(func pid=112678)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=112678)[0m f1_macro: 0.3079744044503692
[2m[36m(func pid=112678)[0m f1_weighted: 0.31980996766894226
[2m[36m(func pid=112678)[0m f1_per_class: [0.34, 0.208, 0.759, 0.442, 0.082, 0.375, 0.275, 0.274, 0.188, 0.137]
[2m[36m(func pid=112678)[0m 
== Status ==
Current time: 2024-01-07 01:37:01 (running for 00:03:21.96)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.608 |      0.164 |                   29 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.955 |      0.286 |                   29 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.05  |      0.343 |                   28 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.012 |      0.308 |                   29 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.20615671641791045
[2m[36m(func pid=111467)[0m top5: 0.6786380597014925
[2m[36m(func pid=111467)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=111467)[0m f1_macro: 0.16414055235580982
[2m[36m(func pid=111467)[0m f1_weighted: 0.22901690933083205
[2m[36m(func pid=111467)[0m f1_per_class: [0.169, 0.169, 0.096, 0.274, 0.032, 0.369, 0.216, 0.177, 0.036, 0.104]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.3530783582089552
[2m[36m(func pid=112258)[0m top5: 0.8843283582089553
[2m[36m(func pid=112258)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=112258)[0m f1_macro: 0.33960825315012316
[2m[36m(func pid=112258)[0m f1_weighted: 0.38320934967258213
[2m[36m(func pid=112258)[0m f1_per_class: [0.328, 0.315, 0.815, 0.383, 0.087, 0.376, 0.487, 0.229, 0.195, 0.181]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3069029850746269
[2m[36m(func pid=111842)[0m top5: 0.8465485074626866
[2m[36m(func pid=111842)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=111842)[0m f1_macro: 0.29324440580225924
[2m[36m(func pid=111842)[0m f1_weighted: 0.3288195951054831
[2m[36m(func pid=111842)[0m f1_per_class: [0.267, 0.264, 0.478, 0.361, 0.069, 0.414, 0.326, 0.357, 0.184, 0.212]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3637 | Steps: 2 | Val loss: 15.3253 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6380 | Steps: 2 | Val loss: 2.2199 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0353 | Steps: 2 | Val loss: 3.8723 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.9509 | Steps: 2 | Val loss: 1.9040 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=112678)[0m top1: 0.2994402985074627
[2m[36m(func pid=112678)[0m top5: 0.8269589552238806
[2m[36m(func pid=112678)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=112678)[0m f1_macro: 0.309087945617495
[2m[36m(func pid=112678)[0m f1_weighted: 0.3202031262343223
[2m[36m(func pid=112678)[0m f1_per_class: [0.36, 0.197, 0.733, 0.451, 0.078, 0.369, 0.272, 0.289, 0.205, 0.137]
[2m[36m(func pid=112678)[0m 
== Status ==
Current time: 2024-01-07 01:37:06 (running for 00:03:27.08)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.638 |      0.169 |                   30 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.876 |      0.293 |                   30 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.134 |      0.34  |                   29 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.364 |      0.309 |                   30 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.20755597014925373
[2m[36m(func pid=111467)[0m top5: 0.6847014925373134
[2m[36m(func pid=111467)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=111467)[0m f1_macro: 0.16862844650100445
[2m[36m(func pid=111467)[0m f1_weighted: 0.2312884076758223
[2m[36m(func pid=111467)[0m f1_per_class: [0.173, 0.17, 0.092, 0.279, 0.033, 0.361, 0.22, 0.177, 0.035, 0.146]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.35494402985074625
[2m[36m(func pid=112258)[0m top5: 0.882929104477612
[2m[36m(func pid=112258)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=112258)[0m f1_macro: 0.33986805971012324
[2m[36m(func pid=112258)[0m f1_weighted: 0.38494808504973155
[2m[36m(func pid=112258)[0m f1_per_class: [0.319, 0.312, 0.815, 0.378, 0.087, 0.379, 0.498, 0.223, 0.203, 0.183]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3125
[2m[36m(func pid=111842)[0m top5: 0.8484141791044776
[2m[36m(func pid=111842)[0m f1_micro: 0.3125
[2m[36m(func pid=111842)[0m f1_macro: 0.30000232313884967
[2m[36m(func pid=111842)[0m f1_weighted: 0.3355096363575949
[2m[36m(func pid=111842)[0m f1_per_class: [0.276, 0.27, 0.524, 0.363, 0.07, 0.411, 0.344, 0.36, 0.176, 0.206]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0001 | Steps: 2 | Val loss: 16.0463 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.5813 | Steps: 2 | Val loss: 2.2158 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0628 | Steps: 2 | Val loss: 3.9986 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.8784 | Steps: 2 | Val loss: 1.9130 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=112678)[0m top1: 0.29151119402985076
[2m[36m(func pid=112678)[0m top5: 0.8222947761194029
[2m[36m(func pid=112678)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=112678)[0m f1_macro: 0.30410365333633577
[2m[36m(func pid=112678)[0m f1_weighted: 0.310213419061217
[2m[36m(func pid=112678)[0m f1_per_class: [0.356, 0.181, 0.733, 0.443, 0.072, 0.373, 0.255, 0.286, 0.198, 0.144]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111467)[0m top1: 0.20942164179104478
[2m[36m(func pid=111467)[0m top5: 0.6884328358208955
[2m[36m(func pid=111467)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=111467)[0m f1_macro: 0.17170173270249928
[2m[36m(func pid=111467)[0m f1_weighted: 0.2342305938161943
[2m[36m(func pid=111467)[0m f1_per_class: [0.192, 0.16, 0.086, 0.288, 0.034, 0.351, 0.23, 0.174, 0.036, 0.167]
== Status ==
Current time: 2024-01-07 01:37:11 (running for 00:03:32.18)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.581 |      0.172 |                   31 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.951 |      0.3   |                   31 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.035 |      0.34  |                   30 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.304 |                   31 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.35074626865671643
[2m[36m(func pid=112258)[0m top5: 0.8801305970149254
[2m[36m(func pid=112258)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=112258)[0m f1_macro: 0.33698317505755876
[2m[36m(func pid=112258)[0m f1_weighted: 0.3819184933628359
[2m[36m(func pid=112258)[0m f1_per_class: [0.316, 0.306, 0.815, 0.37, 0.083, 0.371, 0.501, 0.23, 0.201, 0.176]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3138992537313433
[2m[36m(func pid=111842)[0m top5: 0.8484141791044776
[2m[36m(func pid=111842)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=111842)[0m f1_macro: 0.3029001022664225
[2m[36m(func pid=111842)[0m f1_weighted: 0.3371773644614721
[2m[36m(func pid=111842)[0m f1_per_class: [0.282, 0.273, 0.537, 0.364, 0.068, 0.412, 0.344, 0.369, 0.175, 0.205]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.1362 | Steps: 2 | Val loss: 16.5726 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.5677 | Steps: 2 | Val loss: 2.2088 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8836 | Steps: 2 | Val loss: 1.9201 | Batch size: 32 | lr: 0.001 | Duration: 2.55s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0466 | Steps: 2 | Val loss: 4.1380 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=112678)[0m top1: 0.28777985074626866
[2m[36m(func pid=112678)[0m top5: 0.8213619402985075
[2m[36m(func pid=112678)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=112678)[0m f1_macro: 0.30533415670448016
[2m[36m(func pid=112678)[0m f1_weighted: 0.30726901829550646
[2m[36m(func pid=112678)[0m f1_per_class: [0.379, 0.178, 0.733, 0.443, 0.066, 0.364, 0.247, 0.28, 0.215, 0.148]
[2m[36m(func pid=112678)[0m 
== Status ==
Current time: 2024-01-07 01:37:16 (running for 00:03:37.40)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.568 |      0.179 |                   32 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.878 |      0.303 |                   32 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.063 |      0.337 |                   31 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.136 |      0.305 |                   32 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.2150186567164179
[2m[36m(func pid=111467)[0m top5: 0.7005597014925373
[2m[36m(func pid=111467)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=111467)[0m f1_macro: 0.17934077373228124
[2m[36m(func pid=111467)[0m f1_weighted: 0.2400509652250121
[2m[36m(func pid=111467)[0m f1_per_class: [0.201, 0.16, 0.09, 0.296, 0.034, 0.362, 0.236, 0.171, 0.038, 0.207]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3138992537313433
[2m[36m(func pid=111842)[0m top5: 0.851679104477612
[2m[36m(func pid=111842)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=111842)[0m f1_macro: 0.3119635966201907
[2m[36m(func pid=111842)[0m f1_weighted: 0.33779709431190164
[2m[36m(func pid=111842)[0m f1_per_class: [0.266, 0.271, 0.629, 0.366, 0.067, 0.42, 0.342, 0.362, 0.18, 0.217]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112258)[0m top1: 0.3474813432835821
[2m[36m(func pid=112258)[0m top5: 0.8782649253731343
[2m[36m(func pid=112258)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=112258)[0m f1_macro: 0.3338734947569144
[2m[36m(func pid=112258)[0m f1_weighted: 0.37899289783156437
[2m[36m(func pid=112258)[0m f1_per_class: [0.308, 0.297, 0.815, 0.364, 0.081, 0.376, 0.503, 0.226, 0.195, 0.174]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0039 | Steps: 2 | Val loss: 17.0425 | Batch size: 32 | lr: 0.1 | Duration: 2.54s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.5685 | Steps: 2 | Val loss: 2.2039 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.8520 | Steps: 2 | Val loss: 1.9195 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=112678)[0m top1: 0.28591417910447764
[2m[36m(func pid=112678)[0m top5: 0.8236940298507462
[2m[36m(func pid=112678)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=112678)[0m f1_macro: 0.3041881355141093
[2m[36m(func pid=112678)[0m f1_weighted: 0.30735141059860505
[2m[36m(func pid=112678)[0m f1_per_class: [0.394, 0.189, 0.71, 0.446, 0.062, 0.345, 0.244, 0.289, 0.213, 0.15]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.1333 | Steps: 2 | Val loss: 4.2507 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 01:37:21 (running for 00:03:42.68)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.568 |      0.183 |                   33 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.884 |      0.312 |                   33 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.047 |      0.334 |                   32 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.004 |      0.304 |                   33 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.21688432835820895
[2m[36m(func pid=111467)[0m top5: 0.7122201492537313
[2m[36m(func pid=111467)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=111467)[0m f1_macro: 0.18253942387267524
[2m[36m(func pid=111467)[0m f1_weighted: 0.24247900572540493
[2m[36m(func pid=111467)[0m f1_per_class: [0.198, 0.151, 0.083, 0.302, 0.034, 0.362, 0.238, 0.193, 0.04, 0.225]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m top1: 0.31716417910447764
[2m[36m(func pid=111842)[0m top5: 0.8586753731343284
[2m[36m(func pid=111842)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=111842)[0m f1_macro: 0.3192438005765525
[2m[36m(func pid=111842)[0m f1_weighted: 0.3419645717635585
[2m[36m(func pid=111842)[0m f1_per_class: [0.275, 0.267, 0.667, 0.361, 0.067, 0.415, 0.361, 0.369, 0.191, 0.22]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.7008 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=112258)[0m top1: 0.3423507462686567
[2m[36m(func pid=112258)[0m top5: 0.8736007462686567
[2m[36m(func pid=112258)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=112258)[0m f1_macro: 0.330469900233635
[2m[36m(func pid=112258)[0m f1_weighted: 0.374943799663717
[2m[36m(func pid=112258)[0m f1_per_class: [0.291, 0.292, 0.815, 0.357, 0.077, 0.373, 0.5, 0.231, 0.194, 0.174]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.5277 | Steps: 2 | Val loss: 2.1982 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=112678)[0m top1: 0.27611940298507465
[2m[36m(func pid=112678)[0m top5: 0.8208955223880597
[2m[36m(func pid=112678)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=112678)[0m f1_macro: 0.29621997643976383
[2m[36m(func pid=112678)[0m f1_weighted: 0.29826136460945146
[2m[36m(func pid=112678)[0m f1_per_class: [0.371, 0.19, 0.688, 0.436, 0.057, 0.333, 0.228, 0.297, 0.205, 0.158]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.7359 | Steps: 2 | Val loss: 1.9220 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.1444 | Steps: 2 | Val loss: 4.3146 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 01:37:27 (running for 00:03:47.82)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.528 |      0.186 |                   34 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.852 |      0.319 |                   34 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.133 |      0.33  |                   33 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.296 |                   34 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.22061567164179105
[2m[36m(func pid=111467)[0m top5: 0.7131529850746269
[2m[36m(func pid=111467)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=111467)[0m f1_macro: 0.1862393972061786
[2m[36m(func pid=111467)[0m f1_weighted: 0.24613949939497215
[2m[36m(func pid=111467)[0m f1_per_class: [0.226, 0.157, 0.081, 0.299, 0.037, 0.36, 0.249, 0.189, 0.039, 0.225]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3208955223880597
[2m[36m(func pid=111842)[0m top5: 0.8628731343283582
[2m[36m(func pid=111842)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=111842)[0m f1_macro: 0.32131761719488716
[2m[36m(func pid=111842)[0m f1_weighted: 0.3456921478785144
[2m[36m(func pid=111842)[0m f1_per_class: [0.29, 0.268, 0.667, 0.37, 0.067, 0.415, 0.364, 0.364, 0.186, 0.221]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.2343 | Steps: 2 | Val loss: 17.9739 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=112258)[0m top1: 0.34468283582089554
[2m[36m(func pid=112258)[0m top5: 0.8740671641791045
[2m[36m(func pid=112258)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=112258)[0m f1_macro: 0.33107440661953913
[2m[36m(func pid=112258)[0m f1_weighted: 0.3774839197719427
[2m[36m(func pid=112258)[0m f1_per_class: [0.293, 0.285, 0.815, 0.361, 0.079, 0.371, 0.511, 0.226, 0.199, 0.172]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.4970 | Steps: 2 | Val loss: 2.1925 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=112678)[0m top1: 0.2751865671641791
[2m[36m(func pid=112678)[0m top5: 0.8246268656716418
[2m[36m(func pid=112678)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=112678)[0m f1_macro: 0.3005856459295134
[2m[36m(func pid=112678)[0m f1_weighted: 0.2982949367007078
[2m[36m(func pid=112678)[0m f1_per_class: [0.371, 0.199, 0.733, 0.434, 0.057, 0.331, 0.226, 0.293, 0.203, 0.159]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6603 | Steps: 2 | Val loss: 1.9164 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0224 | Steps: 2 | Val loss: 4.4044 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:37:32 (running for 00:03:53.03)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.497 |      0.188 |                   35 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.736 |      0.321 |                   35 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.144 |      0.331 |                   34 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  1.234 |      0.301 |                   35 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.22434701492537312
[2m[36m(func pid=111467)[0m top5: 0.7192164179104478
[2m[36m(func pid=111467)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=111467)[0m f1_macro: 0.18754952586846807
[2m[36m(func pid=111467)[0m f1_weighted: 0.2508444643261708
[2m[36m(func pid=111467)[0m f1_per_class: [0.228, 0.165, 0.078, 0.303, 0.038, 0.36, 0.259, 0.178, 0.04, 0.227]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m top1: 0.32882462686567165
[2m[36m(func pid=111842)[0m top5: 0.867070895522388
[2m[36m(func pid=111842)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=111842)[0m f1_macro: 0.328096047479364
[2m[36m(func pid=111842)[0m f1_weighted: 0.35403091495174394
[2m[36m(func pid=111842)[0m f1_per_class: [0.298, 0.273, 0.688, 0.385, 0.071, 0.415, 0.374, 0.361, 0.194, 0.223]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0088 | Steps: 2 | Val loss: 17.9938 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=112258)[0m top1: 0.3451492537313433
[2m[36m(func pid=112258)[0m top5: 0.8675373134328358
[2m[36m(func pid=112258)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=112258)[0m f1_macro: 0.33082877229995866
[2m[36m(func pid=112258)[0m f1_weighted: 0.37782597489851233
[2m[36m(func pid=112258)[0m f1_per_class: [0.284, 0.279, 0.815, 0.363, 0.079, 0.365, 0.515, 0.228, 0.209, 0.172]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6850 | Steps: 2 | Val loss: 1.9280 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.5079 | Steps: 2 | Val loss: 2.1880 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=112678)[0m top1: 0.2737873134328358
[2m[36m(func pid=112678)[0m top5: 0.835820895522388
[2m[36m(func pid=112678)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=112678)[0m f1_macro: 0.3012806211491723
[2m[36m(func pid=112678)[0m f1_weighted: 0.29639512498324816
[2m[36m(func pid=112678)[0m f1_per_class: [0.379, 0.219, 0.759, 0.414, 0.062, 0.325, 0.232, 0.28, 0.18, 0.163]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0232 | Steps: 2 | Val loss: 4.4769 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 01:37:37 (running for 00:03:58.21)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.497 |      0.188 |                   35 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.685 |      0.33  |                   37 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.022 |      0.331 |                   35 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.009 |      0.301 |                   36 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.33115671641791045
[2m[36m(func pid=111842)[0m top5: 0.8656716417910447
[2m[36m(func pid=111842)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=111842)[0m f1_macro: 0.33037271307138705
[2m[36m(func pid=111842)[0m f1_weighted: 0.3571304578626691
[2m[36m(func pid=111842)[0m f1_per_class: [0.284, 0.271, 0.71, 0.392, 0.073, 0.422, 0.377, 0.369, 0.185, 0.222]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m top1: 0.22901119402985073
[2m[36m(func pid=111467)[0m top5: 0.7276119402985075
[2m[36m(func pid=111467)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=111467)[0m f1_macro: 0.1953340887564114
[2m[36m(func pid=111467)[0m f1_weighted: 0.2561567177263805
[2m[36m(func pid=111467)[0m f1_per_class: [0.24, 0.161, 0.077, 0.313, 0.036, 0.367, 0.261, 0.191, 0.051, 0.255]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0003 | Steps: 2 | Val loss: 18.1250 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=112258)[0m top1: 0.3516791044776119
[2m[36m(func pid=112258)[0m top5: 0.8647388059701493
[2m[36m(func pid=112258)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=112258)[0m f1_macro: 0.3311001163942479
[2m[36m(func pid=112258)[0m f1_weighted: 0.3855753361864507
[2m[36m(func pid=112258)[0m f1_per_class: [0.289, 0.275, 0.786, 0.38, 0.08, 0.371, 0.524, 0.23, 0.212, 0.164]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.5748 | Steps: 2 | Val loss: 1.9224 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=112678)[0m top1: 0.27658582089552236
[2m[36m(func pid=112678)[0m top5: 0.8386194029850746
[2m[36m(func pid=112678)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=112678)[0m f1_macro: 0.3031965275024805
[2m[36m(func pid=112678)[0m f1_weighted: 0.2981533442810192
[2m[36m(func pid=112678)[0m f1_per_class: [0.359, 0.243, 0.759, 0.403, 0.065, 0.333, 0.23, 0.296, 0.176, 0.168]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.4935 | Steps: 2 | Val loss: 2.1822 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0195 | Steps: 2 | Val loss: 4.5891 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=111842)[0m top1: 0.333955223880597
[2m[36m(func pid=111842)[0m top5: 0.871268656716418
[2m[36m(func pid=111842)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=111842)[0m f1_macro: 0.3316233090413854
[2m[36m(func pid=111842)[0m f1_weighted: 0.3592608306182128
[2m[36m(func pid=111842)[0m f1_per_class: [0.296, 0.272, 0.688, 0.394, 0.074, 0.418, 0.381, 0.364, 0.204, 0.224]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 01:37:43 (running for 00:04:03.77)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.493 |      0.2   |                   37 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.575 |      0.332 |                   38 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.023 |      0.331 |                   36 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.303 |                   37 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.2332089552238806
[2m[36m(func pid=111467)[0m top5: 0.7346082089552238
[2m[36m(func pid=111467)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=111467)[0m f1_macro: 0.199811461183648
[2m[36m(func pid=111467)[0m f1_weighted: 0.2600765780060712
[2m[36m(func pid=111467)[0m f1_per_class: [0.239, 0.166, 0.076, 0.318, 0.037, 0.367, 0.265, 0.19, 0.053, 0.286]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0713 | Steps: 2 | Val loss: 18.5898 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=112258)[0m top1: 0.3493470149253731
[2m[36m(func pid=112258)[0m top5: 0.867070895522388
[2m[36m(func pid=112258)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=112258)[0m f1_macro: 0.33149156495951254
[2m[36m(func pid=112258)[0m f1_weighted: 0.3829438968007577
[2m[36m(func pid=112258)[0m f1_per_class: [0.284, 0.274, 0.815, 0.38, 0.08, 0.366, 0.52, 0.213, 0.222, 0.16]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.5947 | Steps: 2 | Val loss: 1.9286 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=112678)[0m top1: 0.27705223880597013
[2m[36m(func pid=112678)[0m top5: 0.8390858208955224
[2m[36m(func pid=112678)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=112678)[0m f1_macro: 0.30524385150936906
[2m[36m(func pid=112678)[0m f1_weighted: 0.2982518398470422
[2m[36m(func pid=112678)[0m f1_per_class: [0.372, 0.264, 0.759, 0.386, 0.069, 0.332, 0.234, 0.293, 0.179, 0.165]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.4957 | Steps: 2 | Val loss: 2.1733 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0570 | Steps: 2 | Val loss: 4.6793 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=111842)[0m top1: 0.3362873134328358
[2m[36m(func pid=111842)[0m top5: 0.8717350746268657
[2m[36m(func pid=111842)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=111842)[0m f1_macro: 0.3350542720654876
[2m[36m(func pid=111842)[0m f1_weighted: 0.3621646276123213
[2m[36m(func pid=111842)[0m f1_per_class: [0.305, 0.277, 0.733, 0.4, 0.076, 0.416, 0.388, 0.335, 0.2, 0.221]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0004 | Steps: 2 | Val loss: 18.9955 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 01:37:48 (running for 00:04:09.02)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.496 |      0.202 |                   38 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.595 |      0.335 |                   39 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.02  |      0.331 |                   37 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.071 |      0.305 |                   38 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.24113805970149255
[2m[36m(func pid=111467)[0m top5: 0.7397388059701493
[2m[36m(func pid=111467)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=111467)[0m f1_macro: 0.2021793777275549
[2m[36m(func pid=111467)[0m f1_weighted: 0.26862728554589443
[2m[36m(func pid=111467)[0m f1_per_class: [0.242, 0.17, 0.079, 0.337, 0.038, 0.351, 0.278, 0.199, 0.053, 0.274]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.34701492537313433
[2m[36m(func pid=112258)[0m top5: 0.8647388059701493
[2m[36m(func pid=112258)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=112258)[0m f1_macro: 0.33016076173633596
[2m[36m(func pid=112258)[0m f1_weighted: 0.3821082287701777
[2m[36m(func pid=112258)[0m f1_per_class: [0.278, 0.271, 0.815, 0.382, 0.078, 0.365, 0.518, 0.22, 0.213, 0.163]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6552 | Steps: 2 | Val loss: 1.9307 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=112678)[0m top1: 0.27238805970149255
[2m[36m(func pid=112678)[0m top5: 0.8334888059701493
[2m[36m(func pid=112678)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=112678)[0m f1_macro: 0.3033014052313624
[2m[36m(func pid=112678)[0m f1_weighted: 0.2933213947912332
[2m[36m(func pid=112678)[0m f1_per_class: [0.364, 0.265, 0.759, 0.366, 0.072, 0.333, 0.235, 0.293, 0.19, 0.158]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.4575 | Steps: 2 | Val loss: 2.1681 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0117 | Steps: 2 | Val loss: 4.7745 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=111842)[0m top1: 0.33861940298507465
[2m[36m(func pid=111842)[0m top5: 0.8731343283582089
[2m[36m(func pid=111842)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=111842)[0m f1_macro: 0.3328595184781705
[2m[36m(func pid=111842)[0m f1_weighted: 0.3641498992123985
[2m[36m(func pid=111842)[0m f1_per_class: [0.301, 0.278, 0.71, 0.402, 0.073, 0.414, 0.395, 0.319, 0.214, 0.223]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3474 | Steps: 2 | Val loss: 19.1993 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 01:37:53 (running for 00:04:14.33)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.457 |      0.205 |                   39 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.655 |      0.333 |                   40 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.057 |      0.33  |                   38 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.303 |                   39 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.24580223880597016
[2m[36m(func pid=111467)[0m top5: 0.7402052238805971
[2m[36m(func pid=111467)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=111467)[0m f1_macro: 0.2053659412426493
[2m[36m(func pid=111467)[0m f1_weighted: 0.2737026035117901
[2m[36m(func pid=111467)[0m f1_per_class: [0.238, 0.178, 0.079, 0.342, 0.039, 0.358, 0.282, 0.202, 0.064, 0.271]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.34375
[2m[36m(func pid=112258)[0m top5: 0.8628731343283582
[2m[36m(func pid=112258)[0m f1_micro: 0.34375
[2m[36m(func pid=112258)[0m f1_macro: 0.3220066555723388
[2m[36m(func pid=112258)[0m f1_weighted: 0.3784567256548291
[2m[36m(func pid=112258)[0m f1_per_class: [0.268, 0.259, 0.786, 0.388, 0.08, 0.349, 0.517, 0.211, 0.2, 0.162]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.27098880597014924
[2m[36m(func pid=112678)[0m top5: 0.8339552238805971
[2m[36m(func pid=112678)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=112678)[0m f1_macro: 0.2985099754269064
[2m[36m(func pid=112678)[0m f1_weighted: 0.29104461010736443
[2m[36m(func pid=112678)[0m f1_per_class: [0.339, 0.264, 0.759, 0.36, 0.079, 0.327, 0.24, 0.285, 0.179, 0.153]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.5857 | Steps: 2 | Val loss: 1.9425 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.4051 | Steps: 2 | Val loss: 2.1585 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0082 | Steps: 2 | Val loss: 4.8783 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=111842)[0m top1: 0.33488805970149255
[2m[36m(func pid=111842)[0m top5: 0.8763992537313433
[2m[36m(func pid=111842)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=111842)[0m f1_macro: 0.33188357774406274
[2m[36m(func pid=111842)[0m f1_weighted: 0.36050245890765
[2m[36m(func pid=111842)[0m f1_per_class: [0.32, 0.276, 0.733, 0.399, 0.07, 0.407, 0.393, 0.297, 0.202, 0.22]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0681 | Steps: 2 | Val loss: 19.4301 | Batch size: 32 | lr: 0.1 | Duration: 2.53s
== Status ==
Current time: 2024-01-07 01:37:58 (running for 00:04:19.68)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.405 |      0.208 |                   40 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.586 |      0.332 |                   41 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.012 |      0.322 |                   39 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.347 |      0.299 |                   40 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.25419776119402987
[2m[36m(func pid=111467)[0m top5: 0.7509328358208955
[2m[36m(func pid=111467)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=111467)[0m f1_macro: 0.20816179383626499
[2m[36m(func pid=111467)[0m f1_weighted: 0.28080129491954736
[2m[36m(func pid=111467)[0m f1_per_class: [0.24, 0.184, 0.084, 0.361, 0.04, 0.34, 0.289, 0.207, 0.078, 0.257]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.345615671641791
[2m[36m(func pid=112258)[0m top5: 0.8600746268656716
[2m[36m(func pid=112258)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=112258)[0m f1_macro: 0.3244517997356978
[2m[36m(func pid=112258)[0m f1_weighted: 0.3817345407174835
[2m[36m(func pid=112258)[0m f1_per_class: [0.271, 0.253, 0.786, 0.4, 0.079, 0.353, 0.516, 0.23, 0.199, 0.158]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.27005597014925375
[2m[36m(func pid=112678)[0m top5: 0.8297574626865671
[2m[36m(func pid=112678)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=112678)[0m f1_macro: 0.2950662584353754
[2m[36m(func pid=112678)[0m f1_weighted: 0.2914650625844077
[2m[36m(func pid=112678)[0m f1_per_class: [0.316, 0.265, 0.759, 0.348, 0.068, 0.33, 0.253, 0.286, 0.178, 0.148]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4297 | Steps: 2 | Val loss: 1.9529 | Batch size: 32 | lr: 0.001 | Duration: 2.62s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.3276 | Steps: 2 | Val loss: 2.1518 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0187 | Steps: 2 | Val loss: 4.9414 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=111842)[0m top1: 0.3353544776119403
[2m[36m(func pid=111842)[0m top5: 0.8782649253731343
[2m[36m(func pid=111842)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=111842)[0m f1_macro: 0.33117957174044604
[2m[36m(func pid=111842)[0m f1_weighted: 0.3617332461514479
[2m[36m(func pid=111842)[0m f1_per_class: [0.32, 0.278, 0.733, 0.406, 0.07, 0.394, 0.395, 0.3, 0.201, 0.215]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0000 | Steps: 2 | Val loss: 19.7220 | Batch size: 32 | lr: 0.1 | Duration: 2.54s
== Status ==
Current time: 2024-01-07 01:38:04 (running for 00:04:24.83)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.328 |      0.215 |                   41 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.43  |      0.331 |                   42 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.008 |      0.324 |                   40 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.068 |      0.295 |                   41 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.26026119402985076
[2m[36m(func pid=111467)[0m top5: 0.753731343283582
[2m[36m(func pid=111467)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=111467)[0m f1_macro: 0.2149769307750306
[2m[36m(func pid=111467)[0m f1_weighted: 0.28583374753346463
[2m[36m(func pid=111467)[0m f1_per_class: [0.251, 0.189, 0.083, 0.363, 0.043, 0.347, 0.294, 0.223, 0.079, 0.277]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.3498134328358209
[2m[36m(func pid=112258)[0m top5: 0.8563432835820896
[2m[36m(func pid=112258)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=112258)[0m f1_macro: 0.32501416621282175
[2m[36m(func pid=112258)[0m f1_weighted: 0.3856907183808901
[2m[36m(func pid=112258)[0m f1_per_class: [0.282, 0.244, 0.786, 0.423, 0.078, 0.351, 0.516, 0.212, 0.201, 0.158]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.2789179104477612
[2m[36m(func pid=112678)[0m top5: 0.8325559701492538
[2m[36m(func pid=112678)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=112678)[0m f1_macro: 0.29825107913729515
[2m[36m(func pid=112678)[0m f1_weighted: 0.29638949207242626
[2m[36m(func pid=112678)[0m f1_per_class: [0.313, 0.299, 0.759, 0.338, 0.075, 0.347, 0.254, 0.283, 0.169, 0.146]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.5471 | Steps: 2 | Val loss: 1.9609 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.3078 | Steps: 2 | Val loss: 2.1459 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=111842)[0m top1: 0.3376865671641791
[2m[36m(func pid=111842)[0m top5: 0.8805970149253731
[2m[36m(func pid=111842)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=111842)[0m f1_macro: 0.3333438033960191
[2m[36m(func pid=111842)[0m f1_weighted: 0.3636967442125345
[2m[36m(func pid=111842)[0m f1_per_class: [0.322, 0.28, 0.733, 0.406, 0.07, 0.395, 0.398, 0.3, 0.208, 0.222]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0121 | Steps: 2 | Val loss: 20.1611 | Batch size: 32 | lr: 0.1 | Duration: 2.61s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0527 | Steps: 2 | Val loss: 5.0038 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:38:09 (running for 00:04:30.02)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.308 |      0.218 |                   42 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.547 |      0.333 |                   43 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.019 |      0.325 |                   41 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.298 |                   42 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.261660447761194
[2m[36m(func pid=111467)[0m top5: 0.7546641791044776
[2m[36m(func pid=111467)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=111467)[0m f1_macro: 0.21756014762617207
[2m[36m(func pid=111467)[0m f1_weighted: 0.2861875371148745
[2m[36m(func pid=111467)[0m f1_per_class: [0.247, 0.188, 0.084, 0.357, 0.046, 0.348, 0.299, 0.226, 0.091, 0.288]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112678)[0m top1: 0.2798507462686567
[2m[36m(func pid=112678)[0m top5: 0.8278917910447762
[2m[36m(func pid=112678)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=112678)[0m f1_macro: 0.29777748453180675
[2m[36m(func pid=112678)[0m f1_weighted: 0.2952904656327759
[2m[36m(func pid=112678)[0m f1_per_class: [0.308, 0.313, 0.759, 0.333, 0.073, 0.338, 0.25, 0.285, 0.176, 0.144]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112258)[0m top1: 0.35027985074626866
[2m[36m(func pid=112258)[0m top5: 0.8558768656716418
[2m[36m(func pid=112258)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=112258)[0m f1_macro: 0.32558434016554927
[2m[36m(func pid=112258)[0m f1_weighted: 0.3857240649897194
[2m[36m(func pid=112258)[0m f1_per_class: [0.283, 0.247, 0.786, 0.426, 0.083, 0.347, 0.512, 0.214, 0.204, 0.155]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5484 | Steps: 2 | Val loss: 1.9749 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.3078 | Steps: 2 | Val loss: 2.1378 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0000 | Steps: 2 | Val loss: 20.4965 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=111842)[0m top1: 0.3381529850746269
[2m[36m(func pid=111842)[0m top5: 0.8838619402985075
[2m[36m(func pid=111842)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=111842)[0m f1_macro: 0.33189102459917463
[2m[36m(func pid=111842)[0m f1_weighted: 0.36398201374008066
[2m[36m(func pid=111842)[0m f1_per_class: [0.337, 0.279, 0.733, 0.407, 0.071, 0.395, 0.403, 0.283, 0.2, 0.211]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0334 | Steps: 2 | Val loss: 5.0883 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 01:38:14 (running for 00:04:35.27)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.308 |      0.223 |                   43 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.548 |      0.332 |                   44 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.053 |      0.326 |                   42 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.012 |      0.298 |                   43 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.2681902985074627
[2m[36m(func pid=111467)[0m top5: 0.7597947761194029
[2m[36m(func pid=111467)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=111467)[0m f1_macro: 0.22326717955858877
[2m[36m(func pid=111467)[0m f1_weighted: 0.2927898223297882
[2m[36m(func pid=111467)[0m f1_per_class: [0.271, 0.185, 0.086, 0.364, 0.046, 0.356, 0.31, 0.234, 0.092, 0.288]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112678)[0m top1: 0.28078358208955223
[2m[36m(func pid=112678)[0m top5: 0.8311567164179104
[2m[36m(func pid=112678)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=112678)[0m f1_macro: 0.29747342872354976
[2m[36m(func pid=112678)[0m f1_weighted: 0.2951498594917435
[2m[36m(func pid=112678)[0m f1_per_class: [0.3, 0.315, 0.759, 0.33, 0.069, 0.346, 0.248, 0.29, 0.174, 0.144]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112258)[0m top1: 0.3516791044776119
[2m[36m(func pid=112258)[0m top5: 0.8535447761194029
[2m[36m(func pid=112258)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=112258)[0m f1_macro: 0.3284735840852784
[2m[36m(func pid=112258)[0m f1_weighted: 0.38842361674826265
[2m[36m(func pid=112258)[0m f1_per_class: [0.269, 0.242, 0.786, 0.431, 0.081, 0.357, 0.51, 0.244, 0.21, 0.155]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5072 | Steps: 2 | Val loss: 1.9997 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.3490 | Steps: 2 | Val loss: 2.1285 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0000 | Steps: 2 | Val loss: 21.1013 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=111842)[0m top1: 0.3362873134328358
[2m[36m(func pid=111842)[0m top5: 0.8871268656716418
[2m[36m(func pid=111842)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=111842)[0m f1_macro: 0.33324755990852606
[2m[36m(func pid=111842)[0m f1_weighted: 0.36262891401142905
[2m[36m(func pid=111842)[0m f1_per_class: [0.326, 0.276, 0.759, 0.404, 0.073, 0.399, 0.4, 0.289, 0.202, 0.205]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0444 | Steps: 2 | Val loss: 5.2449 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 01:38:19 (running for 00:04:40.55)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.349 |      0.224 |                   44 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.507 |      0.333 |                   45 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.033 |      0.328 |                   43 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.297 |                   44 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.2728544776119403
[2m[36m(func pid=111467)[0m top5: 0.7658582089552238
[2m[36m(func pid=111467)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=111467)[0m f1_macro: 0.22420447173532998
[2m[36m(func pid=111467)[0m f1_weighted: 0.29610776122194277
[2m[36m(func pid=111467)[0m f1_per_class: [0.275, 0.179, 0.089, 0.373, 0.048, 0.332, 0.324, 0.238, 0.095, 0.288]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112678)[0m top1: 0.28125
[2m[36m(func pid=112678)[0m top5: 0.8260261194029851
[2m[36m(func pid=112678)[0m f1_micro: 0.28125
[2m[36m(func pid=112678)[0m f1_macro: 0.2975111892449048
[2m[36m(func pid=112678)[0m f1_weighted: 0.29360124938905857
[2m[36m(func pid=112678)[0m f1_per_class: [0.304, 0.324, 0.759, 0.318, 0.074, 0.347, 0.25, 0.281, 0.174, 0.145]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.6149 | Steps: 2 | Val loss: 2.0017 | Batch size: 32 | lr: 0.001 | Duration: 2.60s
[2m[36m(func pid=112258)[0m top1: 0.35027985074626866
[2m[36m(func pid=112258)[0m top5: 0.8512126865671642
[2m[36m(func pid=112258)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=112258)[0m f1_macro: 0.32734811661506646
[2m[36m(func pid=112258)[0m f1_weighted: 0.3882012715408897
[2m[36m(func pid=112258)[0m f1_per_class: [0.267, 0.237, 0.786, 0.435, 0.079, 0.359, 0.509, 0.241, 0.209, 0.152]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.2380 | Steps: 2 | Val loss: 2.1220 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=111842)[0m top1: 0.341884328358209
[2m[36m(func pid=111842)[0m top5: 0.8899253731343284
[2m[36m(func pid=111842)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=111842)[0m f1_macro: 0.3412905698388474
[2m[36m(func pid=111842)[0m f1_weighted: 0.3683215334273856
[2m[36m(func pid=111842)[0m f1_per_class: [0.337, 0.28, 0.786, 0.408, 0.074, 0.396, 0.412, 0.292, 0.206, 0.223]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0162 | Steps: 2 | Val loss: 21.5140 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0295 | Steps: 2 | Val loss: 5.3310 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 01:38:25 (running for 00:04:45.90)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.238 |      0.226 |                   45 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.615 |      0.341 |                   46 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.044 |      0.327 |                   44 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.298 |                   45 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.27658582089552236
[2m[36m(func pid=111467)[0m top5: 0.7672574626865671
[2m[36m(func pid=111467)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=111467)[0m f1_macro: 0.2258423988702945
[2m[36m(func pid=111467)[0m f1_weighted: 0.2988680281804569
[2m[36m(func pid=111467)[0m f1_per_class: [0.274, 0.182, 0.09, 0.381, 0.052, 0.334, 0.321, 0.248, 0.098, 0.279]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4394 | Steps: 2 | Val loss: 2.0137 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=112678)[0m top1: 0.27798507462686567
[2m[36m(func pid=112678)[0m top5: 0.820429104477612
[2m[36m(func pid=112678)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=112678)[0m f1_macro: 0.29256467233902705
[2m[36m(func pid=112678)[0m f1_weighted: 0.2904410237467088
[2m[36m(func pid=112678)[0m f1_per_class: [0.279, 0.324, 0.759, 0.302, 0.069, 0.341, 0.259, 0.281, 0.168, 0.144]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112258)[0m top1: 0.34375
[2m[36m(func pid=112258)[0m top5: 0.847481343283582
[2m[36m(func pid=112258)[0m f1_micro: 0.34375
[2m[36m(func pid=112258)[0m f1_macro: 0.32474973208251506
[2m[36m(func pid=112258)[0m f1_weighted: 0.38113231895803906
[2m[36m(func pid=112258)[0m f1_per_class: [0.26, 0.231, 0.786, 0.427, 0.099, 0.365, 0.496, 0.238, 0.2, 0.147]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3423507462686567
[2m[36m(func pid=111842)[0m top5: 0.8917910447761194
[2m[36m(func pid=111842)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=111842)[0m f1_macro: 0.3393097921363386
[2m[36m(func pid=111842)[0m f1_weighted: 0.36934551498650336
[2m[36m(func pid=111842)[0m f1_per_class: [0.355, 0.278, 0.786, 0.418, 0.074, 0.395, 0.41, 0.278, 0.196, 0.203]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.2397 | Steps: 2 | Val loss: 2.1179 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3786 | Steps: 2 | Val loss: 21.6917 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0094 | Steps: 2 | Val loss: 5.4045 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 01:38:30 (running for 00:04:51.29)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.24  |      0.228 |                   46 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.439 |      0.339 |                   47 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.029 |      0.325 |                   45 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.016 |      0.293 |                   46 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.2756529850746269
[2m[36m(func pid=111467)[0m top5: 0.7691231343283582
[2m[36m(func pid=111467)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=111467)[0m f1_macro: 0.22770919207121193
[2m[36m(func pid=111467)[0m f1_weighted: 0.29646051833673354
[2m[36m(func pid=111467)[0m f1_per_class: [0.286, 0.192, 0.092, 0.376, 0.051, 0.338, 0.307, 0.261, 0.1, 0.274]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3480 | Steps: 2 | Val loss: 2.0251 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=112678)[0m top1: 0.2826492537313433
[2m[36m(func pid=112678)[0m top5: 0.8255597014925373
[2m[36m(func pid=112678)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=112678)[0m f1_macro: 0.29694939803996767
[2m[36m(func pid=112678)[0m f1_weighted: 0.29315566326251224
[2m[36m(func pid=112678)[0m f1_per_class: [0.282, 0.328, 0.786, 0.299, 0.069, 0.345, 0.266, 0.285, 0.16, 0.15]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112258)[0m top1: 0.34421641791044777
[2m[36m(func pid=112258)[0m top5: 0.8465485074626866
[2m[36m(func pid=112258)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=112258)[0m f1_macro: 0.3260102343716723
[2m[36m(func pid=112258)[0m f1_weighted: 0.3820067651555712
[2m[36m(func pid=112258)[0m f1_per_class: [0.262, 0.23, 0.786, 0.431, 0.097, 0.363, 0.494, 0.249, 0.2, 0.148]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3451492537313433
[2m[36m(func pid=111842)[0m top5: 0.8941231343283582
[2m[36m(func pid=111842)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=111842)[0m f1_macro: 0.34166957508646506
[2m[36m(func pid=111842)[0m f1_weighted: 0.37211947275742974
[2m[36m(func pid=111842)[0m f1_per_class: [0.36, 0.279, 0.786, 0.42, 0.075, 0.394, 0.416, 0.278, 0.207, 0.202]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.1756 | Steps: 2 | Val loss: 2.1128 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0062 | Steps: 2 | Val loss: 21.6486 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0950 | Steps: 2 | Val loss: 5.4611 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 01:38:35 (running for 00:04:56.50)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.24  |      0.228 |                   46 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.348 |      0.342 |                   48 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.009 |      0.326 |                   46 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.006 |      0.3   |                   48 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.2756529850746269
[2m[36m(func pid=111467)[0m top5: 0.7728544776119403
[2m[36m(func pid=111467)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=111467)[0m f1_macro: 0.22881684509382727
[2m[36m(func pid=111467)[0m f1_weighted: 0.2961614613595355
[2m[36m(func pid=111467)[0m f1_per_class: [0.302, 0.189, 0.089, 0.374, 0.051, 0.319, 0.315, 0.261, 0.102, 0.286]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3665 | Steps: 2 | Val loss: 2.0348 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=112678)[0m top1: 0.2891791044776119
[2m[36m(func pid=112678)[0m top5: 0.8306902985074627
[2m[36m(func pid=112678)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=112678)[0m f1_macro: 0.300297110042572
[2m[36m(func pid=112678)[0m f1_weighted: 0.29977404729788254
[2m[36m(func pid=112678)[0m f1_per_class: [0.286, 0.335, 0.786, 0.315, 0.072, 0.351, 0.266, 0.284, 0.17, 0.139]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112258)[0m top1: 0.34375
[2m[36m(func pid=112258)[0m top5: 0.8498134328358209
[2m[36m(func pid=112258)[0m f1_micro: 0.34375
[2m[36m(func pid=112258)[0m f1_macro: 0.3244064454688613
[2m[36m(func pid=112258)[0m f1_weighted: 0.38209337729906473
[2m[36m(func pid=112258)[0m f1_per_class: [0.261, 0.236, 0.786, 0.429, 0.081, 0.364, 0.493, 0.249, 0.196, 0.149]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m top1: 0.34654850746268656
[2m[36m(func pid=111842)[0m top5: 0.8941231343283582
[2m[36m(func pid=111842)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=111842)[0m f1_macro: 0.34276249435535955
[2m[36m(func pid=111842)[0m f1_weighted: 0.37337254618344967
[2m[36m(func pid=111842)[0m f1_per_class: [0.374, 0.282, 0.786, 0.424, 0.075, 0.387, 0.417, 0.276, 0.204, 0.203]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0000 | Steps: 2 | Val loss: 21.8088 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.2168 | Steps: 2 | Val loss: 2.1092 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0165 | Steps: 2 | Val loss: 5.5246 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 01:38:40 (running for 00:05:01.58)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.176 |      0.229 |                   47 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.366 |      0.343 |                   49 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.095 |      0.324 |                   47 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.295 |                   49 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=112678)[0m top1: 0.28451492537313433
[2m[36m(func pid=112678)[0m top5: 0.8288246268656716
[2m[36m(func pid=112678)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=112678)[0m f1_macro: 0.29540180725603127
[2m[36m(func pid=112678)[0m f1_weighted: 0.29389759046551567
[2m[36m(func pid=112678)[0m f1_per_class: [0.284, 0.33, 0.759, 0.313, 0.074, 0.345, 0.254, 0.289, 0.164, 0.143]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3590 | Steps: 2 | Val loss: 2.0508 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=111467)[0m top1: 0.27705223880597013
[2m[36m(func pid=111467)[0m top5: 0.7761194029850746
[2m[36m(func pid=111467)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=111467)[0m f1_macro: 0.23072179487144234
[2m[36m(func pid=111467)[0m f1_weighted: 0.2968674481134975
[2m[36m(func pid=111467)[0m f1_per_class: [0.29, 0.179, 0.089, 0.377, 0.063, 0.318, 0.321, 0.262, 0.105, 0.304]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.3451492537313433
[2m[36m(func pid=112258)[0m top5: 0.8479477611940298
[2m[36m(func pid=112258)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=112258)[0m f1_macro: 0.32579228558751927
[2m[36m(func pid=112258)[0m f1_weighted: 0.38236562005190095
[2m[36m(func pid=112258)[0m f1_per_class: [0.256, 0.23, 0.786, 0.433, 0.085, 0.382, 0.487, 0.244, 0.2, 0.155]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m top1: 0.34468283582089554
[2m[36m(func pid=111842)[0m top5: 0.8955223880597015
[2m[36m(func pid=111842)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=111842)[0m f1_macro: 0.33888032894246434
[2m[36m(func pid=111842)[0m f1_weighted: 0.37139183595163816
[2m[36m(func pid=111842)[0m f1_per_class: [0.369, 0.283, 0.786, 0.424, 0.075, 0.371, 0.42, 0.26, 0.19, 0.209]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7459 | Steps: 2 | Val loss: 21.7496 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.1728 | Steps: 2 | Val loss: 2.1030 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0077 | Steps: 2 | Val loss: 5.5703 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 01:38:45 (running for 00:05:06.70)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.217 |      0.231 |                   48 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.359 |      0.339 |                   50 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.016 |      0.326 |                   48 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.746 |      0.3   |                   50 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=112678)[0m top1: 0.2896455223880597
[2m[36m(func pid=112678)[0m top5: 0.8362873134328358
[2m[36m(func pid=112678)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=112678)[0m f1_macro: 0.29976193024089426
[2m[36m(func pid=112678)[0m f1_weighted: 0.29971178656899927
[2m[36m(func pid=112678)[0m f1_per_class: [0.26, 0.329, 0.786, 0.32, 0.071, 0.36, 0.262, 0.288, 0.174, 0.148]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3117 | Steps: 2 | Val loss: 2.0605 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=111467)[0m top1: 0.2789179104477612
[2m[36m(func pid=111467)[0m top5: 0.7756529850746269
[2m[36m(func pid=111467)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=111467)[0m f1_macro: 0.23323474723137885
[2m[36m(func pid=111467)[0m f1_weighted: 0.2963215469786202
[2m[36m(func pid=111467)[0m f1_per_class: [0.296, 0.185, 0.092, 0.382, 0.063, 0.319, 0.305, 0.277, 0.11, 0.302]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.34281716417910446
[2m[36m(func pid=112258)[0m top5: 0.8493470149253731
[2m[36m(func pid=112258)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=112258)[0m f1_macro: 0.32434711456010973
[2m[36m(func pid=112258)[0m f1_weighted: 0.37958562079802005
[2m[36m(func pid=112258)[0m f1_per_class: [0.245, 0.234, 0.786, 0.425, 0.089, 0.369, 0.487, 0.257, 0.198, 0.154]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3451492537313433
[2m[36m(func pid=111842)[0m top5: 0.8959888059701493
[2m[36m(func pid=111842)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=111842)[0m f1_macro: 0.33853896731765376
[2m[36m(func pid=111842)[0m f1_weighted: 0.37173443672396367
[2m[36m(func pid=111842)[0m f1_per_class: [0.362, 0.285, 0.786, 0.422, 0.077, 0.376, 0.422, 0.254, 0.193, 0.209]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0000 | Steps: 2 | Val loss: 21.8656 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.2359 | Steps: 2 | Val loss: 2.0955 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0071 | Steps: 2 | Val loss: 5.6394 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=112678)[0m top1: 0.29011194029850745
[2m[36m(func pid=112678)[0m top5: 0.840018656716418
[2m[36m(func pid=112678)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=112678)[0m f1_macro: 0.29650636212722303
[2m[36m(func pid=112678)[0m f1_weighted: 0.29978202520603797
[2m[36m(func pid=112678)[0m f1_per_class: [0.243, 0.33, 0.759, 0.334, 0.072, 0.361, 0.248, 0.291, 0.182, 0.145]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2677 | Steps: 2 | Val loss: 2.0888 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 01:38:51 (running for 00:05:12.35)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.173 |      0.233 |                   49 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.312 |      0.339 |                   51 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.007 |      0.323 |                   50 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.297 |                   51 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=112258)[0m top1: 0.3414179104477612
[2m[36m(func pid=112258)[0m top5: 0.8470149253731343
[2m[36m(func pid=112258)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=112258)[0m f1_macro: 0.32286629458736343
[2m[36m(func pid=112258)[0m f1_weighted: 0.37823126704881904
[2m[36m(func pid=112258)[0m f1_per_class: [0.24, 0.229, 0.786, 0.428, 0.09, 0.368, 0.483, 0.252, 0.197, 0.156]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111467)[0m top1: 0.28777985074626866
[2m[36m(func pid=111467)[0m top5: 0.7761194029850746
[2m[36m(func pid=111467)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=111467)[0m f1_macro: 0.24113175794731007
[2m[36m(func pid=111467)[0m f1_weighted: 0.30592807560737084
[2m[36m(func pid=111467)[0m f1_per_class: [0.322, 0.195, 0.092, 0.398, 0.063, 0.307, 0.32, 0.273, 0.11, 0.331]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0000 | Steps: 2 | Val loss: 21.8023 | Batch size: 32 | lr: 0.1 | Duration: 2.53s
[2m[36m(func pid=111842)[0m top1: 0.3423507462686567
[2m[36m(func pid=111842)[0m top5: 0.8936567164179104
[2m[36m(func pid=111842)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=111842)[0m f1_macro: 0.3375004668146652
[2m[36m(func pid=111842)[0m f1_weighted: 0.36891541862763433
[2m[36m(func pid=111842)[0m f1_per_class: [0.366, 0.283, 0.786, 0.42, 0.077, 0.373, 0.417, 0.254, 0.191, 0.209]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.0794 | Steps: 2 | Val loss: 2.0901 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0057 | Steps: 2 | Val loss: 5.7653 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=112678)[0m top1: 0.2905783582089552
[2m[36m(func pid=112678)[0m top5: 0.8386194029850746
[2m[36m(func pid=112678)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=112678)[0m f1_macro: 0.29682801325731933
[2m[36m(func pid=112678)[0m f1_weighted: 0.29931423887342073
[2m[36m(func pid=112678)[0m f1_per_class: [0.233, 0.331, 0.759, 0.338, 0.076, 0.356, 0.242, 0.299, 0.188, 0.147]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3842 | Steps: 2 | Val loss: 2.0993 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 01:38:57 (running for 00:05:17.71)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.236 |      0.241 |                   50 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.268 |      0.338 |                   52 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.006 |      0.321 |                   51 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.297 |                   52 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.2868470149253731
[2m[36m(func pid=111467)[0m top5: 0.7789179104477612
[2m[36m(func pid=111467)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=111467)[0m f1_macro: 0.23753136732320415
[2m[36m(func pid=111467)[0m f1_weighted: 0.30438116328156756
[2m[36m(func pid=111467)[0m f1_per_class: [0.302, 0.186, 0.095, 0.399, 0.065, 0.303, 0.323, 0.266, 0.11, 0.326]
[2m[36m(func pid=112258)[0m top1: 0.33488805970149255
[2m[36m(func pid=112258)[0m top5: 0.8465485074626866
[2m[36m(func pid=112258)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=112258)[0m f1_macro: 0.3210714523057717
[2m[36m(func pid=112258)[0m f1_weighted: 0.3712167287768294
[2m[36m(func pid=112258)[0m f1_per_class: [0.234, 0.231, 0.786, 0.418, 0.09, 0.372, 0.465, 0.258, 0.2, 0.156]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0000 | Steps: 2 | Val loss: 21.8040 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=111842)[0m top1: 0.34328358208955223
[2m[36m(func pid=111842)[0m top5: 0.8969216417910447
[2m[36m(func pid=111842)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=111842)[0m f1_macro: 0.3378979007889758
[2m[36m(func pid=111842)[0m f1_weighted: 0.36989165130332674
[2m[36m(func pid=111842)[0m f1_per_class: [0.377, 0.284, 0.786, 0.42, 0.078, 0.357, 0.425, 0.251, 0.198, 0.204]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.1296 | Steps: 2 | Val loss: 2.0873 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2504 | Steps: 2 | Val loss: 5.8013 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=112678)[0m top1: 0.29151119402985076
[2m[36m(func pid=112678)[0m top5: 0.8423507462686567
[2m[36m(func pid=112678)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=112678)[0m f1_macro: 0.2961436367066847
[2m[36m(func pid=112678)[0m f1_weighted: 0.3004814470401942
[2m[36m(func pid=112678)[0m f1_per_class: [0.22, 0.324, 0.759, 0.357, 0.08, 0.353, 0.234, 0.307, 0.18, 0.148]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3791 | Steps: 2 | Val loss: 2.1054 | Batch size: 32 | lr: 0.001 | Duration: 2.59s
[2m[36m(func pid=111467)[0m top1: 0.2887126865671642
[2m[36m(func pid=111467)[0m top5: 0.7817164179104478
[2m[36m(func pid=111467)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=111467)[0m f1_macro: 0.2394440180067193
[2m[36m(func pid=111467)[0m f1_weighted: 0.30676151498087395
[2m[36m(func pid=111467)[0m f1_per_class: [0.303, 0.189, 0.097, 0.397, 0.063, 0.305, 0.33, 0.266, 0.11, 0.333]
[2m[36m(func pid=111467)[0m 
== Status ==
Current time: 2024-01-07 01:39:02 (running for 00:05:22.99)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.13  |      0.239 |                   52 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.384 |      0.338 |                   53 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.006 |      0.321 |                   51 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.296 |                   53 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=112258)[0m top1: 0.333955223880597
[2m[36m(func pid=112258)[0m top5: 0.8451492537313433
[2m[36m(func pid=112258)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=112258)[0m f1_macro: 0.32058619870954175
[2m[36m(func pid=112258)[0m f1_weighted: 0.37010829394552486
[2m[36m(func pid=112258)[0m f1_per_class: [0.235, 0.234, 0.786, 0.422, 0.105, 0.361, 0.461, 0.259, 0.191, 0.153]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m top1: 0.345615671641791
[2m[36m(func pid=111842)[0m top5: 0.8978544776119403
[2m[36m(func pid=111842)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=111842)[0m f1_macro: 0.3389511947293983
[2m[36m(func pid=111842)[0m f1_weighted: 0.3729093510001658
[2m[36m(func pid=111842)[0m f1_per_class: [0.372, 0.287, 0.786, 0.419, 0.078, 0.372, 0.429, 0.248, 0.195, 0.203]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0000 | Steps: 2 | Val loss: 22.0889 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.1296 | Steps: 2 | Val loss: 2.0832 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0223 | Steps: 2 | Val loss: 5.8939 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=112678)[0m top1: 0.2891791044776119
[2m[36m(func pid=112678)[0m top5: 0.8460820895522388
[2m[36m(func pid=112678)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=112678)[0m f1_macro: 0.29261915908410174
[2m[36m(func pid=112678)[0m f1_weighted: 0.2991072675682775
[2m[36m(func pid=112678)[0m f1_per_class: [0.203, 0.311, 0.759, 0.364, 0.088, 0.356, 0.234, 0.289, 0.176, 0.148]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3792 | Steps: 2 | Val loss: 2.1264 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 01:39:07 (running for 00:05:28.13)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.13  |      0.24  |                   53 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.379 |      0.339 |                   54 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.25  |      0.321 |                   52 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.293 |                   54 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.28824626865671643
[2m[36m(func pid=111467)[0m top5: 0.7807835820895522
[2m[36m(func pid=111467)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=111467)[0m f1_macro: 0.23993681358317986
[2m[36m(func pid=111467)[0m f1_weighted: 0.3071136350725931
[2m[36m(func pid=111467)[0m f1_per_class: [0.3, 0.202, 0.095, 0.394, 0.061, 0.313, 0.322, 0.278, 0.102, 0.331]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.33115671641791045
[2m[36m(func pid=112258)[0m top5: 0.8414179104477612
[2m[36m(func pid=112258)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=112258)[0m f1_macro: 0.31759394300294824
[2m[36m(func pid=112258)[0m f1_weighted: 0.36858734432096163
[2m[36m(func pid=112258)[0m f1_per_class: [0.241, 0.228, 0.786, 0.42, 0.08, 0.354, 0.463, 0.259, 0.193, 0.151]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3474813432835821
[2m[36m(func pid=111842)[0m top5: 0.8969216417910447
[2m[36m(func pid=111842)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=111842)[0m f1_macro: 0.34140358575722
[2m[36m(func pid=111842)[0m f1_weighted: 0.3754345472621749
[2m[36m(func pid=111842)[0m f1_per_class: [0.378, 0.286, 0.786, 0.427, 0.076, 0.362, 0.431, 0.26, 0.195, 0.212]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0052 | Steps: 2 | Val loss: 22.1885 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.0581 | Steps: 2 | Val loss: 2.0795 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0112 | Steps: 2 | Val loss: 5.9771 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=112678)[0m top1: 0.29384328358208955
[2m[36m(func pid=112678)[0m top5: 0.847481343283582
[2m[36m(func pid=112678)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=112678)[0m f1_macro: 0.2972653977956051
[2m[36m(func pid=112678)[0m f1_weighted: 0.3042740946809162
[2m[36m(func pid=112678)[0m f1_per_class: [0.207, 0.308, 0.759, 0.376, 0.088, 0.35, 0.239, 0.302, 0.198, 0.146]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2705 | Steps: 2 | Val loss: 2.1454 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 01:39:12 (running for 00:05:33.47)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.058 |      0.238 |                   54 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.379 |      0.341 |                   55 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.022 |      0.318 |                   53 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.005 |      0.297 |                   55 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.28638059701492535
[2m[36m(func pid=111467)[0m top5: 0.7817164179104478
[2m[36m(func pid=111467)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=111467)[0m f1_macro: 0.23795318550310585
[2m[36m(func pid=111467)[0m f1_weighted: 0.30530169497814147
[2m[36m(func pid=111467)[0m f1_per_class: [0.296, 0.204, 0.097, 0.388, 0.061, 0.316, 0.321, 0.273, 0.104, 0.319]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.3316231343283582
[2m[36m(func pid=112258)[0m top5: 0.8386194029850746
[2m[36m(func pid=112258)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=112258)[0m f1_macro: 0.31846372630311526
[2m[36m(func pid=112258)[0m f1_weighted: 0.36874211442977467
[2m[36m(func pid=112258)[0m f1_per_class: [0.221, 0.231, 0.786, 0.425, 0.088, 0.353, 0.456, 0.28, 0.19, 0.155]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0000 | Steps: 2 | Val loss: 22.3753 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=111842)[0m top1: 0.3451492537313433
[2m[36m(func pid=111842)[0m top5: 0.8959888059701493
[2m[36m(func pid=111842)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=111842)[0m f1_macro: 0.33930227851439554
[2m[36m(func pid=111842)[0m f1_weighted: 0.3735717001455495
[2m[36m(func pid=111842)[0m f1_per_class: [0.37, 0.282, 0.786, 0.423, 0.075, 0.356, 0.435, 0.258, 0.198, 0.21]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.0013 | Steps: 2 | Val loss: 2.0726 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0024 | Steps: 2 | Val loss: 6.0219 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=112678)[0m top1: 0.29617537313432835
[2m[36m(func pid=112678)[0m top5: 0.8502798507462687
[2m[36m(func pid=112678)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=112678)[0m f1_macro: 0.29549092499768786
[2m[36m(func pid=112678)[0m f1_weighted: 0.30645073057089417
[2m[36m(func pid=112678)[0m f1_per_class: [0.206, 0.304, 0.759, 0.386, 0.077, 0.35, 0.242, 0.295, 0.193, 0.145]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4101 | Steps: 2 | Val loss: 2.1644 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 01:39:18 (running for 00:05:38.87)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.001 |      0.241 |                   55 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.27  |      0.339 |                   56 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.318 |                   54 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.295 |                   56 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.29244402985074625
[2m[36m(func pid=111467)[0m top5: 0.7854477611940298
[2m[36m(func pid=111467)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=111467)[0m f1_macro: 0.2414978052587294
[2m[36m(func pid=111467)[0m f1_weighted: 0.3118021184444765
[2m[36m(func pid=111467)[0m f1_per_class: [0.302, 0.205, 0.105, 0.398, 0.059, 0.322, 0.331, 0.273, 0.104, 0.317]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.3362873134328358
[2m[36m(func pid=112258)[0m top5: 0.8414179104477612
[2m[36m(func pid=112258)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=112258)[0m f1_macro: 0.3214202554076437
[2m[36m(func pid=112258)[0m f1_weighted: 0.37277552689307986
[2m[36m(func pid=112258)[0m f1_per_class: [0.231, 0.229, 0.786, 0.432, 0.089, 0.354, 0.462, 0.28, 0.198, 0.154]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0028 | Steps: 2 | Val loss: 22.6347 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=111842)[0m top1: 0.3498134328358209
[2m[36m(func pid=111842)[0m top5: 0.8978544776119403
[2m[36m(func pid=111842)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=111842)[0m f1_macro: 0.3439373142949589
[2m[36m(func pid=111842)[0m f1_weighted: 0.3789945747100665
[2m[36m(func pid=111842)[0m f1_per_class: [0.367, 0.283, 0.815, 0.422, 0.074, 0.359, 0.45, 0.265, 0.202, 0.202]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.9877 | Steps: 2 | Val loss: 2.0641 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0060 | Steps: 2 | Val loss: 6.0862 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=112678)[0m top1: 0.2947761194029851
[2m[36m(func pid=112678)[0m top5: 0.8465485074626866
[2m[36m(func pid=112678)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=112678)[0m f1_macro: 0.2967832629596395
[2m[36m(func pid=112678)[0m f1_weighted: 0.30216718793360475
[2m[36m(func pid=112678)[0m f1_per_class: [0.201, 0.291, 0.786, 0.395, 0.079, 0.339, 0.228, 0.296, 0.203, 0.149]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2161 | Steps: 2 | Val loss: 2.1948 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=111467)[0m top1: 0.29757462686567165
[2m[36m(func pid=111467)[0m top5: 0.7887126865671642
[2m[36m(func pid=111467)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=111467)[0m f1_macro: 0.24319744323870074
[2m[36m(func pid=111467)[0m f1_weighted: 0.3167373062801743
[2m[36m(func pid=111467)[0m f1_per_class: [0.297, 0.198, 0.111, 0.409, 0.059, 0.324, 0.34, 0.276, 0.104, 0.314]
== Status ==
Current time: 2024-01-07 01:39:23 (running for 00:05:44.17)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.988 |      0.243 |                   56 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.41  |      0.344 |                   57 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.002 |      0.321 |                   55 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.003 |      0.297 |                   57 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.33115671641791045
[2m[36m(func pid=112258)[0m top5: 0.8395522388059702
[2m[36m(func pid=112258)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=112258)[0m f1_macro: 0.3188065285755187
[2m[36m(func pid=112258)[0m f1_weighted: 0.36692258252804716
[2m[36m(func pid=112258)[0m f1_per_class: [0.229, 0.225, 0.786, 0.428, 0.085, 0.34, 0.452, 0.285, 0.204, 0.154]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.1330 | Steps: 2 | Val loss: 22.9595 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=111842)[0m top1: 0.35074626865671643
[2m[36m(func pid=111842)[0m top5: 0.8969216417910447
[2m[36m(func pid=111842)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=111842)[0m f1_macro: 0.34615109429521806
[2m[36m(func pid=111842)[0m f1_weighted: 0.37921553430922045
[2m[36m(func pid=111842)[0m f1_per_class: [0.382, 0.287, 0.815, 0.415, 0.076, 0.355, 0.454, 0.269, 0.208, 0.2]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.0159 | Steps: 2 | Val loss: 2.0609 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=112678)[0m top1: 0.2957089552238806
[2m[36m(func pid=112678)[0m top5: 0.8479477611940298
[2m[36m(func pid=112678)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=112678)[0m f1_macro: 0.29582462614846217
[2m[36m(func pid=112678)[0m f1_weighted: 0.3026628929493376
[2m[36m(func pid=112678)[0m f1_per_class: [0.194, 0.289, 0.786, 0.404, 0.083, 0.344, 0.223, 0.288, 0.203, 0.145]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0797 | Steps: 2 | Val loss: 6.2148 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3950 | Steps: 2 | Val loss: 2.2159 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=111467)[0m top1: 0.2989738805970149
== Status ==
Current time: 2024-01-07 01:39:28 (running for 00:05:49.60)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.988 |      0.243 |                   56 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.216 |      0.346 |                   58 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.08  |      0.316 |                   57 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.133 |      0.296 |                   58 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=111467)[0m top5: 0.7882462686567164
[2m[36m(func pid=111467)[0m f1_micro: 0.2989738805970149

[2m[36m(func pid=111467)[0m f1_macro: 0.24260559202994308
[2m[36m(func pid=111467)[0m f1_weighted: 0.31731105833247353
[2m[36m(func pid=111467)[0m f1_per_class: [0.278, 0.2, 0.116, 0.415, 0.061, 0.319, 0.339, 0.265, 0.106, 0.327]
[2m[36m(func pid=112258)[0m top1: 0.3255597014925373
[2m[36m(func pid=112258)[0m top5: 0.8376865671641791
[2m[36m(func pid=112258)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=112258)[0m f1_macro: 0.3156884025044841
[2m[36m(func pid=112258)[0m f1_weighted: 0.3612151614980548
[2m[36m(func pid=112258)[0m f1_per_class: [0.222, 0.228, 0.786, 0.421, 0.093, 0.342, 0.441, 0.272, 0.195, 0.156]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0004 | Steps: 2 | Val loss: 23.3059 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=111842)[0m top1: 0.3530783582089552
[2m[36m(func pid=111842)[0m top5: 0.9006529850746269
[2m[36m(func pid=111842)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=111842)[0m f1_macro: 0.3473767760457469
[2m[36m(func pid=111842)[0m f1_weighted: 0.3809130464385273
[2m[36m(func pid=111842)[0m f1_per_class: [0.387, 0.289, 0.815, 0.416, 0.078, 0.356, 0.457, 0.258, 0.224, 0.193]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m top1: 0.29524253731343286
[2m[36m(func pid=112678)[0m top5: 0.8479477611940298
[2m[36m(func pid=112678)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=112678)[0m f1_macro: 0.2916658521552506
[2m[36m(func pid=112678)[0m f1_weighted: 0.300047774071017
[2m[36m(func pid=112678)[0m f1_per_class: [0.192, 0.287, 0.786, 0.412, 0.066, 0.343, 0.212, 0.274, 0.2, 0.145]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0315 | Steps: 2 | Val loss: 6.3280 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.9785 | Steps: 2 | Val loss: 2.0515 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3062 | Steps: 2 | Val loss: 2.2411 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 01:39:34 (running for 00:05:54.95)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.016 |      0.243 |                   57 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.395 |      0.347 |                   59 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.032 |      0.318 |                   58 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.292 |                   59 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=112258)[0m top1: 0.3255597014925373
[2m[36m(func pid=112258)[0m top5: 0.8386194029850746
[2m[36m(func pid=112258)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=112258)[0m f1_macro: 0.3177503886621785
[2m[36m(func pid=112258)[0m f1_weighted: 0.3608136909201406
[2m[36m(func pid=112258)[0m f1_per_class: [0.222, 0.231, 0.786, 0.414, 0.094, 0.34, 0.441, 0.293, 0.2, 0.157]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111467)[0m top1: 0.3050373134328358
[2m[36m(func pid=111467)[0m top5: 0.7947761194029851
[2m[36m(func pid=111467)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=111467)[0m f1_macro: 0.24830416488859383
[2m[36m(func pid=111467)[0m f1_weighted: 0.3234396857533122
[2m[36m(func pid=111467)[0m f1_per_class: [0.275, 0.217, 0.13, 0.416, 0.07, 0.329, 0.342, 0.284, 0.105, 0.315]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3516791044776119
[2m[36m(func pid=111842)[0m top5: 0.8969216417910447
[2m[36m(func pid=111842)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=111842)[0m f1_macro: 0.34659438913703255
[2m[36m(func pid=111842)[0m f1_weighted: 0.3785979961196729
[2m[36m(func pid=111842)[0m f1_per_class: [0.382, 0.292, 0.815, 0.409, 0.079, 0.352, 0.456, 0.256, 0.229, 0.195]
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2277 | Steps: 2 | Val loss: 23.6620 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m top1: 0.2947761194029851
[2m[36m(func pid=112678)[0m top5: 0.8442164179104478
[2m[36m(func pid=112678)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=112678)[0m f1_macro: 0.2908060771537451
[2m[36m(func pid=112678)[0m f1_weighted: 0.29757483076295976
[2m[36m(func pid=112678)[0m f1_per_class: [0.186, 0.281, 0.786, 0.416, 0.069, 0.35, 0.201, 0.273, 0.198, 0.149]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0172 | Steps: 2 | Val loss: 6.3783 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.0137 | Steps: 2 | Val loss: 2.0483 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2626 | Steps: 2 | Val loss: 2.2629 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 01:39:39 (running for 00:06:00.28)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.978 |      0.248 |                   58 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.306 |      0.347 |                   60 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.017 |      0.319 |                   59 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.228 |      0.291 |                   60 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=112258)[0m top1: 0.32509328358208955
[2m[36m(func pid=112258)[0m top5: 0.8395522388059702
[2m[36m(func pid=112258)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=112258)[0m f1_macro: 0.3193696867166649
[2m[36m(func pid=112258)[0m f1_weighted: 0.36045116580737574
[2m[36m(func pid=112258)[0m f1_per_class: [0.225, 0.23, 0.786, 0.408, 0.091, 0.354, 0.439, 0.293, 0.209, 0.159]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0000 | Steps: 2 | Val loss: 23.9916 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=111467)[0m top1: 0.3050373134328358
[2m[36m(func pid=111467)[0m top5: 0.7933768656716418
[2m[36m(func pid=111467)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=111467)[0m f1_macro: 0.24816258804005026
[2m[36m(func pid=111467)[0m f1_weighted: 0.32300774361860257
[2m[36m(func pid=111467)[0m f1_per_class: [0.278, 0.22, 0.131, 0.418, 0.071, 0.321, 0.34, 0.278, 0.104, 0.32]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3512126865671642
[2m[36m(func pid=111842)[0m top5: 0.8950559701492538
[2m[36m(func pid=111842)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=111842)[0m f1_macro: 0.3466584264792477
[2m[36m(func pid=111842)[0m f1_weighted: 0.37793794463554364
[2m[36m(func pid=111842)[0m f1_per_class: [0.382, 0.293, 0.815, 0.412, 0.08, 0.356, 0.449, 0.257, 0.232, 0.191]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m top1: 0.29011194029850745
[2m[36m(func pid=112678)[0m top5: 0.8423507462686567
[2m[36m(func pid=112678)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=112678)[0m f1_macro: 0.2881376529500714
[2m[36m(func pid=112678)[0m f1_weighted: 0.2912036809605174
[2m[36m(func pid=112678)[0m f1_per_class: [0.176, 0.275, 0.786, 0.416, 0.07, 0.345, 0.185, 0.276, 0.201, 0.152]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2550 | Steps: 2 | Val loss: 2.2881 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.0000 | Steps: 2 | Val loss: 2.0453 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0013 | Steps: 2 | Val loss: 6.4260 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0001 | Steps: 2 | Val loss: 24.3506 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 01:39:44 (running for 00:06:05.37)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2.014 |      0.248 |                   59 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.255 |      0.346 |                   62 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.017 |      0.319 |                   59 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.288 |                   61 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.3498134328358209
[2m[36m(func pid=111842)[0m top5: 0.8950559701492538
[2m[36m(func pid=111842)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=111842)[0m f1_macro: 0.346319528637281
[2m[36m(func pid=111842)[0m f1_weighted: 0.37718993231575954
[2m[36m(func pid=111842)[0m f1_per_class: [0.375, 0.292, 0.815, 0.409, 0.08, 0.349, 0.449, 0.278, 0.227, 0.189]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m top1: 0.3064365671641791
[2m[36m(func pid=111467)[0m top5: 0.7947761194029851
[2m[36m(func pid=111467)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=111467)[0m f1_macro: 0.24876533839307377
[2m[36m(func pid=111467)[0m f1_weighted: 0.3238644642643114
[2m[36m(func pid=111467)[0m f1_per_class: [0.282, 0.217, 0.136, 0.427, 0.068, 0.325, 0.334, 0.281, 0.104, 0.314]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.32322761194029853
[2m[36m(func pid=112258)[0m top5: 0.8376865671641791
[2m[36m(func pid=112258)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=112258)[0m f1_macro: 0.31833342542763465
[2m[36m(func pid=112258)[0m f1_weighted: 0.35917294306751996
[2m[36m(func pid=112258)[0m f1_per_class: [0.219, 0.23, 0.786, 0.402, 0.096, 0.355, 0.44, 0.293, 0.207, 0.155]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.2868470149253731
[2m[36m(func pid=112678)[0m top5: 0.840018656716418
[2m[36m(func pid=112678)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=112678)[0m f1_macro: 0.2868959864244277
[2m[36m(func pid=112678)[0m f1_weighted: 0.2887838213133146
[2m[36m(func pid=112678)[0m f1_per_class: [0.172, 0.275, 0.786, 0.411, 0.07, 0.348, 0.181, 0.282, 0.195, 0.15]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.9254 | Steps: 2 | Val loss: 2.0372 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3131 | Steps: 2 | Val loss: 2.3083 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0225 | Steps: 2 | Val loss: 6.5153 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:39:49 (running for 00:06:10.47)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  2     |      0.249 |                   60 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.313 |      0.342 |                   63 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.001 |      0.318 |                   60 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.287 |                   62 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.3474813432835821
[2m[36m(func pid=111842)[0m top5: 0.8927238805970149
[2m[36m(func pid=111842)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=111842)[0m f1_macro: 0.34240819151845925
[2m[36m(func pid=111842)[0m f1_weighted: 0.3748456972340441
[2m[36m(func pid=111842)[0m f1_per_class: [0.353, 0.293, 0.815, 0.407, 0.081, 0.352, 0.446, 0.276, 0.208, 0.194]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0000 | Steps: 2 | Val loss: 24.6518 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=111467)[0m top1: 0.30923507462686567
[2m[36m(func pid=111467)[0m top5: 0.7980410447761194
[2m[36m(func pid=111467)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=111467)[0m f1_macro: 0.2509527725773455
[2m[36m(func pid=111467)[0m f1_weighted: 0.3268626825918722
[2m[36m(func pid=111467)[0m f1_per_class: [0.286, 0.228, 0.144, 0.427, 0.067, 0.325, 0.338, 0.281, 0.104, 0.31]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.322294776119403
[2m[36m(func pid=112258)[0m top5: 0.8372201492537313
[2m[36m(func pid=112258)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=112258)[0m f1_macro: 0.3188619675052209
[2m[36m(func pid=112258)[0m f1_weighted: 0.3585798818095348
[2m[36m(func pid=112258)[0m f1_per_class: [0.22, 0.226, 0.786, 0.398, 0.093, 0.368, 0.438, 0.301, 0.203, 0.156]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.28591417910447764
[2m[36m(func pid=112678)[0m top5: 0.8386194029850746
[2m[36m(func pid=112678)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=112678)[0m f1_macro: 0.28766519510236377
[2m[36m(func pid=112678)[0m f1_weighted: 0.2880565809282965
[2m[36m(func pid=112678)[0m f1_per_class: [0.166, 0.266, 0.786, 0.41, 0.074, 0.369, 0.176, 0.286, 0.195, 0.15]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.1683 | Steps: 2 | Val loss: 2.3170 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.9141 | Steps: 2 | Val loss: 2.0293 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0024 | Steps: 2 | Val loss: 6.5713 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
== Status ==
Current time: 2024-01-07 01:39:54 (running for 00:06:15.69)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.925 |      0.251 |                   61 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.168 |      0.338 |                   64 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.022 |      0.319 |                   61 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.288 |                   63 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.34794776119402987
[2m[36m(func pid=111842)[0m top5: 0.8903917910447762
[2m[36m(func pid=111842)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=111842)[0m f1_macro: 0.33829188514863323
[2m[36m(func pid=111842)[0m f1_weighted: 0.3755207923709303
[2m[36m(func pid=111842)[0m f1_per_class: [0.349, 0.295, 0.786, 0.405, 0.08, 0.347, 0.452, 0.278, 0.199, 0.192]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0026 | Steps: 2 | Val loss: 24.9365 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=111467)[0m top1: 0.30970149253731344
[2m[36m(func pid=111467)[0m top5: 0.800839552238806
[2m[36m(func pid=111467)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=111467)[0m f1_macro: 0.25186664694177374
[2m[36m(func pid=111467)[0m f1_weighted: 0.3269715581941914
[2m[36m(func pid=111467)[0m f1_per_class: [0.291, 0.232, 0.155, 0.425, 0.066, 0.328, 0.338, 0.273, 0.104, 0.306]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.32136194029850745
[2m[36m(func pid=112258)[0m top5: 0.8367537313432836
[2m[36m(func pid=112258)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=112258)[0m f1_macro: 0.31736390544200116
[2m[36m(func pid=112258)[0m f1_weighted: 0.3580540267234648
[2m[36m(func pid=112258)[0m f1_per_class: [0.208, 0.222, 0.786, 0.399, 0.091, 0.37, 0.44, 0.288, 0.21, 0.16]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.28171641791044777
[2m[36m(func pid=112678)[0m top5: 0.8409514925373134
[2m[36m(func pid=112678)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=112678)[0m f1_macro: 0.2852935969936298
[2m[36m(func pid=112678)[0m f1_weighted: 0.2833777835440831
[2m[36m(func pid=112678)[0m f1_per_class: [0.159, 0.253, 0.786, 0.407, 0.077, 0.367, 0.171, 0.28, 0.2, 0.153]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.1783 | Steps: 2 | Val loss: 2.3436 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0036 | Steps: 2 | Val loss: 6.6336 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.9641 | Steps: 2 | Val loss: 2.0282 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4409 | Steps: 2 | Val loss: 25.4038 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 01:40:00 (running for 00:06:20.87)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.914 |      0.252 |                   62 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.178 |      0.341 |                   65 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.002 |      0.317 |                   62 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.003 |      0.285 |                   64 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.34794776119402987
[2m[36m(func pid=111842)[0m top5: 0.8899253731343284
[2m[36m(func pid=111842)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=111842)[0m f1_macro: 0.34105772290434344
[2m[36m(func pid=111842)[0m f1_weighted: 0.3752871755130017
[2m[36m(func pid=111842)[0m f1_per_class: [0.345, 0.294, 0.815, 0.407, 0.079, 0.349, 0.45, 0.274, 0.198, 0.201]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m top1: 0.30783582089552236
[2m[36m(func pid=111467)[0m top5: 0.8013059701492538
[2m[36m(func pid=111467)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=111467)[0m f1_macro: 0.2515796067435948
[2m[36m(func pid=111467)[0m f1_weighted: 0.3253868695207679
[2m[36m(func pid=111467)[0m f1_per_class: [0.292, 0.236, 0.151, 0.419, 0.067, 0.324, 0.337, 0.276, 0.104, 0.31]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.32276119402985076
[2m[36m(func pid=112258)[0m top5: 0.8348880597014925
[2m[36m(func pid=112258)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=112258)[0m f1_macro: 0.32008368753942773
[2m[36m(func pid=112258)[0m f1_weighted: 0.3595377443523819
[2m[36m(func pid=112258)[0m f1_per_class: [0.21, 0.223, 0.786, 0.401, 0.096, 0.373, 0.438, 0.3, 0.215, 0.159]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.2835820895522388
[2m[36m(func pid=112678)[0m top5: 0.8395522388059702
[2m[36m(func pid=112678)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=112678)[0m f1_macro: 0.2866263317769428
[2m[36m(func pid=112678)[0m f1_weighted: 0.28363443800995
[2m[36m(func pid=112678)[0m f1_per_class: [0.166, 0.264, 0.786, 0.408, 0.078, 0.37, 0.164, 0.279, 0.195, 0.157]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.1625 | Steps: 2 | Val loss: 2.3737 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0018 | Steps: 2 | Val loss: 6.6584 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.8442 | Steps: 2 | Val loss: 2.0178 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 01:40:05 (running for 00:06:25.88)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.964 |      0.252 |                   63 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.162 |      0.34  |                   66 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.004 |      0.32  |                   63 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.441 |      0.287 |                   65 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.345615671641791
[2m[36m(func pid=111842)[0m top5: 0.8885261194029851
[2m[36m(func pid=111842)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=111842)[0m f1_macro: 0.33958431114550425
[2m[36m(func pid=111842)[0m f1_weighted: 0.3739841361479186
[2m[36m(func pid=111842)[0m f1_per_class: [0.347, 0.291, 0.815, 0.408, 0.077, 0.343, 0.448, 0.273, 0.202, 0.192]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0832 | Steps: 2 | Val loss: 25.6484 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=111467)[0m top1: 0.3125
[2m[36m(func pid=111467)[0m top5: 0.8031716417910447
[2m[36m(func pid=111467)[0m f1_micro: 0.3125
[2m[36m(func pid=111467)[0m f1_macro: 0.2559044142652922
[2m[36m(func pid=111467)[0m f1_weighted: 0.3302159584405897
[2m[36m(func pid=111467)[0m f1_per_class: [0.297, 0.251, 0.159, 0.419, 0.066, 0.322, 0.343, 0.288, 0.106, 0.31]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.32136194029850745
[2m[36m(func pid=112258)[0m top5: 0.8344216417910447
[2m[36m(func pid=112258)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=112258)[0m f1_macro: 0.31912577484248783
[2m[36m(func pid=112258)[0m f1_weighted: 0.3577094192603416
[2m[36m(func pid=112258)[0m f1_per_class: [0.21, 0.226, 0.786, 0.405, 0.095, 0.371, 0.43, 0.283, 0.221, 0.165]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.28031716417910446
[2m[36m(func pid=112678)[0m top5: 0.8390858208955224
[2m[36m(func pid=112678)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=112678)[0m f1_macro: 0.2853149806530876
[2m[36m(func pid=112678)[0m f1_weighted: 0.28200531579447985
[2m[36m(func pid=112678)[0m f1_per_class: [0.158, 0.257, 0.786, 0.405, 0.078, 0.372, 0.165, 0.282, 0.196, 0.155]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2694 | Steps: 2 | Val loss: 2.3741 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.8196 | Steps: 2 | Val loss: 2.0127 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0341 | Steps: 2 | Val loss: 6.8201 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.1997 | Steps: 2 | Val loss: 26.0117 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 01:40:10 (running for 00:06:31.04)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.844 |      0.256 |                   64 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.269 |      0.341 |                   67 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.002 |      0.319 |                   64 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.083 |      0.285 |                   66 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.3474813432835821
[2m[36m(func pid=111842)[0m top5: 0.8908582089552238
[2m[36m(func pid=111842)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=111842)[0m f1_macro: 0.34136930827076795
[2m[36m(func pid=111842)[0m f1_weighted: 0.3762172108954318
[2m[36m(func pid=111842)[0m f1_per_class: [0.339, 0.29, 0.815, 0.412, 0.08, 0.353, 0.447, 0.28, 0.211, 0.187]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m top1: 0.31203358208955223
[2m[36m(func pid=111467)[0m top5: 0.8078358208955224
[2m[36m(func pid=111467)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=111467)[0m f1_macro: 0.256997110343987
[2m[36m(func pid=111467)[0m f1_weighted: 0.3289829619053734
[2m[36m(func pid=111467)[0m f1_per_class: [0.295, 0.24, 0.176, 0.421, 0.064, 0.327, 0.339, 0.298, 0.109, 0.302]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.3148320895522388
[2m[36m(func pid=112258)[0m top5: 0.8325559701492538
[2m[36m(func pid=112258)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=112258)[0m f1_macro: 0.31512571268484757
[2m[36m(func pid=112258)[0m f1_weighted: 0.3516007319006712
[2m[36m(func pid=112258)[0m f1_per_class: [0.198, 0.224, 0.786, 0.392, 0.094, 0.355, 0.428, 0.289, 0.223, 0.162]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.2756529850746269
[2m[36m(func pid=112678)[0m top5: 0.8330223880597015
[2m[36m(func pid=112678)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=112678)[0m f1_macro: 0.28207870552179204
[2m[36m(func pid=112678)[0m f1_weighted: 0.2774693982031778
[2m[36m(func pid=112678)[0m f1_per_class: [0.149, 0.242, 0.759, 0.402, 0.078, 0.381, 0.155, 0.298, 0.201, 0.156]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.1845 | Steps: 2 | Val loss: 2.4077 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.8174 | Steps: 2 | Val loss: 2.0053 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0062 | Steps: 2 | Val loss: 6.8343 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0328 | Steps: 2 | Val loss: 26.0722 | Batch size: 32 | lr: 0.1 | Duration: 2.60s
== Status ==
Current time: 2024-01-07 01:40:15 (running for 00:06:36.21)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.82  |      0.257 |                   65 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.185 |      0.336 |                   68 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.034 |      0.315 |                   65 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.2   |      0.282 |                   67 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.34468283582089554
[2m[36m(func pid=111842)[0m top5: 0.886660447761194
[2m[36m(func pid=111842)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=111842)[0m f1_macro: 0.3364192755190012
[2m[36m(func pid=111842)[0m f1_weighted: 0.3736269638667066
[2m[36m(func pid=111842)[0m f1_per_class: [0.325, 0.284, 0.786, 0.407, 0.079, 0.354, 0.446, 0.288, 0.209, 0.187]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=111467)[0m top1: 0.31203358208955223
[2m[36m(func pid=111467)[0m top5: 0.8083022388059702
[2m[36m(func pid=111467)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=111467)[0m f1_macro: 0.25742537900054197
[2m[36m(func pid=111467)[0m f1_weighted: 0.3286231006579978
[2m[36m(func pid=111467)[0m f1_per_class: [0.291, 0.239, 0.182, 0.425, 0.061, 0.327, 0.331, 0.314, 0.117, 0.288]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.314365671641791
[2m[36m(func pid=112258)[0m top5: 0.8344216417910447
[2m[36m(func pid=112258)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=112258)[0m f1_macro: 0.31541405090952285
[2m[36m(func pid=112258)[0m f1_weighted: 0.35065324414404403
[2m[36m(func pid=112258)[0m f1_per_class: [0.198, 0.221, 0.786, 0.389, 0.092, 0.358, 0.426, 0.303, 0.215, 0.166]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.2751865671641791
[2m[36m(func pid=112678)[0m top5: 0.8334888059701493
[2m[36m(func pid=112678)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=112678)[0m f1_macro: 0.28352149890751344
[2m[36m(func pid=112678)[0m f1_weighted: 0.2785153203896467
[2m[36m(func pid=112678)[0m f1_per_class: [0.148, 0.236, 0.759, 0.402, 0.073, 0.39, 0.156, 0.317, 0.201, 0.155]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.1438 | Steps: 2 | Val loss: 2.4366 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.8260 | Steps: 2 | Val loss: 2.0004 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0029 | Steps: 2 | Val loss: 6.9031 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=111842)[0m top1: 0.341884328358209
[2m[36m(func pid=111842)[0m top5: 0.8819962686567164
[2m[36m(func pid=111842)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=111842)[0m f1_macro: 0.3339026629081351
[2m[36m(func pid=111842)[0m f1_weighted: 0.3715101658682775
[2m[36m(func pid=111842)[0m f1_per_class: [0.315, 0.281, 0.786, 0.406, 0.075, 0.353, 0.444, 0.288, 0.201, 0.191]
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0000 | Steps: 2 | Val loss: 26.2711 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 01:40:21 (running for 00:06:42.34)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.826 |      0.259 |                   67 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.144 |      0.334 |                   69 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.006 |      0.315 |                   66 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.033 |      0.284 |                   68 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.3138992537313433
[2m[36m(func pid=111467)[0m top5: 0.8101679104477612
[2m[36m(func pid=111467)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=111467)[0m f1_macro: 0.2585637465042132
[2m[36m(func pid=111467)[0m f1_weighted: 0.33067229281776717
[2m[36m(func pid=111467)[0m f1_per_class: [0.292, 0.239, 0.185, 0.426, 0.062, 0.325, 0.337, 0.317, 0.116, 0.286]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.31669776119402987
[2m[36m(func pid=112258)[0m top5: 0.8325559701492538
[2m[36m(func pid=112258)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=112258)[0m f1_macro: 0.31760614022161937
[2m[36m(func pid=112258)[0m f1_weighted: 0.35396786646791584
[2m[36m(func pid=112258)[0m f1_per_class: [0.201, 0.221, 0.786, 0.391, 0.088, 0.364, 0.431, 0.312, 0.213, 0.167]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.27005597014925375
[2m[36m(func pid=112678)[0m top5: 0.8316231343283582
[2m[36m(func pid=112678)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=112678)[0m f1_macro: 0.28043645466806927
[2m[36m(func pid=112678)[0m f1_weighted: 0.27523752638607274
[2m[36m(func pid=112678)[0m f1_per_class: [0.142, 0.224, 0.759, 0.395, 0.073, 0.388, 0.161, 0.308, 0.2, 0.155]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.1321 | Steps: 2 | Val loss: 2.4599 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.7385 | Steps: 2 | Val loss: 1.9916 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0044 | Steps: 2 | Val loss: 6.9597 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0005 | Steps: 2 | Val loss: 26.7168 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=111842)[0m top1: 0.33861940298507465
[2m[36m(func pid=111842)[0m top5: 0.8801305970149254
[2m[36m(func pid=111842)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=111842)[0m f1_macro: 0.33259542010376936
[2m[36m(func pid=111842)[0m f1_weighted: 0.36845165423199067
[2m[36m(func pid=111842)[0m f1_per_class: [0.313, 0.274, 0.786, 0.405, 0.073, 0.348, 0.439, 0.294, 0.207, 0.187]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 01:40:26 (running for 00:06:47.59)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.738 |      0.261 |                   68 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.132 |      0.333 |                   70 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.003 |      0.318 |                   67 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.28  |                   69 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.3162313432835821
[2m[36m(func pid=111467)[0m top5: 0.8120335820895522
[2m[36m(func pid=111467)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=111467)[0m f1_macro: 0.26054817264909785
[2m[36m(func pid=111467)[0m f1_weighted: 0.33280056310812733
[2m[36m(func pid=111467)[0m f1_per_class: [0.292, 0.239, 0.195, 0.429, 0.062, 0.331, 0.34, 0.311, 0.118, 0.289]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.31763059701492535
[2m[36m(func pid=112258)[0m top5: 0.8334888059701493
[2m[36m(func pid=112258)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=112258)[0m f1_macro: 0.3176387366417172
[2m[36m(func pid=112258)[0m f1_weighted: 0.3545871200029304
[2m[36m(func pid=112258)[0m f1_per_class: [0.198, 0.224, 0.786, 0.383, 0.09, 0.359, 0.442, 0.304, 0.218, 0.172]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.2644589552238806
[2m[36m(func pid=112678)[0m top5: 0.8260261194029851
[2m[36m(func pid=112678)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=112678)[0m f1_macro: 0.27524886282547173
[2m[36m(func pid=112678)[0m f1_weighted: 0.2695931749681998
[2m[36m(func pid=112678)[0m f1_per_class: [0.134, 0.221, 0.733, 0.389, 0.073, 0.387, 0.151, 0.303, 0.209, 0.153]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.1464 | Steps: 2 | Val loss: 2.4875 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.7455 | Steps: 2 | Val loss: 1.9951 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0005 | Steps: 2 | Val loss: 7.0028 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0232 | Steps: 2 | Val loss: 27.2246 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=111842)[0m top1: 0.3358208955223881
[2m[36m(func pid=111842)[0m top5: 0.8805970149253731
[2m[36m(func pid=111842)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=111842)[0m f1_macro: 0.33047841741916717
[2m[36m(func pid=111842)[0m f1_weighted: 0.3659509503570414
[2m[36m(func pid=111842)[0m f1_per_class: [0.297, 0.271, 0.786, 0.398, 0.073, 0.345, 0.439, 0.3, 0.208, 0.187]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 01:40:32 (running for 00:06:52.97)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.746 |      0.257 |                   69 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.146 |      0.33  |                   71 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.004 |      0.318 |                   68 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.275 |                   70 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.3111007462686567
[2m[36m(func pid=111467)[0m top5: 0.8120335820895522
[2m[36m(func pid=111467)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=111467)[0m f1_macro: 0.2567379690179393
[2m[36m(func pid=111467)[0m f1_weighted: 0.32891829455430316
[2m[36m(func pid=111467)[0m f1_per_class: [0.272, 0.244, 0.193, 0.421, 0.061, 0.334, 0.333, 0.307, 0.114, 0.289]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.3180970149253731
[2m[36m(func pid=112258)[0m top5: 0.832089552238806
[2m[36m(func pid=112258)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=112258)[0m f1_macro: 0.31696978551346217
[2m[36m(func pid=112258)[0m f1_weighted: 0.3567699843727416
[2m[36m(func pid=112258)[0m f1_per_class: [0.196, 0.224, 0.786, 0.394, 0.086, 0.363, 0.441, 0.287, 0.222, 0.17]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.2583955223880597
[2m[36m(func pid=112678)[0m top5: 0.8218283582089553
[2m[36m(func pid=112678)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=112678)[0m f1_macro: 0.2729726794767219
[2m[36m(func pid=112678)[0m f1_weighted: 0.2626988643296614
[2m[36m(func pid=112678)[0m f1_per_class: [0.135, 0.223, 0.733, 0.378, 0.071, 0.379, 0.138, 0.309, 0.217, 0.148]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.1445 | Steps: 2 | Val loss: 2.4982 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.6772 | Steps: 2 | Val loss: 1.9884 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0004 | Steps: 2 | Val loss: 7.0725 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0661 | Steps: 2 | Val loss: 27.4217 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=111842)[0m top1: 0.3362873134328358
[2m[36m(func pid=111842)[0m top5: 0.8819962686567164
[2m[36m(func pid=111842)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=111842)[0m f1_macro: 0.3320639750968929
[2m[36m(func pid=111842)[0m f1_weighted: 0.36597060964127687
[2m[36m(func pid=111842)[0m f1_per_class: [0.298, 0.274, 0.786, 0.396, 0.073, 0.355, 0.436, 0.298, 0.217, 0.189]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 01:40:37 (running for 00:06:58.32)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.677 |      0.262 |                   70 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.144 |      0.332 |                   72 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.001 |      0.317 |                   69 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.023 |      0.273 |                   71 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.3148320895522388
[2m[36m(func pid=111467)[0m top5: 0.8157649253731343
[2m[36m(func pid=111467)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=111467)[0m f1_macro: 0.26153617218475517
[2m[36m(func pid=111467)[0m f1_weighted: 0.33193180720523924
[2m[36m(func pid=111467)[0m f1_per_class: [0.284, 0.242, 0.196, 0.424, 0.061, 0.336, 0.337, 0.31, 0.138, 0.287]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.31669776119402987
[2m[36m(func pid=112258)[0m top5: 0.8334888059701493
[2m[36m(func pid=112258)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=112258)[0m f1_macro: 0.3162107998749587
[2m[36m(func pid=112258)[0m f1_weighted: 0.3551615110400823
[2m[36m(func pid=112258)[0m f1_per_class: [0.197, 0.224, 0.786, 0.384, 0.087, 0.356, 0.447, 0.298, 0.215, 0.169]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=112678)[0m top1: 0.2583955223880597
[2m[36m(func pid=112678)[0m top5: 0.8185634328358209
[2m[36m(func pid=112678)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=112678)[0m f1_macro: 0.27236573588114543
[2m[36m(func pid=112678)[0m f1_weighted: 0.2648948003056684
[2m[36m(func pid=112678)[0m f1_per_class: [0.134, 0.224, 0.733, 0.377, 0.065, 0.368, 0.151, 0.305, 0.217, 0.15]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.1540 | Steps: 2 | Val loss: 2.5222 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.7769 | Steps: 2 | Val loss: 1.9875 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0007 | Steps: 2 | Val loss: 7.1784 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0003 | Steps: 2 | Val loss: 27.4771 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=111842)[0m top1: 0.33348880597014924
[2m[36m(func pid=111842)[0m top5: 0.8768656716417911
[2m[36m(func pid=111842)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=111842)[0m f1_macro: 0.33161220776749734
[2m[36m(func pid=111842)[0m f1_weighted: 0.36449604790989165
[2m[36m(func pid=111842)[0m f1_per_class: [0.3, 0.267, 0.786, 0.4, 0.071, 0.358, 0.428, 0.306, 0.225, 0.176]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 01:40:42 (running for 00:07:03.57)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.677 |      0.262 |                   70 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.154 |      0.332 |                   73 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0     |      0.316 |                   70 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.266 |                   73 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=112678)[0m top1: 0.25279850746268656
[2m[36m(func pid=112678)[0m top5: 0.8180970149253731
[2m[36m(func pid=112678)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=112678)[0m f1_macro: 0.2656193476807029
[2m[36m(func pid=112678)[0m f1_weighted: 0.26039775289839673
[2m[36m(func pid=112678)[0m f1_per_class: [0.132, 0.223, 0.71, 0.367, 0.066, 0.36, 0.153, 0.289, 0.205, 0.151]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=111467)[0m top1: 0.310634328358209
[2m[36m(func pid=111467)[0m top5: 0.8129664179104478
[2m[36m(func pid=111467)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=111467)[0m f1_macro: 0.2574639971582078
[2m[36m(func pid=111467)[0m f1_weighted: 0.32841762084197335
[2m[36m(func pid=111467)[0m f1_per_class: [0.267, 0.235, 0.198, 0.42, 0.062, 0.338, 0.334, 0.309, 0.134, 0.277]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=112258)[0m top1: 0.3111007462686567
[2m[36m(func pid=112258)[0m top5: 0.8334888059701493
[2m[36m(func pid=112258)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=112258)[0m f1_macro: 0.3122119829427135
[2m[36m(func pid=112258)[0m f1_weighted: 0.3503013538985549
[2m[36m(func pid=112258)[0m f1_per_class: [0.19, 0.227, 0.786, 0.38, 0.085, 0.354, 0.437, 0.285, 0.211, 0.168]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.1232 | Steps: 2 | Val loss: 2.5264 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.7181 | Steps: 2 | Val loss: 27.4095 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.1337 | Steps: 2 | Val loss: 7.2625 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.6874 | Steps: 2 | Val loss: 1.9837 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=111842)[0m top1: 0.33675373134328357
[2m[36m(func pid=111842)[0m top5: 0.8791977611940298
[2m[36m(func pid=111842)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=111842)[0m f1_macro: 0.3349005887525801
[2m[36m(func pid=111842)[0m f1_weighted: 0.366611662686223
[2m[36m(func pid=111842)[0m f1_per_class: [0.302, 0.274, 0.786, 0.4, 0.072, 0.364, 0.427, 0.302, 0.238, 0.184]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 01:40:47 (running for 00:07:08.70)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING  | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.777 |      0.257 |                   71 |
| train_66d79_00001 | RUNNING  | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.123 |      0.335 |                   74 |
| train_66d79_00002 | RUNNING  | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.001 |      0.312 |                   71 |
| train_66d79_00003 | RUNNING  | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0.718 |      0.265 |                   74 |
| train_66d79_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=112678)[0m top1: 0.2513992537313433
[2m[36m(func pid=112678)[0m top5: 0.8148320895522388
[2m[36m(func pid=112678)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=112678)[0m f1_macro: 0.2649673519589838
[2m[36m(func pid=112678)[0m f1_weighted: 0.25729174747311073
[2m[36m(func pid=112678)[0m f1_per_class: [0.136, 0.223, 0.71, 0.363, 0.065, 0.364, 0.143, 0.3, 0.196, 0.148]
[2m[36m(func pid=112678)[0m 
[2m[36m(func pid=112258)[0m top1: 0.3101679104477612
[2m[36m(func pid=112258)[0m top5: 0.8353544776119403
[2m[36m(func pid=112258)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=112258)[0m f1_macro: 0.3118942679539006
[2m[36m(func pid=112258)[0m f1_weighted: 0.3493663029598065
[2m[36m(func pid=112258)[0m f1_per_class: [0.191, 0.228, 0.786, 0.374, 0.085, 0.354, 0.438, 0.289, 0.208, 0.167]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=111467)[0m top1: 0.3125
[2m[36m(func pid=111467)[0m top5: 0.8111007462686567
[2m[36m(func pid=111467)[0m f1_micro: 0.3125
[2m[36m(func pid=111467)[0m f1_macro: 0.25924058628308855
[2m[36m(func pid=111467)[0m f1_weighted: 0.33088160609927686
[2m[36m(func pid=111467)[0m f1_per_class: [0.27, 0.243, 0.2, 0.419, 0.061, 0.336, 0.337, 0.319, 0.137, 0.271]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.1184 | Steps: 2 | Val loss: 2.5468 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=112678)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0000 | Steps: 2 | Val loss: 27.3854 | Batch size: 32 | lr: 0.1 | Duration: 2.60s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.6608 | Steps: 2 | Val loss: 1.9747 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0085 | Steps: 2 | Val loss: 7.2874 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=111842)[0m top1: 0.3376865671641791
[2m[36m(func pid=111842)[0m top5: 0.8796641791044776
[2m[36m(func pid=111842)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=111842)[0m f1_macro: 0.3363549830212693
[2m[36m(func pid=111842)[0m f1_weighted: 0.36818744121840374
[2m[36m(func pid=111842)[0m f1_per_class: [0.305, 0.274, 0.786, 0.402, 0.072, 0.362, 0.429, 0.318, 0.24, 0.178]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112678)[0m top1: 0.2537313432835821
[2m[36m(func pid=112678)[0m top5: 0.8138992537313433
[2m[36m(func pid=112678)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=112678)[0m f1_macro: 0.26732394784901337
[2m[36m(func pid=112678)[0m f1_weighted: 0.2601731749951663
[2m[36m(func pid=112678)[0m f1_per_class: [0.14, 0.248, 0.71, 0.35, 0.065, 0.359, 0.154, 0.292, 0.208, 0.149]
[2m[36m(func pid=111467)[0m top1: 0.3138992537313433
[2m[36m(func pid=111467)[0m top5: 0.8180970149253731
[2m[36m(func pid=111467)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=111467)[0m f1_macro: 0.26018378348031973
[2m[36m(func pid=111467)[0m f1_weighted: 0.33176704327217876
[2m[36m(func pid=111467)[0m f1_per_class: [0.272, 0.242, 0.22, 0.421, 0.06, 0.328, 0.343, 0.317, 0.124, 0.275]
[2m[36m(func pid=112258)[0m top1: 0.31203358208955223
[2m[36m(func pid=112258)[0m top5: 0.8348880597014925
[2m[36m(func pid=112258)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=112258)[0m f1_macro: 0.31355198626471803
[2m[36m(func pid=112258)[0m f1_weighted: 0.3513470899949802
[2m[36m(func pid=112258)[0m f1_per_class: [0.186, 0.232, 0.786, 0.376, 0.081, 0.354, 0.44, 0.285, 0.216, 0.179]
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2681 | Steps: 2 | Val loss: 2.5879 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=111842)[0m top1: 0.3362873134328358
[2m[36m(func pid=111842)[0m top5: 0.8782649253731343
[2m[36m(func pid=111842)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=111842)[0m f1_macro: 0.3349682782845518
[2m[36m(func pid=111842)[0m f1_weighted: 0.36677496005830035
[2m[36m(func pid=111842)[0m f1_per_class: [0.294, 0.272, 0.786, 0.399, 0.071, 0.354, 0.43, 0.317, 0.243, 0.183]
== Status ==
Current time: 2024-01-07 01:40:53 (running for 00:07:14.12)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.31875000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING    | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.687 |      0.259 |                   72 |
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.118 |      0.336 |                   75 |
| train_66d79_00002 | RUNNING    | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.134 |      0.312 |                   72 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 01:41:01 (running for 00:07:22.41)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.31875000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING    | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.661 |      0.26  |                   73 |
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.118 |      0.336 |                   75 |
| train_66d79_00002 | RUNNING    | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.134 |      0.312 |                   72 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=128882)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=128882)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=128882)[0m Configuration completed!
[2m[36m(func pid=128882)[0m New optimizer parameters:
[2m[36m(func pid=128882)[0m SGD (
[2m[36m(func pid=128882)[0m Parameter Group 0
[2m[36m(func pid=128882)[0m     dampening: 0
[2m[36m(func pid=128882)[0m     differentiable: False
[2m[36m(func pid=128882)[0m     foreach: None
[2m[36m(func pid=128882)[0m     lr: 0.0001
[2m[36m(func pid=128882)[0m     maximize: False
[2m[36m(func pid=128882)[0m     momentum: 0.9
[2m[36m(func pid=128882)[0m     nesterov: False
[2m[36m(func pid=128882)[0m     weight_decay: 0
[2m[36m(func pid=128882)[0m )
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.1461 | Steps: 2 | Val loss: 2.6036 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.5787 | Steps: 2 | Val loss: 1.9678 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0014 | Steps: 2 | Val loss: 7.2942 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9790 | Steps: 2 | Val loss: 2.3199 | Batch size: 32 | lr: 0.0001 | Duration: 4.44s
== Status ==
Current time: 2024-01-07 01:41:06 (running for 00:07:27.44)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.31875000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING    | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.661 |      0.26  |                   73 |
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.268 |      0.335 |                   76 |
| train_66d79_00002 | RUNNING    | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.008 |      0.314 |                   73 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_66d79_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.31576492537313433
[2m[36m(func pid=111467)[0m top5: 0.820429104477612
[2m[36m(func pid=111467)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=111467)[0m f1_macro: 0.2636679311992844
[2m[36m(func pid=111467)[0m f1_weighted: 0.3337210897290512
[2m[36m(func pid=111467)[0m f1_per_class: [0.275, 0.245, 0.232, 0.423, 0.059, 0.352, 0.337, 0.316, 0.125, 0.274]
[2m[36m(func pid=111467)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3353544776119403
[2m[36m(func pid=111842)[0m top5: 0.8791977611940298
[2m[36m(func pid=111842)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=111842)[0m f1_macro: 0.3356822475462888
[2m[36m(func pid=111842)[0m f1_weighted: 0.36585704968055066
[2m[36m(func pid=111842)[0m f1_per_class: [0.298, 0.271, 0.786, 0.398, 0.071, 0.368, 0.425, 0.311, 0.245, 0.185]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=112258)[0m top1: 0.31296641791044777
[2m[36m(func pid=112258)[0m top5: 0.8376865671641791
[2m[36m(func pid=112258)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=112258)[0m f1_macro: 0.31343539746064214
[2m[36m(func pid=112258)[0m f1_weighted: 0.3522507554794021
[2m[36m(func pid=112258)[0m f1_per_class: [0.198, 0.234, 0.786, 0.369, 0.074, 0.35, 0.45, 0.288, 0.207, 0.179]
[2m[36m(func pid=112258)[0m 
[2m[36m(func pid=128882)[0m top1: 0.17490671641791045
[2m[36m(func pid=128882)[0m top5: 0.5331156716417911
[2m[36m(func pid=128882)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=128882)[0m f1_macro: 0.11782595561766149
[2m[36m(func pid=128882)[0m f1_weighted: 0.1231797451060116
[2m[36m(func pid=128882)[0m f1_per_class: [0.312, 0.347, 0.0, 0.091, 0.0, 0.211, 0.018, 0.0, 0.0, 0.2]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2252 | Steps: 2 | Val loss: 2.6178 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=111467)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.6124 | Steps: 2 | Val loss: 1.9626 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=112258)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0114 | Steps: 2 | Val loss: 7.2756 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9837 | Steps: 2 | Val loss: 2.3229 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=111842)[0m top1: 0.33348880597014924
[2m[36m(func pid=111842)[0m top5: 0.8759328358208955
[2m[36m(func pid=111842)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=111842)[0m f1_macro: 0.3342251949108783
[2m[36m(func pid=111842)[0m f1_weighted: 0.3645056118523755
[2m[36m(func pid=111842)[0m f1_per_class: [0.298, 0.262, 0.786, 0.398, 0.071, 0.36, 0.426, 0.331, 0.236, 0.175]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 01:41:12 (running for 00:07:32.96)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.31875000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | RUNNING    | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.579 |      0.264 |                   74 |
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.225 |      0.334 |                   78 |
| train_66d79_00002 | RUNNING    | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.001 |      0.313 |                   74 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.979 |      0.118 |                    1 |
| train_66d79_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111467)[0m top1: 0.31296641791044777
[2m[36m(func pid=111467)[0m top5: 0.8171641791044776
[2m[36m(func pid=111467)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=111467)[0m f1_macro: 0.2631417617770391
[2m[36m(func pid=111467)[0m f1_weighted: 0.32925348638786606
[2m[36m(func pid=111467)[0m f1_per_class: [0.265, 0.242, 0.25, 0.428, 0.057, 0.366, 0.313, 0.33, 0.107, 0.274]
[2m[36m(func pid=112258)[0m top1: 0.31529850746268656
[2m[36m(func pid=112258)[0m top5: 0.8404850746268657
[2m[36m(func pid=112258)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=112258)[0m f1_macro: 0.3153916386392178
[2m[36m(func pid=112258)[0m f1_weighted: 0.3540299098919186
[2m[36m(func pid=112258)[0m f1_per_class: [0.202, 0.24, 0.786, 0.369, 0.075, 0.353, 0.451, 0.287, 0.212, 0.18]
[2m[36m(func pid=128882)[0m top1: 0.17863805970149255
[2m[36m(func pid=128882)[0m top5: 0.5307835820895522
[2m[36m(func pid=128882)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=128882)[0m f1_macro: 0.1077351976571524
[2m[36m(func pid=128882)[0m f1_weighted: 0.12606279294192985
[2m[36m(func pid=128882)[0m f1_per_class: [0.219, 0.327, 0.0, 0.096, 0.01, 0.273, 0.015, 0.023, 0.0, 0.113]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.1755 | Steps: 2 | Val loss: 2.6182 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9901 | Steps: 2 | Val loss: 2.3322 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=111842)[0m top1: 0.3306902985074627
[2m[36m(func pid=111842)[0m top5: 0.8768656716417911
[2m[36m(func pid=111842)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=111842)[0m f1_macro: 0.3314445786033737
[2m[36m(func pid=111842)[0m f1_weighted: 0.36102688967806285
[2m[36m(func pid=111842)[0m f1_per_class: [0.305, 0.264, 0.786, 0.402, 0.071, 0.361, 0.414, 0.311, 0.222, 0.179]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=128882)[0m top1: 0.17397388059701493
[2m[36m(func pid=128882)[0m top5: 0.5153917910447762
[2m[36m(func pid=128882)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=128882)[0m f1_macro: 0.09507051400034214
[2m[36m(func pid=128882)[0m f1_weighted: 0.12229053749063712
[2m[36m(func pid=128882)[0m f1_per_class: [0.133, 0.302, 0.0, 0.103, 0.01, 0.301, 0.009, 0.011, 0.0, 0.082]
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2025 | Steps: 2 | Val loss: 2.6532 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
[2m[36m(func pid=129725)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129725)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=129725)[0m Configuration completed!
[2m[36m(func pid=129725)[0m New optimizer parameters:
[2m[36m(func pid=129725)[0m SGD (
[2m[36m(func pid=129725)[0m Parameter Group 0
[2m[36m(func pid=129725)[0m     dampening: 0
[2m[36m(func pid=129725)[0m     differentiable: False
[2m[36m(func pid=129725)[0m     foreach: None
[2m[36m(func pid=129725)[0m     lr: 0.001
[2m[36m(func pid=129725)[0m     maximize: False
[2m[36m(func pid=129725)[0m     momentum: 0.9
[2m[36m(func pid=129725)[0m     nesterov: False
[2m[36m(func pid=129725)[0m     weight_decay: 0
[2m[36m(func pid=129725)[0m )
[2m[36m(func pid=129725)[0m 
== Status ==
Current time: 2024-01-07 01:41:17 (running for 00:07:38.05)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.176 |      0.331 |                   79 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.984 |      0.108 |                    2 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129730)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=129730)[0m Configuration completed!
[2m[36m(func pid=129730)[0m New optimizer parameters:
[2m[36m(func pid=129730)[0m SGD (
[2m[36m(func pid=129730)[0m Parameter Group 0
[2m[36m(func pid=129730)[0m     dampening: 0
[2m[36m(func pid=129730)[0m     differentiable: False
[2m[36m(func pid=129730)[0m     foreach: None
[2m[36m(func pid=129730)[0m     lr: 0.01
[2m[36m(func pid=129730)[0m     maximize: False
[2m[36m(func pid=129730)[0m     momentum: 0.9
[2m[36m(func pid=129730)[0m     nesterov: False
[2m[36m(func pid=129730)[0m     weight_decay: 0
[2m[36m(func pid=129730)[0m )
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=111842)[0m top1: 0.33115671641791045
[2m[36m(func pid=111842)[0m top5: 0.8791977611940298
[2m[36m(func pid=111842)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=111842)[0m f1_macro: 0.3343644886719584
[2m[36m(func pid=111842)[0m f1_weighted: 0.36141871355553984
[2m[36m(func pid=111842)[0m f1_per_class: [0.302, 0.268, 0.815, 0.398, 0.071, 0.358, 0.417, 0.31, 0.221, 0.184]
[2m[36m(func pid=111842)[0m 
== Status ==
Current time: 2024-01-07 01:41:22 (running for 00:07:43.05)
Memory usage on this node: 23.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.202 |      0.334 |                   80 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.99  |      0.095 |                    3 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9870 | Steps: 2 | Val loss: 2.3362 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0967 | Steps: 2 | Val loss: 2.6574 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9946 | Steps: 2 | Val loss: 2.3169 | Batch size: 32 | lr: 0.001 | Duration: 4.34s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9679 | Steps: 2 | Val loss: 2.3140 | Batch size: 32 | lr: 0.01 | Duration: 4.38s
[2m[36m(func pid=128882)[0m top1: 0.1730410447761194
[2m[36m(func pid=128882)[0m top5: 0.5111940298507462
[2m[36m(func pid=128882)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=128882)[0m f1_macro: 0.0958214940804962
[2m[36m(func pid=128882)[0m f1_weighted: 0.1252187777011088
[2m[36m(func pid=128882)[0m f1_per_class: [0.128, 0.287, 0.0, 0.113, 0.009, 0.314, 0.012, 0.02, 0.0, 0.074]
[2m[36m(func pid=128882)[0m 
== Status ==
Current time: 2024-01-07 01:41:27 (running for 00:07:48.18)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.097 |      0.332 |                   81 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.987 |      0.096 |                    4 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |        |            |                      |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |        |            |                      |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.3316231343283582
[2m[36m(func pid=111842)[0m top5: 0.8773320895522388
[2m[36m(func pid=111842)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=111842)[0m f1_macro: 0.3315908200896579
[2m[36m(func pid=111842)[0m f1_weighted: 0.36173749071409483
[2m[36m(func pid=111842)[0m f1_per_class: [0.3, 0.27, 0.786, 0.4, 0.073, 0.36, 0.416, 0.308, 0.224, 0.18]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=129725)[0m top1: 0.17583955223880596
[2m[36m(func pid=129725)[0m top5: 0.5368470149253731
[2m[36m(func pid=129725)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=129725)[0m f1_macro: 0.11882921856110691
[2m[36m(func pid=129725)[0m f1_weighted: 0.12590637721577505
[2m[36m(func pid=129725)[0m f1_per_class: [0.304, 0.348, 0.0, 0.091, 0.0, 0.211, 0.024, 0.014, 0.0, 0.197]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.1837686567164179
[2m[36m(func pid=129730)[0m top5: 0.534981343283582
[2m[36m(func pid=129730)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=129730)[0m f1_macro: 0.1301671012935232
[2m[36m(func pid=129730)[0m f1_weighted: 0.13409178519805623
[2m[36m(func pid=129730)[0m f1_per_class: [0.386, 0.346, 0.0, 0.116, 0.0, 0.194, 0.027, 0.027, 0.0, 0.205]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9915 | Steps: 2 | Val loss: 2.3417 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.1372 | Steps: 2 | Val loss: 2.6728 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9924 | Steps: 2 | Val loss: 2.3171 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8762 | Steps: 2 | Val loss: 2.3001 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=128882)[0m top1: 0.16324626865671643
[2m[36m(func pid=128882)[0m top5: 0.49906716417910446
[2m[36m(func pid=128882)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=128882)[0m f1_macro: 0.08761490723116168
[2m[36m(func pid=128882)[0m f1_weighted: 0.12082993571340393
[2m[36m(func pid=128882)[0m f1_per_class: [0.086, 0.267, 0.0, 0.112, 0.009, 0.303, 0.018, 0.019, 0.0, 0.062]
[2m[36m(func pid=128882)[0m 
== Status ==
Current time: 2024-01-07 01:41:32 (running for 00:07:53.25)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.097 |      0.332 |                   81 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.991 |      0.088 |                    5 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.992 |      0.12  |                    2 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  2.968 |      0.13  |                    1 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.1828358208955224
[2m[36m(func pid=129725)[0m top5: 0.5359141791044776
[2m[36m(func pid=129725)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=129725)[0m f1_macro: 0.12024754167323629
[2m[36m(func pid=129725)[0m f1_weighted: 0.13240808039364751
[2m[36m(func pid=129725)[0m f1_per_class: [0.321, 0.329, 0.0, 0.103, 0.01, 0.276, 0.018, 0.035, 0.0, 0.109]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3306902985074627
[2m[36m(func pid=111842)[0m top5: 0.8759328358208955
[2m[36m(func pid=111842)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=111842)[0m f1_macro: 0.33268374127004713
[2m[36m(func pid=111842)[0m f1_weighted: 0.3608592684392703
[2m[36m(func pid=111842)[0m f1_per_class: [0.308, 0.269, 0.786, 0.4, 0.072, 0.36, 0.41, 0.322, 0.222, 0.178]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=129730)[0m top1: 0.20335820895522388
[2m[36m(func pid=129730)[0m top5: 0.5382462686567164
[2m[36m(func pid=129730)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=129730)[0m f1_macro: 0.16003625494594825
[2m[36m(func pid=129730)[0m f1_weighted: 0.16447314373670205
[2m[36m(func pid=129730)[0m f1_per_class: [0.331, 0.329, 0.151, 0.191, 0.016, 0.281, 0.021, 0.11, 0.0, 0.17]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9520 | Steps: 2 | Val loss: 2.3453 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9725 | Steps: 2 | Val loss: 2.3179 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.1080 | Steps: 2 | Val loss: 2.6860 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.6955 | Steps: 2 | Val loss: 2.2658 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=128882)[0m top1: 0.15438432835820895
[2m[36m(func pid=128882)[0m top5: 0.4962686567164179
[2m[36m(func pid=128882)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=128882)[0m f1_macro: 0.08529849478354865
[2m[36m(func pid=128882)[0m f1_weighted: 0.11850633569353809
[2m[36m(func pid=128882)[0m f1_per_class: [0.103, 0.242, 0.0, 0.111, 0.017, 0.298, 0.026, 0.027, 0.0, 0.03]
[2m[36m(func pid=128882)[0m 
== Status ==
Current time: 2024-01-07 01:41:37 (running for 00:07:58.27)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.137 |      0.333 |                   82 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.952 |      0.085 |                    6 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.972 |      0.12  |                    3 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  2.876 |      0.16  |                    2 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.18703358208955223
[2m[36m(func pid=129725)[0m top5: 0.5289179104477612
[2m[36m(func pid=129725)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=129725)[0m f1_macro: 0.12044620393089338
[2m[36m(func pid=129725)[0m f1_weighted: 0.13853666127279696
[2m[36m(func pid=129725)[0m f1_per_class: [0.282, 0.313, 0.0, 0.119, 0.01, 0.323, 0.018, 0.032, 0.0, 0.105]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3302238805970149
[2m[36m(func pid=111842)[0m top5: 0.8777985074626866
[2m[36m(func pid=111842)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=111842)[0m f1_macro: 0.3320650768888707
[2m[36m(func pid=111842)[0m f1_weighted: 0.3598721521954293
[2m[36m(func pid=111842)[0m f1_per_class: [0.309, 0.27, 0.786, 0.394, 0.073, 0.36, 0.413, 0.312, 0.224, 0.18]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=129730)[0m top1: 0.19402985074626866
[2m[36m(func pid=129730)[0m top5: 0.5834888059701493
[2m[36m(func pid=129730)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=129730)[0m f1_macro: 0.16183395766736924
[2m[36m(func pid=129730)[0m f1_weighted: 0.17599404462901036
[2m[36m(func pid=129730)[0m f1_per_class: [0.243, 0.251, 0.133, 0.217, 0.038, 0.323, 0.052, 0.203, 0.0, 0.157]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9649 | Steps: 2 | Val loss: 2.3458 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8982 | Steps: 2 | Val loss: 2.3203 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.1526 | Steps: 2 | Val loss: 2.7047 | Batch size: 32 | lr: 0.001 | Duration: 2.59s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.4543 | Steps: 2 | Val loss: 2.2074 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=128882)[0m top1: 0.14832089552238806
[2m[36m(func pid=128882)[0m top5: 0.5013992537313433
[2m[36m(func pid=128882)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=128882)[0m f1_macro: 0.08192029167753015
[2m[36m(func pid=128882)[0m f1_weighted: 0.12145805744698131
[2m[36m(func pid=128882)[0m f1_per_class: [0.088, 0.212, 0.0, 0.116, 0.016, 0.297, 0.048, 0.042, 0.0, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m top1: 0.17863805970149255
[2m[36m(func pid=129725)[0m top5: 0.5298507462686567
[2m[36m(func pid=129725)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=129725)[0m f1_macro: 0.11946823595727638
[2m[36m(func pid=129725)[0m f1_weighted: 0.13779663557659982
[2m[36m(func pid=129725)[0m f1_per_class: [0.315, 0.301, 0.0, 0.12, 0.01, 0.318, 0.024, 0.019, 0.021, 0.068]
[2m[36m(func pid=129725)[0m 
== Status ==
Current time: 2024-01-07 01:41:42 (running for 00:08:03.52)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.153 |      0.329 |                   84 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.965 |      0.082 |                    7 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.898 |      0.119 |                    4 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  2.695 |      0.162 |                    3 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.3302238805970149
[2m[36m(func pid=111842)[0m top5: 0.8759328358208955
[2m[36m(func pid=111842)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=111842)[0m f1_macro: 0.32897001791979036
[2m[36m(func pid=111842)[0m f1_weighted: 0.36046673790252576
[2m[36m(func pid=111842)[0m f1_per_class: [0.302, 0.271, 0.786, 0.397, 0.075, 0.363, 0.415, 0.31, 0.193, 0.177]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=129730)[0m top1: 0.21175373134328357
[2m[36m(func pid=129730)[0m top5: 0.6665111940298507
[2m[36m(func pid=129730)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=129730)[0m f1_macro: 0.18580214654545787
[2m[36m(func pid=129730)[0m f1_weighted: 0.20590119876162585
[2m[36m(func pid=129730)[0m f1_per_class: [0.209, 0.19, 0.089, 0.271, 0.106, 0.372, 0.106, 0.225, 0.073, 0.217]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9379 | Steps: 2 | Val loss: 2.3422 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8869 | Steps: 2 | Val loss: 2.3167 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.1005 | Steps: 2 | Val loss: 2.7222 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=128882)[0m top1: 0.14925373134328357
[2m[36m(func pid=128882)[0m top5: 0.5121268656716418
[2m[36m(func pid=128882)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=128882)[0m f1_macro: 0.08427323724740444
[2m[36m(func pid=128882)[0m f1_weighted: 0.12477495150520844
[2m[36m(func pid=128882)[0m f1_per_class: [0.062, 0.209, 0.0, 0.119, 0.037, 0.296, 0.056, 0.065, 0.0, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m top1: 0.17537313432835822
[2m[36m(func pid=129725)[0m top5: 0.5307835820895522
[2m[36m(func pid=129725)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=129725)[0m f1_macro: 0.12328595043425106
[2m[36m(func pid=129725)[0m f1_weighted: 0.14503390824885057
[2m[36m(func pid=129725)[0m f1_per_class: [0.27, 0.284, 0.054, 0.13, 0.01, 0.319, 0.048, 0.026, 0.031, 0.062]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.2466 | Steps: 2 | Val loss: 2.1435 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 01:41:47 (running for 00:08:08.68)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.101 |      0.33  |                   85 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.938 |      0.084 |                    8 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.887 |      0.123 |                    5 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  2.454 |      0.186 |                    4 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.33115671641791045
[2m[36m(func pid=111842)[0m top5: 0.875
[2m[36m(func pid=111842)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=111842)[0m f1_macro: 0.3299386757348799
[2m[36m(func pid=111842)[0m f1_weighted: 0.360896976442279
[2m[36m(func pid=111842)[0m f1_per_class: [0.308, 0.277, 0.786, 0.398, 0.074, 0.36, 0.413, 0.313, 0.192, 0.179]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8841 | Steps: 2 | Val loss: 2.3086 | Batch size: 32 | lr: 0.001 | Duration: 2.62s
[2m[36m(func pid=129730)[0m top1: 0.24673507462686567
[2m[36m(func pid=129730)[0m top5: 0.7206156716417911
[2m[36m(func pid=129730)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=129730)[0m f1_macro: 0.2165277334384687
[2m[36m(func pid=129730)[0m f1_weighted: 0.24541076481418192
[2m[36m(func pid=129730)[0m f1_per_class: [0.232, 0.152, 0.089, 0.323, 0.104, 0.407, 0.176, 0.297, 0.105, 0.279]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9448 | Steps: 2 | Val loss: 2.3380 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0667 | Steps: 2 | Val loss: 2.7297 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=129725)[0m top1: 0.17723880597014927
[2m[36m(func pid=129725)[0m top5: 0.542910447761194
[2m[36m(func pid=129725)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=129725)[0m f1_macro: 0.1376656994381323
[2m[36m(func pid=129725)[0m f1_weighted: 0.15778573124255837
[2m[36m(func pid=129725)[0m f1_per_class: [0.236, 0.265, 0.122, 0.14, 0.009, 0.336, 0.076, 0.073, 0.036, 0.082]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.15111940298507462
[2m[36m(func pid=128882)[0m top5: 0.5135261194029851
[2m[36m(func pid=128882)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=128882)[0m f1_macro: 0.08588632856167253
[2m[36m(func pid=128882)[0m f1_weighted: 0.13320281508694015
[2m[36m(func pid=128882)[0m f1_per_class: [0.058, 0.195, 0.0, 0.14, 0.023, 0.295, 0.071, 0.078, 0.0, 0.0]
[2m[36m(func pid=128882)[0m 
== Status ==
Current time: 2024-01-07 01:41:53 (running for 00:08:13.82)
Memory usage on this node: 24.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.067 |      0.33  |                   86 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.945 |      0.086 |                    9 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.884 |      0.138 |                    6 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  2.247 |      0.217 |                    5 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.3316231343283582
[2m[36m(func pid=111842)[0m top5: 0.8763992537313433
[2m[36m(func pid=111842)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=111842)[0m f1_macro: 0.33039825720653926
[2m[36m(func pid=111842)[0m f1_weighted: 0.36120793333715717
[2m[36m(func pid=111842)[0m f1_per_class: [0.305, 0.282, 0.786, 0.395, 0.074, 0.359, 0.413, 0.318, 0.192, 0.18]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.1090 | Steps: 2 | Val loss: 2.0555 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8242 | Steps: 2 | Val loss: 2.2983 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9861 | Steps: 2 | Val loss: 2.3356 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=129730)[0m top1: 0.28638059701492535
[2m[36m(func pid=129730)[0m top5: 0.7765858208955224
[2m[36m(func pid=129730)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=129730)[0m f1_macro: 0.24016069844624172
[2m[36m(func pid=129730)[0m f1_weighted: 0.286966680085687
[2m[36m(func pid=129730)[0m f1_per_class: [0.254, 0.188, 0.124, 0.384, 0.086, 0.41, 0.234, 0.28, 0.132, 0.309]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.1260 | Steps: 2 | Val loss: 2.7368 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=129725)[0m top1: 0.17863805970149255
[2m[36m(func pid=129725)[0m top5: 0.5671641791044776
[2m[36m(func pid=129725)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=129725)[0m f1_macro: 0.14387866111528214
[2m[36m(func pid=129725)[0m f1_weighted: 0.16937964475900671
[2m[36m(func pid=129725)[0m f1_per_class: [0.206, 0.246, 0.136, 0.152, 0.018, 0.351, 0.105, 0.106, 0.03, 0.09]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.1455223880597015
[2m[36m(func pid=128882)[0m top5: 0.5121268656716418
[2m[36m(func pid=128882)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=128882)[0m f1_macro: 0.08326312430481839
[2m[36m(func pid=128882)[0m f1_weighted: 0.13116718458550178
[2m[36m(func pid=128882)[0m f1_per_class: [0.052, 0.174, 0.0, 0.151, 0.021, 0.289, 0.068, 0.077, 0.0, 0.0]
[2m[36m(func pid=128882)[0m 
== Status ==
Current time: 2024-01-07 01:41:58 (running for 00:08:18.90)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.126 |      0.33  |                   87 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.986 |      0.083 |                   10 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.824 |      0.144 |                    7 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  2.109 |      0.24  |                    6 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.3292910447761194
[2m[36m(func pid=111842)[0m top5: 0.8759328358208955
[2m[36m(func pid=111842)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=111842)[0m f1_macro: 0.32981406512376776
[2m[36m(func pid=111842)[0m f1_weighted: 0.35868185926225254
[2m[36m(func pid=111842)[0m f1_per_class: [0.31, 0.274, 0.786, 0.399, 0.075, 0.364, 0.404, 0.306, 0.203, 0.177]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.7517 | Steps: 2 | Val loss: 1.9811 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7494 | Steps: 2 | Val loss: 2.2904 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9398 | Steps: 2 | Val loss: 2.3294 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.1369 | Steps: 2 | Val loss: 2.7529 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=129730)[0m top1: 0.3087686567164179
[2m[36m(func pid=129730)[0m top5: 0.8185634328358209
[2m[36m(func pid=129730)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=129730)[0m f1_macro: 0.2651985756658084
[2m[36m(func pid=129730)[0m f1_weighted: 0.3133035944451816
[2m[36m(func pid=129730)[0m f1_per_class: [0.262, 0.251, 0.229, 0.394, 0.087, 0.417, 0.269, 0.29, 0.138, 0.316]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.1814365671641791
[2m[36m(func pid=129725)[0m top5: 0.5783582089552238
[2m[36m(func pid=129725)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=129725)[0m f1_macro: 0.1464723543203496
[2m[36m(func pid=129725)[0m f1_weighted: 0.1791099356697481
[2m[36m(func pid=129725)[0m f1_per_class: [0.194, 0.233, 0.154, 0.167, 0.017, 0.371, 0.122, 0.127, 0.019, 0.061]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.14458955223880596
[2m[36m(func pid=128882)[0m top5: 0.5247201492537313
[2m[36m(func pid=128882)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=128882)[0m f1_macro: 0.08465777463601545
[2m[36m(func pid=128882)[0m f1_weighted: 0.13302043588845133
[2m[36m(func pid=128882)[0m f1_per_class: [0.051, 0.16, 0.0, 0.163, 0.027, 0.284, 0.071, 0.092, 0.0, 0.0]
[2m[36m(func pid=128882)[0m 
== Status ==
Current time: 2024-01-07 01:42:03 (running for 00:08:23.94)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.137 |      0.326 |                   88 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.94  |      0.085 |                   11 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.749 |      0.146 |                    8 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  1.752 |      0.265 |                    7 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.32742537313432835
[2m[36m(func pid=111842)[0m top5: 0.8777985074626866
[2m[36m(func pid=111842)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=111842)[0m f1_macro: 0.3263134297667948
[2m[36m(func pid=111842)[0m f1_weighted: 0.35642225553824974
[2m[36m(func pid=111842)[0m f1_per_class: [0.313, 0.271, 0.786, 0.398, 0.077, 0.355, 0.406, 0.303, 0.179, 0.176]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.6107 | Steps: 2 | Val loss: 1.9416 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7613 | Steps: 2 | Val loss: 2.2801 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9455 | Steps: 2 | Val loss: 2.3249 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.1167 | Steps: 2 | Val loss: 2.7567 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=129730)[0m top1: 0.30363805970149255
[2m[36m(func pid=129730)[0m top5: 0.8372201492537313
[2m[36m(func pid=129730)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=129730)[0m f1_macro: 0.27145130660017364
[2m[36m(func pid=129730)[0m f1_weighted: 0.31631946608312667
[2m[36m(func pid=129730)[0m f1_per_class: [0.199, 0.262, 0.333, 0.372, 0.094, 0.427, 0.295, 0.276, 0.126, 0.331]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.17583955223880596
[2m[36m(func pid=129725)[0m top5: 0.5918843283582089
[2m[36m(func pid=129725)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=129725)[0m f1_macro: 0.14620231833501157
[2m[36m(func pid=129725)[0m f1_weighted: 0.17988468301908012
[2m[36m(func pid=129725)[0m f1_per_class: [0.164, 0.222, 0.164, 0.167, 0.023, 0.354, 0.137, 0.136, 0.018, 0.077]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.14738805970149255
[2m[36m(func pid=128882)[0m top5: 0.5293843283582089
[2m[36m(func pid=128882)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=128882)[0m f1_macro: 0.08657257415934294
[2m[36m(func pid=128882)[0m f1_weighted: 0.13772913434891318
[2m[36m(func pid=128882)[0m f1_per_class: [0.046, 0.152, 0.0, 0.171, 0.027, 0.291, 0.08, 0.099, 0.0, 0.0]
[2m[36m(func pid=128882)[0m 
== Status ==
Current time: 2024-01-07 01:42:08 (running for 00:08:29.07)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.117 |      0.331 |                   89 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.945 |      0.087 |                   12 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.761 |      0.146 |                    9 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  1.611 |      0.271 |                    8 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.33255597014925375
[2m[36m(func pid=111842)[0m top5: 0.8740671641791045
[2m[36m(func pid=111842)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=111842)[0m f1_macro: 0.3314049021854623
[2m[36m(func pid=111842)[0m f1_weighted: 0.36215982150538306
[2m[36m(func pid=111842)[0m f1_per_class: [0.315, 0.275, 0.786, 0.402, 0.08, 0.366, 0.412, 0.303, 0.203, 0.173]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.4094 | Steps: 2 | Val loss: 1.8980 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7105 | Steps: 2 | Val loss: 2.2666 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.9267 | Steps: 2 | Val loss: 2.3201 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0952 | Steps: 2 | Val loss: 2.7617 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=129730)[0m top1: 0.3050373134328358
[2m[36m(func pid=129730)[0m top5: 0.8568097014925373
[2m[36m(func pid=129730)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=129730)[0m f1_macro: 0.29046711766931343
[2m[36m(func pid=129730)[0m f1_weighted: 0.317966869025019
[2m[36m(func pid=129730)[0m f1_per_class: [0.218, 0.288, 0.444, 0.323, 0.082, 0.423, 0.32, 0.3, 0.177, 0.331]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.18796641791044777
[2m[36m(func pid=129725)[0m top5: 0.6142723880597015
[2m[36m(func pid=129725)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=129725)[0m f1_macro: 0.16246202933630333
[2m[36m(func pid=129725)[0m f1_weighted: 0.19459375080123906
[2m[36m(func pid=129725)[0m f1_per_class: [0.174, 0.221, 0.231, 0.197, 0.023, 0.362, 0.153, 0.126, 0.028, 0.11]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.14972014925373134
[2m[36m(func pid=128882)[0m top5: 0.53125
[2m[36m(func pid=128882)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=128882)[0m f1_macro: 0.0975592129423691
[2m[36m(func pid=128882)[0m f1_weighted: 0.14030390234082105
[2m[36m(func pid=128882)[0m f1_per_class: [0.043, 0.154, 0.103, 0.173, 0.026, 0.295, 0.082, 0.1, 0.0, 0.0]
[2m[36m(func pid=128882)[0m 
== Status ==
Current time: 2024-01-07 01:42:13 (running for 00:08:34.10)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.095 |      0.332 |                   90 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.927 |      0.098 |                   13 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.711 |      0.162 |                   10 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  1.409 |      0.29  |                    9 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.3306902985074627
[2m[36m(func pid=111842)[0m top5: 0.871268656716418
[2m[36m(func pid=111842)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=111842)[0m f1_macro: 0.331698933093704
[2m[36m(func pid=111842)[0m f1_weighted: 0.36017626761043275
[2m[36m(func pid=111842)[0m f1_per_class: [0.318, 0.269, 0.786, 0.404, 0.08, 0.356, 0.409, 0.3, 0.224, 0.172]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.6939 | Steps: 2 | Val loss: 2.2516 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.2680 | Steps: 2 | Val loss: 1.8921 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8881 | Steps: 2 | Val loss: 2.3155 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.1612 | Steps: 2 | Val loss: 2.7778 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=129725)[0m top1: 0.19449626865671643
[2m[36m(func pid=129725)[0m top5: 0.6338619402985075
[2m[36m(func pid=129725)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=129725)[0m f1_macro: 0.1713814187097847
[2m[36m(func pid=129725)[0m f1_weighted: 0.20422985554237338
[2m[36m(func pid=129725)[0m f1_per_class: [0.185, 0.204, 0.233, 0.215, 0.031, 0.362, 0.173, 0.14, 0.038, 0.134]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.2994402985074627
[2m[36m(func pid=129730)[0m top5: 0.8549440298507462
[2m[36m(func pid=129730)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=129730)[0m f1_macro: 0.2919519804796863
[2m[36m(func pid=129730)[0m f1_weighted: 0.3150290235560699
[2m[36m(func pid=129730)[0m f1_per_class: [0.22, 0.277, 0.5, 0.322, 0.07, 0.414, 0.317, 0.314, 0.175, 0.309]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=128882)[0m top1: 0.14878731343283583
[2m[36m(func pid=128882)[0m top5: 0.5382462686567164
[2m[36m(func pid=128882)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=128882)[0m f1_macro: 0.09566819443630459
[2m[36m(func pid=128882)[0m f1_weighted: 0.13955089676908006
[2m[36m(func pid=128882)[0m f1_per_class: [0.043, 0.149, 0.1, 0.171, 0.02, 0.299, 0.085, 0.092, 0.0, 0.0]
[2m[36m(func pid=128882)[0m 
== Status ==
Current time: 2024-01-07 01:42:18 (running for 00:08:39.24)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.161 |      0.327 |                   91 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.888 |      0.096 |                   14 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.694 |      0.171 |                   11 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  1.268 |      0.292 |                   10 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.32649253731343286
[2m[36m(func pid=111842)[0m top5: 0.8722014925373134
[2m[36m(func pid=111842)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=111842)[0m f1_macro: 0.327110098140733
[2m[36m(func pid=111842)[0m f1_weighted: 0.35732782873669877
[2m[36m(func pid=111842)[0m f1_per_class: [0.31, 0.262, 0.786, 0.407, 0.077, 0.355, 0.405, 0.293, 0.208, 0.169]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.6251 | Steps: 2 | Val loss: 2.2397 | Batch size: 32 | lr: 0.001 | Duration: 2.59s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.1909 | Steps: 2 | Val loss: 1.8652 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.9327 | Steps: 2 | Val loss: 2.3137 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.1418 | Steps: 2 | Val loss: 2.7939 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=129725)[0m top1: 0.20149253731343283
[2m[36m(func pid=129725)[0m top5: 0.6585820895522388
[2m[36m(func pid=129725)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=129725)[0m f1_macro: 0.17846910976400537
[2m[36m(func pid=129725)[0m f1_weighted: 0.21272783085682287
[2m[36m(func pid=129725)[0m f1_per_class: [0.198, 0.199, 0.213, 0.221, 0.037, 0.371, 0.19, 0.153, 0.039, 0.164]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.31949626865671643
[2m[36m(func pid=129730)[0m top5: 0.8582089552238806
[2m[36m(func pid=129730)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=129730)[0m f1_macro: 0.31182256782250223
[2m[36m(func pid=129730)[0m f1_weighted: 0.3365508936721264
[2m[36m(func pid=129730)[0m f1_per_class: [0.27, 0.28, 0.595, 0.387, 0.067, 0.402, 0.323, 0.347, 0.16, 0.289]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=128882)[0m top1: 0.14878731343283583
[2m[36m(func pid=128882)[0m top5: 0.5410447761194029
[2m[36m(func pid=128882)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=128882)[0m f1_macro: 0.10116545817063378
[2m[36m(func pid=128882)[0m f1_weighted: 0.14083546624482304
[2m[36m(func pid=128882)[0m f1_per_class: [0.056, 0.139, 0.14, 0.175, 0.019, 0.295, 0.089, 0.098, 0.0, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3283582089552239
[2m[36m(func pid=111842)[0m top5: 0.8717350746268657
[2m[36m(func pid=111842)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=111842)[0m f1_macro: 0.3311304858636513
[2m[36m(func pid=111842)[0m f1_weighted: 0.35824643220654984
[2m[36m(func pid=111842)[0m f1_per_class: [0.312, 0.265, 0.786, 0.409, 0.085, 0.356, 0.4, 0.302, 0.226, 0.171]
== Status ==
Current time: 2024-01-07 01:42:23 (running for 00:08:44.30)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.142 |      0.331 |                   92 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.933 |      0.101 |                   15 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.625 |      0.178 |                   12 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  1.191 |      0.312 |                   11 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.5793 | Steps: 2 | Val loss: 2.2316 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.0646 | Steps: 2 | Val loss: 1.8428 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0474 | Steps: 2 | Val loss: 2.8055 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8987 | Steps: 2 | Val loss: 2.3108 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=129725)[0m top1: 0.20289179104477612
[2m[36m(func pid=129725)[0m top5: 0.6651119402985075
[2m[36m(func pid=129725)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=129725)[0m f1_macro: 0.17744958992008675
[2m[36m(func pid=129725)[0m f1_weighted: 0.219895198763917
[2m[36m(func pid=129725)[0m f1_per_class: [0.178, 0.183, 0.177, 0.226, 0.035, 0.358, 0.221, 0.173, 0.04, 0.183]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.32649253731343286
[2m[36m(func pid=129730)[0m top5: 0.8577425373134329
[2m[36m(func pid=129730)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=129730)[0m f1_macro: 0.32385660953298273
[2m[36m(func pid=129730)[0m f1_weighted: 0.342595794410659
[2m[36m(func pid=129730)[0m f1_per_class: [0.286, 0.268, 0.647, 0.417, 0.071, 0.403, 0.313, 0.359, 0.193, 0.28]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=111842)[0m top1: 0.3260261194029851
[2m[36m(func pid=111842)[0m top5: 0.8703358208955224
[2m[36m(func pid=111842)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=111842)[0m f1_macro: 0.3275312523970214
[2m[36m(func pid=111842)[0m f1_weighted: 0.3561109174108474
[2m[36m(func pid=111842)[0m f1_per_class: [0.307, 0.266, 0.786, 0.405, 0.075, 0.344, 0.401, 0.298, 0.226, 0.166]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=128882)[0m top1: 0.1515858208955224
[2m[36m(func pid=128882)[0m top5: 0.5443097014925373
[2m[36m(func pid=128882)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=128882)[0m f1_macro: 0.10165894629122532
[2m[36m(func pid=128882)[0m f1_weighted: 0.14426551700943843
[2m[36m(func pid=128882)[0m f1_per_class: [0.054, 0.14, 0.133, 0.183, 0.019, 0.297, 0.092, 0.098, 0.0, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5579 | Steps: 2 | Val loss: 2.2218 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8983 | Steps: 2 | Val loss: 1.8224 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0499 | Steps: 2 | Val loss: 2.8388 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8862 | Steps: 2 | Val loss: 2.3076 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 01:42:32 (running for 00:08:52.78)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.047 |      0.328 |                   93 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.899 |      0.102 |                   16 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.558 |      0.183 |                   14 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  1.065 |      0.324 |                   12 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.20569029850746268
[2m[36m(func pid=129725)[0m top5: 0.6823694029850746
[2m[36m(func pid=129725)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=129725)[0m f1_macro: 0.18325887974343474
[2m[36m(func pid=129725)[0m f1_weighted: 0.22267915454605716
[2m[36m(func pid=129725)[0m f1_per_class: [0.183, 0.178, 0.185, 0.23, 0.041, 0.357, 0.223, 0.205, 0.043, 0.188]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3376865671641791
[2m[36m(func pid=129730)[0m top5: 0.8577425373134329
[2m[36m(func pid=129730)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=129730)[0m f1_macro: 0.3298129565752802
[2m[36m(func pid=129730)[0m f1_weighted: 0.3528965265327191
[2m[36m(func pid=129730)[0m f1_per_class: [0.314, 0.254, 0.667, 0.452, 0.078, 0.403, 0.324, 0.345, 0.197, 0.265]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=111842)[0m top1: 0.324160447761194
[2m[36m(func pid=111842)[0m top5: 0.8684701492537313
[2m[36m(func pid=111842)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=111842)[0m f1_macro: 0.3281924924065678
[2m[36m(func pid=111842)[0m f1_weighted: 0.3538764866056188
[2m[36m(func pid=111842)[0m f1_per_class: [0.298, 0.259, 0.786, 0.399, 0.074, 0.364, 0.396, 0.299, 0.232, 0.175]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=128882)[0m top1: 0.1525186567164179
[2m[36m(func pid=128882)[0m top5: 0.5485074626865671
[2m[36m(func pid=128882)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=128882)[0m f1_macro: 0.10195171624948625
[2m[36m(func pid=128882)[0m f1_weighted: 0.1458137571402063
[2m[36m(func pid=128882)[0m f1_per_class: [0.052, 0.14, 0.13, 0.189, 0.019, 0.3, 0.091, 0.098, 0.0, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.4845 | Steps: 2 | Val loss: 2.2152 | Batch size: 32 | lr: 0.001 | Duration: 2.59s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.1828 | Steps: 2 | Val loss: 2.8626 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.9084 | Steps: 2 | Val loss: 2.3041 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7878 | Steps: 2 | Val loss: 1.8208 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=129725)[0m top1: 0.2042910447761194
[2m[36m(func pid=129725)[0m top5: 0.691231343283582
[2m[36m(func pid=129725)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=129725)[0m f1_macro: 0.1810615442422267
[2m[36m(func pid=129725)[0m f1_weighted: 0.2222996548541414
[2m[36m(func pid=129725)[0m f1_per_class: [0.181, 0.17, 0.168, 0.238, 0.041, 0.348, 0.222, 0.204, 0.045, 0.194]
[2m[36m(func pid=129725)[0m 
== Status ==
Current time: 2024-01-07 01:42:38 (running for 00:08:59.52)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.183 |      0.33  |                   95 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.886 |      0.102 |                   17 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.484 |      0.181 |                   15 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.898 |      0.33  |                   13 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.3260261194029851
[2m[36m(func pid=111842)[0m top5: 0.8675373134328358
[2m[36m(func pid=111842)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=111842)[0m f1_macro: 0.3297543590128208
[2m[36m(func pid=111842)[0m f1_weighted: 0.35570560093310877
[2m[36m(func pid=111842)[0m f1_per_class: [0.308, 0.274, 0.786, 0.393, 0.075, 0.37, 0.398, 0.29, 0.231, 0.173]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3423507462686567
[2m[36m(func pid=129730)[0m top5: 0.8605410447761194
[2m[36m(func pid=129730)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=129730)[0m f1_macro: 0.33386314905828185
[2m[36m(func pid=129730)[0m f1_weighted: 0.35858039250799345
[2m[36m(func pid=129730)[0m f1_per_class: [0.321, 0.244, 0.688, 0.459, 0.076, 0.401, 0.343, 0.327, 0.213, 0.267]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=128882)[0m top1: 0.15111940298507462
[2m[36m(func pid=128882)[0m top5: 0.5527052238805971
[2m[36m(func pid=128882)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=128882)[0m f1_macro: 0.0980586655249824
[2m[36m(func pid=128882)[0m f1_weighted: 0.14467618552886993
[2m[36m(func pid=128882)[0m f1_per_class: [0.054, 0.148, 0.109, 0.183, 0.019, 0.298, 0.093, 0.076, 0.0, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.4471 | Steps: 2 | Val loss: 2.2019 | Batch size: 32 | lr: 0.001 | Duration: 2.54s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.1234 | Steps: 2 | Val loss: 2.8582 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.8998 | Steps: 2 | Val loss: 2.3040 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=129725)[0m top1: 0.21082089552238806
[2m[36m(func pid=129725)[0m top5: 0.7098880597014925
[2m[36m(func pid=129725)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=129725)[0m f1_macro: 0.1849356283985029
[2m[36m(func pid=129725)[0m f1_weighted: 0.23001247650478746
[2m[36m(func pid=129725)[0m f1_per_class: [0.17, 0.17, 0.154, 0.247, 0.043, 0.351, 0.236, 0.214, 0.049, 0.215]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.9307 | Steps: 2 | Val loss: 1.8386 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 01:42:44 (running for 00:09:04.85)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.123 |      0.328 |                   96 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.908 |      0.098 |                   18 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.447 |      0.185 |                   16 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.788 |      0.334 |                   14 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.3278917910447761
[2m[36m(func pid=111842)[0m top5: 0.871268656716418
[2m[36m(func pid=111842)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=111842)[0m f1_macro: 0.32825439292692804
[2m[36m(func pid=111842)[0m f1_weighted: 0.3571895061824127
[2m[36m(func pid=111842)[0m f1_per_class: [0.291, 0.272, 0.759, 0.395, 0.076, 0.376, 0.399, 0.295, 0.239, 0.18]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=128882)[0m top1: 0.1501865671641791
[2m[36m(func pid=128882)[0m top5: 0.554570895522388
[2m[36m(func pid=128882)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=128882)[0m f1_macro: 0.099207683664175
[2m[36m(func pid=128882)[0m f1_weighted: 0.14444920762129537
[2m[36m(func pid=128882)[0m f1_per_class: [0.051, 0.141, 0.105, 0.181, 0.018, 0.307, 0.092, 0.085, 0.012, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33908582089552236
[2m[36m(func pid=129730)[0m top5: 0.8605410447761194
[2m[36m(func pid=129730)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=129730)[0m f1_macro: 0.33533595596942883
[2m[36m(func pid=129730)[0m f1_weighted: 0.3581470809139869
[2m[36m(func pid=129730)[0m f1_per_class: [0.32, 0.236, 0.688, 0.451, 0.071, 0.407, 0.347, 0.342, 0.237, 0.254]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.4281 | Steps: 2 | Val loss: 2.1935 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0649 | Steps: 2 | Val loss: 2.8819 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=129725)[0m top1: 0.21222014925373134
[2m[36m(func pid=129725)[0m top5: 0.7126865671641791
[2m[36m(func pid=129725)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=129725)[0m f1_macro: 0.18769265363881382
[2m[36m(func pid=129725)[0m f1_weighted: 0.23157395584242474
[2m[36m(func pid=129725)[0m f1_per_class: [0.171, 0.174, 0.144, 0.245, 0.043, 0.363, 0.236, 0.211, 0.051, 0.239]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.8843 | Steps: 2 | Val loss: 2.3037 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8437 | Steps: 2 | Val loss: 1.8693 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 01:42:49 (running for 00:09:10.10)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.065 |      0.33  |                   97 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.9   |      0.099 |                   19 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.428 |      0.188 |                   17 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.931 |      0.335 |                   15 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.3302238805970149
[2m[36m(func pid=111842)[0m top5: 0.8703358208955224
[2m[36m(func pid=111842)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=111842)[0m f1_macro: 0.3297438232730515
[2m[36m(func pid=111842)[0m f1_weighted: 0.36003580814541475
[2m[36m(func pid=111842)[0m f1_per_class: [0.292, 0.273, 0.759, 0.402, 0.075, 0.378, 0.4, 0.299, 0.241, 0.178]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=128882)[0m top1: 0.15205223880597016
[2m[36m(func pid=128882)[0m top5: 0.5564365671641791
[2m[36m(func pid=128882)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=128882)[0m f1_macro: 0.10078459603114473
[2m[36m(func pid=128882)[0m f1_weighted: 0.14761026301642863
[2m[36m(func pid=128882)[0m f1_per_class: [0.051, 0.137, 0.107, 0.189, 0.018, 0.306, 0.097, 0.091, 0.012, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.3625 | Steps: 2 | Val loss: 2.1837 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=129730)[0m top1: 0.3362873134328358
[2m[36m(func pid=129730)[0m top5: 0.8591417910447762
[2m[36m(func pid=129730)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=129730)[0m f1_macro: 0.33386508634769213
[2m[36m(func pid=129730)[0m f1_weighted: 0.3614940630342633
[2m[36m(func pid=129730)[0m f1_per_class: [0.314, 0.254, 0.71, 0.435, 0.071, 0.401, 0.368, 0.339, 0.225, 0.223]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.21455223880597016
[2m[36m(func pid=129725)[0m top5: 0.722481343283582
[2m[36m(func pid=129725)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=129725)[0m f1_macro: 0.19353520529480148
[2m[36m(func pid=129725)[0m f1_weighted: 0.2333717240488418
[2m[36m(func pid=129725)[0m f1_per_class: [0.175, 0.177, 0.145, 0.248, 0.042, 0.351, 0.235, 0.244, 0.053, 0.267]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0537 | Steps: 2 | Val loss: 2.9035 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.9375 | Steps: 2 | Val loss: 2.3000 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6569 | Steps: 2 | Val loss: 1.8977 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=111842)[0m top1: 0.3269589552238806
[2m[36m(func pid=111842)[0m top5: 0.8680037313432836
[2m[36m(func pid=111842)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=111842)[0m f1_macro: 0.3277298817797957
[2m[36m(func pid=111842)[0m f1_weighted: 0.3568349811007594
[2m[36m(func pid=111842)[0m f1_per_class: [0.295, 0.271, 0.759, 0.392, 0.075, 0.376, 0.402, 0.293, 0.239, 0.176]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.2764 | Steps: 2 | Val loss: 2.1771 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 01:42:54 (running for 00:09:15.65)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.054 |      0.328 |                   98 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.884 |      0.101 |                   20 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.363 |      0.194 |                   18 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.844 |      0.334 |                   16 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.15298507462686567
[2m[36m(func pid=128882)[0m top5: 0.558768656716418
[2m[36m(func pid=128882)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=128882)[0m f1_macro: 0.10158130978653102
[2m[36m(func pid=128882)[0m f1_weighted: 0.14890309491520345
[2m[36m(func pid=128882)[0m f1_per_class: [0.048, 0.139, 0.098, 0.186, 0.018, 0.31, 0.099, 0.105, 0.012, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m top1: 0.332089552238806
[2m[36m(func pid=129730)[0m top5: 0.8610074626865671
[2m[36m(func pid=129730)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=129730)[0m f1_macro: 0.32773136342391157
[2m[36m(func pid=129730)[0m f1_weighted: 0.3617558722220808
[2m[36m(func pid=129730)[0m f1_per_class: [0.317, 0.243, 0.688, 0.425, 0.064, 0.383, 0.395, 0.319, 0.23, 0.212]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.21361940298507462
[2m[36m(func pid=129725)[0m top5: 0.726679104477612
[2m[36m(func pid=129725)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=129725)[0m f1_macro: 0.19252862948369726
[2m[36m(func pid=129725)[0m f1_weighted: 0.23078165181204774
[2m[36m(func pid=129725)[0m f1_per_class: [0.176, 0.183, 0.152, 0.241, 0.05, 0.336, 0.234, 0.254, 0.042, 0.257]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0963 | Steps: 2 | Val loss: 2.9135 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.8720 | Steps: 2 | Val loss: 2.2971 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.7106 | Steps: 2 | Val loss: 1.8900 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.3150 | Steps: 2 | Val loss: 2.1646 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 01:43:00 (running for 00:09:20.73)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00001 | RUNNING    | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.096 |      0.332 |                   99 |
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.937 |      0.102 |                   21 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.276 |      0.193 |                   19 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.657 |      0.328 |                   17 |
| train_66d79_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=111842)[0m top1: 0.3292910447761194
[2m[36m(func pid=111842)[0m top5: 0.867070895522388
[2m[36m(func pid=111842)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=111842)[0m f1_macro: 0.3323103342127615
[2m[36m(func pid=111842)[0m f1_weighted: 0.3591878563474993
[2m[36m(func pid=111842)[0m f1_per_class: [0.305, 0.277, 0.786, 0.394, 0.074, 0.378, 0.402, 0.294, 0.239, 0.174]
[2m[36m(func pid=111842)[0m 
[2m[36m(func pid=128882)[0m top1: 0.15764925373134328
[2m[36m(func pid=128882)[0m top5: 0.5652985074626866
[2m[36m(func pid=128882)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=128882)[0m f1_macro: 0.1039311958383953
[2m[36m(func pid=128882)[0m f1_weighted: 0.1554975241066
[2m[36m(func pid=128882)[0m f1_per_class: [0.047, 0.14, 0.094, 0.202, 0.018, 0.313, 0.104, 0.111, 0.012, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33908582089552236
[2m[36m(func pid=129730)[0m top5: 0.8661380597014925
[2m[36m(func pid=129730)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=129730)[0m f1_macro: 0.3305232368053404
[2m[36m(func pid=129730)[0m f1_weighted: 0.3695774637733507
[2m[36m(func pid=129730)[0m f1_per_class: [0.325, 0.243, 0.688, 0.415, 0.079, 0.378, 0.434, 0.314, 0.22, 0.209]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.21921641791044777
[2m[36m(func pid=129725)[0m top5: 0.7369402985074627
[2m[36m(func pid=129725)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=129725)[0m f1_macro: 0.19807656569837942
[2m[36m(func pid=129725)[0m f1_weighted: 0.23552512519998473
[2m[36m(func pid=129725)[0m f1_per_class: [0.186, 0.178, 0.155, 0.259, 0.051, 0.333, 0.234, 0.256, 0.046, 0.283]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=111842)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.1433 | Steps: 2 | Val loss: 2.9204 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.8943 | Steps: 2 | Val loss: 2.2948 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.5910 | Steps: 2 | Val loss: 1.8951 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.3043 | Steps: 2 | Val loss: 2.1562 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=111842)[0m top1: 0.33488805970149255
[2m[36m(func pid=111842)[0m top5: 0.8689365671641791
[2m[36m(func pid=111842)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=111842)[0m f1_macro: 0.33330849824952963
[2m[36m(func pid=111842)[0m f1_weighted: 0.36444611575168345
[2m[36m(func pid=111842)[0m f1_per_class: [0.312, 0.281, 0.759, 0.404, 0.075, 0.378, 0.407, 0.295, 0.252, 0.171]
[2m[36m(func pid=128882)[0m top1: 0.15951492537313433
[2m[36m(func pid=128882)[0m top5: 0.5722947761194029
[2m[36m(func pid=128882)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=128882)[0m f1_macro: 0.10650074906687783
[2m[36m(func pid=128882)[0m f1_weighted: 0.15828007415672987
[2m[36m(func pid=128882)[0m f1_per_class: [0.057, 0.144, 0.095, 0.207, 0.018, 0.317, 0.104, 0.111, 0.012, 0.0]
[2m[36m(func pid=129730)[0m top1: 0.34328358208955223
[2m[36m(func pid=129730)[0m top5: 0.8694029850746269
[2m[36m(func pid=129730)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=129730)[0m f1_macro: 0.331979066731985
[2m[36m(func pid=129730)[0m f1_weighted: 0.3734539217282237
[2m[36m(func pid=129730)[0m f1_per_class: [0.291, 0.246, 0.71, 0.405, 0.077, 0.389, 0.452, 0.323, 0.213, 0.215]
[2m[36m(func pid=129725)[0m top1: 0.22108208955223882
[2m[36m(func pid=129725)[0m top5: 0.7388059701492538
[2m[36m(func pid=129725)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=129725)[0m f1_macro: 0.20434103041624468
[2m[36m(func pid=129725)[0m f1_weighted: 0.23536425312318177
[2m[36m(func pid=129725)[0m f1_per_class: [0.185, 0.193, 0.162, 0.258, 0.043, 0.339, 0.22, 0.262, 0.047, 0.333]
== Status ==
Current time: 2024-01-07 01:43:05 (running for 00:09:26.04)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.872 |      0.104 |                   22 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.315 |      0.198 |                   20 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.711 |      0.331 |                   18 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 01:43:13 (running for 00:09:34.47)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.872 |      0.104 |                   22 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.315 |      0.198 |                   20 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.591 |      0.332 |                   19 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=134827)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=134827)[0m Configuration completed!
[2m[36m(func pid=134827)[0m New optimizer parameters:
[2m[36m(func pid=134827)[0m SGD (
[2m[36m(func pid=134827)[0m Parameter Group 0
[2m[36m(func pid=134827)[0m     dampening: 0
[2m[36m(func pid=134827)[0m     differentiable: False
[2m[36m(func pid=134827)[0m     foreach: None
[2m[36m(func pid=134827)[0m     lr: 0.1
[2m[36m(func pid=134827)[0m     maximize: False
[2m[36m(func pid=134827)[0m     momentum: 0.9
[2m[36m(func pid=134827)[0m     nesterov: False
[2m[36m(func pid=134827)[0m     weight_decay: 0
[2m[36m(func pid=134827)[0m )
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.2170 | Steps: 2 | Val loss: 2.1425 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.8843 | Steps: 2 | Val loss: 2.2914 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5535 | Steps: 2 | Val loss: 1.8905 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1140 | Steps: 2 | Val loss: 2.2655 | Batch size: 32 | lr: 0.1 | Duration: 4.44s
== Status ==
Current time: 2024-01-07 01:43:18 (running for 00:09:39.51)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.894 |      0.107 |                   23 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.304 |      0.204 |                   21 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.591 |      0.332 |                   19 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |        |            |                      |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.2234141791044776
[2m[36m(func pid=129725)[0m top5: 0.7509328358208955
[2m[36m(func pid=129725)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=129725)[0m f1_macro: 0.20592689142172854
[2m[36m(func pid=129725)[0m f1_weighted: 0.23778793012676835
[2m[36m(func pid=129725)[0m f1_per_class: [0.193, 0.19, 0.183, 0.265, 0.043, 0.332, 0.225, 0.268, 0.049, 0.312]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.15578358208955223
[2m[36m(func pid=128882)[0m top5: 0.5722947761194029
[2m[36m(func pid=128882)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=128882)[0m f1_macro: 0.10255638833886009
[2m[36m(func pid=128882)[0m f1_weighted: 0.15494621467328173
[2m[36m(func pid=128882)[0m f1_per_class: [0.045, 0.137, 0.091, 0.203, 0.018, 0.309, 0.105, 0.104, 0.012, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m top1: 0.34701492537313433
[2m[36m(func pid=129730)[0m top5: 0.878731343283582
[2m[36m(func pid=129730)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=129730)[0m f1_macro: 0.32730810773970276
[2m[36m(func pid=129730)[0m f1_weighted: 0.3766607736552661
[2m[36m(func pid=129730)[0m f1_per_class: [0.278, 0.263, 0.71, 0.398, 0.073, 0.385, 0.47, 0.295, 0.185, 0.217]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m top1: 0.1310634328358209
[2m[36m(func pid=134827)[0m top5: 0.6021455223880597
[2m[36m(func pid=134827)[0m f1_micro: 0.1310634328358209
[2m[36m(func pid=134827)[0m f1_macro: 0.12000320226869121
[2m[36m(func pid=134827)[0m f1_weighted: 0.11406150892098203
[2m[36m(func pid=134827)[0m f1_per_class: [0.11, 0.216, 0.231, 0.157, 0.0, 0.092, 0.024, 0.145, 0.036, 0.188]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.8508 | Steps: 2 | Val loss: 2.2908 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.1678 | Steps: 2 | Val loss: 2.1317 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5354 | Steps: 2 | Val loss: 1.8915 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.1469 | Steps: 2 | Val loss: 2.1555 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 01:43:24 (running for 00:09:44.85)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.851 |      0.103 |                   25 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.217 |      0.206 |                   22 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.553 |      0.327 |                   20 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  3.114 |      0.12  |                    1 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.1553171641791045
[2m[36m(func pid=128882)[0m top5: 0.5778917910447762
[2m[36m(func pid=128882)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=128882)[0m f1_macro: 0.10338793294943735
[2m[36m(func pid=128882)[0m f1_weighted: 0.15389951837116925
[2m[36m(func pid=128882)[0m f1_per_class: [0.046, 0.137, 0.09, 0.196, 0.019, 0.313, 0.105, 0.105, 0.023, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m top1: 0.2294776119402985
[2m[36m(func pid=129725)[0m top5: 0.7583955223880597
[2m[36m(func pid=129725)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=129725)[0m f1_macro: 0.21377514249944646
[2m[36m(func pid=129725)[0m f1_weighted: 0.2438107327007322
[2m[36m(func pid=129725)[0m f1_per_class: [0.19, 0.202, 0.202, 0.266, 0.049, 0.335, 0.234, 0.271, 0.05, 0.339]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.34794776119402987
[2m[36m(func pid=129730)[0m top5: 0.8824626865671642
[2m[36m(func pid=129730)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=129730)[0m f1_macro: 0.3265998167013525
[2m[36m(func pid=129730)[0m f1_weighted: 0.3763492570239877
[2m[36m(func pid=129730)[0m f1_per_class: [0.294, 0.269, 0.71, 0.403, 0.068, 0.385, 0.467, 0.257, 0.194, 0.22]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m top1: 0.17257462686567165
[2m[36m(func pid=134827)[0m top5: 0.7653917910447762
[2m[36m(func pid=134827)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=134827)[0m f1_macro: 0.16997957970480743
[2m[36m(func pid=134827)[0m f1_weighted: 0.17460266475093295
[2m[36m(func pid=134827)[0m f1_per_class: [0.121, 0.016, 0.462, 0.199, 0.0, 0.387, 0.204, 0.032, 0.045, 0.235]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.8571 | Steps: 2 | Val loss: 2.2882 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.0911 | Steps: 2 | Val loss: 2.1163 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4741 | Steps: 2 | Val loss: 1.9028 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.4886 | Steps: 2 | Val loss: 2.0613 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:43:29 (running for 00:09:50.06)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.857 |      0.106 |                   26 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.168 |      0.214 |                   23 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.535 |      0.327 |                   21 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  2.147 |      0.17  |                    2 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.15811567164179105
[2m[36m(func pid=128882)[0m top5: 0.5830223880597015
[2m[36m(func pid=128882)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=128882)[0m f1_macro: 0.10647930151506138
[2m[36m(func pid=128882)[0m f1_weighted: 0.15699256003799753
[2m[36m(func pid=128882)[0m f1_per_class: [0.057, 0.139, 0.086, 0.2, 0.025, 0.319, 0.107, 0.109, 0.023, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m top1: 0.23833955223880596
[2m[36m(func pid=129725)[0m top5: 0.7681902985074627
[2m[36m(func pid=129725)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=129725)[0m f1_macro: 0.22057925210778526
[2m[36m(func pid=129725)[0m f1_weighted: 0.25437137964047823
[2m[36m(func pid=129725)[0m f1_per_class: [0.201, 0.194, 0.232, 0.277, 0.049, 0.342, 0.26, 0.273, 0.051, 0.328]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.34888059701492535
[2m[36m(func pid=129730)[0m top5: 0.8819962686567164
[2m[36m(func pid=129730)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=129730)[0m f1_macro: 0.32882710770162005
[2m[36m(func pid=129730)[0m f1_weighted: 0.3768427351698934
[2m[36m(func pid=129730)[0m f1_per_class: [0.31, 0.278, 0.71, 0.404, 0.068, 0.385, 0.462, 0.25, 0.2, 0.222]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m top1: 0.24440298507462688
[2m[36m(func pid=134827)[0m top5: 0.8577425373134329
[2m[36m(func pid=134827)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=134827)[0m f1_macro: 0.25285899998039874
[2m[36m(func pid=134827)[0m f1_weighted: 0.28012633591868846
[2m[36m(func pid=134827)[0m f1_per_class: [0.093, 0.174, 0.462, 0.222, 0.108, 0.388, 0.389, 0.304, 0.024, 0.366]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.8710 | Steps: 2 | Val loss: 2.2882 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.2644 | Steps: 2 | Val loss: 2.1072 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5437 | Steps: 2 | Val loss: 1.9148 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.9561 | Steps: 2 | Val loss: 2.2388 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:43:34 (running for 00:09:55.07)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.871 |      0.11  |                   27 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.091 |      0.221 |                   24 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.474 |      0.329 |                   22 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  1.489 |      0.253 |                    3 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.16277985074626866
[2m[36m(func pid=128882)[0m top5: 0.5811567164179104
[2m[36m(func pid=128882)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=128882)[0m f1_macro: 0.11049153747491511
[2m[36m(func pid=128882)[0m f1_weighted: 0.1628771293707336
[2m[36m(func pid=128882)[0m f1_per_class: [0.056, 0.147, 0.088, 0.21, 0.025, 0.326, 0.107, 0.123, 0.023, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m top1: 0.24067164179104478
[2m[36m(func pid=129725)[0m top5: 0.7747201492537313
[2m[36m(func pid=129725)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=129725)[0m f1_macro: 0.22396606934397173
[2m[36m(func pid=129725)[0m f1_weighted: 0.25608239026355684
[2m[36m(func pid=129725)[0m f1_per_class: [0.207, 0.213, 0.244, 0.27, 0.049, 0.335, 0.264, 0.266, 0.051, 0.339]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3493470149253731
[2m[36m(func pid=129730)[0m top5: 0.8796641791044776
[2m[36m(func pid=129730)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=129730)[0m f1_macro: 0.33209088870671744
[2m[36m(func pid=129730)[0m f1_weighted: 0.37691661037895763
[2m[36m(func pid=129730)[0m f1_per_class: [0.304, 0.289, 0.71, 0.398, 0.071, 0.38, 0.455, 0.284, 0.213, 0.217]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m top1: 0.23460820895522388
[2m[36m(func pid=134827)[0m top5: 0.8036380597014925
[2m[36m(func pid=134827)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=134827)[0m f1_macro: 0.25189315173784504
[2m[36m(func pid=134827)[0m f1_weighted: 0.2341275429032767
[2m[36m(func pid=134827)[0m f1_per_class: [0.13, 0.151, 0.471, 0.305, 0.095, 0.391, 0.145, 0.382, 0.074, 0.376]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.1279 | Steps: 2 | Val loss: 2.0935 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.8283 | Steps: 2 | Val loss: 2.2857 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4038 | Steps: 2 | Val loss: 1.9266 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.6454 | Steps: 2 | Val loss: 2.1702 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 01:43:39 (running for 00:10:00.09)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.871 |      0.11  |                   27 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.128 |      0.232 |                   26 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.544 |      0.332 |                   23 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.956 |      0.252 |                    4 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.16044776119402984
[2m[36m(func pid=128882)[0m top5: 0.5839552238805971
[2m[36m(func pid=128882)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=128882)[0m f1_macro: 0.10953316392882613
[2m[36m(func pid=128882)[0m f1_weighted: 0.16255642340122198
[2m[36m(func pid=128882)[0m f1_per_class: [0.055, 0.137, 0.087, 0.217, 0.024, 0.324, 0.106, 0.123, 0.022, 0.0]
[2m[36m(func pid=129725)[0m top1: 0.2513992537313433
[2m[36m(func pid=129725)[0m top5: 0.7831156716417911
[2m[36m(func pid=129725)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=129725)[0m f1_macro: 0.23180065548639384
[2m[36m(func pid=129725)[0m f1_weighted: 0.26726587674274843
[2m[36m(func pid=129725)[0m f1_per_class: [0.23, 0.228, 0.259, 0.284, 0.048, 0.33, 0.278, 0.273, 0.055, 0.333]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m top1: 0.35074626865671643
[2m[36m(func pid=129730)[0m top5: 0.878731343283582
[2m[36m(func pid=129730)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=129730)[0m f1_macro: 0.33734271464322907
[2m[36m(func pid=129730)[0m f1_weighted: 0.37814794596862683
[2m[36m(func pid=129730)[0m f1_per_class: [0.315, 0.291, 0.71, 0.392, 0.075, 0.394, 0.452, 0.306, 0.228, 0.211]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m top1: 0.29244402985074625
[2m[36m(func pid=134827)[0m top5: 0.8348880597014925
[2m[36m(func pid=134827)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=134827)[0m f1_macro: 0.29067345968176017
[2m[36m(func pid=134827)[0m f1_weighted: 0.31688096699084634
[2m[36m(func pid=134827)[0m f1_per_class: [0.206, 0.141, 0.48, 0.352, 0.118, 0.404, 0.376, 0.363, 0.081, 0.386]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.0629 | Steps: 2 | Val loss: 2.0833 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.9026 | Steps: 2 | Val loss: 2.2835 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3376 | Steps: 2 | Val loss: 1.9254 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4437 | Steps: 2 | Val loss: 2.2408 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 01:43:44 (running for 00:10:05.21)
Memory usage on this node: 24.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.828 |      0.11  |                   28 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.063 |      0.242 |                   27 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.404 |      0.337 |                   24 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.645 |      0.291 |                    5 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.166044776119403
[2m[36m(func pid=128882)[0m top5: 0.5904850746268657
[2m[36m(func pid=128882)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=128882)[0m f1_macro: 0.11582974276610591
[2m[36m(func pid=128882)[0m f1_weighted: 0.16812775773274605
[2m[36m(func pid=128882)[0m f1_per_class: [0.054, 0.151, 0.114, 0.22, 0.024, 0.327, 0.112, 0.124, 0.033, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m top1: 0.25699626865671643
[2m[36m(func pid=129725)[0m top5: 0.7854477611940298
[2m[36m(func pid=129725)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=129725)[0m f1_macro: 0.24150781362808776
[2m[36m(func pid=129725)[0m f1_weighted: 0.27478392435490895
[2m[36m(func pid=129725)[0m f1_per_class: [0.235, 0.223, 0.282, 0.294, 0.048, 0.32, 0.293, 0.271, 0.118, 0.331]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.35447761194029853
[2m[36m(func pid=129730)[0m top5: 0.8791977611940298
[2m[36m(func pid=129730)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=129730)[0m f1_macro: 0.3340869493490383
[2m[36m(func pid=129730)[0m f1_weighted: 0.3808338788991767
[2m[36m(func pid=129730)[0m f1_per_class: [0.325, 0.297, 0.647, 0.405, 0.08, 0.395, 0.445, 0.309, 0.229, 0.209]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m top1: 0.3376865671641791
[2m[36m(func pid=134827)[0m top5: 0.8577425373134329
[2m[36m(func pid=134827)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=134827)[0m f1_macro: 0.30309121408147743
[2m[36m(func pid=134827)[0m f1_weighted: 0.3592079038657651
[2m[36m(func pid=134827)[0m f1_per_class: [0.21, 0.183, 0.649, 0.4, 0.124, 0.391, 0.494, 0.137, 0.097, 0.347]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.0413 | Steps: 2 | Val loss: 2.0764 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.8915 | Steps: 2 | Val loss: 2.2838 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3545 | Steps: 2 | Val loss: 1.9310 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=128882)[0m top1: 0.1646455223880597
[2m[36m(func pid=128882)[0m top5: 0.5886194029850746
[2m[36m(func pid=128882)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=128882)[0m f1_macro: 0.11474841151820428
[2m[36m(func pid=128882)[0m f1_weighted: 0.16720692765767717
[2m[36m(func pid=128882)[0m f1_per_class: [0.056, 0.144, 0.116, 0.225, 0.03, 0.322, 0.111, 0.123, 0.022, 0.0]
[2m[36m(func pid=129725)[0m top1: 0.2579291044776119
[2m[36m(func pid=129725)[0m top5: 0.7882462686567164
[2m[36m(func pid=129725)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=129725)[0m f1_macro: 0.24419600892833754
[2m[36m(func pid=129725)[0m f1_weighted: 0.27641200648166486
[2m[36m(func pid=129725)[0m f1_per_class: [0.236, 0.224, 0.289, 0.304, 0.045, 0.322, 0.288, 0.268, 0.116, 0.349]
[2m[36m(func pid=129725)[0m 
== Status ==
Current time: 2024-01-07 01:43:49 (running for 00:10:10.38)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.903 |      0.116 |                   29 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  2.041 |      0.244 |                   28 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.338 |      0.334 |                   25 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.444 |      0.303 |                    6 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.2972 | Steps: 2 | Val loss: 2.4854 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=129730)[0m top1: 0.35074626865671643
[2m[36m(func pid=129730)[0m top5: 0.8754664179104478
[2m[36m(func pid=129730)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=129730)[0m f1_macro: 0.32402076254874007
[2m[36m(func pid=129730)[0m f1_weighted: 0.376868753548891
[2m[36m(func pid=129730)[0m f1_per_class: [0.337, 0.292, 0.579, 0.407, 0.08, 0.395, 0.437, 0.3, 0.212, 0.201]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.9699 | Steps: 2 | Val loss: 2.0669 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=134827)[0m top1: 0.3208955223880597
[2m[36m(func pid=134827)[0m top5: 0.8488805970149254
[2m[36m(func pid=134827)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=134827)[0m f1_macro: 0.29502101815098253
[2m[36m(func pid=134827)[0m f1_weighted: 0.3389025971436114
[2m[36m(func pid=134827)[0m f1_per_class: [0.22, 0.209, 0.632, 0.411, 0.116, 0.38, 0.393, 0.194, 0.127, 0.269]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.8658 | Steps: 2 | Val loss: 2.2846 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=129725)[0m top1: 0.2630597014925373
[2m[36m(func pid=129725)[0m top5: 0.7947761194029851
[2m[36m(func pid=129725)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=129725)[0m f1_macro: 0.24947078632008476
[2m[36m(func pid=129725)[0m f1_weighted: 0.2809224805656111
[2m[36m(func pid=129725)[0m f1_per_class: [0.239, 0.225, 0.293, 0.315, 0.046, 0.336, 0.284, 0.275, 0.118, 0.362]
[2m[36m(func pid=129725)[0m 
== Status ==
Current time: 2024-01-07 01:43:54 (running for 00:10:15.68)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.866 |      0.115 |                   31 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.97  |      0.249 |                   29 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.355 |      0.324 |                   26 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.297 |      0.295 |                    7 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.16324626865671643
[2m[36m(func pid=128882)[0m top5: 0.5890858208955224
[2m[36m(func pid=128882)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=128882)[0m f1_macro: 0.1148807128046484
[2m[36m(func pid=128882)[0m f1_weighted: 0.16587648530404722
[2m[36m(func pid=128882)[0m f1_per_class: [0.056, 0.146, 0.113, 0.219, 0.03, 0.321, 0.11, 0.123, 0.032, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.1822 | Steps: 2 | Val loss: 2.8734 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4316 | Steps: 2 | Val loss: 1.9471 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.9481 | Steps: 2 | Val loss: 2.0564 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=134827)[0m top1: 0.2943097014925373
[2m[36m(func pid=134827)[0m top5: 0.8386194029850746
[2m[36m(func pid=134827)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=134827)[0m f1_macro: 0.28510527225367527
[2m[36m(func pid=134827)[0m f1_weighted: 0.3116474460687982
[2m[36m(func pid=134827)[0m f1_per_class: [0.187, 0.209, 0.649, 0.396, 0.088, 0.363, 0.307, 0.286, 0.148, 0.218]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.8477 | Steps: 2 | Val loss: 2.2816 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=129730)[0m top1: 0.34794776119402987
[2m[36m(func pid=129730)[0m top5: 0.875
[2m[36m(func pid=129730)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=129730)[0m f1_macro: 0.3226752199439783
[2m[36m(func pid=129730)[0m f1_weighted: 0.3725747196057723
[2m[36m(func pid=129730)[0m f1_per_class: [0.35, 0.297, 0.55, 0.402, 0.081, 0.404, 0.419, 0.304, 0.22, 0.2]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.27098880597014924
[2m[36m(func pid=129725)[0m top5: 0.7975746268656716
[2m[36m(func pid=129725)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=129725)[0m f1_macro: 0.255331583594927
[2m[36m(func pid=129725)[0m f1_weighted: 0.28972461198535765
[2m[36m(func pid=129725)[0m f1_per_class: [0.245, 0.23, 0.31, 0.335, 0.045, 0.356, 0.283, 0.285, 0.122, 0.343]
[2m[36m(func pid=129725)[0m 
== Status ==
Current time: 2024-01-07 01:44:00 (running for 00:10:20.82)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.848 |      0.117 |                   32 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.948 |      0.255 |                   30 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.432 |      0.323 |                   27 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.182 |      0.285 |                    8 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.16791044776119404
[2m[36m(func pid=128882)[0m top5: 0.5914179104477612
[2m[36m(func pid=128882)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=128882)[0m f1_macro: 0.11675593176082932
[2m[36m(func pid=128882)[0m f1_weighted: 0.17232709823706166
[2m[36m(func pid=128882)[0m f1_per_class: [0.056, 0.152, 0.113, 0.226, 0.03, 0.323, 0.122, 0.124, 0.022, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3776 | Steps: 2 | Val loss: 1.9603 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.0879 | Steps: 2 | Val loss: 3.2457 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.8924 | Steps: 2 | Val loss: 2.0543 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.8254 | Steps: 2 | Val loss: 2.2775 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=129730)[0m top1: 0.3474813432835821
[2m[36m(func pid=129730)[0m top5: 0.8708022388059702
[2m[36m(func pid=129730)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=129730)[0m f1_macro: 0.3218887222864657
[2m[36m(func pid=129730)[0m f1_weighted: 0.37310268717933187
[2m[36m(func pid=129730)[0m f1_per_class: [0.335, 0.287, 0.564, 0.404, 0.083, 0.404, 0.426, 0.309, 0.217, 0.191]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m top1: 0.27705223880597013
[2m[36m(func pid=134827)[0m top5: 0.8269589552238806
[2m[36m(func pid=134827)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=134827)[0m f1_macro: 0.27790180292539934
[2m[36m(func pid=134827)[0m f1_weighted: 0.28417315574779417
[2m[36m(func pid=134827)[0m f1_per_class: [0.2, 0.231, 0.686, 0.395, 0.067, 0.336, 0.21, 0.291, 0.17, 0.194]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m top1: 0.27238805970149255
[2m[36m(func pid=129725)[0m top5: 0.7957089552238806
[2m[36m(func pid=129725)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=129725)[0m f1_macro: 0.2571300328418487
[2m[36m(func pid=129725)[0m f1_weighted: 0.2912634019075394
[2m[36m(func pid=129725)[0m f1_per_class: [0.253, 0.223, 0.328, 0.345, 0.044, 0.359, 0.279, 0.299, 0.122, 0.319]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.16884328358208955
[2m[36m(func pid=128882)[0m top5: 0.6012126865671642
[2m[36m(func pid=128882)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=128882)[0m f1_macro: 0.11608783824976643
[2m[36m(func pid=128882)[0m f1_weighted: 0.17482847102665808
[2m[36m(func pid=128882)[0m f1_per_class: [0.048, 0.146, 0.108, 0.24, 0.029, 0.327, 0.12, 0.121, 0.022, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4857 | Steps: 2 | Val loss: 1.9731 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.1439 | Steps: 2 | Val loss: 3.4886 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.7987 | Steps: 2 | Val loss: 2.2804 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.9375 | Steps: 2 | Val loss: 2.0547 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 01:44:08 (running for 00:10:29.09)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.825 |      0.116 |                   33 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.892 |      0.257 |                   31 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.486 |      0.326 |                   29 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.088 |      0.278 |                    9 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129730)[0m top1: 0.341884328358209
[2m[36m(func pid=129730)[0m top5: 0.8675373134328358
[2m[36m(func pid=129730)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=129730)[0m f1_macro: 0.32597595386338224
[2m[36m(func pid=129730)[0m f1_weighted: 0.3673384046395278
[2m[36m(func pid=129730)[0m f1_per_class: [0.326, 0.286, 0.647, 0.402, 0.079, 0.397, 0.413, 0.291, 0.213, 0.205]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m top1: 0.2681902985074627
[2m[36m(func pid=134827)[0m top5: 0.8222947761194029
[2m[36m(func pid=134827)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=134827)[0m f1_macro: 0.27883330782209453
[2m[36m(func pid=134827)[0m f1_weighted: 0.26982672064153707
[2m[36m(func pid=134827)[0m f1_per_class: [0.227, 0.256, 0.727, 0.381, 0.064, 0.325, 0.162, 0.282, 0.183, 0.181]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m top1: 0.166044776119403
[2m[36m(func pid=128882)[0m top5: 0.5965485074626866
[2m[36m(func pid=128882)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=128882)[0m f1_macro: 0.11408176196662392
[2m[36m(func pid=128882)[0m f1_weighted: 0.17269805954000503
[2m[36m(func pid=128882)[0m f1_per_class: [0.057, 0.144, 0.1, 0.239, 0.03, 0.323, 0.117, 0.12, 0.01, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m top1: 0.2691231343283582
[2m[36m(func pid=129725)[0m top5: 0.7947761194029851
[2m[36m(func pid=129725)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=129725)[0m f1_macro: 0.25460947121419303
[2m[36m(func pid=129725)[0m f1_weighted: 0.28806231537733296
[2m[36m(func pid=129725)[0m f1_per_class: [0.247, 0.227, 0.314, 0.343, 0.042, 0.362, 0.265, 0.31, 0.123, 0.313]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.0954 | Steps: 2 | Val loss: 3.5604 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3198 | Steps: 2 | Val loss: 1.9923 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.8806 | Steps: 2 | Val loss: 2.0461 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.7947 | Steps: 2 | Val loss: 2.2801 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 01:44:13 (running for 00:10:34.44)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.799 |      0.114 |                   34 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.938 |      0.255 |                   32 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.486 |      0.326 |                   29 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.095 |      0.277 |                   11 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=134827)[0m top1: 0.26399253731343286
[2m[36m(func pid=134827)[0m top5: 0.8255597014925373
[2m[36m(func pid=134827)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=134827)[0m f1_macro: 0.2769536000138624
[2m[36m(func pid=134827)[0m f1_weighted: 0.2728478154617004
[2m[36m(func pid=134827)[0m f1_per_class: [0.241, 0.256, 0.75, 0.365, 0.058, 0.306, 0.2, 0.254, 0.176, 0.164]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33861940298507465
[2m[36m(func pid=129730)[0m top5: 0.863339552238806
[2m[36m(func pid=129730)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=129730)[0m f1_macro: 0.3269551338613496
[2m[36m(func pid=129730)[0m f1_weighted: 0.3656580315916002
[2m[36m(func pid=129730)[0m f1_per_class: [0.313, 0.275, 0.71, 0.409, 0.073, 0.387, 0.414, 0.281, 0.202, 0.206]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.27052238805970147
[2m[36m(func pid=129725)[0m top5: 0.7966417910447762
[2m[36m(func pid=129725)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=129725)[0m f1_macro: 0.25644836666741
[2m[36m(func pid=129725)[0m f1_weighted: 0.28956633465769216
[2m[36m(func pid=129725)[0m f1_per_class: [0.254, 0.221, 0.338, 0.344, 0.043, 0.352, 0.276, 0.309, 0.124, 0.303]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.16791044776119404
[2m[36m(func pid=128882)[0m top5: 0.5984141791044776
[2m[36m(func pid=128882)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=128882)[0m f1_macro: 0.11668935169249448
[2m[36m(func pid=128882)[0m f1_weighted: 0.1728590529614463
[2m[36m(func pid=128882)[0m f1_per_class: [0.075, 0.14, 0.096, 0.238, 0.031, 0.33, 0.117, 0.119, 0.021, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.0677 | Steps: 2 | Val loss: 3.5121 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2876 | Steps: 2 | Val loss: 2.0144 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.8104 | Steps: 2 | Val loss: 2.0408 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.8050 | Steps: 2 | Val loss: 2.2784 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 01:44:19 (running for 00:10:39.79)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.795 |      0.117 |                   35 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.881 |      0.256 |                   33 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.32  |      0.327 |                   30 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.068 |      0.284 |                   12 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=134827)[0m top1: 0.2849813432835821
[2m[36m(func pid=134827)[0m top5: 0.8460820895522388
[2m[36m(func pid=134827)[0m f1_micro: 0.2849813432835821
[2m[36m(func pid=134827)[0m f1_macro: 0.2838530242114491
[2m[36m(func pid=134827)[0m f1_weighted: 0.3076973318777188
[2m[36m(func pid=134827)[0m f1_per_class: [0.212, 0.277, 0.727, 0.361, 0.06, 0.35, 0.303, 0.212, 0.172, 0.165]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3353544776119403
[2m[36m(func pid=129730)[0m top5: 0.8610074626865671
[2m[36m(func pid=129730)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=129730)[0m f1_macro: 0.32006113556627314
[2m[36m(func pid=129730)[0m f1_weighted: 0.3645742792370478
[2m[36m(func pid=129730)[0m f1_per_class: [0.316, 0.258, 0.667, 0.413, 0.073, 0.375, 0.424, 0.272, 0.2, 0.202]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.2719216417910448
[2m[36m(func pid=129725)[0m top5: 0.7947761194029851
[2m[36m(func pid=129725)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=129725)[0m f1_macro: 0.2588488262436564
[2m[36m(func pid=129725)[0m f1_weighted: 0.2910330079808323
[2m[36m(func pid=129725)[0m f1_per_class: [0.248, 0.22, 0.367, 0.357, 0.043, 0.35, 0.269, 0.311, 0.123, 0.301]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.16791044776119404
[2m[36m(func pid=128882)[0m top5: 0.6040111940298507
[2m[36m(func pid=128882)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=128882)[0m f1_macro: 0.11569215835025182
[2m[36m(func pid=128882)[0m f1_weighted: 0.17284496352556303
[2m[36m(func pid=128882)[0m f1_per_class: [0.074, 0.136, 0.088, 0.243, 0.031, 0.332, 0.114, 0.117, 0.022, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0603 | Steps: 2 | Val loss: 3.5631 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3663 | Steps: 2 | Val loss: 2.0294 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.8602 | Steps: 2 | Val loss: 2.0417 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.8322 | Steps: 2 | Val loss: 2.2780 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 01:44:24 (running for 00:10:45.23)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.805 |      0.116 |                   36 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.81  |      0.259 |                   34 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.288 |      0.32  |                   31 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.06  |      0.286 |                   13 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=134827)[0m top1: 0.29244402985074625
[2m[36m(func pid=134827)[0m top5: 0.8610074626865671
[2m[36m(func pid=134827)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=134827)[0m f1_macro: 0.28592979525562956
[2m[36m(func pid=134827)[0m f1_weighted: 0.3173917429972476
[2m[36m(func pid=134827)[0m f1_per_class: [0.166, 0.294, 0.774, 0.351, 0.062, 0.349, 0.343, 0.184, 0.174, 0.163]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3353544776119403
[2m[36m(func pid=129730)[0m top5: 0.8596082089552238
[2m[36m(func pid=129730)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=129730)[0m f1_macro: 0.32280144123764654
[2m[36m(func pid=129730)[0m f1_weighted: 0.36596613403966416
[2m[36m(func pid=129730)[0m f1_per_class: [0.312, 0.248, 0.71, 0.415, 0.07, 0.363, 0.437, 0.265, 0.2, 0.207]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=128882)[0m top1: 0.16837686567164178
[2m[36m(func pid=128882)[0m top5: 0.6058768656716418
[2m[36m(func pid=128882)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=128882)[0m f1_macro: 0.1158478525521169
[2m[36m(func pid=128882)[0m f1_weighted: 0.17423816338034362
[2m[36m(func pid=128882)[0m f1_per_class: [0.074, 0.136, 0.083, 0.242, 0.032, 0.334, 0.119, 0.118, 0.021, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m top1: 0.27052238805970147
[2m[36m(func pid=129725)[0m top5: 0.7966417910447762
[2m[36m(func pid=129725)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=129725)[0m f1_macro: 0.25436361807543906
[2m[36m(func pid=129725)[0m f1_weighted: 0.29001560435952567
[2m[36m(func pid=129725)[0m f1_per_class: [0.244, 0.224, 0.344, 0.355, 0.042, 0.349, 0.268, 0.316, 0.107, 0.295]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0666 | Steps: 2 | Val loss: 3.5709 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2606 | Steps: 2 | Val loss: 2.0331 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.8736 | Steps: 2 | Val loss: 2.0353 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.8307 | Steps: 2 | Val loss: 2.2745 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 01:44:30 (running for 00:10:50.73)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.832 |      0.116 |                   37 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.86  |      0.254 |                   35 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.366 |      0.323 |                   32 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.067 |      0.284 |                   14 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=134827)[0m top1: 0.29757462686567165
[2m[36m(func pid=134827)[0m top5: 0.8647388059701493
[2m[36m(func pid=134827)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=134827)[0m f1_macro: 0.28395163450715655
[2m[36m(func pid=134827)[0m f1_weighted: 0.32307856779933875
[2m[36m(func pid=134827)[0m f1_per_class: [0.16, 0.291, 0.733, 0.361, 0.063, 0.359, 0.35, 0.186, 0.182, 0.154]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3353544776119403
[2m[36m(func pid=129730)[0m top5: 0.863339552238806
[2m[36m(func pid=129730)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=129730)[0m f1_macro: 0.321076504858844
[2m[36m(func pid=129730)[0m f1_weighted: 0.3655639654139271
[2m[36m(func pid=129730)[0m f1_per_class: [0.311, 0.256, 0.71, 0.407, 0.071, 0.363, 0.443, 0.249, 0.197, 0.205]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.269589552238806
[2m[36m(func pid=129725)[0m top5: 0.7938432835820896
[2m[36m(func pid=129725)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=129725)[0m f1_macro: 0.2579668076494715
[2m[36m(func pid=129725)[0m f1_weighted: 0.2906768204365578
[2m[36m(func pid=129725)[0m f1_per_class: [0.232, 0.214, 0.379, 0.354, 0.046, 0.337, 0.28, 0.314, 0.125, 0.298]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.16930970149253732
[2m[36m(func pid=128882)[0m top5: 0.6105410447761194
[2m[36m(func pid=128882)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=128882)[0m f1_macro: 0.11725392028231692
[2m[36m(func pid=128882)[0m f1_weighted: 0.1766019368501616
[2m[36m(func pid=128882)[0m f1_per_class: [0.073, 0.135, 0.083, 0.243, 0.031, 0.332, 0.125, 0.13, 0.02, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0434 | Steps: 2 | Val loss: 3.6232 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3574 | Steps: 2 | Val loss: 2.0357 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.8721 | Steps: 2 | Val loss: 2.0297 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.8171 | Steps: 2 | Val loss: 2.2744 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 01:44:35 (running for 00:10:55.99)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.831 |      0.117 |                   38 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.874 |      0.258 |                   36 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.261 |      0.321 |                   33 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.043 |      0.283 |                   15 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=134827)[0m top1: 0.29384328358208955
[2m[36m(func pid=134827)[0m top5: 0.8628731343283582
[2m[36m(func pid=134827)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=134827)[0m f1_macro: 0.28325522988516955
[2m[36m(func pid=134827)[0m f1_weighted: 0.31834604171868863
[2m[36m(func pid=134827)[0m f1_per_class: [0.163, 0.288, 0.733, 0.363, 0.062, 0.362, 0.331, 0.201, 0.168, 0.161]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33861940298507465
[2m[36m(func pid=129730)[0m top5: 0.867070895522388
[2m[36m(func pid=129730)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=129730)[0m f1_macro: 0.3230898667026668
[2m[36m(func pid=129730)[0m f1_weighted: 0.3692139244725597
[2m[36m(func pid=129730)[0m f1_per_class: [0.315, 0.269, 0.71, 0.406, 0.081, 0.35, 0.452, 0.252, 0.203, 0.193]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.2733208955223881
[2m[36m(func pid=129725)[0m top5: 0.7952425373134329
[2m[36m(func pid=129725)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=129725)[0m f1_macro: 0.261588358965028
[2m[36m(func pid=129725)[0m f1_weighted: 0.2934288236091617
[2m[36m(func pid=129725)[0m f1_per_class: [0.242, 0.222, 0.379, 0.363, 0.046, 0.333, 0.275, 0.324, 0.124, 0.308]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.16884328358208955
[2m[36m(func pid=128882)[0m top5: 0.6124067164179104
[2m[36m(func pid=128882)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=128882)[0m f1_macro: 0.11840907823817748
[2m[36m(func pid=128882)[0m f1_weighted: 0.17690720892051967
[2m[36m(func pid=128882)[0m f1_per_class: [0.073, 0.138, 0.079, 0.236, 0.031, 0.33, 0.129, 0.138, 0.031, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.1167 | Steps: 2 | Val loss: 3.6577 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2331 | Steps: 2 | Val loss: 2.0563 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.8855 | Steps: 2 | Val loss: 2.0213 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.7737 | Steps: 2 | Val loss: 2.2749 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 01:44:40 (running for 00:11:01.33)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.817 |      0.118 |                   39 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.872 |      0.262 |                   37 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.357 |      0.323 |                   34 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.117 |      0.28  |                   16 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=134827)[0m top1: 0.28451492537313433
[2m[36m(func pid=134827)[0m top5: 0.8484141791044776
[2m[36m(func pid=134827)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=134827)[0m f1_macro: 0.2802020834753958
[2m[36m(func pid=134827)[0m f1_weighted: 0.3049520752939348
[2m[36m(func pid=134827)[0m f1_per_class: [0.178, 0.262, 0.71, 0.372, 0.071, 0.35, 0.292, 0.213, 0.182, 0.171]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m top1: 0.27472014925373134
[2m[36m(func pid=129725)[0m top5: 0.7966417910447762
[2m[36m(func pid=129725)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=129725)[0m f1_macro: 0.2600121413135607
[2m[36m(func pid=129725)[0m f1_weighted: 0.29464579071044833
[2m[36m(func pid=129725)[0m f1_per_class: [0.253, 0.219, 0.379, 0.367, 0.048, 0.322, 0.284, 0.315, 0.118, 0.296]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33955223880597013
[2m[36m(func pid=129730)[0m top5: 0.8656716417910447
[2m[36m(func pid=129730)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=129730)[0m f1_macro: 0.3246943641472389
[2m[36m(func pid=129730)[0m f1_weighted: 0.3708548480781809
[2m[36m(func pid=129730)[0m f1_per_class: [0.319, 0.278, 0.71, 0.402, 0.08, 0.345, 0.456, 0.259, 0.209, 0.189]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=128882)[0m top1: 0.16977611940298507
[2m[36m(func pid=128882)[0m top5: 0.6105410447761194
[2m[36m(func pid=128882)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=128882)[0m f1_macro: 0.11914772803671991
[2m[36m(func pid=128882)[0m f1_weighted: 0.17833996488024084
[2m[36m(func pid=128882)[0m f1_per_class: [0.072, 0.141, 0.077, 0.238, 0.031, 0.332, 0.129, 0.141, 0.03, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0159 | Steps: 2 | Val loss: 3.7780 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.6982 | Steps: 2 | Val loss: 2.0111 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3739 | Steps: 2 | Val loss: 2.0573 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.7945 | Steps: 2 | Val loss: 2.2739 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 01:44:45 (running for 00:11:06.66)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.774 |      0.119 |                   40 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.885 |      0.26  |                   38 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.233 |      0.325 |                   35 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.016 |      0.274 |                   17 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=134827)[0m top1: 0.28031716417910446
[2m[36m(func pid=134827)[0m top5: 0.8255597014925373
[2m[36m(func pid=134827)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=134827)[0m f1_macro: 0.27404784011924044
[2m[36m(func pid=134827)[0m f1_weighted: 0.29908089109855623
[2m[36m(func pid=134827)[0m f1_per_class: [0.203, 0.25, 0.629, 0.394, 0.061, 0.351, 0.255, 0.224, 0.203, 0.17]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m top1: 0.28078358208955223
[2m[36m(func pid=129725)[0m top5: 0.8017723880597015
[2m[36m(func pid=129725)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=129725)[0m f1_macro: 0.26733060328155517
[2m[36m(func pid=129725)[0m f1_weighted: 0.3007440165546127
[2m[36m(func pid=129725)[0m f1_per_class: [0.267, 0.215, 0.407, 0.378, 0.048, 0.333, 0.288, 0.321, 0.132, 0.284]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.17117537313432835
[2m[36m(func pid=128882)[0m top5: 0.6114738805970149
[2m[36m(func pid=128882)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=128882)[0m f1_macro: 0.11960978928635368
[2m[36m(func pid=128882)[0m f1_weighted: 0.17825487550783856
[2m[36m(func pid=128882)[0m f1_per_class: [0.07, 0.147, 0.077, 0.228, 0.032, 0.341, 0.132, 0.138, 0.031, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33908582089552236
[2m[36m(func pid=129730)[0m top5: 0.8703358208955224
[2m[36m(func pid=129730)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=129730)[0m f1_macro: 0.3264816024501881
[2m[36m(func pid=129730)[0m f1_weighted: 0.3700127397385445
[2m[36m(func pid=129730)[0m f1_per_class: [0.331, 0.284, 0.71, 0.392, 0.083, 0.345, 0.458, 0.261, 0.212, 0.19]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0264 | Steps: 2 | Val loss: 3.9277 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.6557 | Steps: 2 | Val loss: 2.0022 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.7841 | Steps: 2 | Val loss: 2.2712 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2530 | Steps: 2 | Val loss: 2.0653 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 01:44:51 (running for 00:11:12.05)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.795 |      0.12  |                   41 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.698 |      0.267 |                   39 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.374 |      0.326 |                   36 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.026 |      0.27  |                   18 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.2896455223880597
[2m[36m(func pid=129725)[0m top5: 0.8036380597014925
[2m[36m(func pid=129725)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=129725)[0m f1_macro: 0.2802442527933156
[2m[36m(func pid=129725)[0m f1_weighted: 0.30897447125657557
[2m[36m(func pid=129725)[0m f1_per_class: [0.299, 0.217, 0.468, 0.396, 0.049, 0.339, 0.289, 0.334, 0.129, 0.282]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m top1: 0.2737873134328358
[2m[36m(func pid=134827)[0m top5: 0.8036380597014925
[2m[36m(func pid=134827)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=134827)[0m f1_macro: 0.2701532622298949
[2m[36m(func pid=134827)[0m f1_weighted: 0.2871372180637095
[2m[36m(func pid=134827)[0m f1_per_class: [0.213, 0.229, 0.615, 0.412, 0.055, 0.329, 0.215, 0.236, 0.211, 0.186]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m top1: 0.16884328358208955
[2m[36m(func pid=128882)[0m top5: 0.6114738805970149
[2m[36m(func pid=128882)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=128882)[0m f1_macro: 0.11697205352627689
[2m[36m(func pid=128882)[0m f1_weighted: 0.1786101287884155
[2m[36m(func pid=128882)[0m f1_per_class: [0.072, 0.14, 0.071, 0.239, 0.031, 0.334, 0.133, 0.12, 0.03, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m top1: 0.345615671641791
[2m[36m(func pid=129730)[0m top5: 0.8703358208955224
[2m[36m(func pid=129730)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=129730)[0m f1_macro: 0.331063292551138
[2m[36m(func pid=129730)[0m f1_weighted: 0.3762637769656849
[2m[36m(func pid=129730)[0m f1_per_class: [0.347, 0.294, 0.71, 0.395, 0.088, 0.347, 0.467, 0.277, 0.201, 0.186]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.6615 | Steps: 2 | Val loss: 1.9966 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0505 | Steps: 2 | Val loss: 3.9720 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.7801 | Steps: 2 | Val loss: 2.2733 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2255 | Steps: 2 | Val loss: 2.0924 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 01:44:56 (running for 00:11:17.14)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.784 |      0.117 |                   42 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.662 |      0.28  |                   41 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.253 |      0.331 |                   37 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.026 |      0.27  |                   18 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.28777985074626866
[2m[36m(func pid=129725)[0m top5: 0.8083022388059702
[2m[36m(func pid=129725)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=129725)[0m f1_macro: 0.28034288714352096
[2m[36m(func pid=129725)[0m f1_weighted: 0.3072066733971812
[2m[36m(func pid=129725)[0m f1_per_class: [0.296, 0.222, 0.458, 0.386, 0.048, 0.337, 0.289, 0.336, 0.14, 0.291]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m top1: 0.2798507462686567
[2m[36m(func pid=134827)[0m top5: 0.7980410447761194
[2m[36m(func pid=134827)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=134827)[0m f1_macro: 0.2692009901181337
[2m[36m(func pid=134827)[0m f1_weighted: 0.29270582528586125
[2m[36m(func pid=134827)[0m f1_per_class: [0.208, 0.213, 0.558, 0.42, 0.064, 0.32, 0.234, 0.25, 0.229, 0.195]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m top1: 0.1669776119402985
[2m[36m(func pid=128882)[0m top5: 0.6124067164179104
[2m[36m(func pid=128882)[0m f1_micro: 0.1669776119402985
[2m[36m(func pid=128882)[0m f1_macro: 0.11834533466646628
[2m[36m(func pid=128882)[0m f1_weighted: 0.17657503691377
[2m[36m(func pid=128882)[0m f1_per_class: [0.081, 0.145, 0.077, 0.226, 0.032, 0.329, 0.134, 0.13, 0.029, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3414179104477612
[2m[36m(func pid=129730)[0m top5: 0.8666044776119403
[2m[36m(func pid=129730)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=129730)[0m f1_macro: 0.3280443011098153
[2m[36m(func pid=129730)[0m f1_weighted: 0.37193548242587726
[2m[36m(func pid=129730)[0m f1_per_class: [0.354, 0.298, 0.71, 0.384, 0.081, 0.348, 0.461, 0.269, 0.192, 0.183]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.6255 | Steps: 2 | Val loss: 1.9866 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0639 | Steps: 2 | Val loss: 3.9738 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.8162 | Steps: 2 | Val loss: 2.2741 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3544 | Steps: 2 | Val loss: 2.1223 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 01:45:01 (running for 00:11:22.29)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.78  |      0.118 |                   43 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.625 |      0.283 |                   42 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.226 |      0.328 |                   38 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.051 |      0.269 |                   19 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.2896455223880597
[2m[36m(func pid=129725)[0m top5: 0.8134328358208955
[2m[36m(func pid=129725)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=129725)[0m f1_macro: 0.28327232758490883
[2m[36m(func pid=129725)[0m f1_weighted: 0.309038569647693
[2m[36m(func pid=129725)[0m f1_per_class: [0.317, 0.223, 0.458, 0.383, 0.048, 0.352, 0.292, 0.325, 0.138, 0.295]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.16930970149253732
[2m[36m(func pid=128882)[0m top5: 0.6128731343283582
[2m[36m(func pid=128882)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=128882)[0m f1_macro: 0.12186842784375011
[2m[36m(func pid=128882)[0m f1_weighted: 0.1801986140556618
[2m[36m(func pid=128882)[0m f1_per_class: [0.081, 0.143, 0.093, 0.234, 0.031, 0.334, 0.137, 0.137, 0.028, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m top1: 0.28824626865671643
[2m[36m(func pid=134827)[0m top5: 0.792910447761194
[2m[36m(func pid=134827)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=134827)[0m f1_macro: 0.26221210104562764
[2m[36m(func pid=134827)[0m f1_weighted: 0.3055263827250072
[2m[36m(func pid=134827)[0m f1_per_class: [0.204, 0.209, 0.48, 0.431, 0.07, 0.309, 0.28, 0.236, 0.212, 0.19]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3353544776119403
[2m[36m(func pid=129730)[0m top5: 0.8628731343283582
[2m[36m(func pid=129730)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=129730)[0m f1_macro: 0.323808028690835
[2m[36m(func pid=129730)[0m f1_weighted: 0.36540588083360565
[2m[36m(func pid=129730)[0m f1_per_class: [0.337, 0.298, 0.71, 0.378, 0.083, 0.351, 0.446, 0.267, 0.19, 0.179]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.6923 | Steps: 2 | Val loss: 1.9768 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.7758 | Steps: 2 | Val loss: 2.2710 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0318 | Steps: 2 | Val loss: 3.9694 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2381 | Steps: 2 | Val loss: 2.1382 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 01:45:06 (running for 00:11:27.33)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.816 |      0.122 |                   44 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.692 |      0.28  |                   43 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.354 |      0.324 |                   39 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.064 |      0.262 |                   20 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.292910447761194
[2m[36m(func pid=129725)[0m top5: 0.816231343283582
[2m[36m(func pid=129725)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=129725)[0m f1_macro: 0.2800892551892942
[2m[36m(func pid=129725)[0m f1_weighted: 0.3134549672364758
[2m[36m(func pid=129725)[0m f1_per_class: [0.323, 0.225, 0.44, 0.398, 0.049, 0.331, 0.303, 0.316, 0.134, 0.282]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.17210820895522388
[2m[36m(func pid=128882)[0m top5: 0.617070895522388
[2m[36m(func pid=128882)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=128882)[0m f1_macro: 0.1251352862544422
[2m[36m(func pid=128882)[0m f1_weighted: 0.18254049863017527
[2m[36m(func pid=128882)[0m f1_per_class: [0.101, 0.14, 0.091, 0.236, 0.032, 0.342, 0.139, 0.143, 0.028, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m top1: 0.292910447761194
[2m[36m(func pid=134827)[0m top5: 0.7938432835820896
[2m[36m(func pid=134827)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=134827)[0m f1_macro: 0.2574457865857237
[2m[36m(func pid=134827)[0m f1_weighted: 0.3138875536523566
[2m[36m(func pid=134827)[0m f1_per_class: [0.208, 0.2, 0.414, 0.437, 0.068, 0.308, 0.311, 0.221, 0.222, 0.184]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3316231343283582
[2m[36m(func pid=129730)[0m top5: 0.8563432835820896
[2m[36m(func pid=129730)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=129730)[0m f1_macro: 0.32151270646274044
[2m[36m(func pid=129730)[0m f1_weighted: 0.3619384767460911
[2m[36m(func pid=129730)[0m f1_per_class: [0.333, 0.296, 0.71, 0.374, 0.081, 0.348, 0.441, 0.266, 0.19, 0.176]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.5600 | Steps: 2 | Val loss: 1.9704 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.7761 | Steps: 2 | Val loss: 2.2667 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0142 | Steps: 2 | Val loss: 3.9099 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2373 | Steps: 2 | Val loss: 2.1448 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 01:45:11 (running for 00:11:32.45)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.776 |      0.125 |                   45 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.56  |      0.285 |                   44 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.238 |      0.322 |                   40 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.032 |      0.257 |                   21 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.2971082089552239
[2m[36m(func pid=129725)[0m top5: 0.8199626865671642
[2m[36m(func pid=129725)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=129725)[0m f1_macro: 0.2850669545984014
[2m[36m(func pid=129725)[0m f1_weighted: 0.3168107051617511
[2m[36m(func pid=129725)[0m f1_per_class: [0.356, 0.225, 0.449, 0.406, 0.051, 0.338, 0.302, 0.312, 0.129, 0.282]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.177705223880597
[2m[36m(func pid=128882)[0m top5: 0.6217350746268657
[2m[36m(func pid=128882)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=128882)[0m f1_macro: 0.13025956439883934
[2m[36m(func pid=128882)[0m f1_weighted: 0.19003417321125599
[2m[36m(func pid=128882)[0m f1_per_class: [0.102, 0.144, 0.108, 0.247, 0.032, 0.341, 0.15, 0.152, 0.027, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m top1: 0.3031716417910448
[2m[36m(func pid=134827)[0m top5: 0.7985074626865671
[2m[36m(func pid=134827)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=134827)[0m f1_macro: 0.2619912349793584
[2m[36m(func pid=134827)[0m f1_weighted: 0.32801545044587316
[2m[36m(func pid=134827)[0m f1_per_class: [0.216, 0.208, 0.393, 0.437, 0.065, 0.312, 0.35, 0.241, 0.207, 0.19]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33302238805970147
[2m[36m(func pid=129730)[0m top5: 0.8549440298507462
[2m[36m(func pid=129730)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=129730)[0m f1_macro: 0.32315641157415625
[2m[36m(func pid=129730)[0m f1_weighted: 0.3628917553566838
[2m[36m(func pid=129730)[0m f1_per_class: [0.335, 0.293, 0.71, 0.384, 0.083, 0.35, 0.435, 0.268, 0.197, 0.177]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.6090 | Steps: 2 | Val loss: 1.9656 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.7770 | Steps: 2 | Val loss: 2.2630 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0131 | Steps: 2 | Val loss: 3.8912 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2424 | Steps: 2 | Val loss: 2.1650 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 01:45:16 (running for 00:11:37.58)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.776 |      0.13  |                   46 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.609 |      0.287 |                   45 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.237 |      0.323 |                   41 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.014 |      0.262 |                   22 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.2957089552238806
[2m[36m(func pid=129725)[0m top5: 0.8218283582089553
[2m[36m(func pid=129725)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=129725)[0m f1_macro: 0.2869766766596207
[2m[36m(func pid=129725)[0m f1_weighted: 0.3164815662423509
[2m[36m(func pid=129725)[0m f1_per_class: [0.343, 0.224, 0.458, 0.395, 0.05, 0.347, 0.306, 0.324, 0.138, 0.284]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.17817164179104478
[2m[36m(func pid=128882)[0m top5: 0.6268656716417911
[2m[36m(func pid=128882)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=128882)[0m f1_macro: 0.12994165762578347
[2m[36m(func pid=128882)[0m f1_weighted: 0.1919961204971474
[2m[36m(func pid=128882)[0m f1_per_class: [0.11, 0.131, 0.103, 0.265, 0.037, 0.335, 0.15, 0.149, 0.018, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m top1: 0.30830223880597013
[2m[36m(func pid=134827)[0m top5: 0.7980410447761194
[2m[36m(func pid=134827)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=134827)[0m f1_macro: 0.26304200104990827
[2m[36m(func pid=134827)[0m f1_weighted: 0.33594839290407297
[2m[36m(func pid=134827)[0m f1_per_class: [0.22, 0.21, 0.387, 0.442, 0.062, 0.303, 0.375, 0.237, 0.205, 0.189]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.32975746268656714
[2m[36m(func pid=129730)[0m top5: 0.8521455223880597
[2m[36m(func pid=129730)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=129730)[0m f1_macro: 0.3231134541648315
[2m[36m(func pid=129730)[0m f1_weighted: 0.3592944613549733
[2m[36m(func pid=129730)[0m f1_per_class: [0.317, 0.285, 0.71, 0.377, 0.091, 0.359, 0.428, 0.277, 0.207, 0.18]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.6245 | Steps: 2 | Val loss: 1.9627 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.7621 | Steps: 2 | Val loss: 2.2606 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.1029 | Steps: 2 | Val loss: 3.8335 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.1972 | Steps: 2 | Val loss: 2.1752 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 01:45:22 (running for 00:11:42.75)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.777 |      0.13  |                   47 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.624 |      0.289 |                   46 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.242 |      0.323 |                   42 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.013 |      0.263 |                   23 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.2966417910447761
[2m[36m(func pid=129725)[0m top5: 0.824160447761194
[2m[36m(func pid=129725)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=129725)[0m f1_macro: 0.2887415611686385
[2m[36m(func pid=129725)[0m f1_weighted: 0.3175461202346593
[2m[36m(func pid=129725)[0m f1_per_class: [0.349, 0.225, 0.468, 0.397, 0.05, 0.349, 0.306, 0.319, 0.147, 0.277]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m top1: 0.18190298507462688
[2m[36m(func pid=128882)[0m top5: 0.6296641791044776
[2m[36m(func pid=128882)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=128882)[0m f1_macro: 0.1323067043504847
[2m[36m(func pid=128882)[0m f1_weighted: 0.19625506552454047
[2m[36m(func pid=128882)[0m f1_per_class: [0.101, 0.142, 0.115, 0.266, 0.037, 0.342, 0.156, 0.147, 0.018, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m top1: 0.314365671641791
[2m[36m(func pid=134827)[0m top5: 0.8050373134328358
[2m[36m(func pid=134827)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=134827)[0m f1_macro: 0.2669867676321887
[2m[36m(func pid=134827)[0m f1_weighted: 0.3452518018870166
[2m[36m(func pid=134827)[0m f1_per_class: [0.202, 0.223, 0.407, 0.432, 0.059, 0.313, 0.406, 0.236, 0.205, 0.188]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3278917910447761
[2m[36m(func pid=129730)[0m top5: 0.8502798507462687
[2m[36m(func pid=129730)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=129730)[0m f1_macro: 0.3221170673276811
[2m[36m(func pid=129730)[0m f1_weighted: 0.35839277917567597
[2m[36m(func pid=129730)[0m f1_per_class: [0.314, 0.273, 0.71, 0.383, 0.087, 0.368, 0.423, 0.278, 0.207, 0.177]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.5221 | Steps: 2 | Val loss: 1.9587 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.7722 | Steps: 2 | Val loss: 2.2581 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0280 | Steps: 2 | Val loss: 3.7762 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2177 | Steps: 2 | Val loss: 2.1619 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=129725)[0m top1: 0.30177238805970147
[2m[36m(func pid=129725)[0m top5: 0.8250932835820896
[2m[36m(func pid=129725)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=129725)[0m f1_macro: 0.290206470967162
[2m[36m(func pid=129725)[0m f1_weighted: 0.3225032683143132
[2m[36m(func pid=129725)[0m f1_per_class: [0.35, 0.225, 0.458, 0.408, 0.051, 0.353, 0.311, 0.313, 0.156, 0.275]
[2m[36m(func pid=129725)[0m 
== Status ==
Current time: 2024-01-07 01:45:27 (running for 00:11:47.83)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.762 |      0.132 |                   48 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.522 |      0.29  |                   47 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.197 |      0.322 |                   43 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.103 |      0.267 |                   24 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.1791044776119403
[2m[36m(func pid=128882)[0m top5: 0.632929104477612
[2m[36m(func pid=128882)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=128882)[0m f1_macro: 0.13228872029321587
[2m[36m(func pid=128882)[0m f1_weighted: 0.19308998900732277
[2m[36m(func pid=128882)[0m f1_per_class: [0.09, 0.143, 0.117, 0.258, 0.035, 0.331, 0.153, 0.158, 0.037, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m top1: 0.31763059701492535
[2m[36m(func pid=134827)[0m top5: 0.8283582089552238
[2m[36m(func pid=134827)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=134827)[0m f1_macro: 0.26552421706997675
[2m[36m(func pid=134827)[0m f1_weighted: 0.34906458638282273
[2m[36m(func pid=134827)[0m f1_per_class: [0.191, 0.251, 0.414, 0.405, 0.073, 0.327, 0.43, 0.218, 0.174, 0.172]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3278917910447761
[2m[36m(func pid=129730)[0m top5: 0.8479477611940298
[2m[36m(func pid=129730)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=129730)[0m f1_macro: 0.3197660690290262
[2m[36m(func pid=129730)[0m f1_weighted: 0.35974599249651334
[2m[36m(func pid=129730)[0m f1_per_class: [0.288, 0.249, 0.71, 0.385, 0.089, 0.367, 0.442, 0.272, 0.218, 0.178]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.5922 | Steps: 2 | Val loss: 1.9505 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.7596 | Steps: 2 | Val loss: 2.2579 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0099 | Steps: 2 | Val loss: 3.7545 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=129725)[0m top1: 0.30363805970149255
[2m[36m(func pid=129725)[0m top5: 0.8269589552238806
[2m[36m(func pid=129725)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=129725)[0m f1_macro: 0.2948200219903817
[2m[36m(func pid=129725)[0m f1_weighted: 0.3240988207982487
[2m[36m(func pid=129725)[0m f1_per_class: [0.358, 0.227, 0.468, 0.405, 0.052, 0.365, 0.312, 0.313, 0.169, 0.279]
== Status ==
Current time: 2024-01-07 01:45:32 (running for 00:11:52.92)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.772 |      0.132 |                   49 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.592 |      0.295 |                   48 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.218 |      0.32  |                   44 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.028 |      0.266 |                   25 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2202 | Steps: 2 | Val loss: 2.1494 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=128882)[0m top1: 0.17723880597014927
[2m[36m(func pid=128882)[0m top5: 0.6347947761194029
[2m[36m(func pid=128882)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=128882)[0m f1_macro: 0.1324981423712524
[2m[36m(func pid=128882)[0m f1_weighted: 0.19205204056545055
[2m[36m(func pid=128882)[0m f1_per_class: [0.091, 0.144, 0.121, 0.246, 0.034, 0.334, 0.159, 0.158, 0.037, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m top1: 0.324160447761194
[2m[36m(func pid=134827)[0m top5: 0.8442164179104478
[2m[36m(func pid=134827)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=134827)[0m f1_macro: 0.27755885378502465
[2m[36m(func pid=134827)[0m f1_weighted: 0.3543875078007425
[2m[36m(func pid=134827)[0m f1_per_class: [0.202, 0.274, 0.5, 0.377, 0.071, 0.327, 0.458, 0.217, 0.172, 0.176]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3292910447761194
[2m[36m(func pid=129730)[0m top5: 0.8498134328358209
[2m[36m(func pid=129730)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=129730)[0m f1_macro: 0.31939335471019203
[2m[36m(func pid=129730)[0m f1_weighted: 0.36004687188579254
[2m[36m(func pid=129730)[0m f1_per_class: [0.291, 0.243, 0.71, 0.386, 0.092, 0.374, 0.444, 0.269, 0.208, 0.177]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.4615 | Steps: 2 | Val loss: 1.9530 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.7346 | Steps: 2 | Val loss: 2.2600 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0211 | Steps: 2 | Val loss: 3.7376 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=129725)[0m top1: 0.2971082089552239
[2m[36m(func pid=129725)[0m top5: 0.8269589552238806
[2m[36m(func pid=129725)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=129725)[0m f1_macro: 0.28880263314281407
[2m[36m(func pid=129725)[0m f1_weighted: 0.3181263853944647
[2m[36m(func pid=129725)[0m f1_per_class: [0.35, 0.229, 0.458, 0.389, 0.052, 0.364, 0.31, 0.305, 0.154, 0.277]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2176 | Steps: 2 | Val loss: 2.1289 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 01:45:38 (running for 00:11:58.77)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.735 |      0.132 |                   51 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.461 |      0.289 |                   49 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.22  |      0.319 |                   45 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.01  |      0.278 |                   26 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.177705223880597
[2m[36m(func pid=128882)[0m top5: 0.6343283582089553
[2m[36m(func pid=128882)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=128882)[0m f1_macro: 0.13226348189577583
[2m[36m(func pid=128882)[0m f1_weighted: 0.1921939329624872
[2m[36m(func pid=128882)[0m f1_per_class: [0.088, 0.145, 0.119, 0.245, 0.036, 0.338, 0.159, 0.157, 0.036, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m top1: 0.3316231343283582
[2m[36m(func pid=134827)[0m top5: 0.8577425373134329
[2m[36m(func pid=134827)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=134827)[0m f1_macro: 0.2841287647224983
[2m[36m(func pid=134827)[0m f1_weighted: 0.3607605115068263
[2m[36m(func pid=134827)[0m f1_per_class: [0.215, 0.288, 0.533, 0.377, 0.08, 0.335, 0.468, 0.216, 0.164, 0.164]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.5041 | Steps: 2 | Val loss: 1.9518 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=129730)[0m top1: 0.333955223880597
[2m[36m(func pid=129730)[0m top5: 0.8563432835820896
[2m[36m(func pid=129730)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=129730)[0m f1_macro: 0.3226568948657031
[2m[36m(func pid=129730)[0m f1_weighted: 0.36340527428112046
[2m[36m(func pid=129730)[0m f1_per_class: [0.303, 0.241, 0.71, 0.4, 0.095, 0.377, 0.442, 0.262, 0.215, 0.182]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.7446 | Steps: 2 | Val loss: 2.2624 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=129725)[0m top1: 0.2966417910447761
[2m[36m(func pid=129725)[0m top5: 0.8255597014925373
[2m[36m(func pid=129725)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=129725)[0m f1_macro: 0.2871943309508416
[2m[36m(func pid=129725)[0m f1_weighted: 0.31623580460076905
[2m[36m(func pid=129725)[0m f1_per_class: [0.332, 0.238, 0.44, 0.392, 0.051, 0.37, 0.294, 0.31, 0.151, 0.295]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0595 | Steps: 2 | Val loss: 3.7629 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 01:45:43 (running for 00:12:03.98)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.745 |      0.133 |                   52 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.504 |      0.287 |                   50 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.218 |      0.323 |                   46 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.021 |      0.284 |                   27 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.17630597014925373
[2m[36m(func pid=128882)[0m top5: 0.6315298507462687
[2m[36m(func pid=128882)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=128882)[0m f1_macro: 0.13325060362636593
[2m[36m(func pid=128882)[0m f1_weighted: 0.18932492683436494
[2m[36m(func pid=128882)[0m f1_per_class: [0.106, 0.141, 0.115, 0.23, 0.038, 0.348, 0.159, 0.161, 0.035, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.1658 | Steps: 2 | Val loss: 2.1122 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=134827)[0m top1: 0.33302238805970147
[2m[36m(func pid=134827)[0m top5: 0.8582089552238806
[2m[36m(func pid=134827)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=134827)[0m f1_macro: 0.28601598082636176
[2m[36m(func pid=134827)[0m f1_weighted: 0.3614065540711621
[2m[36m(func pid=134827)[0m f1_per_class: [0.224, 0.299, 0.558, 0.376, 0.071, 0.333, 0.467, 0.211, 0.158, 0.163]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.4133 | Steps: 2 | Val loss: 1.9481 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=129730)[0m top1: 0.33675373134328357
[2m[36m(func pid=129730)[0m top5: 0.8642723880597015
[2m[36m(func pid=129730)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=129730)[0m f1_macro: 0.32150383052003384
[2m[36m(func pid=129730)[0m f1_weighted: 0.36672126827209145
[2m[36m(func pid=129730)[0m f1_per_class: [0.306, 0.242, 0.71, 0.409, 0.096, 0.377, 0.447, 0.257, 0.199, 0.173]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.7998 | Steps: 2 | Val loss: 2.2617 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=129725)[0m top1: 0.29757462686567165
[2m[36m(func pid=129725)[0m top5: 0.8260261194029851
[2m[36m(func pid=129725)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=129725)[0m f1_macro: 0.28806558977348645
[2m[36m(func pid=129725)[0m f1_weighted: 0.318492615636953
[2m[36m(func pid=129725)[0m f1_per_class: [0.323, 0.242, 0.449, 0.386, 0.051, 0.37, 0.304, 0.311, 0.157, 0.288]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0181 | Steps: 2 | Val loss: 3.7181 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 01:45:48 (running for 00:12:09.06)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.8   |      0.134 |                   53 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.413 |      0.288 |                   51 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.166 |      0.322 |                   47 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.06  |      0.286 |                   28 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.17817164179104478
[2m[36m(func pid=128882)[0m top5: 0.6305970149253731
[2m[36m(func pid=128882)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=128882)[0m f1_macro: 0.13392102874828948
[2m[36m(func pid=128882)[0m f1_weighted: 0.19054885945071567
[2m[36m(func pid=128882)[0m f1_per_class: [0.096, 0.157, 0.115, 0.224, 0.038, 0.355, 0.158, 0.161, 0.036, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3345 | Steps: 2 | Val loss: 2.0983 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.4344 | Steps: 2 | Val loss: 1.9423 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=134827)[0m top1: 0.33255597014925375
[2m[36m(func pid=134827)[0m top5: 0.8610074626865671
[2m[36m(func pid=134827)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=134827)[0m f1_macro: 0.2916830148572766
[2m[36m(func pid=134827)[0m f1_weighted: 0.3616661942733889
[2m[36m(func pid=134827)[0m f1_per_class: [0.249, 0.282, 0.571, 0.388, 0.085, 0.338, 0.46, 0.221, 0.164, 0.159]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3414179104477612
[2m[36m(func pid=129730)[0m top5: 0.8652052238805971
[2m[36m(func pid=129730)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=129730)[0m f1_macro: 0.32549911208377297
[2m[36m(func pid=129730)[0m f1_weighted: 0.3716588656209353
[2m[36m(func pid=129730)[0m f1_per_class: [0.301, 0.252, 0.733, 0.42, 0.1, 0.369, 0.45, 0.259, 0.2, 0.171]
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.7337 | Steps: 2 | Val loss: 2.2588 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.3045708955223881
[2m[36m(func pid=129725)[0m top5: 0.8269589552238806
[2m[36m(func pid=129725)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=129725)[0m f1_macro: 0.29272735808491523
[2m[36m(func pid=129725)[0m f1_weighted: 0.32612649636158025
[2m[36m(func pid=129725)[0m f1_per_class: [0.341, 0.249, 0.449, 0.396, 0.052, 0.371, 0.313, 0.315, 0.164, 0.277]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0248 | Steps: 2 | Val loss: 3.7058 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 01:45:53 (running for 00:12:14.31)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.734 |      0.135 |                   54 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.434 |      0.293 |                   52 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.334 |      0.325 |                   48 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.018 |      0.292 |                   29 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.18050373134328357
[2m[36m(func pid=128882)[0m top5: 0.6352611940298507
[2m[36m(func pid=128882)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=128882)[0m f1_macro: 0.13522115968581233
[2m[36m(func pid=128882)[0m f1_weighted: 0.19420199232712354
[2m[36m(func pid=128882)[0m f1_per_class: [0.108, 0.156, 0.111, 0.233, 0.038, 0.345, 0.166, 0.159, 0.037, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.1529 | Steps: 2 | Val loss: 2.0977 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.5271 | Steps: 2 | Val loss: 1.9455 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=134827)[0m top1: 0.3292910447761194
[2m[36m(func pid=134827)[0m top5: 0.8605410447761194
[2m[36m(func pid=134827)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=134827)[0m f1_macro: 0.2867997059653725
[2m[36m(func pid=134827)[0m f1_weighted: 0.3588061904018708
[2m[36m(func pid=134827)[0m f1_per_class: [0.27, 0.275, 0.524, 0.403, 0.075, 0.328, 0.442, 0.23, 0.155, 0.165]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.7341 | Steps: 2 | Val loss: 2.2569 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=129730)[0m top1: 0.33955223880597013
[2m[36m(func pid=129730)[0m top5: 0.8647388059701493
[2m[36m(func pid=129730)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=129730)[0m f1_macro: 0.3212153054166583
[2m[36m(func pid=129730)[0m f1_weighted: 0.3696293478479592
[2m[36m(func pid=129730)[0m f1_per_class: [0.29, 0.261, 0.71, 0.416, 0.09, 0.361, 0.445, 0.254, 0.209, 0.176]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.30223880597014924
[2m[36m(func pid=129725)[0m top5: 0.8269589552238806
[2m[36m(func pid=129725)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=129725)[0m f1_macro: 0.29074421969033615
[2m[36m(func pid=129725)[0m f1_weighted: 0.3238872902137867
[2m[36m(func pid=129725)[0m f1_per_class: [0.321, 0.253, 0.458, 0.389, 0.051, 0.379, 0.309, 0.312, 0.157, 0.277]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0163 | Steps: 2 | Val loss: 3.6916 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 01:45:58 (running for 00:12:19.69)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.734 |      0.139 |                   55 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.527 |      0.291 |                   53 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.153 |      0.321 |                   49 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.025 |      0.287 |                   30 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.18470149253731344
[2m[36m(func pid=128882)[0m top5: 0.6431902985074627
[2m[36m(func pid=128882)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=128882)[0m f1_macro: 0.13905728047371968
[2m[36m(func pid=128882)[0m f1_weighted: 0.19805871408432082
[2m[36m(func pid=128882)[0m f1_per_class: [0.111, 0.152, 0.13, 0.24, 0.04, 0.355, 0.171, 0.157, 0.037, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.1647 | Steps: 2 | Val loss: 2.1085 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.4297 | Steps: 2 | Val loss: 1.9441 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=134827)[0m top1: 0.33348880597014924
[2m[36m(func pid=134827)[0m top5: 0.8558768656716418
[2m[36m(func pid=134827)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=134827)[0m f1_macro: 0.28886796468753956
[2m[36m(func pid=134827)[0m f1_weighted: 0.3633252308185054
[2m[36m(func pid=134827)[0m f1_per_class: [0.273, 0.283, 0.524, 0.425, 0.07, 0.313, 0.437, 0.232, 0.169, 0.164]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.7066 | Steps: 2 | Val loss: 2.2555 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=129730)[0m top1: 0.3362873134328358
[2m[36m(func pid=129730)[0m top5: 0.8666044776119403
[2m[36m(func pid=129730)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=129730)[0m f1_macro: 0.3210279802936987
[2m[36m(func pid=129730)[0m f1_weighted: 0.3652007980399535
[2m[36m(func pid=129730)[0m f1_per_class: [0.299, 0.269, 0.71, 0.403, 0.092, 0.358, 0.438, 0.261, 0.204, 0.176]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.3031716417910448
[2m[36m(func pid=129725)[0m top5: 0.8255597014925373
[2m[36m(func pid=129725)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=129725)[0m f1_macro: 0.290953333148867
[2m[36m(func pid=129725)[0m f1_weighted: 0.3258007705824389
[2m[36m(func pid=129725)[0m f1_per_class: [0.301, 0.241, 0.458, 0.393, 0.051, 0.371, 0.318, 0.337, 0.17, 0.269]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0320 | Steps: 2 | Val loss: 3.7448 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:46:04 (running for 00:12:24.90)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.707 |      0.137 |                   56 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.43  |      0.291 |                   54 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.165 |      0.321 |                   50 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.016 |      0.289 |                   31 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.1837686567164179
[2m[36m(func pid=128882)[0m top5: 0.6427238805970149
[2m[36m(func pid=128882)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=128882)[0m f1_macro: 0.13653066948232945
[2m[36m(func pid=128882)[0m f1_weighted: 0.19837662039088425
[2m[36m(func pid=128882)[0m f1_per_class: [0.092, 0.154, 0.123, 0.232, 0.04, 0.352, 0.181, 0.154, 0.038, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3131 | Steps: 2 | Val loss: 2.1083 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.3706 | Steps: 2 | Val loss: 1.9337 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=134827)[0m top1: 0.33115671641791045
[2m[36m(func pid=134827)[0m top5: 0.8484141791044776
[2m[36m(func pid=134827)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=134827)[0m f1_macro: 0.29412113909916154
[2m[36m(func pid=134827)[0m f1_weighted: 0.3602688933612436
[2m[36m(func pid=134827)[0m f1_per_class: [0.269, 0.273, 0.558, 0.434, 0.066, 0.326, 0.415, 0.232, 0.2, 0.168]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.7075 | Steps: 2 | Val loss: 2.2518 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=129730)[0m top1: 0.34375
[2m[36m(func pid=129730)[0m top5: 0.8726679104477612
[2m[36m(func pid=129730)[0m f1_micro: 0.34375
[2m[36m(func pid=129730)[0m f1_macro: 0.32495428341772564
[2m[36m(func pid=129730)[0m f1_weighted: 0.37269962609646345
[2m[36m(func pid=129730)[0m f1_per_class: [0.315, 0.287, 0.71, 0.405, 0.085, 0.36, 0.449, 0.262, 0.206, 0.17]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.3069029850746269
[2m[36m(func pid=129725)[0m top5: 0.8278917910447762
[2m[36m(func pid=129725)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=129725)[0m f1_macro: 0.291453350669756
[2m[36m(func pid=129725)[0m f1_weighted: 0.32939043128027984
[2m[36m(func pid=129725)[0m f1_per_class: [0.307, 0.246, 0.458, 0.397, 0.052, 0.358, 0.329, 0.328, 0.165, 0.272]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0187 | Steps: 2 | Val loss: 3.7703 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:46:09 (running for 00:12:30.02)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.708 |      0.14  |                   57 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.371 |      0.291 |                   55 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.313 |      0.325 |                   51 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.032 |      0.294 |                   32 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.18983208955223882
[2m[36m(func pid=128882)[0m top5: 0.6501865671641791
[2m[36m(func pid=128882)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=128882)[0m f1_macro: 0.14005107473335218
[2m[36m(func pid=128882)[0m f1_weighted: 0.20621628178116205
[2m[36m(func pid=128882)[0m f1_per_class: [0.103, 0.155, 0.127, 0.25, 0.04, 0.35, 0.191, 0.147, 0.038, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.1624 | Steps: 2 | Val loss: 2.1332 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.3517 | Steps: 2 | Val loss: 1.9278 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=134827)[0m top1: 0.32136194029850745
[2m[36m(func pid=134827)[0m top5: 0.8428171641791045
[2m[36m(func pid=134827)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=134827)[0m f1_macro: 0.2947680459562164
[2m[36m(func pid=134827)[0m f1_weighted: 0.3485750321184016
[2m[36m(func pid=134827)[0m f1_per_class: [0.283, 0.243, 0.571, 0.433, 0.062, 0.326, 0.385, 0.264, 0.212, 0.168]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.6785 | Steps: 2 | Val loss: 2.2507 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=129725)[0m top1: 0.30970149253731344
[2m[36m(func pid=129725)[0m top5: 0.8292910447761194
[2m[36m(func pid=129725)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=129725)[0m f1_macro: 0.2963143414632758
[2m[36m(func pid=129725)[0m f1_weighted: 0.33217001943804
[2m[36m(func pid=129725)[0m f1_per_class: [0.31, 0.248, 0.478, 0.401, 0.051, 0.36, 0.331, 0.331, 0.176, 0.277]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3400186567164179
[2m[36m(func pid=129730)[0m top5: 0.8656716417910447
[2m[36m(func pid=129730)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=129730)[0m f1_macro: 0.322885360068137
[2m[36m(func pid=129730)[0m f1_weighted: 0.3692457203981531
[2m[36m(func pid=129730)[0m f1_per_class: [0.302, 0.286, 0.71, 0.399, 0.084, 0.364, 0.442, 0.266, 0.204, 0.172]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=128882)[0m top1: 0.18983208955223882
[2m[36m(func pid=128882)[0m top5: 0.6487873134328358
[2m[36m(func pid=128882)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=128882)[0m f1_macro: 0.14029408259045853
[2m[36m(func pid=128882)[0m f1_weighted: 0.20613596801044828
[2m[36m(func pid=128882)[0m f1_per_class: [0.103, 0.143, 0.124, 0.257, 0.041, 0.357, 0.187, 0.154, 0.038, 0.0]
== Status ==
Current time: 2024-01-07 01:46:14 (running for 00:12:35.13)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.678 |      0.14  |                   58 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.352 |      0.296 |                   56 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.162 |      0.323 |                   52 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.019 |      0.295 |                   33 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0062 | Steps: 2 | Val loss: 3.7639 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.3502 | Steps: 2 | Val loss: 1.9231 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.1752 | Steps: 2 | Val loss: 2.1555 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=134827)[0m top1: 0.31949626865671643
[2m[36m(func pid=134827)[0m top5: 0.8409514925373134
[2m[36m(func pid=134827)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=134827)[0m f1_macro: 0.2891740930007139
[2m[36m(func pid=134827)[0m f1_weighted: 0.34559491393413283
[2m[36m(func pid=134827)[0m f1_per_class: [0.286, 0.24, 0.537, 0.435, 0.069, 0.322, 0.379, 0.258, 0.2, 0.167]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.6944 | Steps: 2 | Val loss: 2.2499 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=129725)[0m top1: 0.3111007462686567
[2m[36m(func pid=129725)[0m top5: 0.8302238805970149
[2m[36m(func pid=129725)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=129725)[0m f1_macro: 0.29819705023922316
[2m[36m(func pid=129725)[0m f1_weighted: 0.334317796039599
[2m[36m(func pid=129725)[0m f1_per_class: [0.313, 0.246, 0.489, 0.395, 0.052, 0.359, 0.345, 0.331, 0.172, 0.28]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3344216417910448
[2m[36m(func pid=129730)[0m top5: 0.8605410447761194
[2m[36m(func pid=129730)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=129730)[0m f1_macro: 0.31850035570066715
[2m[36m(func pid=129730)[0m f1_weighted: 0.3646178233545366
[2m[36m(func pid=129730)[0m f1_per_class: [0.29, 0.274, 0.71, 0.394, 0.083, 0.356, 0.442, 0.265, 0.202, 0.168]
[2m[36m(func pid=129730)[0m 
== Status ==
Current time: 2024-01-07 01:46:19 (running for 00:12:40.27)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.694 |      0.143 |                   59 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.35  |      0.298 |                   57 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.175 |      0.319 |                   53 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.006 |      0.289 |                   34 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.1921641791044776
[2m[36m(func pid=128882)[0m top5: 0.652518656716418
[2m[36m(func pid=128882)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=128882)[0m f1_macro: 0.1426240232726162
[2m[36m(func pid=128882)[0m f1_weighted: 0.2085553682700597
[2m[36m(func pid=128882)[0m f1_per_class: [0.129, 0.144, 0.126, 0.257, 0.035, 0.36, 0.194, 0.144, 0.038, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0124 | Steps: 2 | Val loss: 3.7742 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.3647 | Steps: 2 | Val loss: 1.9165 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.1673 | Steps: 2 | Val loss: 2.1589 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=134827)[0m top1: 0.3180970149253731
[2m[36m(func pid=134827)[0m top5: 0.8395522388059702
[2m[36m(func pid=134827)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=134827)[0m f1_macro: 0.2885050515314929
[2m[36m(func pid=134827)[0m f1_weighted: 0.3444281638285941
[2m[36m(func pid=134827)[0m f1_per_class: [0.309, 0.238, 0.524, 0.437, 0.066, 0.315, 0.377, 0.249, 0.205, 0.165]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.7043 | Steps: 2 | Val loss: 2.2503 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=129725)[0m top1: 0.310634328358209
[2m[36m(func pid=129725)[0m top5: 0.8353544776119403
[2m[36m(func pid=129725)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=129725)[0m f1_macro: 0.2982165066404233
[2m[36m(func pid=129725)[0m f1_weighted: 0.3331846885011496
[2m[36m(func pid=129725)[0m f1_per_class: [0.312, 0.245, 0.489, 0.394, 0.059, 0.349, 0.345, 0.33, 0.185, 0.274]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3381529850746269
[2m[36m(func pid=129730)[0m top5: 0.8577425373134329
[2m[36m(func pid=129730)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=129730)[0m f1_macro: 0.3203351081787999
[2m[36m(func pid=129730)[0m f1_weighted: 0.3693232815578366
[2m[36m(func pid=129730)[0m f1_per_class: [0.272, 0.269, 0.71, 0.402, 0.084, 0.35, 0.453, 0.279, 0.213, 0.17]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=128882)[0m top1: 0.18796641791044777
[2m[36m(func pid=128882)[0m top5: 0.6511194029850746
[2m[36m(func pid=128882)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=128882)[0m f1_macro: 0.14127397378937348
[2m[36m(func pid=128882)[0m f1_weighted: 0.20328118525762634
[2m[36m(func pid=128882)[0m f1_per_class: [0.121, 0.149, 0.143, 0.246, 0.035, 0.353, 0.187, 0.143, 0.038, 0.0]
[2m[36m(func pid=128882)[0m 
== Status ==
Current time: 2024-01-07 01:46:24 (running for 00:12:45.57)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.704 |      0.141 |                   60 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.365 |      0.298 |                   58 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.167 |      0.32  |                   54 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.012 |      0.289 |                   35 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0104 | Steps: 2 | Val loss: 3.8132 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.3834 | Steps: 2 | Val loss: 1.9100 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.1645 | Steps: 2 | Val loss: 2.1666 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=134827)[0m top1: 0.31529850746268656
[2m[36m(func pid=134827)[0m top5: 0.8386194029850746
[2m[36m(func pid=134827)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=134827)[0m f1_macro: 0.28967493426480234
[2m[36m(func pid=134827)[0m f1_weighted: 0.34194533592184323
[2m[36m(func pid=134827)[0m f1_per_class: [0.309, 0.246, 0.558, 0.437, 0.063, 0.302, 0.37, 0.247, 0.2, 0.165]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.6897 | Steps: 2 | Val loss: 2.2504 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=129725)[0m top1: 0.31156716417910446
[2m[36m(func pid=129725)[0m top5: 0.8339552238805971
[2m[36m(func pid=129725)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=129725)[0m f1_macro: 0.2983338157371761
[2m[36m(func pid=129725)[0m f1_weighted: 0.33517846119754147
[2m[36m(func pid=129725)[0m f1_per_class: [0.311, 0.247, 0.489, 0.393, 0.053, 0.352, 0.35, 0.332, 0.188, 0.267]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33675373134328357
[2m[36m(func pid=129730)[0m top5: 0.8568097014925373
[2m[36m(func pid=129730)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=129730)[0m f1_macro: 0.31925573253920553
[2m[36m(func pid=129730)[0m f1_weighted: 0.36740345765004395
[2m[36m(func pid=129730)[0m f1_per_class: [0.272, 0.273, 0.71, 0.398, 0.08, 0.351, 0.45, 0.265, 0.216, 0.177]
[2m[36m(func pid=129730)[0m 
== Status ==
Current time: 2024-01-07 01:46:30 (running for 00:12:50.88)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.69  |      0.144 |                   61 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.383 |      0.298 |                   59 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.164 |      0.319 |                   55 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.01  |      0.29  |                   36 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.18796641791044777
[2m[36m(func pid=128882)[0m top5: 0.6553171641791045
[2m[36m(func pid=128882)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=128882)[0m f1_macro: 0.1437146269348644
[2m[36m(func pid=128882)[0m f1_weighted: 0.20336414458052618
[2m[36m(func pid=128882)[0m f1_per_class: [0.149, 0.152, 0.138, 0.235, 0.035, 0.355, 0.193, 0.144, 0.037, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0148 | Steps: 2 | Val loss: 3.8011 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.2974 | Steps: 2 | Val loss: 1.9070 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2171 | Steps: 2 | Val loss: 2.1946 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=134827)[0m top1: 0.31529850746268656
[2m[36m(func pid=134827)[0m top5: 0.8381529850746269
[2m[36m(func pid=134827)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=134827)[0m f1_macro: 0.2900464273025999
[2m[36m(func pid=134827)[0m f1_weighted: 0.34340135630713453
[2m[36m(func pid=134827)[0m f1_per_class: [0.313, 0.245, 0.558, 0.434, 0.061, 0.303, 0.376, 0.255, 0.194, 0.161]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.6941 | Steps: 2 | Val loss: 2.2488 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=129725)[0m top1: 0.30970149253731344
[2m[36m(func pid=129725)[0m top5: 0.8362873134328358
[2m[36m(func pid=129725)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=129725)[0m f1_macro: 0.3013388180396614
[2m[36m(func pid=129725)[0m f1_weighted: 0.33287760831544905
[2m[36m(func pid=129725)[0m f1_per_class: [0.333, 0.242, 0.512, 0.382, 0.063, 0.343, 0.359, 0.325, 0.175, 0.279]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3269589552238806
[2m[36m(func pid=129730)[0m top5: 0.8488805970149254
[2m[36m(func pid=129730)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=129730)[0m f1_macro: 0.3128176349753754
[2m[36m(func pid=129730)[0m f1_weighted: 0.3578126542119716
[2m[36m(func pid=129730)[0m f1_per_class: [0.241, 0.261, 0.71, 0.378, 0.079, 0.35, 0.447, 0.263, 0.212, 0.188]
[2m[36m(func pid=129730)[0m 
== Status ==
Current time: 2024-01-07 01:46:35 (running for 00:12:56.16)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.694 |      0.144 |                   62 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.297 |      0.301 |                   60 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.217 |      0.313 |                   56 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.015 |      0.29  |                   37 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.1884328358208955
[2m[36m(func pid=128882)[0m top5: 0.6567164179104478
[2m[36m(func pid=128882)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=128882)[0m f1_macro: 0.143737014947671
[2m[36m(func pid=128882)[0m f1_weighted: 0.2037283143083208
[2m[36m(func pid=128882)[0m f1_per_class: [0.139, 0.151, 0.137, 0.232, 0.042, 0.356, 0.197, 0.147, 0.037, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.2957 | Steps: 2 | Val loss: 1.9017 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0204 | Steps: 2 | Val loss: 3.7935 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2785 | Steps: 2 | Val loss: 2.2226 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=129725)[0m top1: 0.31203358208955223
[2m[36m(func pid=129725)[0m top5: 0.8404850746268657
[2m[36m(func pid=129725)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=129725)[0m f1_macro: 0.3052492129113955
[2m[36m(func pid=129725)[0m f1_weighted: 0.33527822744557995
[2m[36m(func pid=129725)[0m f1_per_class: [0.33, 0.255, 0.537, 0.386, 0.058, 0.349, 0.353, 0.328, 0.177, 0.279]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m top1: 0.3208955223880597
[2m[36m(func pid=134827)[0m top5: 0.8414179104477612
[2m[36m(func pid=134827)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=134827)[0m f1_macro: 0.2964141632178744
[2m[36m(func pid=134827)[0m f1_weighted: 0.3485732518072107
[2m[36m(func pid=134827)[0m f1_per_class: [0.337, 0.25, 0.564, 0.436, 0.071, 0.306, 0.386, 0.255, 0.197, 0.162]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.6771 | Steps: 2 | Val loss: 2.2470 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=129730)[0m top1: 0.3218283582089552
[2m[36m(func pid=129730)[0m top5: 0.8484141791044776
[2m[36m(func pid=129730)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=129730)[0m f1_macro: 0.31019215190966615
[2m[36m(func pid=129730)[0m f1_weighted: 0.3523157280119458
[2m[36m(func pid=129730)[0m f1_per_class: [0.241, 0.261, 0.688, 0.368, 0.082, 0.347, 0.438, 0.272, 0.216, 0.19]
[2m[36m(func pid=129730)[0m 
== Status ==
Current time: 2024-01-07 01:46:40 (running for 00:13:01.25)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.677 |      0.144 |                   63 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.296 |      0.305 |                   61 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.278 |      0.31  |                   57 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.02  |      0.296 |                   38 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.18889925373134328
[2m[36m(func pid=128882)[0m top5: 0.6571828358208955
[2m[36m(func pid=128882)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=128882)[0m f1_macro: 0.14413437348635585
[2m[36m(func pid=128882)[0m f1_weighted: 0.20460721327074016
[2m[36m(func pid=128882)[0m f1_per_class: [0.141, 0.144, 0.129, 0.242, 0.041, 0.365, 0.189, 0.153, 0.036, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.3106 | Steps: 2 | Val loss: 1.8996 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0087 | Steps: 2 | Val loss: 3.7537 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.1889 | Steps: 2 | Val loss: 2.2299 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=129725)[0m top1: 0.3111007462686567
[2m[36m(func pid=129725)[0m top5: 0.8428171641791045
[2m[36m(func pid=129725)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=129725)[0m f1_macro: 0.3086626554257824
[2m[36m(func pid=129725)[0m f1_weighted: 0.3343672574101103
[2m[36m(func pid=129725)[0m f1_per_class: [0.328, 0.255, 0.564, 0.377, 0.058, 0.352, 0.356, 0.33, 0.182, 0.286]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m top1: 0.32322761194029853
[2m[36m(func pid=134827)[0m top5: 0.8432835820895522
[2m[36m(func pid=134827)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=134827)[0m f1_macro: 0.29597172059560484
[2m[36m(func pid=134827)[0m f1_weighted: 0.3511323077835144
[2m[36m(func pid=134827)[0m f1_per_class: [0.339, 0.255, 0.579, 0.439, 0.068, 0.304, 0.393, 0.251, 0.174, 0.159]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.7446 | Steps: 2 | Val loss: 2.2441 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=129730)[0m top1: 0.32322761194029853
[2m[36m(func pid=129730)[0m top5: 0.8498134328358209
[2m[36m(func pid=129730)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=129730)[0m f1_macro: 0.31258468931179695
[2m[36m(func pid=129730)[0m f1_weighted: 0.35425908385721944
[2m[36m(func pid=129730)[0m f1_per_class: [0.238, 0.256, 0.71, 0.373, 0.081, 0.347, 0.442, 0.268, 0.221, 0.19]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.2717 | Steps: 2 | Val loss: 1.8955 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 01:46:45 (running for 00:13:06.57)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.745 |      0.144 |                   64 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.311 |      0.309 |                   62 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.189 |      0.313 |                   58 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.009 |      0.296 |                   39 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.18936567164179105
[2m[36m(func pid=128882)[0m top5: 0.6581156716417911
[2m[36m(func pid=128882)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=128882)[0m f1_macro: 0.1442997862364701
[2m[36m(func pid=128882)[0m f1_weighted: 0.20601844677414172
[2m[36m(func pid=128882)[0m f1_per_class: [0.145, 0.148, 0.13, 0.249, 0.04, 0.349, 0.191, 0.153, 0.037, 0.0]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0033 | Steps: 2 | Val loss: 3.7758 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.1083 | Steps: 2 | Val loss: 2.2065 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=129725)[0m top1: 0.31669776119402987
[2m[36m(func pid=129725)[0m top5: 0.8442164179104478
[2m[36m(func pid=129725)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=129725)[0m f1_macro: 0.3125588825933453
[2m[36m(func pid=129725)[0m f1_weighted: 0.3408299188035082
[2m[36m(func pid=129725)[0m f1_per_class: [0.341, 0.259, 0.564, 0.382, 0.057, 0.363, 0.365, 0.336, 0.18, 0.278]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6858 | Steps: 2 | Val loss: 2.2452 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=134827)[0m top1: 0.32322761194029853
[2m[36m(func pid=134827)[0m top5: 0.8442164179104478
[2m[36m(func pid=134827)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=134827)[0m f1_macro: 0.30049274609088744
[2m[36m(func pid=134827)[0m f1_weighted: 0.3519516811378341
[2m[36m(func pid=134827)[0m f1_per_class: [0.341, 0.259, 0.615, 0.431, 0.067, 0.301, 0.4, 0.252, 0.175, 0.163]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3269589552238806
[2m[36m(func pid=129730)[0m top5: 0.8558768656716418
[2m[36m(func pid=129730)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=129730)[0m f1_macro: 0.3137193500225028
[2m[36m(func pid=129730)[0m f1_weighted: 0.3579210928485613
[2m[36m(func pid=129730)[0m f1_per_class: [0.247, 0.253, 0.71, 0.386, 0.079, 0.343, 0.446, 0.261, 0.224, 0.189]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.3423 | Steps: 2 | Val loss: 1.8821 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 01:46:50 (running for 00:13:11.60)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.686 |      0.146 |                   65 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.272 |      0.313 |                   63 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.108 |      0.314 |                   59 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.003 |      0.3   |                   40 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.18796641791044777
[2m[36m(func pid=128882)[0m top5: 0.6581156716417911
[2m[36m(func pid=128882)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=128882)[0m f1_macro: 0.14601195356116264
[2m[36m(func pid=128882)[0m f1_weighted: 0.20464054643367074
[2m[36m(func pid=128882)[0m f1_per_class: [0.143, 0.154, 0.131, 0.244, 0.04, 0.343, 0.189, 0.152, 0.036, 0.026]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0056 | Steps: 2 | Val loss: 3.7993 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=129725)[0m top1: 0.3218283582089552
[2m[36m(func pid=129725)[0m top5: 0.8479477611940298
[2m[36m(func pid=129725)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=129725)[0m f1_macro: 0.3123026169238087
[2m[36m(func pid=129725)[0m f1_weighted: 0.3456143963893167
[2m[36m(func pid=129725)[0m f1_per_class: [0.344, 0.261, 0.564, 0.393, 0.054, 0.357, 0.377, 0.31, 0.179, 0.284]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2348 | Steps: 2 | Val loss: 2.1792 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.6817 | Steps: 2 | Val loss: 2.2440 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=134827)[0m top1: 0.32276119402985076
[2m[36m(func pid=134827)[0m top5: 0.8451492537313433
[2m[36m(func pid=134827)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=134827)[0m f1_macro: 0.29889087956533006
[2m[36m(func pid=134827)[0m f1_weighted: 0.3520463264196271
[2m[36m(func pid=134827)[0m f1_per_class: [0.337, 0.262, 0.611, 0.429, 0.07, 0.292, 0.406, 0.242, 0.18, 0.159]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33722014925373134
[2m[36m(func pid=129730)[0m top5: 0.8540111940298507
[2m[36m(func pid=129730)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=129730)[0m f1_macro: 0.31724941517577615
[2m[36m(func pid=129730)[0m f1_weighted: 0.3688279352293379
[2m[36m(func pid=129730)[0m f1_per_class: [0.27, 0.25, 0.688, 0.414, 0.081, 0.356, 0.453, 0.261, 0.218, 0.183]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.2448 | Steps: 2 | Val loss: 1.8782 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=128882)[0m top1: 0.1875
[2m[36m(func pid=128882)[0m top5: 0.6571828358208955
[2m[36m(func pid=128882)[0m f1_micro: 0.1875
[2m[36m(func pid=128882)[0m f1_macro: 0.14576059522601145
[2m[36m(func pid=128882)[0m f1_weighted: 0.2050722255951291
[2m[36m(func pid=128882)[0m f1_per_class: [0.146, 0.167, 0.127, 0.246, 0.033, 0.345, 0.182, 0.152, 0.035, 0.025]
[2m[36m(func pid=128882)[0m 
== Status ==
Current time: 2024-01-07 01:46:56 (running for 00:13:16.92)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.682 |      0.146 |                   66 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.342 |      0.312 |                   64 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.235 |      0.317 |                   60 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.006 |      0.299 |                   41 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0094 | Steps: 2 | Val loss: 3.7780 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=129725)[0m top1: 0.32369402985074625
[2m[36m(func pid=129725)[0m top5: 0.8502798507462687
[2m[36m(func pid=129725)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=129725)[0m f1_macro: 0.3160586674545932
[2m[36m(func pid=129725)[0m f1_weighted: 0.3474043238177628
[2m[36m(func pid=129725)[0m f1_per_class: [0.365, 0.262, 0.564, 0.389, 0.049, 0.356, 0.382, 0.319, 0.183, 0.291]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3796 | Steps: 2 | Val loss: 2.1910 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.6840 | Steps: 2 | Val loss: 2.2438 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=134827)[0m top1: 0.3292910447761194
[2m[36m(func pid=134827)[0m top5: 0.8460820895522388
[2m[36m(func pid=134827)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=134827)[0m f1_macro: 0.30173347415479723
[2m[36m(func pid=134827)[0m f1_weighted: 0.359385573875413
[2m[36m(func pid=134827)[0m f1_per_class: [0.331, 0.258, 0.611, 0.434, 0.072, 0.29, 0.428, 0.247, 0.182, 0.164]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.2778 | Steps: 2 | Val loss: 1.8775 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:47:01 (running for 00:13:22.02)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.684 |      0.151 |                   67 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.245 |      0.316 |                   65 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.235 |      0.317 |                   60 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.009 |      0.302 |                   42 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.18983208955223882
[2m[36m(func pid=128882)[0m top5: 0.6576492537313433
[2m[36m(func pid=128882)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=128882)[0m f1_macro: 0.15131806712284063
[2m[36m(func pid=128882)[0m f1_weighted: 0.20713163947176702
[2m[36m(func pid=128882)[0m f1_per_class: [0.144, 0.163, 0.124, 0.246, 0.035, 0.349, 0.186, 0.159, 0.035, 0.072]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33348880597014924
[2m[36m(func pid=129730)[0m top5: 0.8610074626865671
[2m[36m(func pid=129730)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=129730)[0m f1_macro: 0.3159834976437348
[2m[36m(func pid=129730)[0m f1_weighted: 0.3641282000115997
[2m[36m(func pid=129730)[0m f1_per_class: [0.281, 0.258, 0.688, 0.395, 0.077, 0.345, 0.455, 0.253, 0.22, 0.19]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0059 | Steps: 2 | Val loss: 3.7849 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=129725)[0m top1: 0.3255597014925373
[2m[36m(func pid=129725)[0m top5: 0.8470149253731343
[2m[36m(func pid=129725)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=129725)[0m f1_macro: 0.3163610570308701
[2m[36m(func pid=129725)[0m f1_weighted: 0.3490340018483045
[2m[36m(func pid=129725)[0m f1_per_class: [0.352, 0.268, 0.579, 0.386, 0.051, 0.355, 0.389, 0.321, 0.175, 0.288]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.6594 | Steps: 2 | Val loss: 2.2398 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.1453 | Steps: 2 | Val loss: 2.1881 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=134827)[0m top1: 0.33255597014925375
[2m[36m(func pid=134827)[0m top5: 0.847481343283582
[2m[36m(func pid=134827)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=134827)[0m f1_macro: 0.3022056658460147
[2m[36m(func pid=134827)[0m f1_weighted: 0.36286837871296546
[2m[36m(func pid=134827)[0m f1_per_class: [0.331, 0.258, 0.611, 0.434, 0.074, 0.294, 0.44, 0.243, 0.176, 0.161]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.3112 | Steps: 2 | Val loss: 1.8780 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 01:47:06 (running for 00:13:27.22)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.659 |      0.155 |                   68 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.278 |      0.316 |                   66 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.38  |      0.316 |                   61 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.006 |      0.302 |                   43 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.19309701492537312
[2m[36m(func pid=128882)[0m top5: 0.6632462686567164
[2m[36m(func pid=128882)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=128882)[0m f1_macro: 0.15515683688072918
[2m[36m(func pid=128882)[0m f1_weighted: 0.21060976872894338
[2m[36m(func pid=128882)[0m f1_per_class: [0.152, 0.159, 0.129, 0.255, 0.046, 0.348, 0.19, 0.162, 0.035, 0.074]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3344216417910448
[2m[36m(func pid=129730)[0m top5: 0.8600746268656716
[2m[36m(func pid=129730)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=129730)[0m f1_macro: 0.31607376111707414
[2m[36m(func pid=129730)[0m f1_weighted: 0.3661364556814308
[2m[36m(func pid=129730)[0m f1_per_class: [0.297, 0.251, 0.688, 0.405, 0.076, 0.344, 0.458, 0.249, 0.212, 0.183]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.1928 | Steps: 2 | Val loss: 3.6945 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=129725)[0m top1: 0.3278917910447761
[2m[36m(func pid=129725)[0m top5: 0.8512126865671642
[2m[36m(func pid=129725)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=129725)[0m f1_macro: 0.3200442631907845
[2m[36m(func pid=129725)[0m f1_weighted: 0.3519112825344872
[2m[36m(func pid=129725)[0m f1_per_class: [0.358, 0.268, 0.595, 0.394, 0.051, 0.355, 0.39, 0.316, 0.182, 0.291]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.6404 | Steps: 2 | Val loss: 2.2390 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.1410 | Steps: 2 | Val loss: 2.1908 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=134827)[0m top1: 0.345615671641791
[2m[36m(func pid=134827)[0m top5: 0.8638059701492538
[2m[36m(func pid=134827)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=134827)[0m f1_macro: 0.30903344330329685
[2m[36m(func pid=134827)[0m f1_weighted: 0.3751468549747059
[2m[36m(func pid=134827)[0m f1_per_class: [0.329, 0.286, 0.629, 0.433, 0.08, 0.302, 0.465, 0.231, 0.175, 0.161]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.1507 | Steps: 2 | Val loss: 1.8755 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 01:47:11 (running for 00:13:32.29)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.64  |      0.156 |                   69 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.311 |      0.32  |                   67 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.145 |      0.316 |                   62 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.193 |      0.309 |                   44 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.19449626865671643
[2m[36m(func pid=128882)[0m top5: 0.6637126865671642
[2m[36m(func pid=128882)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=128882)[0m f1_macro: 0.15575715087785563
[2m[36m(func pid=128882)[0m f1_weighted: 0.21274865485204356
[2m[36m(func pid=128882)[0m f1_per_class: [0.149, 0.156, 0.132, 0.261, 0.046, 0.35, 0.193, 0.163, 0.036, 0.072]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33675373134328357
[2m[36m(func pid=129730)[0m top5: 0.8624067164179104
[2m[36m(func pid=129730)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=129730)[0m f1_macro: 0.32274566556043893
[2m[36m(func pid=129730)[0m f1_weighted: 0.368963769479488
[2m[36m(func pid=129730)[0m f1_per_class: [0.298, 0.259, 0.733, 0.406, 0.077, 0.347, 0.458, 0.253, 0.217, 0.179]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0109 | Steps: 2 | Val loss: 3.6921 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=129725)[0m top1: 0.3283582089552239
[2m[36m(func pid=129725)[0m top5: 0.8488805970149254
[2m[36m(func pid=129725)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=129725)[0m f1_macro: 0.3178945209814254
[2m[36m(func pid=129725)[0m f1_weighted: 0.35197265881786266
[2m[36m(func pid=129725)[0m f1_per_class: [0.354, 0.274, 0.595, 0.396, 0.051, 0.358, 0.389, 0.303, 0.167, 0.293]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.6518 | Steps: 2 | Val loss: 2.2400 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.1327 | Steps: 2 | Val loss: 2.1946 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=134827)[0m top1: 0.35074626865671643
[2m[36m(func pid=134827)[0m top5: 0.8782649253731343
[2m[36m(func pid=134827)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=134827)[0m f1_macro: 0.31714847466184803
[2m[36m(func pid=134827)[0m f1_weighted: 0.37429044538943523
[2m[36m(func pid=134827)[0m f1_per_class: [0.361, 0.326, 0.667, 0.398, 0.088, 0.305, 0.468, 0.232, 0.165, 0.162]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.2409 | Steps: 2 | Val loss: 1.8797 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 01:47:16 (running for 00:13:37.45)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.652 |      0.159 |                   70 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.151 |      0.318 |                   68 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.141 |      0.323 |                   63 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.011 |      0.317 |                   45 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.19542910447761194
[2m[36m(func pid=128882)[0m top5: 0.6618470149253731
[2m[36m(func pid=128882)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=128882)[0m f1_macro: 0.15936957040694133
[2m[36m(func pid=128882)[0m f1_weighted: 0.2130347921865505
[2m[36m(func pid=128882)[0m f1_per_class: [0.155, 0.175, 0.129, 0.256, 0.046, 0.349, 0.186, 0.167, 0.036, 0.095]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m top1: 0.32882462686567165
[2m[36m(func pid=129730)[0m top5: 0.8624067164179104
[2m[36m(func pid=129730)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=129730)[0m f1_macro: 0.31862757487934934
[2m[36m(func pid=129730)[0m f1_weighted: 0.360679380533601
[2m[36m(func pid=129730)[0m f1_per_class: [0.303, 0.254, 0.733, 0.389, 0.075, 0.34, 0.453, 0.253, 0.208, 0.179]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2558 | Steps: 2 | Val loss: 3.7919 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=129725)[0m top1: 0.3269589552238806
[2m[36m(func pid=129725)[0m top5: 0.8488805970149254
[2m[36m(func pid=129725)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=129725)[0m f1_macro: 0.31218179051559625
[2m[36m(func pid=129725)[0m f1_weighted: 0.35106264873971776
[2m[36m(func pid=129725)[0m f1_per_class: [0.341, 0.271, 0.564, 0.396, 0.051, 0.355, 0.392, 0.292, 0.171, 0.289]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.6499 | Steps: 2 | Val loss: 2.2414 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.1501 | Steps: 2 | Val loss: 2.2038 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=134827)[0m top1: 0.35867537313432835
[2m[36m(func pid=134827)[0m top5: 0.8819962686567164
[2m[36m(func pid=134827)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=134827)[0m f1_macro: 0.32303160945944226
[2m[36m(func pid=134827)[0m f1_weighted: 0.379458786632913
[2m[36m(func pid=134827)[0m f1_per_class: [0.389, 0.342, 0.71, 0.389, 0.088, 0.288, 0.492, 0.23, 0.132, 0.17]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.2360 | Steps: 2 | Val loss: 1.8775 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 01:47:21 (running for 00:13:42.68)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.65  |      0.162 |                   71 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.241 |      0.312 |                   69 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.133 |      0.319 |                   64 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.256 |      0.323 |                   46 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.19682835820895522
[2m[36m(func pid=128882)[0m top5: 0.6627798507462687
[2m[36m(func pid=128882)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=128882)[0m f1_macro: 0.1620682804893812
[2m[36m(func pid=128882)[0m f1_weighted: 0.21360084128878118
[2m[36m(func pid=128882)[0m f1_per_class: [0.148, 0.179, 0.129, 0.252, 0.041, 0.354, 0.186, 0.172, 0.036, 0.123]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3302238805970149
[2m[36m(func pid=129730)[0m top5: 0.8638059701492538
[2m[36m(func pid=129730)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=129730)[0m f1_macro: 0.3201620989912898
[2m[36m(func pid=129730)[0m f1_weighted: 0.3616055078026948
[2m[36m(func pid=129730)[0m f1_per_class: [0.315, 0.26, 0.733, 0.388, 0.076, 0.329, 0.457, 0.246, 0.208, 0.19]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0088 | Steps: 2 | Val loss: 3.9213 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=129725)[0m top1: 0.3246268656716418
[2m[36m(func pid=129725)[0m top5: 0.8498134328358209
[2m[36m(func pid=129725)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=129725)[0m f1_macro: 0.3135460004631507
[2m[36m(func pid=129725)[0m f1_weighted: 0.3487288969206138
[2m[36m(func pid=129725)[0m f1_per_class: [0.343, 0.268, 0.579, 0.39, 0.051, 0.356, 0.389, 0.293, 0.173, 0.293]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.6481 | Steps: 2 | Val loss: 2.2394 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.1250 | Steps: 2 | Val loss: 2.1961 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=134827)[0m top1: 0.353544776119403
[2m[36m(func pid=134827)[0m top5: 0.8759328358208955
[2m[36m(func pid=134827)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=134827)[0m f1_macro: 0.3149648126125598
[2m[36m(func pid=134827)[0m f1_weighted: 0.3695159368886226
[2m[36m(func pid=134827)[0m f1_per_class: [0.394, 0.353, 0.71, 0.36, 0.074, 0.247, 0.5, 0.198, 0.136, 0.177]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.1386 | Steps: 2 | Val loss: 1.8765 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=128882)[0m top1: 0.19542910447761194
[2m[36m(func pid=128882)[0m top5: 0.6655783582089553
[2m[36m(func pid=128882)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=128882)[0m f1_macro: 0.1591705347900864
[2m[36m(func pid=128882)[0m f1_weighted: 0.21252640470292855
[2m[36m(func pid=128882)[0m f1_per_class: [0.144, 0.178, 0.124, 0.247, 0.042, 0.348, 0.192, 0.164, 0.037, 0.116]
[2m[36m(func pid=128882)[0m 
== Status ==
Current time: 2024-01-07 01:47:27 (running for 00:13:48.07)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.648 |      0.159 |                   72 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.236 |      0.314 |                   70 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.15  |      0.32  |                   65 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.009 |      0.315 |                   47 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129730)[0m top1: 0.333955223880597
[2m[36m(func pid=129730)[0m top5: 0.8675373134328358
[2m[36m(func pid=129730)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=129730)[0m f1_macro: 0.3252909923121195
[2m[36m(func pid=129730)[0m f1_weighted: 0.3654905460571638
[2m[36m(func pid=129730)[0m f1_per_class: [0.329, 0.27, 0.759, 0.397, 0.079, 0.326, 0.456, 0.247, 0.215, 0.176]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0572 | Steps: 2 | Val loss: 4.0597 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=129725)[0m top1: 0.32649253731343286
[2m[36m(func pid=129725)[0m top5: 0.8488805970149254
[2m[36m(func pid=129725)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=129725)[0m f1_macro: 0.3140782615073091
[2m[36m(func pid=129725)[0m f1_weighted: 0.3503521342679128
[2m[36m(func pid=129725)[0m f1_per_class: [0.341, 0.269, 0.579, 0.387, 0.052, 0.368, 0.395, 0.279, 0.178, 0.293]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.6440 | Steps: 2 | Val loss: 2.2374 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=134827)[0m top1: 0.34701492537313433
[2m[36m(func pid=134827)[0m top5: 0.8684701492537313
[2m[36m(func pid=134827)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=134827)[0m f1_macro: 0.3046218367604662
[2m[36m(func pid=134827)[0m f1_weighted: 0.36241978733752295
[2m[36m(func pid=134827)[0m f1_per_class: [0.371, 0.349, 0.71, 0.36, 0.058, 0.184, 0.507, 0.188, 0.134, 0.185]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.1283 | Steps: 2 | Val loss: 2.2109 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.1584 | Steps: 2 | Val loss: 1.8742 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
== Status ==
Current time: 2024-01-07 01:47:32 (running for 00:13:53.45)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.644 |      0.16  |                   73 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.139 |      0.314 |                   71 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.125 |      0.325 |                   66 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.057 |      0.305 |                   48 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.1982276119402985
[2m[36m(func pid=128882)[0m top5: 0.6674440298507462
[2m[36m(func pid=128882)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=128882)[0m f1_macro: 0.16023079149904743
[2m[36m(func pid=128882)[0m f1_weighted: 0.21602995584039064
[2m[36m(func pid=128882)[0m f1_per_class: [0.152, 0.176, 0.131, 0.248, 0.041, 0.36, 0.199, 0.166, 0.037, 0.092]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m top1: 0.3278917910447761
[2m[36m(func pid=129725)[0m top5: 0.847481343283582
[2m[36m(func pid=129725)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=129725)[0m f1_macro: 0.31430187067011955
[2m[36m(func pid=129725)[0m f1_weighted: 0.35176474187074597
[2m[36m(func pid=129725)[0m f1_per_class: [0.33, 0.27, 0.579, 0.385, 0.053, 0.368, 0.399, 0.283, 0.193, 0.282]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33302238805970147
[2m[36m(func pid=129730)[0m top5: 0.8656716417910447
[2m[36m(func pid=129730)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=129730)[0m f1_macro: 0.32189171930330546
[2m[36m(func pid=129730)[0m f1_weighted: 0.3640737241973371
[2m[36m(func pid=129730)[0m f1_per_class: [0.324, 0.275, 0.733, 0.393, 0.079, 0.325, 0.451, 0.254, 0.206, 0.178]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0393 | Steps: 2 | Val loss: 4.1289 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.6803 | Steps: 2 | Val loss: 2.2358 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.2378 | Steps: 2 | Val loss: 1.8768 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=134827)[0m top1: 0.32742537313432835
[2m[36m(func pid=134827)[0m top5: 0.8558768656716418
[2m[36m(func pid=134827)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=134827)[0m f1_macro: 0.2846735265642759
[2m[36m(func pid=134827)[0m f1_weighted: 0.34978408787253357
[2m[36m(func pid=134827)[0m f1_per_class: [0.297, 0.309, 0.667, 0.375, 0.045, 0.144, 0.493, 0.192, 0.153, 0.172]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.1373 | Steps: 2 | Val loss: 2.2245 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:47:37 (running for 00:13:58.67)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00004 | RUNNING    | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.68  |      0.16  |                   74 |
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.158 |      0.314 |                   72 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.128 |      0.322 |                   67 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.039 |      0.285 |                   49 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.19869402985074627
[2m[36m(func pid=128882)[0m top5: 0.6707089552238806
[2m[36m(func pid=128882)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=128882)[0m f1_macro: 0.15993165968246337
[2m[36m(func pid=128882)[0m f1_weighted: 0.2163134848000201
[2m[36m(func pid=128882)[0m f1_per_class: [0.143, 0.181, 0.128, 0.241, 0.048, 0.355, 0.206, 0.167, 0.039, 0.092]
[2m[36m(func pid=128882)[0m 
[2m[36m(func pid=129725)[0m top1: 0.32369402985074625
[2m[36m(func pid=129725)[0m top5: 0.847481343283582
[2m[36m(func pid=129725)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=129725)[0m f1_macro: 0.3109687738819213
[2m[36m(func pid=129725)[0m f1_weighted: 0.34784109966315263
[2m[36m(func pid=129725)[0m f1_per_class: [0.318, 0.264, 0.579, 0.374, 0.053, 0.367, 0.402, 0.29, 0.183, 0.28]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.1330 | Steps: 2 | Val loss: 4.2046 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=129730)[0m top1: 0.3344216417910448
[2m[36m(func pid=129730)[0m top5: 0.8647388059701493
[2m[36m(func pid=129730)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=129730)[0m f1_macro: 0.3249858387442134
[2m[36m(func pid=129730)[0m f1_weighted: 0.3655424399185171
[2m[36m(func pid=129730)[0m f1_per_class: [0.319, 0.279, 0.759, 0.395, 0.08, 0.337, 0.448, 0.248, 0.209, 0.175]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=128882)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.6225 | Steps: 2 | Val loss: 2.2355 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.1558 | Steps: 2 | Val loss: 1.8754 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=134827)[0m top1: 0.31296641791044777
[2m[36m(func pid=134827)[0m top5: 0.8353544776119403
[2m[36m(func pid=134827)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=134827)[0m f1_macro: 0.2787733742329718
[2m[36m(func pid=134827)[0m f1_weighted: 0.3422140699860865
[2m[36m(func pid=134827)[0m f1_per_class: [0.234, 0.284, 0.706, 0.379, 0.045, 0.138, 0.483, 0.2, 0.15, 0.169]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.1997 | Steps: 2 | Val loss: 2.2238 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 01:47:43 (running for 00:14:03.92)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.315
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 3 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.238 |      0.311 |                   73 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.137 |      0.325 |                   68 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.133 |      0.279 |                   50 |
| train_66d79_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=128882)[0m top1: 0.197294776119403
[2m[36m(func pid=128882)[0m top5: 0.6674440298507462
[2m[36m(func pid=128882)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=128882)[0m f1_macro: 0.16045527166152956
[2m[36m(func pid=128882)[0m f1_weighted: 0.21441847002693448
[2m[36m(func pid=128882)[0m f1_per_class: [0.152, 0.167, 0.129, 0.238, 0.047, 0.364, 0.204, 0.174, 0.039, 0.09]
[2m[36m(func pid=129725)[0m top1: 0.3260261194029851
[2m[36m(func pid=129725)[0m top5: 0.8493470149253731
[2m[36m(func pid=129725)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=129725)[0m f1_macro: 0.3145586900623881
[2m[36m(func pid=129725)[0m f1_weighted: 0.35043596318704606
[2m[36m(func pid=129725)[0m f1_per_class: [0.325, 0.264, 0.595, 0.375, 0.053, 0.367, 0.408, 0.284, 0.196, 0.279]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0085 | Steps: 2 | Val loss: 4.3251 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=129730)[0m top1: 0.3358208955223881
[2m[36m(func pid=129730)[0m top5: 0.8652052238805971
[2m[36m(func pid=129730)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=129730)[0m f1_macro: 0.3240623327423773
[2m[36m(func pid=129730)[0m f1_weighted: 0.3672105759795535
[2m[36m(func pid=129730)[0m f1_per_class: [0.314, 0.278, 0.759, 0.402, 0.081, 0.335, 0.45, 0.245, 0.205, 0.171]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.0922 | Steps: 2 | Val loss: 1.8754 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=134827)[0m top1: 0.2947761194029851
[2m[36m(func pid=134827)[0m top5: 0.8134328358208955
[2m[36m(func pid=134827)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=134827)[0m f1_macro: 0.2807352297150162
[2m[36m(func pid=134827)[0m f1_weighted: 0.33396466340958303
[2m[36m(func pid=134827)[0m f1_per_class: [0.184, 0.242, 0.727, 0.372, 0.052, 0.182, 0.464, 0.208, 0.204, 0.171]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.1590 | Steps: 2 | Val loss: 2.2312 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=129725)[0m top1: 0.32369402985074625
[2m[36m(func pid=129725)[0m top5: 0.8451492537313433
[2m[36m(func pid=129725)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=129725)[0m f1_macro: 0.31457559923015577
[2m[36m(func pid=129725)[0m f1_weighted: 0.34748470240251306
[2m[36m(func pid=129725)[0m f1_per_class: [0.325, 0.261, 0.595, 0.37, 0.061, 0.365, 0.402, 0.296, 0.199, 0.272]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0140 | Steps: 2 | Val loss: 4.4496 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=129730)[0m top1: 0.33348880597014924
[2m[36m(func pid=129730)[0m top5: 0.8684701492537313
[2m[36m(func pid=129730)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=129730)[0m f1_macro: 0.3247578251162501
[2m[36m(func pid=129730)[0m f1_weighted: 0.3635780494765954
[2m[36m(func pid=129730)[0m f1_per_class: [0.331, 0.278, 0.759, 0.395, 0.085, 0.338, 0.442, 0.251, 0.199, 0.17]
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 1.1474 | Steps: 2 | Val loss: 1.8791 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 01:47:48 (running for 00:14:09.54)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.092 |      0.315 |                   75 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.2   |      0.324 |                   69 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.009 |      0.281 |                   51 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=134827)[0m top1: 0.283115671641791
[2m[36m(func pid=134827)[0m top5: 0.792910447761194
[2m[36m(func pid=134827)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=134827)[0m f1_macro: 0.2754784556411368
[2m[36m(func pid=134827)[0m f1_weighted: 0.32838893530962115
[2m[36m(func pid=134827)[0m f1_per_class: [0.166, 0.233, 0.649, 0.369, 0.048, 0.227, 0.431, 0.256, 0.203, 0.174]
[2m[36m(func pid=146329)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=146329)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=146329)[0m Configuration completed!
[2m[36m(func pid=146329)[0m New optimizer parameters:
[2m[36m(func pid=146329)[0m SGD (
[2m[36m(func pid=146329)[0m Parameter Group 0
[2m[36m(func pid=146329)[0m     dampening: 0
[2m[36m(func pid=146329)[0m     differentiable: False
[2m[36m(func pid=146329)[0m     foreach: None
[2m[36m(func pid=146329)[0m     lr: 0.0001
[2m[36m(func pid=146329)[0m     maximize: False
[2m[36m(func pid=146329)[0m     momentum: 0.99
[2m[36m(func pid=146329)[0m     nesterov: False
[2m[36m(func pid=146329)[0m     weight_decay: 0.0001
[2m[36m(func pid=146329)[0m )
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m 
== Status ==
Current time: 2024-01-07 01:47:53 (running for 00:14:14.63)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.147 |      0.315 |                   76 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.159 |      0.325 |                   70 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.014 |      0.275 |                   52 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.3283582089552239
[2m[36m(func pid=129725)[0m top5: 0.8423507462686567
[2m[36m(func pid=129725)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=129725)[0m f1_macro: 0.3148131318243258
[2m[36m(func pid=129725)[0m f1_weighted: 0.3527153295869035
[2m[36m(func pid=129725)[0m f1_per_class: [0.329, 0.263, 0.595, 0.386, 0.054, 0.366, 0.406, 0.286, 0.203, 0.261]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.1366 | Steps: 2 | Val loss: 2.2272 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0071 | Steps: 2 | Val loss: 4.5991 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9650 | Steps: 2 | Val loss: 2.3199 | Batch size: 32 | lr: 0.0001 | Duration: 4.45s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.0823 | Steps: 2 | Val loss: 1.8746 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=134827)[0m top1: 0.26632462686567165
[2m[36m(func pid=134827)[0m top5: 0.7714552238805971
[2m[36m(func pid=134827)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=134827)[0m f1_macro: 0.2612246828813636
[2m[36m(func pid=134827)[0m f1_weighted: 0.31212010624377257
[2m[36m(func pid=134827)[0m f1_per_class: [0.15, 0.195, 0.579, 0.36, 0.049, 0.243, 0.402, 0.249, 0.214, 0.17]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3362873134328358
[2m[36m(func pid=129730)[0m top5: 0.8722014925373134
[2m[36m(func pid=129730)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=129730)[0m f1_macro: 0.3264435603696488
[2m[36m(func pid=129730)[0m f1_weighted: 0.366908397287441
[2m[36m(func pid=129730)[0m f1_per_class: [0.339, 0.279, 0.759, 0.4, 0.077, 0.35, 0.444, 0.254, 0.194, 0.169]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=146329)[0m top1: 0.1767723880597015
[2m[36m(func pid=146329)[0m top5: 0.534981343283582
[2m[36m(func pid=146329)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=146329)[0m f1_macro: 0.11844342991093386
[2m[36m(func pid=146329)[0m f1_weighted: 0.12635870683815661
[2m[36m(func pid=146329)[0m f1_per_class: [0.3, 0.35, 0.0, 0.093, 0.0, 0.214, 0.024, 0.0, 0.0, 0.203]
[2m[36m(func pid=146329)[0m 
== Status ==
Current time: 2024-01-07 01:47:58 (running for 00:14:19.70)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.082 |      0.313 |                   77 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.137 |      0.326 |                   71 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.007 |      0.261 |                   53 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.965 |      0.118 |                    1 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.3306902985074627
[2m[36m(func pid=129725)[0m top5: 0.8418843283582089
[2m[36m(func pid=129725)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=129725)[0m f1_macro: 0.3133310586790477
[2m[36m(func pid=129725)[0m f1_weighted: 0.35568907839186614
[2m[36m(func pid=129725)[0m f1_per_class: [0.324, 0.257, 0.579, 0.388, 0.057, 0.363, 0.416, 0.31, 0.194, 0.246]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0107 | Steps: 2 | Val loss: 4.7255 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4151 | Steps: 2 | Val loss: 2.2041 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9917 | Steps: 2 | Val loss: 2.3260 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.1993 | Steps: 2 | Val loss: 1.8707 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=134827)[0m top1: 0.251865671641791
[2m[36m(func pid=134827)[0m top5: 0.7551305970149254
[2m[36m(func pid=134827)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=134827)[0m f1_macro: 0.2553194713746427
[2m[36m(func pid=134827)[0m f1_weighted: 0.29595670869705526
[2m[36m(func pid=134827)[0m f1_per_class: [0.146, 0.171, 0.595, 0.347, 0.05, 0.263, 0.367, 0.25, 0.2, 0.163]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.34421641791044777
[2m[36m(func pid=129730)[0m top5: 0.871268656716418
[2m[36m(func pid=129730)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=129730)[0m f1_macro: 0.3314678966967051
[2m[36m(func pid=129730)[0m f1_weighted: 0.37478946724097656
[2m[36m(func pid=129730)[0m f1_per_class: [0.337, 0.286, 0.759, 0.411, 0.091, 0.351, 0.453, 0.265, 0.194, 0.169]
[2m[36m(func pid=129730)[0m 
== Status ==
Current time: 2024-01-07 01:48:04 (running for 00:14:24.83)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.082 |      0.313 |                   77 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.415 |      0.331 |                   72 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.011 |      0.255 |                   54 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.992 |      0.108 |                    2 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=146329)[0m top1: 0.1791044776119403
[2m[36m(func pid=146329)[0m top5: 0.5284514925373134
[2m[36m(func pid=146329)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=146329)[0m f1_macro: 0.10826913425311568
[2m[36m(func pid=146329)[0m f1_weighted: 0.12722277418549455
[2m[36m(func pid=146329)[0m f1_per_class: [0.213, 0.328, 0.0, 0.098, 0.01, 0.271, 0.015, 0.036, 0.0, 0.111]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=129725)[0m top1: 0.3358208955223881
[2m[36m(func pid=129725)[0m top5: 0.8432835820895522
[2m[36m(func pid=129725)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=129725)[0m f1_macro: 0.311921327420709
[2m[36m(func pid=129725)[0m f1_weighted: 0.36091316998449857
[2m[36m(func pid=129725)[0m f1_per_class: [0.327, 0.258, 0.537, 0.397, 0.058, 0.373, 0.421, 0.308, 0.202, 0.238]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0103 | Steps: 2 | Val loss: 4.8261 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.1771 | Steps: 2 | Val loss: 2.2405 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9742 | Steps: 2 | Val loss: 2.3363 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 1.0711 | Steps: 2 | Val loss: 1.8684 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=134827)[0m top1: 0.24347014925373134
[2m[36m(func pid=134827)[0m top5: 0.7458022388059702
[2m[36m(func pid=134827)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=134827)[0m f1_macro: 0.2534128916345072
[2m[36m(func pid=134827)[0m f1_weighted: 0.2860677359479491
[2m[36m(func pid=134827)[0m f1_per_class: [0.14, 0.166, 0.611, 0.337, 0.051, 0.279, 0.341, 0.251, 0.2, 0.158]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33722014925373134
[2m[36m(func pid=129730)[0m top5: 0.867070895522388
[2m[36m(func pid=129730)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=129730)[0m f1_macro: 0.3248892548870134
[2m[36m(func pid=129730)[0m f1_weighted: 0.3680405906086237
[2m[36m(func pid=129730)[0m f1_per_class: [0.339, 0.278, 0.733, 0.416, 0.085, 0.341, 0.436, 0.259, 0.2, 0.163]
[2m[36m(func pid=129730)[0m 
== Status ==
Current time: 2024-01-07 01:48:09 (running for 00:14:29.90)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.199 |      0.312 |                   78 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.177 |      0.325 |                   73 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.01  |      0.253 |                   55 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.974 |      0.101 |                    3 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=146329)[0m top1: 0.17537313432835822
[2m[36m(func pid=146329)[0m top5: 0.5121268656716418
[2m[36m(func pid=146329)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=146329)[0m f1_macro: 0.10059400653905393
[2m[36m(func pid=146329)[0m f1_weighted: 0.12442898091501046
[2m[36m(func pid=146329)[0m f1_per_class: [0.177, 0.304, 0.0, 0.11, 0.01, 0.299, 0.006, 0.011, 0.0, 0.089]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=129725)[0m top1: 0.33488805970149255
[2m[36m(func pid=129725)[0m top5: 0.8432835820895522
[2m[36m(func pid=129725)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=129725)[0m f1_macro: 0.3094475618468051
[2m[36m(func pid=129725)[0m f1_weighted: 0.360149935194487
[2m[36m(func pid=129725)[0m f1_per_class: [0.295, 0.255, 0.55, 0.391, 0.06, 0.378, 0.428, 0.307, 0.182, 0.249]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0731 | Steps: 2 | Val loss: 4.7869 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.1326 | Steps: 2 | Val loss: 2.2386 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 1.0755 | Steps: 2 | Val loss: 1.8685 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9675 | Steps: 2 | Val loss: 2.3425 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=134827)[0m top1: 0.24673507462686567
[2m[36m(func pid=134827)[0m top5: 0.7481343283582089
[2m[36m(func pid=134827)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=134827)[0m f1_macro: 0.25655590211675194
[2m[36m(func pid=134827)[0m f1_weighted: 0.28777570464538804
[2m[36m(func pid=134827)[0m f1_per_class: [0.14, 0.171, 0.611, 0.341, 0.049, 0.301, 0.329, 0.268, 0.203, 0.154]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3376865671641791
[2m[36m(func pid=129730)[0m top5: 0.871268656716418
[2m[36m(func pid=129730)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=129730)[0m f1_macro: 0.323466521919765
[2m[36m(func pid=129730)[0m f1_weighted: 0.3677466499024283
[2m[36m(func pid=129730)[0m f1_per_class: [0.333, 0.279, 0.71, 0.409, 0.085, 0.344, 0.439, 0.256, 0.214, 0.167]
== Status ==
Current time: 2024-01-07 01:48:14 (running for 00:14:34.93)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.075 |      0.31  |                   80 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.133 |      0.323 |                   74 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.073 |      0.257 |                   56 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.974 |      0.101 |                    3 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.33348880597014924
[2m[36m(func pid=129725)[0m top5: 0.8423507462686567
[2m[36m(func pid=129725)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=129725)[0m f1_macro: 0.3098183961732587
[2m[36m(func pid=129725)[0m f1_weighted: 0.3580891472120499
[2m[36m(func pid=129725)[0m f1_per_class: [0.283, 0.255, 0.579, 0.387, 0.071, 0.382, 0.425, 0.311, 0.154, 0.25]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=146329)[0m top1: 0.17024253731343283
[2m[36m(func pid=146329)[0m top5: 0.5009328358208955
[2m[36m(func pid=146329)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=146329)[0m f1_macro: 0.09283870758378691
[2m[36m(func pid=146329)[0m f1_weighted: 0.12304552269164377
[2m[36m(func pid=146329)[0m f1_per_class: [0.107, 0.289, 0.0, 0.111, 0.009, 0.311, 0.009, 0.019, 0.0, 0.074]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0089 | Steps: 2 | Val loss: 4.6737 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.0403 | Steps: 2 | Val loss: 1.8653 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.1744 | Steps: 2 | Val loss: 2.2478 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9561 | Steps: 2 | Val loss: 2.3470 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=134827)[0m top1: 0.2574626865671642
[2m[36m(func pid=134827)[0m top5: 0.7569962686567164
[2m[36m(func pid=134827)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=134827)[0m f1_macro: 0.2652324859232286
[2m[36m(func pid=134827)[0m f1_weighted: 0.2953064913971584
[2m[36m(func pid=134827)[0m f1_per_class: [0.146, 0.2, 0.629, 0.351, 0.059, 0.322, 0.317, 0.265, 0.22, 0.143]
[2m[36m(func pid=134827)[0m 
== Status ==
Current time: 2024-01-07 01:48:19 (running for 00:14:40.16)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.04  |      0.311 |                   81 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.133 |      0.323 |                   74 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.009 |      0.265 |                   57 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.968 |      0.093 |                    4 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.33302238805970147
[2m[36m(func pid=129725)[0m top5: 0.84375
[2m[36m(func pid=129725)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=129725)[0m f1_macro: 0.3113705654161646
[2m[36m(func pid=129725)[0m f1_weighted: 0.3572977566909787
[2m[36m(func pid=129725)[0m f1_per_class: [0.288, 0.253, 0.564, 0.386, 0.072, 0.381, 0.421, 0.316, 0.182, 0.251]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33488805970149255
[2m[36m(func pid=129730)[0m top5: 0.8703358208955224
[2m[36m(func pid=129730)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=129730)[0m f1_macro: 0.32072247474397125
[2m[36m(func pid=129730)[0m f1_weighted: 0.36547737163120236
[2m[36m(func pid=129730)[0m f1_per_class: [0.315, 0.28, 0.733, 0.406, 0.075, 0.333, 0.439, 0.262, 0.201, 0.162]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=146329)[0m top1: 0.16138059701492538
[2m[36m(func pid=146329)[0m top5: 0.49486940298507465
[2m[36m(func pid=146329)[0m f1_micro: 0.16138059701492538
[2m[36m(func pid=146329)[0m f1_macro: 0.0867185314340569
[2m[36m(func pid=146329)[0m f1_weighted: 0.12100649872713302
[2m[36m(func pid=146329)[0m f1_per_class: [0.111, 0.264, 0.0, 0.115, 0.009, 0.301, 0.018, 0.018, 0.0, 0.031]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0236 | Steps: 2 | Val loss: 4.5998 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 1.1982 | Steps: 2 | Val loss: 1.8717 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.1029 | Steps: 2 | Val loss: 2.2431 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9901 | Steps: 2 | Val loss: 2.3489 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=134827)[0m top1: 0.2630597014925373
[2m[36m(func pid=134827)[0m top5: 0.7663246268656716
[2m[36m(func pid=134827)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=134827)[0m f1_macro: 0.2748621235711623
[2m[36m(func pid=134827)[0m f1_weighted: 0.29815702898091123
[2m[36m(func pid=134827)[0m f1_per_class: [0.162, 0.218, 0.71, 0.352, 0.05, 0.331, 0.314, 0.243, 0.216, 0.153]
[2m[36m(func pid=134827)[0m 
== Status ==
Current time: 2024-01-07 01:48:24 (running for 00:14:45.31)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.198 |      0.31  |                   82 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.174 |      0.321 |                   75 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.024 |      0.275 |                   58 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.956 |      0.087 |                    5 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.3278917910447761
[2m[36m(func pid=129725)[0m top5: 0.8428171641791045
[2m[36m(func pid=129725)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=129725)[0m f1_macro: 0.30976560837657013
[2m[36m(func pid=129725)[0m f1_weighted: 0.3516217743991666
[2m[36m(func pid=129725)[0m f1_per_class: [0.299, 0.26, 0.564, 0.372, 0.068, 0.379, 0.412, 0.32, 0.167, 0.257]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=146329)[0m top1: 0.15438432835820895
[2m[36m(func pid=146329)[0m top5: 0.49113805970149255
[2m[36m(func pid=146329)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=146329)[0m f1_macro: 0.08638480278466673
[2m[36m(func pid=146329)[0m f1_weighted: 0.12122544733263521
[2m[36m(func pid=146329)[0m f1_per_class: [0.098, 0.238, 0.0, 0.118, 0.016, 0.301, 0.029, 0.035, 0.0, 0.03]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33908582089552236
[2m[36m(func pid=129730)[0m top5: 0.8689365671641791
[2m[36m(func pid=129730)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=129730)[0m f1_macro: 0.3222710016228909
[2m[36m(func pid=129730)[0m f1_weighted: 0.3703519982373406
[2m[36m(func pid=129730)[0m f1_per_class: [0.309, 0.277, 0.733, 0.412, 0.077, 0.333, 0.452, 0.258, 0.201, 0.17]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 1.0469 | Steps: 2 | Val loss: 1.8714 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2513 | Steps: 2 | Val loss: 4.4365 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9736 | Steps: 2 | Val loss: 2.3476 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.1801 | Steps: 2 | Val loss: 2.2659 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:48:29 (running for 00:14:50.40)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.047 |      0.312 |                   83 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.103 |      0.322 |                   76 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.024 |      0.275 |                   58 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.99  |      0.086 |                    6 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.32975746268656714
[2m[36m(func pid=129725)[0m top5: 0.8442164179104478
[2m[36m(func pid=129725)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=129725)[0m f1_macro: 0.31182723214628216
[2m[36m(func pid=129725)[0m f1_weighted: 0.35436256716686027
[2m[36m(func pid=129725)[0m f1_per_class: [0.294, 0.26, 0.579, 0.387, 0.068, 0.382, 0.405, 0.326, 0.165, 0.251]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m top1: 0.2835820895522388
[2m[36m(func pid=134827)[0m top5: 0.7943097014925373
[2m[36m(func pid=134827)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=134827)[0m f1_macro: 0.2845715646503854
[2m[36m(func pid=134827)[0m f1_weighted: 0.3150945949725765
[2m[36m(func pid=134827)[0m f1_per_class: [0.227, 0.255, 0.688, 0.364, 0.05, 0.343, 0.331, 0.246, 0.203, 0.138]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=146329)[0m top1: 0.1478544776119403
[2m[36m(func pid=146329)[0m top5: 0.49906716417910446
[2m[36m(func pid=146329)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=146329)[0m f1_macro: 0.08247938425084664
[2m[36m(func pid=146329)[0m f1_weighted: 0.12030514909424525
[2m[36m(func pid=146329)[0m f1_per_class: [0.082, 0.211, 0.0, 0.109, 0.023, 0.3, 0.048, 0.05, 0.0, 0.0]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3362873134328358
[2m[36m(func pid=129730)[0m top5: 0.8647388059701493
[2m[36m(func pid=129730)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=129730)[0m f1_macro: 0.31885591317897366
[2m[36m(func pid=129730)[0m f1_weighted: 0.36737497957651966
[2m[36m(func pid=129730)[0m f1_per_class: [0.311, 0.28, 0.71, 0.39, 0.077, 0.336, 0.46, 0.258, 0.199, 0.168]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 1.1246 | Steps: 2 | Val loss: 1.8740 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0575 | Steps: 2 | Val loss: 4.3285 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9445 | Steps: 2 | Val loss: 2.3427 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.1072 | Steps: 2 | Val loss: 2.2472 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 01:48:34 (running for 00:14:55.56)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.125 |      0.311 |                   84 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.18  |      0.319 |                   77 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.251 |      0.285 |                   59 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.974 |      0.082 |                    7 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.3278917910447761
[2m[36m(func pid=129725)[0m top5: 0.8414179104477612
[2m[36m(func pid=129725)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=129725)[0m f1_macro: 0.31071654369361024
[2m[36m(func pid=129725)[0m f1_weighted: 0.35236475821613006
[2m[36m(func pid=129725)[0m f1_per_class: [0.288, 0.257, 0.579, 0.386, 0.069, 0.384, 0.401, 0.317, 0.178, 0.249]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m top1: 0.292910447761194
[2m[36m(func pid=134827)[0m top5: 0.8176305970149254
[2m[36m(func pid=134827)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=134827)[0m f1_macro: 0.2958043307096153
[2m[36m(func pid=134827)[0m f1_weighted: 0.3165905593083584
[2m[36m(func pid=134827)[0m f1_per_class: [0.298, 0.292, 0.71, 0.357, 0.06, 0.357, 0.313, 0.239, 0.188, 0.144]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=146329)[0m top1: 0.1478544776119403
[2m[36m(func pid=146329)[0m top5: 0.5121268656716418
[2m[36m(func pid=146329)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=146329)[0m f1_macro: 0.08551803423435973
[2m[36m(func pid=146329)[0m f1_weighted: 0.12750880078108204
[2m[36m(func pid=146329)[0m f1_per_class: [0.075, 0.198, 0.0, 0.127, 0.029, 0.298, 0.061, 0.066, 0.0, 0.0]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=129730)[0m top1: 0.34421641791044777
[2m[36m(func pid=129730)[0m top5: 0.8675373134328358
[2m[36m(func pid=129730)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=129730)[0m f1_macro: 0.32622928442604165
[2m[36m(func pid=129730)[0m f1_weighted: 0.3756862781641274
[2m[36m(func pid=129730)[0m f1_per_class: [0.311, 0.283, 0.733, 0.396, 0.079, 0.349, 0.474, 0.263, 0.214, 0.161]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.0052 | Steps: 2 | Val loss: 1.8725 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0324 | Steps: 2 | Val loss: 4.2095 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9308 | Steps: 2 | Val loss: 2.3369 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.4402 | Steps: 2 | Val loss: 2.2711 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:48:40 (running for 00:15:00.75)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.005 |      0.312 |                   85 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.107 |      0.326 |                   78 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.058 |      0.296 |                   60 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.944 |      0.086 |                    8 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.32975746268656714
[2m[36m(func pid=129725)[0m top5: 0.8418843283582089
[2m[36m(func pid=129725)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=129725)[0m f1_macro: 0.31205096775162355
[2m[36m(func pid=129725)[0m f1_weighted: 0.35475427913500646
[2m[36m(func pid=129725)[0m f1_per_class: [0.293, 0.254, 0.579, 0.393, 0.066, 0.379, 0.404, 0.325, 0.176, 0.25]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m top1: 0.2994402985074627
[2m[36m(func pid=134827)[0m top5: 0.8381529850746269
[2m[36m(func pid=134827)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=134827)[0m f1_macro: 0.29929494136922935
[2m[36m(func pid=134827)[0m f1_weighted: 0.3198936804113702
[2m[36m(func pid=134827)[0m f1_per_class: [0.283, 0.316, 0.733, 0.363, 0.053, 0.359, 0.304, 0.237, 0.195, 0.15]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=146329)[0m top1: 0.14738805970149255
[2m[36m(func pid=146329)[0m top5: 0.5177238805970149
[2m[36m(func pid=146329)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=146329)[0m f1_macro: 0.08483753368060425
[2m[36m(func pid=146329)[0m f1_weighted: 0.1309971879972351
[2m[36m(func pid=146329)[0m f1_per_class: [0.056, 0.189, 0.0, 0.133, 0.021, 0.301, 0.071, 0.078, 0.0, 0.0]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33908582089552236
[2m[36m(func pid=129730)[0m top5: 0.8605410447761194
[2m[36m(func pid=129730)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=129730)[0m f1_macro: 0.3190833498201268
[2m[36m(func pid=129730)[0m f1_weighted: 0.37068844218459934
[2m[36m(func pid=129730)[0m f1_per_class: [0.308, 0.271, 0.71, 0.392, 0.078, 0.337, 0.475, 0.257, 0.207, 0.157]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.1530 | Steps: 2 | Val loss: 1.8687 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0496 | Steps: 2 | Val loss: 4.0993 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9061 | Steps: 2 | Val loss: 2.3309 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.1266 | Steps: 2 | Val loss: 2.2880 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 01:48:45 (running for 00:15:05.97)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.153 |      0.317 |                   86 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.44  |      0.319 |                   79 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.032 |      0.299 |                   61 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.931 |      0.085 |                    9 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129725)[0m top1: 0.32975746268656714
[2m[36m(func pid=129725)[0m top5: 0.8432835820895522
[2m[36m(func pid=129725)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=129725)[0m f1_macro: 0.3169269998541837
[2m[36m(func pid=129725)[0m f1_weighted: 0.3537242776696321
[2m[36m(func pid=129725)[0m f1_per_class: [0.32, 0.263, 0.579, 0.395, 0.064, 0.374, 0.391, 0.327, 0.199, 0.258]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=146329)[0m top1: 0.14925373134328357
[2m[36m(func pid=146329)[0m top5: 0.5191231343283582
[2m[36m(func pid=146329)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=146329)[0m f1_macro: 0.09599880305936999
[2m[36m(func pid=146329)[0m f1_weighted: 0.13780217685625865
[2m[36m(func pid=146329)[0m f1_per_class: [0.062, 0.179, 0.087, 0.153, 0.02, 0.304, 0.078, 0.077, 0.0, 0.0]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=134827)[0m top1: 0.30550373134328357
[2m[36m(func pid=134827)[0m top5: 0.8530783582089553
[2m[36m(func pid=134827)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=134827)[0m f1_macro: 0.3004289778520711
[2m[36m(func pid=134827)[0m f1_weighted: 0.32341327263438835
[2m[36m(func pid=134827)[0m f1_per_class: [0.308, 0.33, 0.733, 0.383, 0.054, 0.358, 0.293, 0.229, 0.166, 0.15]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.33675373134328357
[2m[36m(func pid=129730)[0m top5: 0.8563432835820896
[2m[36m(func pid=129730)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=129730)[0m f1_macro: 0.31871863337272716
[2m[36m(func pid=129730)[0m f1_weighted: 0.36889168512577947
[2m[36m(func pid=129730)[0m f1_per_class: [0.311, 0.259, 0.71, 0.396, 0.08, 0.335, 0.474, 0.25, 0.207, 0.166]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 1.0107 | Steps: 2 | Val loss: 1.8693 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0734 | Steps: 2 | Val loss: 3.9799 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9575 | Steps: 2 | Val loss: 2.3251 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.1158 | Steps: 2 | Val loss: 2.3193 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=129725)[0m top1: 0.333955223880597
[2m[36m(func pid=129725)[0m top5: 0.8432835820895522
[2m[36m(func pid=129725)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=129725)[0m f1_macro: 0.32308587899292096
[2m[36m(func pid=129725)[0m f1_weighted: 0.35884090168143895
[2m[36m(func pid=129725)[0m f1_per_class: [0.337, 0.26, 0.579, 0.404, 0.06, 0.397, 0.388, 0.333, 0.213, 0.26]
[2m[36m(func pid=129725)[0m 
== Status ==
Current time: 2024-01-07 01:48:50 (running for 00:15:11.37)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.011 |      0.323 |                   87 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.127 |      0.319 |                   80 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.05  |      0.3   |                   62 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.957 |      0.104 |                   11 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=146329)[0m top1: 0.1515858208955224
[2m[36m(func pid=146329)[0m top5: 0.5284514925373134
[2m[36m(func pid=146329)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=146329)[0m f1_macro: 0.10382762261486216
[2m[36m(func pid=146329)[0m f1_weighted: 0.14268123802130123
[2m[36m(func pid=146329)[0m f1_per_class: [0.058, 0.172, 0.122, 0.158, 0.025, 0.305, 0.087, 0.099, 0.011, 0.0]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=134827)[0m top1: 0.3138992537313433
[2m[36m(func pid=134827)[0m top5: 0.8647388059701493
[2m[36m(func pid=134827)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=134827)[0m f1_macro: 0.30449495440907076
[2m[36m(func pid=134827)[0m f1_weighted: 0.3355789819249167
[2m[36m(func pid=134827)[0m f1_per_class: [0.321, 0.327, 0.733, 0.408, 0.054, 0.359, 0.312, 0.223, 0.16, 0.147]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m top1: 0.32882462686567165
[2m[36m(func pid=129730)[0m top5: 0.8502798507462687
[2m[36m(func pid=129730)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=129730)[0m f1_macro: 0.3159456495962797
[2m[36m(func pid=129730)[0m f1_weighted: 0.3614358208141028
[2m[36m(func pid=129730)[0m f1_per_class: [0.304, 0.251, 0.71, 0.382, 0.073, 0.331, 0.466, 0.254, 0.217, 0.172]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.9196 | Steps: 2 | Val loss: 1.8675 | Batch size: 32 | lr: 0.001 | Duration: 2.58s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9039 | Steps: 2 | Val loss: 2.3183 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0286 | Steps: 2 | Val loss: 3.8475 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=129725)[0m top1: 0.33488805970149255
[2m[36m(func pid=129725)[0m top5: 0.84375
[2m[36m(func pid=129725)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=129725)[0m f1_macro: 0.32344106693571717
[2m[36m(func pid=129725)[0m f1_weighted: 0.3607845689331873
[2m[36m(func pid=129725)[0m f1_per_class: [0.329, 0.257, 0.579, 0.404, 0.06, 0.401, 0.395, 0.332, 0.224, 0.254]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.1453 | Steps: 2 | Val loss: 2.3200 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 01:48:55 (running for 00:15:16.54)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.92  |      0.323 |                   88 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.116 |      0.316 |                   81 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.073 |      0.304 |                   63 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.904 |      0.104 |                   12 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=146329)[0m top1: 0.1525186567164179
[2m[36m(func pid=146329)[0m top5: 0.5396455223880597
[2m[36m(func pid=146329)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=146329)[0m f1_macro: 0.10421404143535928
[2m[36m(func pid=146329)[0m f1_weighted: 0.1467857594339553
[2m[36m(func pid=146329)[0m f1_per_class: [0.056, 0.168, 0.118, 0.171, 0.026, 0.303, 0.093, 0.098, 0.011, 0.0]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=134827)[0m top1: 0.33115671641791045
[2m[36m(func pid=134827)[0m top5: 0.8614738805970149
[2m[36m(func pid=134827)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=134827)[0m f1_macro: 0.311893467277768
[2m[36m(func pid=134827)[0m f1_weighted: 0.3571590706902669
[2m[36m(func pid=134827)[0m f1_per_class: [0.344, 0.327, 0.71, 0.439, 0.053, 0.354, 0.358, 0.212, 0.165, 0.157]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 1.2324 | Steps: 2 | Val loss: 1.8694 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=129730)[0m top1: 0.3316231343283582
[2m[36m(func pid=129730)[0m top5: 0.8428171641791045
[2m[36m(func pid=129730)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=129730)[0m f1_macro: 0.31761901716356633
[2m[36m(func pid=129730)[0m f1_weighted: 0.3648876555757516
[2m[36m(func pid=129730)[0m f1_per_class: [0.298, 0.248, 0.71, 0.398, 0.071, 0.336, 0.463, 0.253, 0.222, 0.179]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.9079 | Steps: 2 | Val loss: 2.3114 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0151 | Steps: 2 | Val loss: 3.7967 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=129725)[0m top1: 0.3302238805970149
[2m[36m(func pid=129725)[0m top5: 0.8451492537313433
[2m[36m(func pid=129725)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=129725)[0m f1_macro: 0.31634158680080676
[2m[36m(func pid=129725)[0m f1_weighted: 0.3551108738116658
[2m[36m(func pid=129725)[0m f1_per_class: [0.314, 0.268, 0.55, 0.403, 0.06, 0.4, 0.375, 0.328, 0.205, 0.26]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.1426 | Steps: 2 | Val loss: 2.3470 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=146329)[0m top1: 0.15205223880597016
[2m[36m(func pid=146329)[0m top5: 0.5522388059701493
[2m[36m(func pid=146329)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=146329)[0m f1_macro: 0.10410356661562029
[2m[36m(func pid=146329)[0m f1_weighted: 0.14767902824541831
[2m[36m(func pid=146329)[0m f1_per_class: [0.056, 0.16, 0.102, 0.172, 0.026, 0.304, 0.097, 0.103, 0.021, 0.0]
[2m[36m(func pid=146329)[0m 
== Status ==
Current time: 2024-01-07 01:49:01 (running for 00:15:21.82)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.232 |      0.316 |                   89 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.145 |      0.318 |                   82 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.029 |      0.312 |                   64 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.908 |      0.104 |                   13 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=134827)[0m top1: 0.333955223880597
[2m[36m(func pid=134827)[0m top5: 0.855410447761194
[2m[36m(func pid=134827)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=134827)[0m f1_macro: 0.31105790952309426
[2m[36m(func pid=134827)[0m f1_weighted: 0.3617520541565906
[2m[36m(func pid=134827)[0m f1_per_class: [0.336, 0.315, 0.71, 0.453, 0.05, 0.33, 0.377, 0.206, 0.169, 0.164]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.9984 | Steps: 2 | Val loss: 1.8655 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=129730)[0m top1: 0.324160447761194
[2m[36m(func pid=129730)[0m top5: 0.8353544776119403
[2m[36m(func pid=129730)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=129730)[0m f1_macro: 0.3136337051819857
[2m[36m(func pid=129730)[0m f1_weighted: 0.3582973940523001
[2m[36m(func pid=129730)[0m f1_per_class: [0.294, 0.237, 0.71, 0.396, 0.074, 0.335, 0.451, 0.245, 0.221, 0.175]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.9205 | Steps: 2 | Val loss: 2.3057 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0188 | Steps: 2 | Val loss: 3.7930 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=129725)[0m top1: 0.3344216417910448
[2m[36m(func pid=129725)[0m top5: 0.8484141791044776
[2m[36m(func pid=129725)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=129725)[0m f1_macro: 0.32063508651049455
[2m[36m(func pid=129725)[0m f1_weighted: 0.3596109022346538
[2m[36m(func pid=129725)[0m f1_per_class: [0.321, 0.267, 0.55, 0.41, 0.06, 0.406, 0.38, 0.329, 0.219, 0.264]
[2m[36m(func pid=129725)[0m 
== Status ==
Current time: 2024-01-07 01:49:06 (running for 00:15:26.98)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.998 |      0.321 |                   90 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.143 |      0.314 |                   83 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.015 |      0.311 |                   65 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.921 |      0.109 |                   14 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=146329)[0m top1: 0.1571828358208955
[2m[36m(func pid=146329)[0m top5: 0.5611007462686567
[2m[36m(func pid=146329)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=146329)[0m f1_macro: 0.1086659599587961
[2m[36m(func pid=146329)[0m f1_weighted: 0.1524912681259219
[2m[36m(func pid=146329)[0m f1_per_class: [0.065, 0.161, 0.103, 0.177, 0.027, 0.318, 0.099, 0.115, 0.021, 0.0]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0839 | Steps: 2 | Val loss: 2.3555 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=134827)[0m top1: 0.33908582089552236
[2m[36m(func pid=134827)[0m top5: 0.8502798507462687
[2m[36m(func pid=134827)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=134827)[0m f1_macro: 0.3153122762436162
[2m[36m(func pid=134827)[0m f1_weighted: 0.36670478587293864
[2m[36m(func pid=134827)[0m f1_per_class: [0.342, 0.291, 0.71, 0.459, 0.061, 0.334, 0.397, 0.214, 0.186, 0.161]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.9656 | Steps: 2 | Val loss: 1.8703 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=129730)[0m top1: 0.3260261194029851
[2m[36m(func pid=129730)[0m top5: 0.8339552238805971
[2m[36m(func pid=129730)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=129730)[0m f1_macro: 0.31713059591941634
[2m[36m(func pid=129730)[0m f1_weighted: 0.36081227339401517
[2m[36m(func pid=129730)[0m f1_per_class: [0.29, 0.235, 0.733, 0.4, 0.074, 0.334, 0.455, 0.252, 0.226, 0.172]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8506 | Steps: 2 | Val loss: 2.3042 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0118 | Steps: 2 | Val loss: 3.8232 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=129725)[0m top1: 0.3358208955223881
[2m[36m(func pid=129725)[0m top5: 0.84375
[2m[36m(func pid=129725)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=129725)[0m f1_macro: 0.3207109544261417
[2m[36m(func pid=129725)[0m f1_weighted: 0.3603481638082057
[2m[36m(func pid=129725)[0m f1_per_class: [0.332, 0.267, 0.537, 0.409, 0.061, 0.404, 0.382, 0.333, 0.221, 0.261]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=146329)[0m top1: 0.15671641791044777
[2m[36m(func pid=146329)[0m top5: 0.5606343283582089
[2m[36m(func pid=146329)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=146329)[0m f1_macro: 0.10774243423975283
[2m[36m(func pid=146329)[0m f1_weighted: 0.1554905530644561
[2m[36m(func pid=146329)[0m f1_per_class: [0.058, 0.154, 0.09, 0.178, 0.027, 0.326, 0.11, 0.115, 0.02, 0.0]
[2m[36m(func pid=146329)[0m 
== Status ==
Current time: 2024-01-07 01:49:11 (running for 00:15:32.25)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.966 |      0.321 |                   91 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.084 |      0.317 |                   84 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.019 |      0.315 |                   66 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.851 |      0.108 |                   15 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.1251 | Steps: 2 | Val loss: 2.3633 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=134827)[0m top1: 0.33722014925373134
[2m[36m(func pid=134827)[0m top5: 0.8428171641791045
[2m[36m(func pid=134827)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=134827)[0m f1_macro: 0.31430989201597037
[2m[36m(func pid=134827)[0m f1_weighted: 0.36437171161271664
[2m[36m(func pid=134827)[0m f1_per_class: [0.335, 0.282, 0.71, 0.456, 0.058, 0.324, 0.399, 0.22, 0.195, 0.165]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 1.0185 | Steps: 2 | Val loss: 1.8679 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8540 | Steps: 2 | Val loss: 2.2949 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=129730)[0m top1: 0.32649253731343286
[2m[36m(func pid=129730)[0m top5: 0.8348880597014925
[2m[36m(func pid=129730)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=129730)[0m f1_macro: 0.31791763081588814
[2m[36m(func pid=129730)[0m f1_weighted: 0.3599592336298682
[2m[36m(func pid=129730)[0m f1_per_class: [0.288, 0.228, 0.733, 0.404, 0.089, 0.33, 0.453, 0.261, 0.221, 0.173]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.3344216417910448
[2m[36m(func pid=129725)[0m top5: 0.8442164179104478
[2m[36m(func pid=129725)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=129725)[0m f1_macro: 0.3207849866319258
[2m[36m(func pid=129725)[0m f1_weighted: 0.36033524314662085
[2m[36m(func pid=129725)[0m f1_per_class: [0.326, 0.261, 0.564, 0.41, 0.06, 0.399, 0.388, 0.328, 0.216, 0.254]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0239 | Steps: 2 | Val loss: 3.8916 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 01:49:16 (running for 00:15:37.43)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.018 |      0.321 |                   92 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.125 |      0.318 |                   85 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.012 |      0.314 |                   67 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.854 |      0.11  |                   16 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=146329)[0m top1: 0.15951492537313433
[2m[36m(func pid=146329)[0m top5: 0.5792910447761194
[2m[36m(func pid=146329)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=146329)[0m f1_macro: 0.10980887192489863
[2m[36m(func pid=146329)[0m f1_weighted: 0.15963333638713204
[2m[36m(func pid=146329)[0m f1_per_class: [0.059, 0.163, 0.079, 0.184, 0.026, 0.328, 0.11, 0.128, 0.021, 0.0]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0911 | Steps: 2 | Val loss: 2.3813 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=134827)[0m top1: 0.3353544776119403
[2m[36m(func pid=134827)[0m top5: 0.8367537313432836
[2m[36m(func pid=134827)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=134827)[0m f1_macro: 0.3112517061346105
[2m[36m(func pid=134827)[0m f1_weighted: 0.3648873556065515
[2m[36m(func pid=134827)[0m f1_per_class: [0.298, 0.261, 0.71, 0.454, 0.054, 0.323, 0.414, 0.228, 0.211, 0.159]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.9449 | Steps: 2 | Val loss: 1.8648 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8361 | Steps: 2 | Val loss: 2.2893 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=129730)[0m top1: 0.3269589552238806
[2m[36m(func pid=129730)[0m top5: 0.8330223880597015
[2m[36m(func pid=129730)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=129730)[0m f1_macro: 0.3172088674705964
[2m[36m(func pid=129730)[0m f1_weighted: 0.3616306349543133
[2m[36m(func pid=129730)[0m f1_per_class: [0.272, 0.232, 0.733, 0.403, 0.088, 0.33, 0.457, 0.269, 0.224, 0.165]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m top1: 0.3362873134328358
[2m[36m(func pid=129725)[0m top5: 0.8460820895522388
[2m[36m(func pid=129725)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=129725)[0m f1_macro: 0.32259011979111973
[2m[36m(func pid=129725)[0m f1_weighted: 0.3620949680631575
[2m[36m(func pid=129725)[0m f1_per_class: [0.333, 0.263, 0.564, 0.409, 0.061, 0.396, 0.393, 0.333, 0.219, 0.254]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.1234 | Steps: 2 | Val loss: 3.9916 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=146329)[0m top1: 0.16324626865671643
[2m[36m(func pid=146329)[0m top5: 0.5890858208955224
[2m[36m(func pid=146329)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=146329)[0m f1_macro: 0.11375799041478814
[2m[36m(func pid=146329)[0m f1_weighted: 0.16809418663855602
[2m[36m(func pid=146329)[0m f1_per_class: [0.057, 0.164, 0.093, 0.207, 0.026, 0.324, 0.117, 0.127, 0.021, 0.0]
[2m[36m(func pid=146329)[0m 
== Status ==
Current time: 2024-01-07 01:49:21 (running for 00:15:42.69)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.945 |      0.323 |                   93 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.091 |      0.317 |                   86 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.024 |      0.311 |                   68 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.836 |      0.114 |                   17 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=134827)[0m top1: 0.32742537313432835
[2m[36m(func pid=134827)[0m top5: 0.8250932835820896
[2m[36m(func pid=134827)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=134827)[0m f1_macro: 0.30843363898940507
[2m[36m(func pid=134827)[0m f1_weighted: 0.3591245836556452
[2m[36m(func pid=134827)[0m f1_per_class: [0.252, 0.251, 0.733, 0.438, 0.055, 0.326, 0.416, 0.233, 0.215, 0.165]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.1047 | Steps: 2 | Val loss: 2.3728 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 1.0026 | Steps: 2 | Val loss: 1.8669 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8419 | Steps: 2 | Val loss: 2.2863 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=129725)[0m top1: 0.33488805970149255
[2m[36m(func pid=129725)[0m top5: 0.8442164179104478
[2m[36m(func pid=129725)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=129725)[0m f1_macro: 0.31941828122218735
[2m[36m(func pid=129725)[0m f1_weighted: 0.3601156017127853
[2m[36m(func pid=129725)[0m f1_per_class: [0.325, 0.263, 0.55, 0.404, 0.062, 0.39, 0.395, 0.337, 0.208, 0.26]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.32882462686567165
[2m[36m(func pid=129730)[0m top5: 0.8339552238805971
[2m[36m(func pid=129730)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=129730)[0m f1_macro: 0.3187904262020285
[2m[36m(func pid=129730)[0m f1_weighted: 0.363286974828091
[2m[36m(func pid=129730)[0m f1_per_class: [0.272, 0.227, 0.759, 0.41, 0.091, 0.329, 0.462, 0.255, 0.212, 0.171]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0082 | Steps: 2 | Val loss: 4.0264 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:49:27 (running for 00:15:47.86)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  1.003 |      0.319 |                   94 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.105 |      0.319 |                   87 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.123 |      0.308 |                   69 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.842 |      0.117 |                   18 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=146329)[0m top1: 0.166044776119403
[2m[36m(func pid=146329)[0m top5: 0.5951492537313433
[2m[36m(func pid=146329)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=146329)[0m f1_macro: 0.11730532186525908
[2m[36m(func pid=146329)[0m f1_weighted: 0.1722196263317353
[2m[36m(func pid=146329)[0m f1_per_class: [0.076, 0.159, 0.085, 0.209, 0.026, 0.33, 0.126, 0.14, 0.021, 0.0]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.8621 | Steps: 2 | Val loss: 1.8658 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=134827)[0m top1: 0.32136194029850745
[2m[36m(func pid=134827)[0m top5: 0.8222947761194029
[2m[36m(func pid=134827)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=134827)[0m f1_macro: 0.30245655070389055
[2m[36m(func pid=134827)[0m f1_weighted: 0.3545803168436986
[2m[36m(func pid=134827)[0m f1_per_class: [0.229, 0.249, 0.733, 0.415, 0.062, 0.315, 0.433, 0.217, 0.204, 0.167]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.1073 | Steps: 2 | Val loss: 2.3697 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=129725)[0m top1: 0.332089552238806
[2m[36m(func pid=129725)[0m top5: 0.847481343283582
[2m[36m(func pid=129725)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=129725)[0m f1_macro: 0.31939940472724626
[2m[36m(func pid=129725)[0m f1_weighted: 0.3569779734184926
[2m[36m(func pid=129725)[0m f1_per_class: [0.327, 0.263, 0.579, 0.403, 0.061, 0.392, 0.389, 0.316, 0.2, 0.264]
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7919 | Steps: 2 | Val loss: 2.2800 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m top1: 0.3306902985074627
[2m[36m(func pid=129730)[0m top5: 0.8372201492537313
[2m[36m(func pid=129730)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=129730)[0m f1_macro: 0.31653375280469576
[2m[36m(func pid=129730)[0m f1_weighted: 0.3647915566393646
[2m[36m(func pid=129730)[0m f1_per_class: [0.264, 0.225, 0.759, 0.42, 0.086, 0.334, 0.461, 0.238, 0.212, 0.167]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0081 | Steps: 2 | Val loss: 4.0482 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 01:49:32 (running for 00:15:53.13)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.862 |      0.319 |                   95 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.107 |      0.317 |                   88 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.008 |      0.302 |                   70 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.792 |      0.117 |                   19 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=146329)[0m top1: 0.16744402985074627
[2m[36m(func pid=146329)[0m top5: 0.6040111940298507
[2m[36m(func pid=146329)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=146329)[0m f1_macro: 0.11728499943524691
[2m[36m(func pid=146329)[0m f1_weighted: 0.17638670208107285
[2m[36m(func pid=146329)[0m f1_per_class: [0.077, 0.157, 0.075, 0.219, 0.026, 0.331, 0.134, 0.133, 0.02, 0.0]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.8661 | Steps: 2 | Val loss: 1.8677 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=134827)[0m top1: 0.3180970149253731
[2m[36m(func pid=134827)[0m top5: 0.8232276119402985
[2m[36m(func pid=134827)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=134827)[0m f1_macro: 0.30483824930588
[2m[36m(func pid=134827)[0m f1_weighted: 0.3511202579082264
[2m[36m(func pid=134827)[0m f1_per_class: [0.221, 0.241, 0.759, 0.394, 0.067, 0.311, 0.444, 0.22, 0.218, 0.172]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.1797 | Steps: 2 | Val loss: 2.3723 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=129725)[0m top1: 0.3302238805970149
[2m[36m(func pid=129725)[0m top5: 0.8470149253731343
[2m[36m(func pid=129725)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=129725)[0m f1_macro: 0.3198265819807471
[2m[36m(func pid=129725)[0m f1_weighted: 0.35551856874878945
[2m[36m(func pid=129725)[0m f1_per_class: [0.325, 0.266, 0.595, 0.4, 0.06, 0.39, 0.386, 0.307, 0.204, 0.264]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7991 | Steps: 2 | Val loss: 2.2742 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=129730)[0m top1: 0.32882462686567165
[2m[36m(func pid=129730)[0m top5: 0.840018656716418
[2m[36m(func pid=129730)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=129730)[0m f1_macro: 0.3162943430236977
[2m[36m(func pid=129730)[0m f1_weighted: 0.3623496791988549
[2m[36m(func pid=129730)[0m f1_per_class: [0.268, 0.23, 0.759, 0.408, 0.087, 0.333, 0.461, 0.245, 0.197, 0.174]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0071 | Steps: 2 | Val loss: 4.0802 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:49:37 (running for 00:15:58.35)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.866 |      0.32  |                   96 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.18  |      0.316 |                   89 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.008 |      0.305 |                   71 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.799 |      0.122 |                   20 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=146329)[0m top1: 0.17257462686567165
[2m[36m(func pid=146329)[0m top5: 0.617070895522388
[2m[36m(func pid=146329)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=146329)[0m f1_macro: 0.12204056252622719
[2m[36m(func pid=146329)[0m f1_weighted: 0.18496821516126538
[2m[36m(func pid=146329)[0m f1_per_class: [0.074, 0.158, 0.094, 0.229, 0.026, 0.337, 0.15, 0.132, 0.019, 0.0]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.9250 | Steps: 2 | Val loss: 1.8705 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=134827)[0m top1: 0.31669776119402987
[2m[36m(func pid=134827)[0m top5: 0.8143656716417911
[2m[36m(func pid=134827)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=134827)[0m f1_macro: 0.3025732278957928
[2m[36m(func pid=134827)[0m f1_weighted: 0.34938424150058306
[2m[36m(func pid=134827)[0m f1_per_class: [0.211, 0.245, 0.759, 0.384, 0.074, 0.286, 0.456, 0.221, 0.218, 0.172]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0991 | Steps: 2 | Val loss: 2.3806 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=129725)[0m top1: 0.32882462686567165
[2m[36m(func pid=129725)[0m top5: 0.8484141791044776
[2m[36m(func pid=129725)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=129725)[0m f1_macro: 0.31872016012794147
[2m[36m(func pid=129725)[0m f1_weighted: 0.35394476960553073
[2m[36m(func pid=129725)[0m f1_per_class: [0.317, 0.264, 0.595, 0.4, 0.068, 0.388, 0.385, 0.31, 0.2, 0.263]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7850 | Steps: 2 | Val loss: 2.2679 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=129730)[0m top1: 0.32975746268656714
[2m[36m(func pid=129730)[0m top5: 0.840018656716418
[2m[36m(func pid=129730)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=129730)[0m f1_macro: 0.31954438948435204
[2m[36m(func pid=129730)[0m f1_weighted: 0.36304577387915526
[2m[36m(func pid=129730)[0m f1_per_class: [0.265, 0.231, 0.759, 0.403, 0.091, 0.329, 0.464, 0.251, 0.232, 0.17]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0084 | Steps: 2 | Val loss: 4.1814 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=146329)[0m top1: 0.177705223880597
[2m[36m(func pid=146329)[0m top5: 0.6291977611940298
[2m[36m(func pid=146329)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=146329)[0m f1_macro: 0.1296514633688599
[2m[36m(func pid=146329)[0m f1_weighted: 0.1915147675181676
[2m[36m(func pid=146329)[0m f1_per_class: [0.103, 0.148, 0.116, 0.241, 0.027, 0.343, 0.16, 0.139, 0.019, 0.0]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.8276 | Steps: 2 | Val loss: 1.8639 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 01:49:44 (running for 00:16:05.06)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.925 |      0.319 |                   97 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.099 |      0.32  |                   90 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.008 |      0.295 |                   73 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.785 |      0.13  |                   21 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=134827)[0m top1: 0.31156716417910446
[2m[36m(func pid=134827)[0m top5: 0.808768656716418
[2m[36m(func pid=134827)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=134827)[0m f1_macro: 0.2950360820750151
[2m[36m(func pid=134827)[0m f1_weighted: 0.34605490133561445
[2m[36m(func pid=134827)[0m f1_per_class: [0.192, 0.239, 0.733, 0.374, 0.075, 0.279, 0.464, 0.216, 0.211, 0.167]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.1120 | Steps: 2 | Val loss: 2.3955 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=129725)[0m top1: 0.33302238805970147
[2m[36m(func pid=129725)[0m top5: 0.8512126865671642
[2m[36m(func pid=129725)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=129725)[0m f1_macro: 0.32146340558999964
[2m[36m(func pid=129725)[0m f1_weighted: 0.3580514982087151
[2m[36m(func pid=129725)[0m f1_per_class: [0.32, 0.268, 0.595, 0.404, 0.068, 0.382, 0.392, 0.315, 0.204, 0.267]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7519 | Steps: 2 | Val loss: 2.2616 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=129730)[0m top1: 0.32649253731343286
[2m[36m(func pid=129730)[0m top5: 0.840018656716418
[2m[36m(func pid=129730)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=129730)[0m f1_macro: 0.31892087838345345
[2m[36m(func pid=129730)[0m f1_weighted: 0.3601525600346112
[2m[36m(func pid=129730)[0m f1_per_class: [0.27, 0.232, 0.759, 0.398, 0.085, 0.329, 0.458, 0.246, 0.244, 0.169]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0232 | Steps: 2 | Val loss: 4.1917 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.9440 | Steps: 2 | Val loss: 1.8653 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=146329)[0m top1: 0.1791044776119403
[2m[36m(func pid=146329)[0m top5: 0.6385261194029851
[2m[36m(func pid=146329)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=146329)[0m f1_macro: 0.12951185728089096
[2m[36m(func pid=146329)[0m f1_weighted: 0.19547544058623736
[2m[36m(func pid=146329)[0m f1_per_class: [0.09, 0.146, 0.101, 0.25, 0.035, 0.346, 0.166, 0.141, 0.019, 0.0]
[2m[36m(func pid=146329)[0m 
== Status ==
Current time: 2024-01-07 01:49:49 (running for 00:16:10.47)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.828 |      0.321 |                   98 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.112 |      0.319 |                   91 |
| train_66d79_00007 | RUNNING    | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.023 |      0.293 |                   74 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.752 |      0.13  |                   22 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=134827)[0m top1: 0.31156716417910446
[2m[36m(func pid=134827)[0m top5: 0.8092350746268657
[2m[36m(func pid=134827)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=134827)[0m f1_macro: 0.2930850812484351
[2m[36m(func pid=134827)[0m f1_weighted: 0.3439847149605513
[2m[36m(func pid=134827)[0m f1_per_class: [0.192, 0.231, 0.733, 0.353, 0.075, 0.273, 0.484, 0.214, 0.207, 0.167]
[2m[36m(func pid=134827)[0m 
[2m[36m(func pid=129725)[0m top1: 0.3302238805970149
[2m[36m(func pid=129725)[0m top5: 0.8512126865671642
[2m[36m(func pid=129725)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=129725)[0m f1_macro: 0.3184506871126899
[2m[36m(func pid=129725)[0m f1_weighted: 0.3554145858589954
[2m[36m(func pid=129725)[0m f1_per_class: [0.305, 0.264, 0.595, 0.404, 0.069, 0.379, 0.388, 0.316, 0.2, 0.264]
[2m[36m(func pid=129725)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.1279 | Steps: 2 | Val loss: 2.3928 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7202 | Steps: 2 | Val loss: 2.2589 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=134827)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0037 | Steps: 2 | Val loss: 4.2004 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=129730)[0m top1: 0.3246268656716418
[2m[36m(func pid=129730)[0m top5: 0.8418843283582089
[2m[36m(func pid=129730)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=129730)[0m f1_macro: 0.3183838127477109
[2m[36m(func pid=129730)[0m f1_weighted: 0.3588807589885804
[2m[36m(func pid=129730)[0m f1_per_class: [0.269, 0.233, 0.786, 0.396, 0.08, 0.33, 0.457, 0.242, 0.226, 0.165]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=129725)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.8668 | Steps: 2 | Val loss: 1.8637 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=146329)[0m top1: 0.18003731343283583
[2m[36m(func pid=146329)[0m top5: 0.644589552238806
[2m[36m(func pid=146329)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=146329)[0m f1_macro: 0.13337183191045437
[2m[36m(func pid=146329)[0m f1_weighted: 0.19750219642842964
[2m[36m(func pid=146329)[0m f1_per_class: [0.098, 0.154, 0.113, 0.243, 0.036, 0.342, 0.173, 0.147, 0.028, 0.0]
[2m[36m(func pid=146329)[0m 
== Status ==
Current time: 2024-01-07 01:49:55 (running for 00:16:15.76)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.3165
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 3 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00005 | RUNNING    | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.944 |      0.318 |                   99 |
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.128 |      0.318 |                   92 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.72  |      0.133 |                   23 |
| train_66d79_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=134827)[0m top1: 0.3199626865671642
[2m[36m(func pid=134827)[0m top5: 0.8111007462686567
[2m[36m(func pid=134827)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=134827)[0m f1_macro: 0.2982434008183294
[2m[36m(func pid=134827)[0m f1_weighted: 0.3518318282504953
[2m[36m(func pid=134827)[0m f1_per_class: [0.186, 0.23, 0.759, 0.355, 0.076, 0.278, 0.507, 0.216, 0.204, 0.171]
[2m[36m(func pid=129725)[0m top1: 0.3306902985074627
[2m[36m(func pid=129725)[0m top5: 0.8526119402985075
[2m[36m(func pid=129725)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=129725)[0m f1_macro: 0.31932967951882396
[2m[36m(func pid=129725)[0m f1_weighted: 0.356215084184624
[2m[36m(func pid=129725)[0m f1_per_class: [0.323, 0.263, 0.595, 0.404, 0.067, 0.379, 0.39, 0.317, 0.195, 0.26]
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.1030 | Steps: 2 | Val loss: 2.4010 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7396 | Steps: 2 | Val loss: 2.2486 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=129730)[0m top1: 0.3208955223880597
[2m[36m(func pid=129730)[0m top5: 0.8386194029850746
[2m[36m(func pid=129730)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=129730)[0m f1_macro: 0.3155189011639172
[2m[36m(func pid=129730)[0m f1_weighted: 0.354836134278821
[2m[36m(func pid=129730)[0m f1_per_class: [0.26, 0.233, 0.786, 0.392, 0.082, 0.323, 0.451, 0.241, 0.218, 0.168]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=146329)[0m top1: 0.18889925373134328
[2m[36m(func pid=146329)[0m top5: 0.6553171641791045
[2m[36m(func pid=146329)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=146329)[0m f1_macro: 0.14002017648048753
[2m[36m(func pid=146329)[0m f1_weighted: 0.20815870462398586
[2m[36m(func pid=146329)[0m f1_per_class: [0.119, 0.16, 0.106, 0.259, 0.038, 0.339, 0.188, 0.164, 0.028, 0.0]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.1536 | Steps: 2 | Val loss: 2.3969 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7047 | Steps: 2 | Val loss: 2.2433 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=129730)[0m top1: 0.32322761194029853
[2m[36m(func pid=129730)[0m top5: 0.8423507462686567
[2m[36m(func pid=129730)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=129730)[0m f1_macro: 0.31277293493108865
[2m[36m(func pid=129730)[0m f1_weighted: 0.3563352202956348
[2m[36m(func pid=129730)[0m f1_per_class: [0.269, 0.239, 0.733, 0.385, 0.086, 0.338, 0.454, 0.248, 0.208, 0.167]
[2m[36m(func pid=146329)[0m top1: 0.18889925373134328
[2m[36m(func pid=146329)[0m top5: 0.6618470149253731
[2m[36m(func pid=146329)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=146329)[0m f1_macro: 0.1413191158302126== Status ==
Current time: 2024-01-07 01:50:03 (running for 00:16:24.11)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.103 |      0.316 |                   93 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.74  |      0.14  |                   24 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=146329)[0m f1_weighted: 0.20835716196905402
[2m[36m(func pid=146329)[0m f1_per_class: [0.129, 0.171, 0.11, 0.254, 0.03, 0.333, 0.187, 0.17, 0.03, 0.0]
[2m[36m(func pid=151878)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151878)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=151878)[0m Configuration completed!
[2m[36m(func pid=151878)[0m New optimizer parameters:
[2m[36m(func pid=151878)[0m SGD (
[2m[36m(func pid=151878)[0m Parameter Group 0
[2m[36m(func pid=151878)[0m     dampening: 0
[2m[36m(func pid=151878)[0m     differentiable: False
[2m[36m(func pid=151878)[0m     foreach: None
[2m[36m(func pid=151878)[0m     lr: 0.01
[2m[36m(func pid=151878)[0m     maximize: False
[2m[36m(func pid=151878)[0m     momentum: 0.99
[2m[36m(func pid=151878)[0m     nesterov: False
[2m[36m(func pid=151878)[0m     weight_decay: 0.0001
[2m[36m(func pid=151878)[0m )
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151876)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=151876)[0m Configuration completed!
[2m[36m(func pid=151876)[0m New optimizer parameters:
[2m[36m(func pid=151876)[0m SGD (
[2m[36m(func pid=151876)[0m Parameter Group 0
[2m[36m(func pid=151876)[0m     dampening: 0
[2m[36m(func pid=151876)[0m     differentiable: False
[2m[36m(func pid=151876)[0m     foreach: None
[2m[36m(func pid=151876)[0m     lr: 0.001
[2m[36m(func pid=151876)[0m     maximize: False
[2m[36m(func pid=151876)[0m     momentum: 0.99
[2m[36m(func pid=151876)[0m     nesterov: False
[2m[36m(func pid=151876)[0m     weight_decay: 0.0001
[2m[36m(func pid=151876)[0m )
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.1437 | Steps: 2 | Val loss: 2.3823 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.6582 | Steps: 2 | Val loss: 2.2388 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9832 | Steps: 2 | Val loss: 2.3187 | Batch size: 32 | lr: 0.001 | Duration: 4.46s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0035 | Steps: 2 | Val loss: 2.3148 | Batch size: 32 | lr: 0.01 | Duration: 4.69s
== Status ==
Current time: 2024-01-07 01:50:08 (running for 00:16:29.41)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.154 |      0.313 |                   94 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.705 |      0.141 |                   25 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129730)[0m top1: 0.32742537313432835
[2m[36m(func pid=129730)[0m top5: 0.8446828358208955
[2m[36m(func pid=129730)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=129730)[0m f1_macro: 0.31764675251095825
[2m[36m(func pid=129730)[0m f1_weighted: 0.35960292457757026
[2m[36m(func pid=129730)[0m f1_per_class: [0.27, 0.251, 0.759, 0.404, 0.088, 0.343, 0.438, 0.239, 0.223, 0.161]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=146329)[0m top1: 0.19496268656716417
[2m[36m(func pid=146329)[0m top5: 0.6651119402985075
[2m[36m(func pid=146329)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=146329)[0m f1_macro: 0.14855796025472773
[2m[36m(func pid=146329)[0m f1_weighted: 0.2171227685511825
[2m[36m(func pid=146329)[0m f1_per_class: [0.131, 0.163, 0.108, 0.258, 0.031, 0.346, 0.208, 0.182, 0.03, 0.028]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151876)[0m top1: 0.1767723880597015
[2m[36m(func pid=151876)[0m top5: 0.53125
[2m[36m(func pid=151876)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=151876)[0m f1_macro: 0.11840934520317473
[2m[36m(func pid=151876)[0m f1_weighted: 0.1259778599915281
[2m[36m(func pid=151876)[0m f1_per_class: [0.312, 0.348, 0.0, 0.092, 0.0, 0.214, 0.024, 0.0, 0.0, 0.194]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m top1: 0.18470149253731344
[2m[36m(func pid=151878)[0m top5: 0.5326492537313433
[2m[36m(func pid=151878)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=151878)[0m f1_macro: 0.13201090484597883
[2m[36m(func pid=151878)[0m f1_weighted: 0.1349234113842101
[2m[36m(func pid=151878)[0m f1_per_class: [0.379, 0.35, 0.0, 0.111, 0.0, 0.203, 0.027, 0.039, 0.0, 0.211]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0966 | Steps: 2 | Val loss: 2.3781 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.6980 | Steps: 2 | Val loss: 2.2341 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9723 | Steps: 2 | Val loss: 2.3187 | Batch size: 32 | lr: 0.001 | Duration: 2.60s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8662 | Steps: 2 | Val loss: 2.2659 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=146329)[0m top1: 0.1958955223880597
[2m[36m(func pid=146329)[0m top5: 0.6697761194029851
[2m[36m(func pid=146329)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=146329)[0m f1_macro: 0.1526164702940367
[2m[36m(func pid=146329)[0m f1_weighted: 0.21904540997752692
[2m[36m(func pid=146329)[0m f1_per_class: [0.14, 0.166, 0.105, 0.26, 0.03, 0.339, 0.211, 0.188, 0.029, 0.057]
[2m[36m(func pid=146329)[0m 
== Status ==
Current time: 2024-01-07 01:50:14 (running for 00:16:35.18)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.144 |      0.318 |                   95 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.698 |      0.153 |                   27 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  2.983 |      0.118 |                    1 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  3.003 |      0.132 |                    1 |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129730)[0m top1: 0.32369402985074625
[2m[36m(func pid=129730)[0m top5: 0.8451492537313433
[2m[36m(func pid=129730)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=129730)[0m f1_macro: 0.3168645418690349
[2m[36m(func pid=129730)[0m f1_weighted: 0.35652612885450896
[2m[36m(func pid=129730)[0m f1_per_class: [0.265, 0.231, 0.786, 0.41, 0.085, 0.341, 0.435, 0.241, 0.213, 0.162]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=151876)[0m top1: 0.19076492537313433
[2m[36m(func pid=151876)[0m top5: 0.5298507462686567
[2m[36m(func pid=151876)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=151876)[0m f1_macro: 0.11774175420447504
[2m[36m(func pid=151876)[0m f1_weighted: 0.134149717208308
[2m[36m(func pid=151876)[0m f1_per_class: [0.263, 0.336, 0.0, 0.1, 0.012, 0.307, 0.015, 0.036, 0.0, 0.109]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m top1: 0.2042910447761194
[2m[36m(func pid=151878)[0m top5: 0.5844216417910447
[2m[36m(func pid=151878)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=151878)[0m f1_macro: 0.16262965090640819
[2m[36m(func pid=151878)[0m f1_weighted: 0.16623098616291454
[2m[36m(func pid=151878)[0m f1_per_class: [0.376, 0.33, 0.143, 0.185, 0.0, 0.246, 0.038, 0.137, 0.0, 0.172]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6174 | Steps: 2 | Val loss: 2.2337 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.1224 | Steps: 2 | Val loss: 2.3411 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9666 | Steps: 2 | Val loss: 2.3222 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.6610 | Steps: 2 | Val loss: 2.2127 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=146329)[0m top1: 0.19869402985074627
[2m[36m(func pid=146329)[0m top5: 0.6753731343283582
[2m[36m(func pid=146329)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=146329)[0m f1_macro: 0.1546265053301258
[2m[36m(func pid=146329)[0m f1_weighted: 0.22327259760309187
[2m[36m(func pid=146329)[0m f1_per_class: [0.133, 0.16, 0.103, 0.274, 0.037, 0.346, 0.213, 0.19, 0.03, 0.06]
[2m[36m(func pid=146329)[0m 
== Status ==
Current time: 2024-01-07 01:50:19 (running for 00:16:40.67)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.122 |      0.322 |                   97 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.617 |      0.155 |                   28 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  2.972 |      0.118 |                    2 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  2.866 |      0.163 |                    2 |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129730)[0m top1: 0.33255597014925375
[2m[36m(func pid=129730)[0m top5: 0.8540111940298507
[2m[36m(func pid=129730)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=129730)[0m f1_macro: 0.32236075459288493
[2m[36m(func pid=129730)[0m f1_weighted: 0.3639482544235987
[2m[36m(func pid=129730)[0m f1_per_class: [0.27, 0.24, 0.786, 0.423, 0.09, 0.346, 0.439, 0.246, 0.219, 0.165]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=151876)[0m top1: 0.18936567164179105
[2m[36m(func pid=151876)[0m top5: 0.5186567164179104
[2m[36m(func pid=151876)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=151876)[0m f1_macro: 0.11393852750769776
[2m[36m(func pid=151876)[0m f1_weighted: 0.13716672655140275
[2m[36m(func pid=151876)[0m f1_per_class: [0.247, 0.313, 0.0, 0.12, 0.012, 0.331, 0.015, 0.022, 0.0, 0.078]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m top1: 0.21175373134328357
[2m[36m(func pid=151878)[0m top5: 0.6665111940298507
[2m[36m(func pid=151878)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=151878)[0m f1_macro: 0.18144189397652732
[2m[36m(func pid=151878)[0m f1_weighted: 0.18975933269085826
[2m[36m(func pid=151878)[0m f1_per_class: [0.335, 0.259, 0.115, 0.258, 0.04, 0.257, 0.07, 0.185, 0.052, 0.244]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6668 | Steps: 2 | Val loss: 2.2310 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.1291 | Steps: 2 | Val loss: 2.3276 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9172 | Steps: 2 | Val loss: 2.3197 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.4152 | Steps: 2 | Val loss: 2.1683 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=146329)[0m top1: 0.20102611940298507
[2m[36m(func pid=146329)[0m top5: 0.6777052238805971
[2m[36m(func pid=146329)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=146329)[0m f1_macro: 0.16026790193702423
[2m[36m(func pid=146329)[0m f1_weighted: 0.22596436086502247
[2m[36m(func pid=146329)[0m f1_per_class: [0.142, 0.17, 0.096, 0.267, 0.037, 0.344, 0.22, 0.195, 0.042, 0.091]
[2m[36m(func pid=146329)[0m 
== Status ==
Current time: 2024-01-07 01:50:25 (running for 00:16:46.02)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.129 |      0.321 |                   98 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.667 |      0.16  |                   29 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  2.967 |      0.114 |                    3 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  2.661 |      0.181 |                    3 |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129730)[0m top1: 0.3344216417910448
[2m[36m(func pid=129730)[0m top5: 0.8568097014925373
[2m[36m(func pid=129730)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=129730)[0m f1_macro: 0.32055957591934015
[2m[36m(func pid=129730)[0m f1_weighted: 0.36547620911874934
[2m[36m(func pid=129730)[0m f1_per_class: [0.273, 0.252, 0.759, 0.423, 0.09, 0.34, 0.44, 0.246, 0.213, 0.17]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=151876)[0m top1: 0.18610074626865672
[2m[36m(func pid=151876)[0m top5: 0.5209888059701493
[2m[36m(func pid=151876)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=151876)[0m f1_macro: 0.12632935878447965
[2m[36m(func pid=151876)[0m f1_weighted: 0.1383191795575322
[2m[36m(func pid=151876)[0m f1_per_class: [0.316, 0.306, 0.067, 0.119, 0.011, 0.334, 0.018, 0.02, 0.0, 0.073]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m top1: 0.20988805970149255
[2m[36m(func pid=151878)[0m top5: 0.7019589552238806
[2m[36m(func pid=151878)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=151878)[0m f1_macro: 0.1828412907494748
[2m[36m(func pid=151878)[0m f1_weighted: 0.19373549109589533
[2m[36m(func pid=151878)[0m f1_per_class: [0.229, 0.13, 0.071, 0.308, 0.125, 0.252, 0.101, 0.262, 0.061, 0.29]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.5598 | Steps: 2 | Val loss: 2.2269 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.1034 | Steps: 2 | Val loss: 2.3496 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8913 | Steps: 2 | Val loss: 2.3160 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=146329)[0m top1: 0.20242537313432835
[2m[36m(func pid=146329)[0m top5: 0.683768656716418
[2m[36m(func pid=146329)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=146329)[0m f1_macro: 0.16409194746701056
[2m[36m(func pid=146329)[0m f1_weighted: 0.22780615537676463
[2m[36m(func pid=146329)[0m f1_per_class: [0.158, 0.171, 0.088, 0.261, 0.038, 0.339, 0.229, 0.208, 0.043, 0.107]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.1421 | Steps: 2 | Val loss: 2.1029 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 01:50:30 (running for 00:16:51.32)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00006 | RUNNING    | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.103 |      0.32  |                   99 |
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.56  |      0.164 |                   30 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  2.917 |      0.126 |                    4 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  2.415 |      0.183 |                    4 |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129730)[0m top1: 0.32975746268656714
[2m[36m(func pid=129730)[0m top5: 0.8544776119402985
[2m[36m(func pid=129730)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=129730)[0m f1_macro: 0.31966140565146284
[2m[36m(func pid=129730)[0m f1_weighted: 0.3607130952415386
[2m[36m(func pid=129730)[0m f1_per_class: [0.27, 0.251, 0.759, 0.421, 0.085, 0.344, 0.424, 0.248, 0.227, 0.169]
[2m[36m(func pid=129730)[0m 
[2m[36m(func pid=151876)[0m top1: 0.1828358208955224
[2m[36m(func pid=151876)[0m top5: 0.5284514925373134
[2m[36m(func pid=151876)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=151876)[0m f1_macro: 0.12806977446876816
[2m[36m(func pid=151876)[0m f1_weighted: 0.14490141572937254
[2m[36m(func pid=151876)[0m f1_per_class: [0.262, 0.292, 0.077, 0.128, 0.012, 0.336, 0.038, 0.029, 0.016, 0.091]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m top1: 0.2416044776119403
[2m[36m(func pid=151878)[0m top5: 0.7364738805970149
[2m[36m(func pid=151878)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=151878)[0m f1_macro: 0.19425886606971848
[2m[36m(func pid=151878)[0m f1_weighted: 0.2103101520448294
[2m[36m(func pid=151878)[0m f1_per_class: [0.208, 0.029, 0.085, 0.379, 0.121, 0.275, 0.121, 0.341, 0.101, 0.282]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.6195 | Steps: 2 | Val loss: 2.2238 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=129730)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0828 | Steps: 2 | Val loss: 2.3593 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8334 | Steps: 2 | Val loss: 2.3030 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=146329)[0m top1: 0.20475746268656717
[2m[36m(func pid=146329)[0m top5: 0.6888992537313433
[2m[36m(func pid=146329)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=146329)[0m f1_macro: 0.16898882159022885
[2m[36m(func pid=146329)[0m f1_weighted: 0.2302721022705668
[2m[36m(func pid=146329)[0m f1_per_class: [0.171, 0.162, 0.093, 0.275, 0.039, 0.331, 0.232, 0.201, 0.043, 0.145]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.7908 | Steps: 2 | Val loss: 2.0174 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 01:50:36 (running for 00:16:56.92)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 3 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.619 |      0.169 |                   31 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  2.891 |      0.128 |                    5 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  2.142 |      0.194 |                    5 |
| train_66d79_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=129730)[0m top1: 0.3316231343283582
[2m[36m(func pid=129730)[0m top5: 0.8540111940298507
[2m[36m(func pid=129730)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=129730)[0m f1_macro: 0.31892008151833334
[2m[36m(func pid=129730)[0m f1_weighted: 0.3627671245337878
[2m[36m(func pid=129730)[0m f1_per_class: [0.279, 0.251, 0.733, 0.42, 0.084, 0.35, 0.428, 0.252, 0.222, 0.169]
[2m[36m(func pid=151876)[0m top1: 0.18097014925373134
[2m[36m(func pid=151876)[0m top5: 0.5471082089552238
[2m[36m(func pid=151876)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=151876)[0m f1_macro: 0.13481844582283833
[2m[36m(func pid=151876)[0m f1_weighted: 0.15725069933372363
[2m[36m(func pid=151876)[0m f1_per_class: [0.216, 0.271, 0.1, 0.137, 0.012, 0.351, 0.074, 0.048, 0.037, 0.103]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.5671 | Steps: 2 | Val loss: 2.2169 | Batch size: 32 | lr: 0.0001 | Duration: 2.63s
[2m[36m(func pid=151878)[0m top1: 0.27238805970149255
[2m[36m(func pid=151878)[0m top5: 0.7770522388059702
[2m[36m(func pid=151878)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=151878)[0m f1_macro: 0.21813354720276695
[2m[36m(func pid=151878)[0m f1_weighted: 0.2431293895321984
[2m[36m(func pid=151878)[0m f1_per_class: [0.204, 0.047, 0.139, 0.412, 0.102, 0.326, 0.169, 0.335, 0.1, 0.348]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7605 | Steps: 2 | Val loss: 2.2857 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=146329)[0m top1: 0.208955223880597
[2m[36m(func pid=146329)[0m top5: 0.699160447761194
[2m[36m(func pid=146329)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=146329)[0m f1_macro: 0.1681878757337948
[2m[36m(func pid=146329)[0m f1_weighted: 0.23525023986307766
[2m[36m(func pid=146329)[0m f1_per_class: [0.177, 0.162, 0.091, 0.284, 0.033, 0.332, 0.238, 0.212, 0.033, 0.119]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.4318 | Steps: 2 | Val loss: 1.9418 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=151876)[0m top1: 0.18563432835820895
[2m[36m(func pid=151876)[0m top5: 0.5732276119402985
[2m[36m(func pid=151876)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=151876)[0m f1_macro: 0.1480053294371279
[2m[36m(func pid=151876)[0m f1_weighted: 0.17145800440456344
[2m[36m(func pid=151876)[0m f1_per_class: [0.218, 0.262, 0.12, 0.166, 0.027, 0.359, 0.086, 0.1, 0.03, 0.112]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.5589 | Steps: 2 | Val loss: 2.2130 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=151878)[0m top1: 0.2989738805970149
[2m[36m(func pid=151878)[0m top5: 0.8041044776119403
[2m[36m(func pid=151878)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=151878)[0m f1_macro: 0.2539957241738618
[2m[36m(func pid=151878)[0m f1_weighted: 0.2871831725828257
[2m[36m(func pid=151878)[0m f1_per_class: [0.198, 0.146, 0.264, 0.427, 0.117, 0.357, 0.226, 0.361, 0.098, 0.345]
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.6953 | Steps: 2 | Val loss: 2.2669 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=146329)[0m top1: 0.20942164179104478
[2m[36m(func pid=146329)[0m top5: 0.7047574626865671
[2m[36m(func pid=146329)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=146329)[0m f1_macro: 0.17748623630103202
[2m[36m(func pid=146329)[0m f1_weighted: 0.2351179265207804
[2m[36m(func pid=146329)[0m f1_per_class: [0.172, 0.164, 0.09, 0.282, 0.034, 0.331, 0.236, 0.209, 0.033, 0.225]
== Status ==
Current time: 2024-01-07 01:50:41 (running for 00:17:02.25)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.567 |      0.168 |                   32 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  2.761 |      0.148 |                    7 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  1.791 |      0.218 |                    6 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=154058)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=154058)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=154058)[0m Configuration completed!
[2m[36m(func pid=154058)[0m New optimizer parameters:
[2m[36m(func pid=154058)[0m SGD (
[2m[36m(func pid=154058)[0m Parameter Group 0
[2m[36m(func pid=154058)[0m     dampening: 0
[2m[36m(func pid=154058)[0m     differentiable: False
[2m[36m(func pid=154058)[0m     foreach: None
[2m[36m(func pid=154058)[0m     lr: 0.1
[2m[36m(func pid=154058)[0m     maximize: False
[2m[36m(func pid=154058)[0m     momentum: 0.99
[2m[36m(func pid=154058)[0m     nesterov: False
[2m[36m(func pid=154058)[0m     weight_decay: 0.0001
[2m[36m(func pid=154058)[0m )
[2m[36m(func pid=154058)[0m 
== Status ==
Current time: 2024-01-07 01:50:46 (running for 00:17:07.33)
Memory usage on this node: 24.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.559 |      0.177 |                   33 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  2.695 |      0.159 |                    8 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  1.432 |      0.254 |                    7 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151876)[0m top1: 0.19496268656716417
[2m[36m(func pid=151876)[0m top5: 0.6030783582089553
[2m[36m(func pid=151876)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=151876)[0m f1_macro: 0.15931221866617012
[2m[36m(func pid=151876)[0m f1_weighted: 0.19432662916709406
[2m[36m(func pid=151876)[0m f1_per_class: [0.23, 0.248, 0.104, 0.197, 0.031, 0.358, 0.13, 0.16, 0.028, 0.108]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.6040 | Steps: 2 | Val loss: 2.2077 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.2738 | Steps: 2 | Val loss: 1.8985 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6381 | Steps: 2 | Val loss: 2.2465 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9308 | Steps: 2 | Val loss: 2.3199 | Batch size: 32 | lr: 0.1 | Duration: 4.55s
[2m[36m(func pid=146329)[0m top1: 0.21688432835820895
[2m[36m(func pid=146329)[0m top5: 0.7066231343283582
[2m[36m(func pid=146329)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=146329)[0m f1_macro: 0.18274974358433532
[2m[36m(func pid=146329)[0m f1_weighted: 0.2432787144395535
[2m[36m(func pid=146329)[0m f1_per_class: [0.187, 0.187, 0.087, 0.288, 0.035, 0.321, 0.246, 0.213, 0.033, 0.23]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m top1: 0.3064365671641791
[2m[36m(func pid=151878)[0m top5: 0.8330223880597015
[2m[36m(func pid=151878)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=151878)[0m f1_macro: 0.27116054828086106
[2m[36m(func pid=151878)[0m f1_weighted: 0.3110376629733362
[2m[36m(func pid=151878)[0m f1_per_class: [0.192, 0.186, 0.429, 0.433, 0.097, 0.38, 0.273, 0.346, 0.086, 0.29]
[2m[36m(func pid=151878)[0m 
== Status ==
Current time: 2024-01-07 01:50:51 (running for 00:17:12.41)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.604 |      0.183 |                   34 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  2.638 |      0.173 |                    9 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  1.274 |      0.271 |                    8 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151876)[0m top1: 0.197294776119403
[2m[36m(func pid=151876)[0m top5: 0.6375932835820896
[2m[36m(func pid=151876)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=151876)[0m f1_macro: 0.1731403157180659
[2m[36m(func pid=151876)[0m f1_weighted: 0.20701256398629234
[2m[36m(func pid=151876)[0m f1_per_class: [0.242, 0.224, 0.079, 0.211, 0.035, 0.364, 0.16, 0.195, 0.026, 0.193]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.12546641791044777
[2m[36m(func pid=154058)[0m top5: 0.5419776119402985
[2m[36m(func pid=154058)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=154058)[0m f1_macro: 0.11888935390894853
[2m[36m(func pid=154058)[0m f1_weighted: 0.11894125775785694
[2m[36m(func pid=154058)[0m f1_per_class: [0.097, 0.232, 0.096, 0.149, 0.038, 0.06, 0.03, 0.286, 0.0, 0.202]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5103 | Steps: 2 | Val loss: 2.2023 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.9922 | Steps: 2 | Val loss: 1.8885 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.5382 | Steps: 2 | Val loss: 2.2225 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.0411 | Steps: 2 | Val loss: 2.3126 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=146329)[0m top1: 0.2150186567164179
[2m[36m(func pid=146329)[0m top5: 0.7122201492537313
[2m[36m(func pid=146329)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=146329)[0m f1_macro: 0.1790152985826423
[2m[36m(func pid=146329)[0m f1_weighted: 0.24200392216030944
[2m[36m(func pid=146329)[0m f1_per_class: [0.18, 0.187, 0.084, 0.285, 0.037, 0.31, 0.251, 0.204, 0.034, 0.217]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m top1: 0.31529850746268656
[2m[36m(func pid=151878)[0m top5: 0.8544776119402985
[2m[36m(func pid=151878)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=151878)[0m f1_macro: 0.2961679089961843
[2m[36m(func pid=151878)[0m f1_weighted: 0.3320177182026397
[2m[36m(func pid=151878)[0m f1_per_class: [0.204, 0.22, 0.558, 0.429, 0.075, 0.391, 0.314, 0.348, 0.145, 0.277]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m top1: 0.20335820895522388
[2m[36m(func pid=151876)[0m top5: 0.675839552238806
[2m[36m(func pid=151876)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=151876)[0m f1_macro: 0.1802805947025602
[2m[36m(func pid=151876)[0m f1_weighted: 0.2222644263629906
[2m[36m(func pid=151876)[0m f1_per_class: [0.247, 0.205, 0.066, 0.231, 0.037, 0.361, 0.201, 0.211, 0.034, 0.21]
[2m[36m(func pid=151876)[0m 
== Status ==
Current time: 2024-01-07 01:50:57 (running for 00:17:18.34)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.51  |      0.179 |                   35 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  2.538 |      0.18  |                   10 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.992 |      0.296 |                    9 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  2.041 |      0.179 |                    2 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.15345149253731344
[2m[36m(func pid=154058)[0m top5: 0.6888992537313433
[2m[36m(func pid=154058)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=154058)[0m f1_macro: 0.17929353951086816
[2m[36m(func pid=154058)[0m f1_weighted: 0.15283805720068483
[2m[36m(func pid=154058)[0m f1_per_class: [0.086, 0.124, 0.194, 0.176, 0.083, 0.322, 0.045, 0.4, 0.05, 0.312]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.4673 | Steps: 2 | Val loss: 2.1982 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8925 | Steps: 2 | Val loss: 1.9212 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.4551 | Steps: 2 | Val loss: 2.2040 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0947 | Steps: 2 | Val loss: 2.3786 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=146329)[0m top1: 0.21361940298507462
[2m[36m(func pid=146329)[0m top5: 0.7178171641791045
[2m[36m(func pid=146329)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=146329)[0m f1_macro: 0.18037583550723552
[2m[36m(func pid=146329)[0m f1_weighted: 0.24058909828916325
[2m[36m(func pid=146329)[0m f1_per_class: [0.198, 0.178, 0.082, 0.284, 0.034, 0.301, 0.254, 0.208, 0.035, 0.229]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m top1: 0.3185634328358209
[2m[36m(func pid=151878)[0m top5: 0.8600746268656716
[2m[36m(func pid=151878)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=151878)[0m f1_macro: 0.31513605788308474
[2m[36m(func pid=151878)[0m f1_weighted: 0.3435321485180073
[2m[36m(func pid=151878)[0m f1_per_class: [0.211, 0.264, 0.686, 0.39, 0.082, 0.395, 0.361, 0.323, 0.169, 0.271]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m top1: 0.2080223880597015
[2m[36m(func pid=151876)[0m top5: 0.6921641791044776
[2m[36m(func pid=151876)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=151876)[0m f1_macro: 0.18280301661170037
[2m[36m(func pid=151876)[0m f1_weighted: 0.23381451644113868
[2m[36m(func pid=151876)[0m f1_per_class: [0.245, 0.174, 0.059, 0.248, 0.038, 0.36, 0.24, 0.214, 0.048, 0.201]
[2m[36m(func pid=151876)[0m 
== Status ==
Current time: 2024-01-07 01:51:03 (running for 00:17:23.77)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.467 |      0.18  |                   36 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  2.455 |      0.183 |                   11 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.893 |      0.315 |                   10 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  1.095 |      0.247 |                    3 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.20289179104477612
[2m[36m(func pid=154058)[0m top5: 0.7957089552238806
[2m[36m(func pid=154058)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=154058)[0m f1_macro: 0.24662187528113835
[2m[36m(func pid=154058)[0m f1_weighted: 0.21694541720152147
[2m[36m(func pid=154058)[0m f1_per_class: [0.091, 0.125, 0.545, 0.237, 0.089, 0.4, 0.161, 0.398, 0.08, 0.339]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.4165 | Steps: 2 | Val loss: 2.1906 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8407 | Steps: 2 | Val loss: 1.9528 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.3704 | Steps: 2 | Val loss: 2.1787 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=146329)[0m top1: 0.21875
[2m[36m(func pid=146329)[0m top5: 0.7215485074626866
[2m[36m(func pid=146329)[0m f1_micro: 0.21875
[2m[36m(func pid=146329)[0m f1_macro: 0.183922525965798
[2m[36m(func pid=146329)[0m f1_weighted: 0.2453926952281151
[2m[36m(func pid=146329)[0m f1_per_class: [0.2, 0.187, 0.085, 0.289, 0.035, 0.291, 0.263, 0.207, 0.036, 0.247]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8366 | Steps: 2 | Val loss: 2.6634 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=151878)[0m top1: 0.3302238805970149
[2m[36m(func pid=151878)[0m top5: 0.863339552238806
[2m[36m(func pid=151878)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=151878)[0m f1_macro: 0.32478371788199006
[2m[36m(func pid=151878)[0m f1_weighted: 0.355500062435979
[2m[36m(func pid=151878)[0m f1_per_class: [0.248, 0.287, 0.727, 0.382, 0.081, 0.404, 0.394, 0.301, 0.162, 0.261]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m top1: 0.22294776119402984
[2m[36m(func pid=151876)[0m top5: 0.7182835820895522
[2m[36m(func pid=151876)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=151876)[0m f1_macro: 0.19444286077596046
[2m[36m(func pid=151876)[0m f1_weighted: 0.24959364835558728
[2m[36m(func pid=151876)[0m f1_per_class: [0.257, 0.171, 0.059, 0.269, 0.02, 0.335, 0.275, 0.254, 0.054, 0.252]
[2m[36m(func pid=151876)[0m 
== Status ==
Current time: 2024-01-07 01:51:08 (running for 00:17:29.27)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.416 |      0.184 |                   37 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  2.37  |      0.194 |                   12 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.841 |      0.325 |                   11 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.837 |      0.281 |                    4 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.22061567164179105
[2m[36m(func pid=154058)[0m top5: 0.8409514925373134
[2m[36m(func pid=154058)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=154058)[0m f1_macro: 0.28095579232060525
[2m[36m(func pid=154058)[0m f1_weighted: 0.25636895106192553
[2m[36m(func pid=154058)[0m f1_per_class: [0.105, 0.166, 0.706, 0.24, 0.075, 0.389, 0.273, 0.354, 0.078, 0.423]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.4507 | Steps: 2 | Val loss: 2.1843 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.3613 | Steps: 2 | Val loss: 2.1564 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5818 | Steps: 2 | Val loss: 2.0160 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=146329)[0m top1: 0.21828358208955223
[2m[36m(func pid=146329)[0m top5: 0.7210820895522388
[2m[36m(func pid=146329)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=146329)[0m f1_macro: 0.1841743333468151
[2m[36m(func pid=146329)[0m f1_weighted: 0.24398517876059297
[2m[36m(func pid=146329)[0m f1_per_class: [0.195, 0.18, 0.084, 0.293, 0.035, 0.271, 0.264, 0.218, 0.035, 0.265]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5190 | Steps: 2 | Val loss: 2.9921 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=151876)[0m top1: 0.23274253731343283
[2m[36m(func pid=151876)[0m top5: 0.7369402985074627
[2m[36m(func pid=151876)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=151876)[0m f1_macro: 0.19497473412061805
[2m[36m(func pid=151876)[0m f1_weighted: 0.26198100034955396
[2m[36m(func pid=151876)[0m f1_per_class: [0.237, 0.177, 0.06, 0.278, 0.021, 0.329, 0.31, 0.235, 0.066, 0.237]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m top1: 0.3306902985074627
[2m[36m(func pid=151878)[0m top5: 0.8614738805970149
[2m[36m(func pid=151878)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=151878)[0m f1_macro: 0.32145288887406925
[2m[36m(func pid=151878)[0m f1_weighted: 0.35454192875303164
[2m[36m(func pid=151878)[0m f1_per_class: [0.265, 0.303, 0.727, 0.375, 0.077, 0.405, 0.397, 0.247, 0.169, 0.249]
[2m[36m(func pid=151878)[0m 
== Status ==
Current time: 2024-01-07 01:51:13 (running for 00:17:34.46)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.451 |      0.184 |                   38 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  2.361 |      0.195 |                   13 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.582 |      0.321 |                   12 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.519 |      0.286 |                    5 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.23087686567164178
[2m[36m(func pid=154058)[0m top5: 0.8656716417910447
[2m[36m(func pid=154058)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=154058)[0m f1_macro: 0.285615223840788
[2m[36m(func pid=154058)[0m f1_weighted: 0.2733770621160967
[2m[36m(func pid=154058)[0m f1_per_class: [0.14, 0.195, 0.815, 0.218, 0.088, 0.39, 0.354, 0.247, 0.066, 0.343]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.3338 | Steps: 2 | Val loss: 2.1791 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.2856 | Steps: 2 | Val loss: 2.1353 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4889 | Steps: 2 | Val loss: 2.0799 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=146329)[0m top1: 0.22108208955223882
[2m[36m(func pid=146329)[0m top5: 0.726679104477612
[2m[36m(func pid=146329)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=146329)[0m f1_macro: 0.18486399252629815
[2m[36m(func pid=146329)[0m f1_weighted: 0.24764749945169356
[2m[36m(func pid=146329)[0m f1_per_class: [0.191, 0.17, 0.084, 0.302, 0.035, 0.27, 0.274, 0.214, 0.049, 0.26]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.3649 | Steps: 2 | Val loss: 3.3579 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=151876)[0m top1: 0.2490671641791045
[2m[36m(func pid=151876)[0m top5: 0.7476679104477612
[2m[36m(func pid=151876)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=151876)[0m f1_macro: 0.21064499002929513
[2m[36m(func pid=151876)[0m f1_weighted: 0.27881330670889665
[2m[36m(func pid=151876)[0m f1_per_class: [0.247, 0.199, 0.061, 0.292, 0.054, 0.325, 0.336, 0.252, 0.072, 0.269]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m top1: 0.3381529850746269
[2m[36m(func pid=151878)[0m top5: 0.8656716417910447
[2m[36m(func pid=151878)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=151878)[0m f1_macro: 0.326609731801476
[2m[36m(func pid=151878)[0m f1_weighted: 0.3626236996404633
[2m[36m(func pid=151878)[0m f1_per_class: [0.291, 0.3, 0.727, 0.381, 0.072, 0.415, 0.414, 0.254, 0.178, 0.236]
[2m[36m(func pid=151878)[0m 
== Status ==
Current time: 2024-01-07 01:51:19 (running for 00:17:39.85)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.334 |      0.185 |                   39 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  2.286 |      0.211 |                   14 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.489 |      0.327 |                   13 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.365 |      0.289 |                    6 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.251865671641791
[2m[36m(func pid=154058)[0m top5: 0.8638059701492538
[2m[36m(func pid=154058)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=154058)[0m f1_macro: 0.2890600426228405
[2m[36m(func pid=154058)[0m f1_weighted: 0.2872402962866045
[2m[36m(func pid=154058)[0m f1_per_class: [0.157, 0.209, 0.815, 0.289, 0.104, 0.385, 0.326, 0.253, 0.082, 0.27]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.3910 | Steps: 2 | Val loss: 2.1753 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.1223 | Steps: 2 | Val loss: 2.1153 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3598 | Steps: 2 | Val loss: 2.1697 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=146329)[0m top1: 0.22527985074626866
[2m[36m(func pid=146329)[0m top5: 0.730410447761194
[2m[36m(func pid=146329)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=146329)[0m f1_macro: 0.1868991341470189
[2m[36m(func pid=146329)[0m f1_weighted: 0.25134543986322194
[2m[36m(func pid=146329)[0m f1_per_class: [0.203, 0.164, 0.087, 0.308, 0.034, 0.261, 0.285, 0.217, 0.063, 0.248]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.1878 | Steps: 2 | Val loss: 3.7669 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=151876)[0m top1: 0.25886194029850745
[2m[36m(func pid=151876)[0m top5: 0.7611940298507462
[2m[36m(func pid=151876)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=151876)[0m f1_macro: 0.21760109401532812
[2m[36m(func pid=151876)[0m f1_weighted: 0.2882180030853827
[2m[36m(func pid=151876)[0m f1_per_class: [0.252, 0.203, 0.065, 0.314, 0.066, 0.309, 0.343, 0.28, 0.094, 0.251]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m top1: 0.34281716417910446
[2m[36m(func pid=151878)[0m top5: 0.8694029850746269
[2m[36m(func pid=151878)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=151878)[0m f1_macro: 0.33279493720926884
[2m[36m(func pid=151878)[0m f1_weighted: 0.36652539282908075
[2m[36m(func pid=151878)[0m f1_per_class: [0.299, 0.292, 0.733, 0.381, 0.081, 0.43, 0.421, 0.251, 0.211, 0.229]
[2m[36m(func pid=151878)[0m 
== Status ==
Current time: 2024-01-07 01:51:24 (running for 00:17:44.96)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.391 |      0.187 |                   40 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  2.122 |      0.218 |                   15 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.36  |      0.333 |                   14 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.188 |      0.288 |                    7 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.2635261194029851
[2m[36m(func pid=154058)[0m top5: 0.8493470149253731
[2m[36m(func pid=154058)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=154058)[0m f1_macro: 0.2881877325426321
[2m[36m(func pid=154058)[0m f1_weighted: 0.28796786084091314
[2m[36m(func pid=154058)[0m f1_per_class: [0.153, 0.192, 0.815, 0.327, 0.115, 0.398, 0.294, 0.273, 0.11, 0.206]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.3395 | Steps: 2 | Val loss: 2.1687 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.0151 | Steps: 2 | Val loss: 2.0943 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4502 | Steps: 2 | Val loss: 2.2309 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=146329)[0m top1: 0.23274253731343283
[2m[36m(func pid=146329)[0m top5: 0.7332089552238806
[2m[36m(func pid=146329)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=146329)[0m f1_macro: 0.19123486363540126
[2m[36m(func pid=146329)[0m f1_weighted: 0.25822128832705493
[2m[36m(func pid=146329)[0m f1_per_class: [0.205, 0.17, 0.09, 0.314, 0.036, 0.256, 0.298, 0.223, 0.077, 0.243]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.1289 | Steps: 2 | Val loss: 4.2671 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=151876)[0m top1: 0.2691231343283582
[2m[36m(func pid=151876)[0m top5: 0.7667910447761194
[2m[36m(func pid=151876)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=151876)[0m f1_macro: 0.22069028468664637
[2m[36m(func pid=151876)[0m f1_weighted: 0.2977814584297125
[2m[36m(func pid=151876)[0m f1_per_class: [0.238, 0.217, 0.076, 0.332, 0.064, 0.306, 0.351, 0.277, 0.116, 0.231]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m top1: 0.34794776119402987
[2m[36m(func pid=151878)[0m top5: 0.8708022388059702
[2m[36m(func pid=151878)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=151878)[0m f1_macro: 0.3343588226342269
[2m[36m(func pid=151878)[0m f1_weighted: 0.3726346484179707
[2m[36m(func pid=151878)[0m f1_per_class: [0.316, 0.283, 0.75, 0.411, 0.087, 0.425, 0.424, 0.243, 0.184, 0.221]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.4022 | Steps: 2 | Val loss: 2.1615 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 01:51:29 (running for 00:17:50.48)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.339 |      0.191 |                   41 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  2.015 |      0.221 |                   16 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.45  |      0.334 |                   15 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.129 |      0.291 |                    8 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.27658582089552236
[2m[36m(func pid=154058)[0m top5: 0.8479477611940298
[2m[36m(func pid=154058)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=154058)[0m f1_macro: 0.2908691209445159
[2m[36m(func pid=154058)[0m f1_weighted: 0.2981761153379585
[2m[36m(func pid=154058)[0m f1_per_class: [0.165, 0.199, 0.769, 0.351, 0.123, 0.388, 0.296, 0.299, 0.163, 0.155]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.9211 | Steps: 2 | Val loss: 2.0702 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4268 | Steps: 2 | Val loss: 2.3403 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=146329)[0m top1: 0.23507462686567165
[2m[36m(func pid=146329)[0m top5: 0.7364738805970149
[2m[36m(func pid=146329)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=146329)[0m f1_macro: 0.1930523226920498
[2m[36m(func pid=146329)[0m f1_weighted: 0.25898012195656517
[2m[36m(func pid=146329)[0m f1_per_class: [0.209, 0.175, 0.095, 0.314, 0.036, 0.244, 0.3, 0.235, 0.067, 0.255]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151876)[0m top1: 0.2733208955223881
[2m[36m(func pid=151876)[0m top5: 0.7756529850746269
[2m[36m(func pid=151876)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=151876)[0m f1_macro: 0.2222271720774585
[2m[36m(func pid=151876)[0m f1_weighted: 0.3010026963209045
[2m[36m(func pid=151876)[0m f1_per_class: [0.227, 0.22, 0.086, 0.338, 0.074, 0.287, 0.36, 0.284, 0.121, 0.226]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.0737 | Steps: 2 | Val loss: 4.6858 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=151878)[0m top1: 0.34375
[2m[36m(func pid=151878)[0m top5: 0.8708022388059702
[2m[36m(func pid=151878)[0m f1_micro: 0.34375
[2m[36m(func pid=151878)[0m f1_macro: 0.3365252089243444
[2m[36m(func pid=151878)[0m f1_weighted: 0.36937301650460785
[2m[36m(func pid=151878)[0m f1_per_class: [0.305, 0.28, 0.774, 0.406, 0.088, 0.421, 0.415, 0.271, 0.188, 0.217]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.3206 | Steps: 2 | Val loss: 2.1566 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=154058)[0m top1: 0.2980410447761194
== Status ==
Current time: 2024-01-07 01:51:35 (running for 00:17:55.84)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.402 |      0.193 |                   42 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  1.921 |      0.222 |                   17 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.427 |      0.337 |                   16 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.074 |      0.302 |                    9 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=154058)[0m top5: 0.8484141791044776

[2m[36m(func pid=154058)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=154058)[0m f1_macro: 0.302230953477416
[2m[36m(func pid=154058)[0m f1_weighted: 0.3246540277551251
[2m[36m(func pid=154058)[0m f1_per_class: [0.189, 0.207, 0.815, 0.386, 0.101, 0.385, 0.352, 0.271, 0.172, 0.145]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.8903 | Steps: 2 | Val loss: 2.0407 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=146329)[0m top1: 0.2416044776119403
[2m[36m(func pid=146329)[0m top5: 0.738339552238806
[2m[36m(func pid=146329)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=146329)[0m f1_macro: 0.1973531173051691
[2m[36m(func pid=146329)[0m f1_weighted: 0.26600387376187196
[2m[36m(func pid=146329)[0m f1_per_class: [0.206, 0.173, 0.094, 0.322, 0.044, 0.241, 0.316, 0.246, 0.069, 0.262]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3137 | Steps: 2 | Val loss: 2.3916 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=151876)[0m top1: 0.2957089552238806
[2m[36m(func pid=151876)[0m top5: 0.7873134328358209
[2m[36m(func pid=151876)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=151876)[0m f1_macro: 0.23413226301356338
[2m[36m(func pid=151876)[0m f1_weighted: 0.32307126763739064
[2m[36m(func pid=151876)[0m f1_per_class: [0.227, 0.237, 0.118, 0.367, 0.067, 0.294, 0.392, 0.288, 0.131, 0.219]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.0401 | Steps: 2 | Val loss: 5.1974 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=151878)[0m top1: 0.3493470149253731
[2m[36m(func pid=151878)[0m top5: 0.8759328358208955
[2m[36m(func pid=151878)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=151878)[0m f1_macro: 0.33925325919787896
[2m[36m(func pid=151878)[0m f1_weighted: 0.3753915941355315
[2m[36m(func pid=151878)[0m f1_per_class: [0.322, 0.274, 0.786, 0.434, 0.086, 0.407, 0.417, 0.271, 0.188, 0.208]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.3223 | Steps: 2 | Val loss: 2.1516 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.7184 | Steps: 2 | Val loss: 2.0131 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 01:51:40 (running for 00:18:01.26)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.321 |      0.197 |                   43 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  1.89  |      0.234 |                   18 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.314 |      0.339 |                   17 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.04  |      0.302 |                   10 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.302705223880597
[2m[36m(func pid=154058)[0m top5: 0.8498134328358209
[2m[36m(func pid=154058)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=154058)[0m f1_macro: 0.30241545445101203
[2m[36m(func pid=154058)[0m f1_weighted: 0.33111303021375427
[2m[36m(func pid=154058)[0m f1_per_class: [0.207, 0.217, 0.815, 0.393, 0.101, 0.371, 0.371, 0.243, 0.168, 0.139]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m top1: 0.24207089552238806
[2m[36m(func pid=146329)[0m top5: 0.7406716417910447
[2m[36m(func pid=146329)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=146329)[0m f1_macro: 0.2029217791676939
[2m[36m(func pid=146329)[0m f1_weighted: 0.26695086098740817
[2m[36m(func pid=146329)[0m f1_per_class: [0.201, 0.173, 0.091, 0.324, 0.045, 0.251, 0.31, 0.239, 0.11, 0.286]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2391 | Steps: 2 | Val loss: 2.4884 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=151876)[0m top1: 0.3045708955223881
[2m[36m(func pid=151876)[0m top5: 0.7943097014925373
[2m[36m(func pid=151876)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=151876)[0m f1_macro: 0.24265732117624045
[2m[36m(func pid=151876)[0m f1_weighted: 0.3300448020766112
[2m[36m(func pid=151876)[0m f1_per_class: [0.237, 0.233, 0.14, 0.381, 0.079, 0.296, 0.4, 0.292, 0.152, 0.217]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.0139 | Steps: 2 | Val loss: 5.6948 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=151878)[0m top1: 0.3530783582089552
[2m[36m(func pid=151878)[0m top5: 0.8731343283582089
[2m[36m(func pid=151878)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=151878)[0m f1_macro: 0.341378649492357
[2m[36m(func pid=151878)[0m f1_weighted: 0.3793334482917621
[2m[36m(func pid=151878)[0m f1_per_class: [0.333, 0.276, 0.786, 0.439, 0.085, 0.415, 0.422, 0.262, 0.196, 0.201]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.3492 | Steps: 2 | Val loss: 2.1436 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.6743 | Steps: 2 | Val loss: 1.9877 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 01:51:45 (running for 00:18:06.66)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.322 |      0.203 |                   44 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  1.718 |      0.243 |                   19 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.239 |      0.341 |                   18 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.014 |      0.311 |                   11 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.3148320895522388
[2m[36m(func pid=154058)[0m top5: 0.8544776119402985
[2m[36m(func pid=154058)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=154058)[0m f1_macro: 0.3107508896833041
[2m[36m(func pid=154058)[0m f1_weighted: 0.3463174506752567
[2m[36m(func pid=154058)[0m f1_per_class: [0.234, 0.224, 0.815, 0.408, 0.094, 0.368, 0.401, 0.244, 0.187, 0.132]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m top1: 0.25046641791044777
[2m[36m(func pid=146329)[0m top5: 0.7430037313432836
[2m[36m(func pid=146329)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=146329)[0m f1_macro: 0.204422770381517
[2m[36m(func pid=146329)[0m f1_weighted: 0.2738913195705957
[2m[36m(func pid=146329)[0m f1_per_class: [0.212, 0.16, 0.093, 0.329, 0.049, 0.237, 0.341, 0.245, 0.087, 0.29]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.1620 | Steps: 2 | Val loss: 2.5660 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=151876)[0m top1: 0.30783582089552236
[2m[36m(func pid=151876)[0m top5: 0.8017723880597015
[2m[36m(func pid=151876)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=151876)[0m f1_macro: 0.24498086535440272
[2m[36m(func pid=151876)[0m f1_weighted: 0.3328200477356861
[2m[36m(func pid=151876)[0m f1_per_class: [0.249, 0.24, 0.157, 0.393, 0.074, 0.295, 0.397, 0.262, 0.172, 0.211]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.1507 | Steps: 2 | Val loss: 6.1397 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=151878)[0m top1: 0.35634328358208955
[2m[36m(func pid=151878)[0m top5: 0.878731343283582
[2m[36m(func pid=151878)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=151878)[0m f1_macro: 0.3428931847238591
[2m[36m(func pid=151878)[0m f1_weighted: 0.3819666602696894
[2m[36m(func pid=151878)[0m f1_per_class: [0.347, 0.27, 0.786, 0.453, 0.095, 0.409, 0.424, 0.249, 0.205, 0.191]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.2362 | Steps: 2 | Val loss: 2.1377 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.6077 | Steps: 2 | Val loss: 1.9674 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 01:51:51 (running for 00:18:11.97)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.349 |      0.204 |                   45 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  1.674 |      0.245 |                   20 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.162 |      0.343 |                   19 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.151 |      0.311 |                   12 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.31529850746268656
[2m[36m(func pid=154058)[0m top5: 0.8675373134328358
[2m[36m(func pid=154058)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=154058)[0m f1_macro: 0.3109786641703524
[2m[36m(func pid=154058)[0m f1_weighted: 0.34485543709292277
[2m[36m(func pid=154058)[0m f1_per_class: [0.259, 0.236, 0.815, 0.406, 0.097, 0.368, 0.394, 0.222, 0.18, 0.132]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m top1: 0.2537313432835821
[2m[36m(func pid=146329)[0m top5: 0.7481343283582089
[2m[36m(func pid=146329)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=146329)[0m f1_macro: 0.20701435804913867
[2m[36m(func pid=146329)[0m f1_weighted: 0.2774683903960214
[2m[36m(func pid=146329)[0m f1_per_class: [0.215, 0.165, 0.094, 0.338, 0.048, 0.241, 0.339, 0.251, 0.09, 0.29]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151876)[0m top1: 0.3138992537313433
[2m[36m(func pid=151876)[0m top5: 0.8101679104477612
[2m[36m(func pid=151876)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=151876)[0m f1_macro: 0.2538524715192092
[2m[36m(func pid=151876)[0m f1_weighted: 0.3393131709451281
[2m[36m(func pid=151876)[0m f1_per_class: [0.25, 0.238, 0.186, 0.4, 0.071, 0.296, 0.404, 0.296, 0.19, 0.207]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.2027 | Steps: 2 | Val loss: 2.6775 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0080 | Steps: 2 | Val loss: 6.4254 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.1776 | Steps: 2 | Val loss: 2.1307 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=151878)[0m top1: 0.3572761194029851
[2m[36m(func pid=151878)[0m top5: 0.8731343283582089
[2m[36m(func pid=151878)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=151878)[0m f1_macro: 0.3428478833717939
[2m[36m(func pid=151878)[0m f1_weighted: 0.3831757737267115
[2m[36m(func pid=151878)[0m f1_per_class: [0.341, 0.27, 0.786, 0.464, 0.094, 0.408, 0.417, 0.257, 0.204, 0.187]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.5748 | Steps: 2 | Val loss: 1.9390 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 01:51:56 (running for 00:18:17.50)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.236 |      0.207 |                   46 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  1.608 |      0.254 |                   21 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.203 |      0.343 |                   20 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.008 |      0.311 |                   13 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.31716417910447764
[2m[36m(func pid=154058)[0m top5: 0.8736007462686567
[2m[36m(func pid=154058)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=154058)[0m f1_macro: 0.3112259248393625
[2m[36m(func pid=154058)[0m f1_weighted: 0.34246027914441884
[2m[36m(func pid=154058)[0m f1_per_class: [0.276, 0.241, 0.815, 0.421, 0.084, 0.369, 0.368, 0.228, 0.173, 0.138]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m top1: 0.25513059701492535
[2m[36m(func pid=146329)[0m top5: 0.7509328358208955
[2m[36m(func pid=146329)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=146329)[0m f1_macro: 0.20944976084498212
[2m[36m(func pid=146329)[0m f1_weighted: 0.27904588449501877
[2m[36m(func pid=146329)[0m f1_per_class: [0.211, 0.165, 0.101, 0.333, 0.046, 0.256, 0.343, 0.254, 0.091, 0.295]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151876)[0m top1: 0.31669776119402987
[2m[36m(func pid=151876)[0m top5: 0.820429104477612
[2m[36m(func pid=151876)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=151876)[0m f1_macro: 0.25868185715159936
[2m[36m(func pid=151876)[0m f1_weighted: 0.3412284410622304
[2m[36m(func pid=151876)[0m f1_per_class: [0.258, 0.246, 0.239, 0.399, 0.068, 0.306, 0.404, 0.299, 0.165, 0.203]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.1409 | Steps: 2 | Val loss: 2.7814 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0098 | Steps: 2 | Val loss: 6.7358 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.2048 | Steps: 2 | Val loss: 2.1232 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.4808 | Steps: 2 | Val loss: 1.9188 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=151878)[0m top1: 0.3591417910447761
[2m[36m(func pid=151878)[0m top5: 0.8694029850746269
[2m[36m(func pid=151878)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=151878)[0m f1_macro: 0.34482887701099746
[2m[36m(func pid=151878)[0m f1_weighted: 0.3864641249038895
[2m[36m(func pid=151878)[0m f1_per_class: [0.352, 0.264, 0.786, 0.468, 0.092, 0.404, 0.428, 0.263, 0.207, 0.185]
[2m[36m(func pid=151878)[0m 
== Status ==
Current time: 2024-01-07 01:52:02 (running for 00:18:22.78)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.178 |      0.209 |                   47 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  1.575 |      0.259 |                   22 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.141 |      0.345 |                   21 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.01  |      0.314 |                   14 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.3218283582089552
[2m[36m(func pid=154058)[0m top5: 0.8782649253731343
[2m[36m(func pid=154058)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=154058)[0m f1_macro: 0.31368756801758657
[2m[36m(func pid=154058)[0m f1_weighted: 0.3438909058978221
[2m[36m(func pid=154058)[0m f1_per_class: [0.299, 0.25, 0.786, 0.432, 0.091, 0.388, 0.349, 0.219, 0.196, 0.129]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m top1: 0.2574626865671642
[2m[36m(func pid=146329)[0m top5: 0.7569962686567164
[2m[36m(func pid=146329)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=146329)[0m f1_macro: 0.21150420538141187
[2m[36m(func pid=146329)[0m f1_weighted: 0.28065862292051375
[2m[36m(func pid=146329)[0m f1_per_class: [0.206, 0.159, 0.106, 0.334, 0.046, 0.256, 0.347, 0.268, 0.095, 0.297]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151876)[0m top1: 0.3255597014925373
[2m[36m(func pid=151876)[0m top5: 0.8264925373134329
[2m[36m(func pid=151876)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=151876)[0m f1_macro: 0.26902334535021233
[2m[36m(func pid=151876)[0m f1_weighted: 0.3505744359358663
[2m[36m(func pid=151876)[0m f1_per_class: [0.255, 0.263, 0.286, 0.411, 0.065, 0.306, 0.412, 0.298, 0.183, 0.213]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0699 | Steps: 2 | Val loss: 2.8554 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.1236 | Steps: 2 | Val loss: 7.0433 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.3653 | Steps: 2 | Val loss: 1.9020 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.1956 | Steps: 2 | Val loss: 2.1175 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=151878)[0m top1: 0.36007462686567165
[2m[36m(func pid=151878)[0m top5: 0.8680037313432836
[2m[36m(func pid=151878)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=151878)[0m f1_macro: 0.3443893365424235
[2m[36m(func pid=151878)[0m f1_weighted: 0.3868346718159208
[2m[36m(func pid=151878)[0m f1_per_class: [0.354, 0.261, 0.786, 0.474, 0.097, 0.395, 0.43, 0.254, 0.215, 0.18]
[2m[36m(func pid=151878)[0m 
== Status ==
Current time: 2024-01-07 01:52:07 (running for 00:18:28.05)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.205 |      0.212 |                   48 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  1.481 |      0.269 |                   23 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.07  |      0.344 |                   22 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.124 |      0.32  |                   15 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.324160447761194
[2m[36m(func pid=154058)[0m top5: 0.882929104477612
[2m[36m(func pid=154058)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=154058)[0m f1_macro: 0.31986343918658006
[2m[36m(func pid=154058)[0m f1_weighted: 0.34463756170358295
[2m[36m(func pid=154058)[0m f1_per_class: [0.293, 0.265, 0.786, 0.425, 0.098, 0.386, 0.339, 0.264, 0.208, 0.134]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m top1: 0.332089552238806
[2m[36m(func pid=151876)[0m top5: 0.8306902985074627
[2m[36m(func pid=151876)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=151876)[0m f1_macro: 0.2791844468939729
[2m[36m(func pid=151876)[0m f1_weighted: 0.3567203794343548
[2m[36m(func pid=151876)[0m f1_per_class: [0.255, 0.264, 0.328, 0.42, 0.063, 0.32, 0.412, 0.317, 0.185, 0.228]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=146329)[0m top1: 0.26026119402985076
[2m[36m(func pid=146329)[0m top5: 0.7593283582089553
[2m[36m(func pid=146329)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=146329)[0m f1_macro: 0.2124627089317014
[2m[36m(func pid=146329)[0m f1_weighted: 0.28384887882983295
[2m[36m(func pid=146329)[0m f1_per_class: [0.21, 0.165, 0.106, 0.339, 0.046, 0.256, 0.349, 0.271, 0.097, 0.286]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.1219 | Steps: 2 | Val loss: 2.9550 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0040 | Steps: 2 | Val loss: 7.4029 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.1756 | Steps: 2 | Val loss: 1.8856 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.1036 | Steps: 2 | Val loss: 2.1106 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=151878)[0m top1: 0.3605410447761194
[2m[36m(func pid=151878)[0m top5: 0.8656716417910447
[2m[36m(func pid=151878)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=151878)[0m f1_macro: 0.3454627700328827
[2m[36m(func pid=151878)[0m f1_weighted: 0.3865287819404238
[2m[36m(func pid=151878)[0m f1_per_class: [0.352, 0.261, 0.786, 0.472, 0.099, 0.391, 0.43, 0.259, 0.215, 0.19]
[2m[36m(func pid=151878)[0m 
== Status ==
Current time: 2024-01-07 01:52:12 (running for 00:18:33.56)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.196 |      0.212 |                   49 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  1.365 |      0.279 |                   24 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.122 |      0.345 |                   23 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.004 |      0.323 |                   16 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.32369402985074625
[2m[36m(func pid=154058)[0m top5: 0.8903917910447762
[2m[36m(func pid=154058)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=154058)[0m f1_macro: 0.3228900340291121
[2m[36m(func pid=154058)[0m f1_weighted: 0.3453589262902636
[2m[36m(func pid=154058)[0m f1_per_class: [0.31, 0.291, 0.786, 0.411, 0.096, 0.396, 0.337, 0.257, 0.208, 0.138]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m top1: 0.33488805970149255
[2m[36m(func pid=151876)[0m top5: 0.8334888059701493
[2m[36m(func pid=151876)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=151876)[0m f1_macro: 0.2854183595622138
[2m[36m(func pid=151876)[0m f1_weighted: 0.36032926962098566
[2m[36m(func pid=151876)[0m f1_per_class: [0.263, 0.269, 0.349, 0.427, 0.057, 0.333, 0.405, 0.336, 0.182, 0.232]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=146329)[0m top1: 0.2653917910447761
[2m[36m(func pid=146329)[0m top5: 0.7644589552238806
[2m[36m(func pid=146329)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=146329)[0m f1_macro: 0.21603652578067378
[2m[36m(func pid=146329)[0m f1_weighted: 0.2879045666540989
[2m[36m(func pid=146329)[0m f1_per_class: [0.208, 0.162, 0.11, 0.351, 0.057, 0.261, 0.35, 0.272, 0.098, 0.29]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.1103 | Steps: 2 | Val loss: 3.0591 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0037 | Steps: 2 | Val loss: 7.8249 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.1690 | Steps: 2 | Val loss: 1.8702 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.0781 | Steps: 2 | Val loss: 2.1074 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=151878)[0m top1: 0.3591417910447761
[2m[36m(func pid=151878)[0m top5: 0.8698694029850746
[2m[36m(func pid=151878)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=151878)[0m f1_macro: 0.3428473396393409
[2m[36m(func pid=151878)[0m f1_weighted: 0.3869742104473238
[2m[36m(func pid=151878)[0m f1_per_class: [0.343, 0.258, 0.786, 0.465, 0.086, 0.377, 0.445, 0.268, 0.212, 0.189]
[2m[36m(func pid=151878)[0m 
== Status ==
Current time: 2024-01-07 01:52:18 (running for 00:18:39.22)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.104 |      0.216 |                   50 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  1.176 |      0.285 |                   25 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.11  |      0.343 |                   24 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.004 |      0.318 |                   17 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.31949626865671643
[2m[36m(func pid=154058)[0m top5: 0.8964552238805971
[2m[36m(func pid=154058)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=154058)[0m f1_macro: 0.31816506450557575
[2m[36m(func pid=154058)[0m f1_weighted: 0.3408322478486428
[2m[36m(func pid=154058)[0m f1_per_class: [0.304, 0.299, 0.786, 0.387, 0.09, 0.403, 0.341, 0.259, 0.169, 0.145]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m top1: 0.33488805970149255
[2m[36m(func pid=151876)[0m top5: 0.8367537313432836
[2m[36m(func pid=151876)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=151876)[0m f1_macro: 0.29030223083530654
[2m[36m(func pid=151876)[0m f1_weighted: 0.35997224352445484
[2m[36m(func pid=151876)[0m f1_per_class: [0.267, 0.271, 0.393, 0.425, 0.063, 0.33, 0.404, 0.335, 0.18, 0.235]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=146329)[0m top1: 0.2644589552238806
[2m[36m(func pid=146329)[0m top5: 0.7653917910447762
[2m[36m(func pid=146329)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=146329)[0m f1_macro: 0.21540055596794008
[2m[36m(func pid=146329)[0m f1_weighted: 0.2869045079803386
[2m[36m(func pid=146329)[0m f1_per_class: [0.199, 0.157, 0.116, 0.351, 0.057, 0.259, 0.352, 0.272, 0.099, 0.292]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0639 | Steps: 2 | Val loss: 3.1797 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0376 | Steps: 2 | Val loss: 8.2611 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.1320 | Steps: 2 | Val loss: 1.8566 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.0907 | Steps: 2 | Val loss: 2.1037 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=151878)[0m top1: 0.35867537313432835
[2m[36m(func pid=151878)[0m top5: 0.8666044776119403
[2m[36m(func pid=151878)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=151878)[0m f1_macro: 0.3423732851880671
[2m[36m(func pid=151878)[0m f1_weighted: 0.38675024217195536
[2m[36m(func pid=151878)[0m f1_per_class: [0.337, 0.259, 0.786, 0.46, 0.088, 0.372, 0.451, 0.264, 0.222, 0.186]
[2m[36m(func pid=151878)[0m 
== Status ==
Current time: 2024-01-07 01:52:23 (running for 00:18:44.45)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.078 |      0.215 |                   51 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  1.169 |      0.29  |                   26 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.064 |      0.342 |                   25 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.038 |      0.32  |                   18 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=154058)[0m top1: 0.31576492537313433
[2m[36m(func pid=154058)[0m top5: 0.898320895522388
[2m[36m(func pid=154058)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=154058)[0m f1_macro: 0.31950119232381347
[2m[36m(func pid=154058)[0m f1_weighted: 0.33574794308736056
[2m[36m(func pid=154058)[0m f1_per_class: [0.324, 0.304, 0.786, 0.362, 0.089, 0.4, 0.343, 0.255, 0.178, 0.154]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m top1: 0.3353544776119403
[2m[36m(func pid=151876)[0m top5: 0.8456156716417911
[2m[36m(func pid=151876)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=151876)[0m f1_macro: 0.2940782010681137
[2m[36m(func pid=151876)[0m f1_weighted: 0.3600035695350064
[2m[36m(func pid=151876)[0m f1_per_class: [0.275, 0.275, 0.449, 0.431, 0.067, 0.327, 0.401, 0.324, 0.155, 0.236]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=146329)[0m top1: 0.2658582089552239
[2m[36m(func pid=146329)[0m top5: 0.7677238805970149
[2m[36m(func pid=146329)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=146329)[0m f1_macro: 0.2167319143001735
[2m[36m(func pid=146329)[0m f1_weighted: 0.2892853125795946
[2m[36m(func pid=146329)[0m f1_per_class: [0.196, 0.175, 0.116, 0.346, 0.057, 0.269, 0.35, 0.277, 0.085, 0.296]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2283 | Steps: 2 | Val loss: 3.2936 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.0267 | Steps: 2 | Val loss: 1.8429 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.1603 | Steps: 2 | Val loss: 8.7132 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.1016 | Steps: 2 | Val loss: 2.0970 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=151878)[0m top1: 0.3558768656716418
[2m[36m(func pid=151878)[0m top5: 0.8577425373134329
[2m[36m(func pid=151878)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=151878)[0m f1_macro: 0.3402725232824836
[2m[36m(func pid=151878)[0m f1_weighted: 0.38436797500148095
[2m[36m(func pid=151878)[0m f1_per_class: [0.322, 0.251, 0.786, 0.455, 0.087, 0.374, 0.45, 0.275, 0.219, 0.184]
[2m[36m(func pid=151878)[0m 
== Status ==
Current time: 2024-01-07 01:52:29 (running for 00:18:49.87)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.091 |      0.217 |                   52 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  1.027 |      0.303 |                   28 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.228 |      0.34  |                   26 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.038 |      0.32  |                   18 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151876)[0m top1: 0.3381529850746269
[2m[36m(func pid=151876)[0m top5: 0.8484141791044776
[2m[36m(func pid=151876)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=151876)[0m f1_macro: 0.3027414961975031
[2m[36m(func pid=151876)[0m f1_weighted: 0.36216606688738306
[2m[36m(func pid=151876)[0m f1_per_class: [0.287, 0.277, 0.489, 0.432, 0.073, 0.344, 0.396, 0.328, 0.157, 0.243]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.3148320895522388
[2m[36m(func pid=154058)[0m top5: 0.8950559701492538
[2m[36m(func pid=154058)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=154058)[0m f1_macro: 0.32137342883161557
[2m[36m(func pid=154058)[0m f1_weighted: 0.33446437279091323
[2m[36m(func pid=154058)[0m f1_per_class: [0.331, 0.308, 0.786, 0.357, 0.092, 0.396, 0.341, 0.251, 0.204, 0.149]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m top1: 0.26725746268656714
[2m[36m(func pid=146329)[0m top5: 0.7700559701492538
[2m[36m(func pid=146329)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=146329)[0m f1_macro: 0.2174185914257877
[2m[36m(func pid=146329)[0m f1_weighted: 0.28954381667452017
[2m[36m(func pid=146329)[0m f1_per_class: [0.198, 0.17, 0.122, 0.349, 0.058, 0.266, 0.352, 0.275, 0.086, 0.298]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0768 | Steps: 2 | Val loss: 3.3644 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.0338 | Steps: 2 | Val loss: 1.8357 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0009 | Steps: 2 | Val loss: 9.0726 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.0582 | Steps: 2 | Val loss: 2.0916 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=151878)[0m top1: 0.35494402985074625
[2m[36m(func pid=151878)[0m top5: 0.8647388059701493
[2m[36m(func pid=151878)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=151878)[0m f1_macro: 0.3389232111534951
[2m[36m(func pid=151878)[0m f1_weighted: 0.38262407139009486
[2m[36m(func pid=151878)[0m f1_per_class: [0.319, 0.258, 0.786, 0.448, 0.092, 0.366, 0.451, 0.271, 0.22, 0.179]
[2m[36m(func pid=151878)[0m 
== Status ==
Current time: 2024-01-07 01:52:34 (running for 00:18:55.07)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.102 |      0.217 |                   53 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  1.034 |      0.308 |                   29 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.077 |      0.339 |                   27 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.16  |      0.321 |                   19 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151876)[0m top1: 0.33908582089552236
[2m[36m(func pid=151876)[0m top5: 0.8540111940298507
[2m[36m(func pid=151876)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=151876)[0m f1_macro: 0.307516734367628
[2m[36m(func pid=151876)[0m f1_weighted: 0.3637555970655599
[2m[36m(func pid=151876)[0m f1_per_class: [0.296, 0.275, 0.512, 0.431, 0.071, 0.347, 0.4, 0.337, 0.161, 0.246]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.3162313432835821
[2m[36m(func pid=154058)[0m top5: 0.8959888059701493
[2m[36m(func pid=154058)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=154058)[0m f1_macro: 0.3255272782751263
[2m[36m(func pid=154058)[0m f1_weighted: 0.33447908873840343
[2m[36m(func pid=154058)[0m f1_per_class: [0.35, 0.314, 0.786, 0.345, 0.091, 0.395, 0.345, 0.259, 0.209, 0.162]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m top1: 0.2681902985074627
[2m[36m(func pid=146329)[0m top5: 0.7723880597014925
[2m[36m(func pid=146329)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=146329)[0m f1_macro: 0.2170591839665666
[2m[36m(func pid=146329)[0m f1_weighted: 0.2900717584723475
[2m[36m(func pid=146329)[0m f1_per_class: [0.196, 0.173, 0.126, 0.356, 0.058, 0.257, 0.35, 0.271, 0.087, 0.296]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0452 | Steps: 2 | Val loss: 3.4566 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.9265 | Steps: 2 | Val loss: 1.8343 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4751 | Steps: 2 | Val loss: 9.4575 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.0637 | Steps: 2 | Val loss: 2.0846 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=151878)[0m top1: 0.3614738805970149
[2m[36m(func pid=151878)[0m top5: 0.867070895522388
[2m[36m(func pid=151878)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=151878)[0m f1_macro: 0.34471030529492885
[2m[36m(func pid=151878)[0m f1_weighted: 0.3884376134234893
[2m[36m(func pid=151878)[0m f1_per_class: [0.322, 0.273, 0.786, 0.442, 0.099, 0.377, 0.461, 0.279, 0.22, 0.189]
[2m[36m(func pid=151878)[0m 
== Status ==
Current time: 2024-01-07 01:52:39 (running for 00:19:00.10)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.058 |      0.217 |                   54 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.926 |      0.319 |                   30 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.045 |      0.345 |                   28 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.001 |      0.326 |                   20 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151876)[0m top1: 0.33722014925373134
[2m[36m(func pid=151876)[0m top5: 0.8582089552238806
[2m[36m(func pid=151876)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=151876)[0m f1_macro: 0.3185874967181995
[2m[36m(func pid=151876)[0m f1_weighted: 0.3624517720692037
[2m[36m(func pid=151876)[0m f1_per_class: [0.296, 0.275, 0.629, 0.421, 0.069, 0.357, 0.399, 0.338, 0.159, 0.243]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=146329)[0m top1: 0.27005597014925375
[2m[36m(func pid=146329)[0m top5: 0.7770522388059702
[2m[36m(func pid=146329)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=146329)[0m f1_macro: 0.22133280505478065
[2m[36m(func pid=146329)[0m f1_weighted: 0.291490245472107
[2m[36m(func pid=146329)[0m f1_per_class: [0.199, 0.179, 0.137, 0.361, 0.056, 0.269, 0.341, 0.27, 0.089, 0.312]
[2m[36m(func pid=154058)[0m top1: 0.3125
[2m[36m(func pid=154058)[0m top5: 0.8903917910447762
[2m[36m(func pid=154058)[0m f1_micro: 0.3125
[2m[36m(func pid=154058)[0m f1_macro: 0.32155404258182524
[2m[36m(func pid=154058)[0m f1_weighted: 0.33009109425881483
[2m[36m(func pid=154058)[0m f1_per_class: [0.331, 0.311, 0.786, 0.334, 0.095, 0.389, 0.347, 0.255, 0.198, 0.17]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0284 | Steps: 2 | Val loss: 3.5487 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.9118 | Steps: 2 | Val loss: 1.8364 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.9748 | Steps: 2 | Val loss: 2.0750 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0011 | Steps: 2 | Val loss: 9.8278 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 01:52:44 (running for 00:19:05.10)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.064 |      0.221 |                   55 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.926 |      0.319 |                   30 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.028 |      0.346 |                   29 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.475 |      0.322 |                   21 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151878)[0m top1: 0.3605410447761194
[2m[36m(func pid=151878)[0m top5: 0.8703358208955224
[2m[36m(func pid=151878)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=151878)[0m f1_macro: 0.3456303319410729
[2m[36m(func pid=151878)[0m f1_weighted: 0.3879893805821711
[2m[36m(func pid=151878)[0m f1_per_class: [0.328, 0.27, 0.815, 0.431, 0.101, 0.374, 0.475, 0.265, 0.213, 0.185]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m top1: 0.3362873134328358
[2m[36m(func pid=151876)[0m top5: 0.8591417910447762
[2m[36m(func pid=151876)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=151876)[0m f1_macro: 0.32115680632035637
[2m[36m(func pid=151876)[0m f1_weighted: 0.36132096543973924
[2m[36m(func pid=151876)[0m f1_per_class: [0.303, 0.279, 0.647, 0.414, 0.069, 0.355, 0.398, 0.345, 0.159, 0.243]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=146329)[0m top1: 0.28125
[2m[36m(func pid=146329)[0m top5: 0.7798507462686567
[2m[36m(func pid=146329)[0m f1_micro: 0.28125
[2m[36m(func pid=146329)[0m f1_macro: 0.22769681554120957
[2m[36m(func pid=146329)[0m f1_weighted: 0.30239073655976073
[2m[36m(func pid=146329)[0m f1_per_class: [0.205, 0.199, 0.147, 0.383, 0.058, 0.275, 0.343, 0.271, 0.089, 0.308]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=154058)[0m top1: 0.30363805970149255
[2m[36m(func pid=154058)[0m top5: 0.8824626865671642
[2m[36m(func pid=154058)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=154058)[0m f1_macro: 0.31377706898157176
[2m[36m(func pid=154058)[0m f1_weighted: 0.3216077218711874
[2m[36m(func pid=154058)[0m f1_per_class: [0.302, 0.301, 0.786, 0.331, 0.09, 0.382, 0.334, 0.244, 0.202, 0.167]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.8555 | Steps: 2 | Val loss: 1.8368 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0785 | Steps: 2 | Val loss: 3.6564 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.0023 | Steps: 2 | Val loss: 2.0679 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0030 | Steps: 2 | Val loss: 10.1707 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=151876)[0m top1: 0.33488805970149255
[2m[36m(func pid=151876)[0m top5: 0.8656716417910447
[2m[36m(func pid=151876)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=151876)[0m f1_macro: 0.3221761039741878
[2m[36m(func pid=151876)[0m f1_weighted: 0.3609713260834891
[2m[36m(func pid=151876)[0m f1_per_class: [0.308, 0.279, 0.647, 0.412, 0.067, 0.364, 0.393, 0.344, 0.183, 0.225]
[2m[36m(func pid=151876)[0m 
== Status ==
Current time: 2024-01-07 01:52:49 (running for 00:19:10.36)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.975 |      0.228 |                   56 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.856 |      0.322 |                   32 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.028 |      0.346 |                   29 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.001 |      0.314 |                   22 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151878)[0m top1: 0.3619402985074627
[2m[36m(func pid=151878)[0m top5: 0.8703358208955224
[2m[36m(func pid=151878)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=151878)[0m f1_macro: 0.3462253627753605
[2m[36m(func pid=151878)[0m f1_weighted: 0.3897945588662537
[2m[36m(func pid=151878)[0m f1_per_class: [0.337, 0.271, 0.815, 0.427, 0.102, 0.378, 0.484, 0.253, 0.216, 0.179]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=146329)[0m top1: 0.28218283582089554
[2m[36m(func pid=146329)[0m top5: 0.7868470149253731
[2m[36m(func pid=146329)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=146329)[0m f1_macro: 0.23235560109325
[2m[36m(func pid=146329)[0m f1_weighted: 0.30340166518491096
[2m[36m(func pid=146329)[0m f1_per_class: [0.21, 0.209, 0.155, 0.375, 0.056, 0.275, 0.344, 0.274, 0.106, 0.319]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=154058)[0m top1: 0.29990671641791045
[2m[36m(func pid=154058)[0m top5: 0.8782649253731343
[2m[36m(func pid=154058)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=154058)[0m f1_macro: 0.3130062952962426
[2m[36m(func pid=154058)[0m f1_weighted: 0.3163627073947041
[2m[36m(func pid=154058)[0m f1_per_class: [0.289, 0.286, 0.786, 0.332, 0.105, 0.383, 0.323, 0.24, 0.209, 0.177]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8264 | Steps: 2 | Val loss: 1.8518 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0367 | Steps: 2 | Val loss: 3.7766 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.9909 | Steps: 2 | Val loss: 2.0622 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0216 | Steps: 2 | Val loss: 10.4737 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:52:54 (running for 00:19:15.47)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  2.002 |      0.232 |                   57 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.826 |      0.323 |                   33 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.078 |      0.346 |                   30 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.003 |      0.313 |                   23 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151876)[0m top1: 0.33115671641791045
[2m[36m(func pid=151876)[0m top5: 0.8642723880597015
[2m[36m(func pid=151876)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=151876)[0m f1_macro: 0.3229328567216908
[2m[36m(func pid=151876)[0m f1_weighted: 0.3577725854931912
[2m[36m(func pid=151876)[0m f1_per_class: [0.306, 0.277, 0.647, 0.406, 0.062, 0.385, 0.379, 0.357, 0.179, 0.231]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m top1: 0.3619402985074627
[2m[36m(func pid=151878)[0m top5: 0.8675373134328358
[2m[36m(func pid=151878)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=151878)[0m f1_macro: 0.345806661778103
[2m[36m(func pid=151878)[0m f1_weighted: 0.39044512066615766
[2m[36m(func pid=151878)[0m f1_per_class: [0.337, 0.277, 0.815, 0.416, 0.101, 0.37, 0.496, 0.255, 0.213, 0.177]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=146329)[0m top1: 0.28544776119402987
[2m[36m(func pid=146329)[0m top5: 0.7868470149253731
[2m[36m(func pid=146329)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=146329)[0m f1_macro: 0.2333444002147548
[2m[36m(func pid=146329)[0m f1_weighted: 0.3078945178847484
[2m[36m(func pid=146329)[0m f1_per_class: [0.211, 0.215, 0.157, 0.372, 0.058, 0.284, 0.355, 0.276, 0.104, 0.301]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=154058)[0m top1: 0.2989738805970149
[2m[36m(func pid=154058)[0m top5: 0.8708022388059702
[2m[36m(func pid=154058)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=154058)[0m f1_macro: 0.3122628135473479
[2m[36m(func pid=154058)[0m f1_weighted: 0.3165502659713719
[2m[36m(func pid=154058)[0m f1_per_class: [0.258, 0.28, 0.786, 0.329, 0.115, 0.387, 0.33, 0.236, 0.215, 0.187]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.8484 | Steps: 2 | Val loss: 1.8635 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0230 | Steps: 2 | Val loss: 3.8976 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.9702 | Steps: 2 | Val loss: 2.0571 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=151876)[0m top1: 0.333955223880597
[2m[36m(func pid=151876)[0m top5: 0.8689365671641791
[2m[36m(func pid=151876)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=151876)[0m f1_macro: 0.32820457392312113
[2m[36m(func pid=151876)[0m f1_weighted: 0.36053220816326764
[2m[36m(func pid=151876)[0m f1_per_class: [0.312, 0.286, 0.647, 0.404, 0.063, 0.383, 0.381, 0.361, 0.207, 0.238]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0397 | Steps: 2 | Val loss: 10.8453 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=146329)[0m top1: 0.28404850746268656
[2m[36m(func pid=146329)[0m top5: 0.7901119402985075
[2m[36m(func pid=146329)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=146329)[0m f1_macro: 0.2363710140397895
[2m[36m(func pid=146329)[0m f1_weighted: 0.30582203381564843
[2m[36m(func pid=146329)[0m f1_per_class: [0.204, 0.223, 0.163, 0.367, 0.059, 0.297, 0.341, 0.291, 0.106, 0.313]
== Status ==
Current time: 2024-01-07 01:53:01 (running for 00:19:21.94)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.97  |      0.236 |                   59 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.848 |      0.328 |                   34 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.037 |      0.346 |                   31 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.022 |      0.312 |                   24 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m top1: 0.3591417910447761
[2m[36m(func pid=151878)[0m top5: 0.8652052238805971
[2m[36m(func pid=151878)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=151878)[0m f1_macro: 0.34198990738467827
[2m[36m(func pid=151878)[0m f1_weighted: 0.38753269418396236
[2m[36m(func pid=151878)[0m f1_per_class: [0.337, 0.285, 0.786, 0.401, 0.109, 0.371, 0.496, 0.257, 0.209, 0.167]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.8754 | Steps: 2 | Val loss: 1.8570 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=154058)[0m top1: 0.29384328358208955
[2m[36m(func pid=154058)[0m top5: 0.8619402985074627
[2m[36m(func pid=154058)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=154058)[0m f1_macro: 0.3082801907771973
[2m[36m(func pid=154058)[0m f1_weighted: 0.3116788319200119
[2m[36m(func pid=154058)[0m f1_per_class: [0.25, 0.264, 0.786, 0.321, 0.12, 0.382, 0.334, 0.235, 0.208, 0.183]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.8897 | Steps: 2 | Val loss: 2.0537 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0140 | Steps: 2 | Val loss: 4.0197 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=151876)[0m top1: 0.3376865671641791
[2m[36m(func pid=151876)[0m top5: 0.8745335820895522
[2m[36m(func pid=151876)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=151876)[0m f1_macro: 0.3341775257340614
[2m[36m(func pid=151876)[0m f1_weighted: 0.36448623471858316
[2m[36m(func pid=151876)[0m f1_per_class: [0.333, 0.286, 0.667, 0.404, 0.064, 0.389, 0.39, 0.359, 0.217, 0.233]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0277 | Steps: 2 | Val loss: 11.0886 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 01:53:06 (running for 00:19:27.28)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.89  |      0.236 |                   60 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.875 |      0.334 |                   35 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.023 |      0.342 |                   32 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.04  |      0.308 |                   25 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=146329)[0m top1: 0.28451492537313433
[2m[36m(func pid=146329)[0m top5: 0.7915111940298507
[2m[36m(func pid=146329)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=146329)[0m f1_macro: 0.23621472550706696
[2m[36m(func pid=146329)[0m f1_weighted: 0.30538544428116315
[2m[36m(func pid=146329)[0m f1_per_class: [0.207, 0.225, 0.175, 0.369, 0.059, 0.285, 0.341, 0.291, 0.104, 0.307]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m top1: 0.3558768656716418
[2m[36m(func pid=151878)[0m top5: 0.8675373134328358
[2m[36m(func pid=151878)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=151878)[0m f1_macro: 0.34300384375620385
[2m[36m(func pid=151878)[0m f1_weighted: 0.38269582274470854
[2m[36m(func pid=151878)[0m f1_per_class: [0.356, 0.29, 0.815, 0.395, 0.108, 0.333, 0.496, 0.246, 0.221, 0.17]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6726 | Steps: 2 | Val loss: 1.8666 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=154058)[0m top1: 0.29617537313432835
[2m[36m(func pid=154058)[0m top5: 0.8661380597014925
[2m[36m(func pid=154058)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=154058)[0m f1_macro: 0.3057718139387553
[2m[36m(func pid=154058)[0m f1_weighted: 0.3146519303299487
[2m[36m(func pid=154058)[0m f1_per_class: [0.217, 0.261, 0.786, 0.342, 0.113, 0.383, 0.329, 0.244, 0.186, 0.198]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.8981 | Steps: 2 | Val loss: 2.0500 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0107 | Steps: 2 | Val loss: 4.1472 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=151876)[0m top1: 0.3400186567164179
[2m[36m(func pid=151876)[0m top5: 0.8782649253731343
[2m[36m(func pid=151876)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=151876)[0m f1_macro: 0.34025054220564205
[2m[36m(func pid=151876)[0m f1_weighted: 0.36654616716363514
[2m[36m(func pid=151876)[0m f1_per_class: [0.346, 0.294, 0.688, 0.402, 0.063, 0.388, 0.391, 0.36, 0.225, 0.245]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3904 | Steps: 2 | Val loss: 11.4699 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 01:53:11 (running for 00:19:32.59)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.898 |      0.239 |                   61 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.673 |      0.34  |                   36 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.014 |      0.343 |                   33 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.028 |      0.306 |                   26 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=146329)[0m top1: 0.2868470149253731
[2m[36m(func pid=146329)[0m top5: 0.7910447761194029
[2m[36m(func pid=146329)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=146329)[0m f1_macro: 0.23855004415042863
[2m[36m(func pid=146329)[0m f1_weighted: 0.3085522774539345
[2m[36m(func pid=146329)[0m f1_per_class: [0.206, 0.23, 0.179, 0.37, 0.059, 0.292, 0.344, 0.292, 0.122, 0.293]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m top1: 0.355410447761194
[2m[36m(func pid=151878)[0m top5: 0.8666044776119403
[2m[36m(func pid=151878)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=151878)[0m f1_macro: 0.34022212862256385
[2m[36m(func pid=151878)[0m f1_weighted: 0.3806397955890049
[2m[36m(func pid=151878)[0m f1_per_class: [0.362, 0.292, 0.786, 0.374, 0.107, 0.342, 0.506, 0.237, 0.22, 0.175]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6589 | Steps: 2 | Val loss: 1.8868 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=154058)[0m top1: 0.29524253731343286
[2m[36m(func pid=154058)[0m top5: 0.8656716417910447
[2m[36m(func pid=154058)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=154058)[0m f1_macro: 0.30517097849740804
[2m[36m(func pid=154058)[0m f1_weighted: 0.3140559986692787
[2m[36m(func pid=154058)[0m f1_per_class: [0.195, 0.247, 0.786, 0.346, 0.114, 0.388, 0.328, 0.254, 0.186, 0.207]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.8529 | Steps: 2 | Val loss: 2.0438 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0768 | Steps: 2 | Val loss: 4.2424 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=151876)[0m top1: 0.3376865671641791
[2m[36m(func pid=151876)[0m top5: 0.8745335820895522
[2m[36m(func pid=151876)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=151876)[0m f1_macro: 0.3402230450514534
[2m[36m(func pid=151876)[0m f1_weighted: 0.3637861060726942
[2m[36m(func pid=151876)[0m f1_per_class: [0.333, 0.298, 0.71, 0.397, 0.064, 0.386, 0.386, 0.362, 0.221, 0.247]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0025 | Steps: 2 | Val loss: 11.8072 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 01:53:17 (running for 00:19:37.93)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.853 |      0.237 |                   62 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.659 |      0.34  |                   37 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.011 |      0.34  |                   34 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.39  |      0.305 |                   27 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=146329)[0m top1: 0.28638059701492535
[2m[36m(func pid=146329)[0m top5: 0.7924440298507462
[2m[36m(func pid=146329)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=146329)[0m f1_macro: 0.23706501931512144
[2m[36m(func pid=146329)[0m f1_weighted: 0.3075828123561552
[2m[36m(func pid=146329)[0m f1_per_class: [0.205, 0.232, 0.18, 0.368, 0.06, 0.292, 0.344, 0.288, 0.106, 0.297]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151878)[0m top1: 0.353544776119403
[2m[36m(func pid=151878)[0m top5: 0.8661380597014925
[2m[36m(func pid=151878)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=151878)[0m f1_macro: 0.33809662466783297
[2m[36m(func pid=151878)[0m f1_weighted: 0.37835555985132585
[2m[36m(func pid=151878)[0m f1_per_class: [0.365, 0.287, 0.786, 0.367, 0.108, 0.322, 0.516, 0.243, 0.212, 0.176]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6060 | Steps: 2 | Val loss: 1.8948 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=154058)[0m top1: 0.3003731343283582
[2m[36m(func pid=154058)[0m top5: 0.8666044776119403
[2m[36m(func pid=154058)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=154058)[0m f1_macro: 0.30897799629095973
[2m[36m(func pid=154058)[0m f1_weighted: 0.3217364097694639
[2m[36m(func pid=154058)[0m f1_per_class: [0.182, 0.247, 0.786, 0.345, 0.118, 0.4, 0.349, 0.263, 0.195, 0.206]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.8713 | Steps: 2 | Val loss: 2.0369 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=151876)[0m top1: 0.33722014925373134
[2m[36m(func pid=151876)[0m top5: 0.8773320895522388
[2m[36m(func pid=151876)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=151876)[0m f1_macro: 0.34305985417684426
[2m[36m(func pid=151876)[0m f1_weighted: 0.36336788950881277
[2m[36m(func pid=151876)[0m f1_per_class: [0.35, 0.296, 0.733, 0.404, 0.061, 0.393, 0.378, 0.348, 0.213, 0.253]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0089 | Steps: 2 | Val loss: 4.3432 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0007 | Steps: 2 | Val loss: 12.2310 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=146329)[0m top1: 0.2868470149253731
[2m[36m(func pid=146329)[0m top5: 0.7975746268656716
[2m[36m(func pid=146329)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=146329)[0m f1_macro: 0.23818944615833376
[2m[36m(func pid=146329)[0m f1_weighted: 0.30892777002102456
[2m[36m(func pid=146329)[0m f1_per_class: [0.192, 0.228, 0.202, 0.372, 0.061, 0.291, 0.347, 0.291, 0.107, 0.291]
[2m[36m(func pid=146329)[0m 
== Status ==
Current time: 2024-01-07 01:53:22 (running for 00:19:43.56)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.871 |      0.238 |                   63 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.606 |      0.343 |                   38 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.009 |      0.339 |                   36 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.002 |      0.309 |                   28 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151878)[0m top1: 0.35447761194029853
[2m[36m(func pid=151878)[0m top5: 0.8680037313432836
[2m[36m(func pid=151878)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=151878)[0m f1_macro: 0.3388969825801692
[2m[36m(func pid=151878)[0m f1_weighted: 0.37837101477338864
[2m[36m(func pid=151878)[0m f1_per_class: [0.382, 0.282, 0.786, 0.361, 0.108, 0.32, 0.524, 0.238, 0.215, 0.174]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6527 | Steps: 2 | Val loss: 1.8924 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=154058)[0m top1: 0.2989738805970149
[2m[36m(func pid=154058)[0m top5: 0.8638059701492538
[2m[36m(func pid=154058)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=154058)[0m f1_macro: 0.3052623072247959
[2m[36m(func pid=154058)[0m f1_weighted: 0.3229089983762731
[2m[36m(func pid=154058)[0m f1_per_class: [0.17, 0.236, 0.786, 0.354, 0.104, 0.399, 0.352, 0.272, 0.184, 0.197]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.8035 | Steps: 2 | Val loss: 2.0327 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=151876)[0m top1: 0.3376865671641791
[2m[36m(func pid=151876)[0m top5: 0.8805970149253731
[2m[36m(func pid=151876)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=151876)[0m f1_macro: 0.34360541114247745
[2m[36m(func pid=151876)[0m f1_weighted: 0.3628461420484363
[2m[36m(func pid=151876)[0m f1_per_class: [0.356, 0.295, 0.733, 0.404, 0.063, 0.397, 0.376, 0.338, 0.218, 0.255]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0140 | Steps: 2 | Val loss: 4.4602 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=146329)[0m top1: 0.2887126865671642
[2m[36m(func pid=146329)[0m top5: 0.7994402985074627
[2m[36m(func pid=146329)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=146329)[0m f1_macro: 0.2405183544418029
[2m[36m(func pid=146329)[0m f1_weighted: 0.3109158180514312
[2m[36m(func pid=146329)[0m f1_per_class: [0.196, 0.229, 0.202, 0.374, 0.059, 0.311, 0.342, 0.296, 0.11, 0.286]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.1296 | Steps: 2 | Val loss: 12.7974 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.5697 | Steps: 2 | Val loss: 1.9186 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 01:53:28 (running for 00:19:48.89)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.804 |      0.241 |                   64 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.653 |      0.344 |                   39 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.014 |      0.34  |                   37 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.001 |      0.305 |                   29 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151878)[0m top1: 0.35261194029850745
[2m[36m(func pid=151878)[0m top5: 0.8698694029850746
[2m[36m(func pid=151878)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=151878)[0m f1_macro: 0.3396437464197196
[2m[36m(func pid=151878)[0m f1_weighted: 0.37659179033524454
[2m[36m(func pid=151878)[0m f1_per_class: [0.389, 0.288, 0.815, 0.355, 0.107, 0.32, 0.522, 0.233, 0.198, 0.169]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=154058)[0m top1: 0.2896455223880597
[2m[36m(func pid=154058)[0m top5: 0.8614738805970149
[2m[36m(func pid=154058)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=154058)[0m f1_macro: 0.29790197812090996
[2m[36m(func pid=154058)[0m f1_weighted: 0.31464349267163005
[2m[36m(func pid=154058)[0m f1_per_class: [0.157, 0.223, 0.786, 0.356, 0.098, 0.382, 0.34, 0.264, 0.179, 0.195]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.7685 | Steps: 2 | Val loss: 2.0266 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=151876)[0m top1: 0.3362873134328358
[2m[36m(func pid=151876)[0m top5: 0.8777985074626866
[2m[36m(func pid=151876)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=151876)[0m f1_macro: 0.34449862907133455
[2m[36m(func pid=151876)[0m f1_weighted: 0.36201105208637213
[2m[36m(func pid=151876)[0m f1_per_class: [0.365, 0.289, 0.733, 0.401, 0.061, 0.408, 0.374, 0.344, 0.214, 0.256]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0145 | Steps: 2 | Val loss: 4.5728 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=146329)[0m top1: 0.28824626865671643
[2m[36m(func pid=146329)[0m top5: 0.8017723880597015
[2m[36m(func pid=146329)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=146329)[0m f1_macro: 0.24142538843068279
[2m[36m(func pid=146329)[0m f1_weighted: 0.3106993318039278
[2m[36m(func pid=146329)[0m f1_per_class: [0.199, 0.229, 0.21, 0.37, 0.058, 0.318, 0.342, 0.295, 0.111, 0.282]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0619 | Steps: 2 | Val loss: 12.9974 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.5015 | Steps: 2 | Val loss: 1.9341 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 01:53:33 (running for 00:19:54.40)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.769 |      0.241 |                   65 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.57  |      0.344 |                   40 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.014 |      0.342 |                   38 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.13  |      0.298 |                   30 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151878)[0m top1: 0.35774253731343286
[2m[36m(func pid=151878)[0m top5: 0.8694029850746269
[2m[36m(func pid=151878)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=151878)[0m f1_macro: 0.3422757325019364
[2m[36m(func pid=151878)[0m f1_weighted: 0.38020923870410844
[2m[36m(func pid=151878)[0m f1_per_class: [0.389, 0.298, 0.815, 0.355, 0.11, 0.321, 0.529, 0.232, 0.198, 0.176]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.7333 | Steps: 2 | Val loss: 2.0229 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=151876)[0m top1: 0.3358208955223881
[2m[36m(func pid=151876)[0m top5: 0.8773320895522388
[2m[36m(func pid=151876)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=151876)[0m f1_macro: 0.3474504870880268
[2m[36m(func pid=151876)[0m f1_weighted: 0.36158463382659
[2m[36m(func pid=151876)[0m f1_per_class: [0.371, 0.293, 0.759, 0.397, 0.059, 0.413, 0.372, 0.339, 0.211, 0.26]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.28824626865671643
[2m[36m(func pid=154058)[0m top5: 0.8614738805970149
[2m[36m(func pid=154058)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=154058)[0m f1_macro: 0.29635278292205647
[2m[36m(func pid=154058)[0m f1_weighted: 0.3131788132992939
[2m[36m(func pid=154058)[0m f1_per_class: [0.15, 0.208, 0.786, 0.364, 0.099, 0.38, 0.337, 0.262, 0.18, 0.198]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0146 | Steps: 2 | Val loss: 4.6863 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=146329)[0m top1: 0.2891791044776119
[2m[36m(func pid=146329)[0m top5: 0.8013059701492538
[2m[36m(func pid=146329)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=146329)[0m f1_macro: 0.2429415268004822
[2m[36m(func pid=146329)[0m f1_weighted: 0.31110534833721754
[2m[36m(func pid=146329)[0m f1_per_class: [0.202, 0.227, 0.222, 0.376, 0.058, 0.314, 0.34, 0.298, 0.109, 0.284]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6935 | Steps: 2 | Val loss: 1.9381 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0003 | Steps: 2 | Val loss: 13.2373 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 01:53:39 (running for 00:19:59.87)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.733 |      0.243 |                   66 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.502 |      0.347 |                   41 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.015 |      0.34  |                   39 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.062 |      0.296 |                   31 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151878)[0m top1: 0.35867537313432835
[2m[36m(func pid=151878)[0m top5: 0.8694029850746269
[2m[36m(func pid=151878)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=151878)[0m f1_macro: 0.3402248170217184
[2m[36m(func pid=151878)[0m f1_weighted: 0.37832028538306406
[2m[36m(func pid=151878)[0m f1_per_class: [0.389, 0.306, 0.815, 0.348, 0.115, 0.303, 0.533, 0.222, 0.194, 0.177]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.7182 | Steps: 2 | Val loss: 2.0191 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=151876)[0m top1: 0.3376865671641791
[2m[36m(func pid=151876)[0m top5: 0.878731343283582
[2m[36m(func pid=151876)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=151876)[0m f1_macro: 0.34624997698267085
[2m[36m(func pid=151876)[0m f1_weighted: 0.36300680349913955
[2m[36m(func pid=151876)[0m f1_per_class: [0.383, 0.294, 0.733, 0.403, 0.066, 0.411, 0.371, 0.34, 0.207, 0.253]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.291044776119403
[2m[36m(func pid=154058)[0m top5: 0.863339552238806
[2m[36m(func pid=154058)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=154058)[0m f1_macro: 0.296401140978195
[2m[36m(func pid=154058)[0m f1_weighted: 0.3176762346152478
[2m[36m(func pid=154058)[0m f1_per_class: [0.145, 0.202, 0.786, 0.371, 0.104, 0.372, 0.353, 0.259, 0.178, 0.193]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0105 | Steps: 2 | Val loss: 4.7799 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=146329)[0m top1: 0.2905783582089552
[2m[36m(func pid=146329)[0m top5: 0.8017723880597015
[2m[36m(func pid=146329)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=146329)[0m f1_macro: 0.24482546361852667
[2m[36m(func pid=146329)[0m f1_weighted: 0.3134717945132537
[2m[36m(func pid=146329)[0m f1_per_class: [0.202, 0.23, 0.227, 0.378, 0.055, 0.322, 0.341, 0.3, 0.109, 0.284]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4761 | Steps: 2 | Val loss: 1.9566 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0118 | Steps: 2 | Val loss: 13.2431 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 01:53:44 (running for 00:20:05.39)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.718 |      0.245 |                   67 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.693 |      0.346 |                   42 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.011 |      0.342 |                   40 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0     |      0.296 |                   32 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151878)[0m top1: 0.36007462686567165
[2m[36m(func pid=151878)[0m top5: 0.8703358208955224
[2m[36m(func pid=151878)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=151878)[0m f1_macro: 0.3415105468985361
[2m[36m(func pid=151878)[0m f1_weighted: 0.37979794287543656
[2m[36m(func pid=151878)[0m f1_per_class: [0.406, 0.304, 0.815, 0.353, 0.116, 0.306, 0.533, 0.217, 0.194, 0.172]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.8220 | Steps: 2 | Val loss: 2.0133 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=151876)[0m top1: 0.33908582089552236
[2m[36m(func pid=151876)[0m top5: 0.882929104477612
[2m[36m(func pid=151876)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=151876)[0m f1_macro: 0.3481462033931112
[2m[36m(func pid=151876)[0m f1_weighted: 0.36473075354482576
[2m[36m(func pid=151876)[0m f1_per_class: [0.378, 0.292, 0.759, 0.407, 0.062, 0.414, 0.374, 0.34, 0.199, 0.256]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.3045708955223881
[2m[36m(func pid=154058)[0m top5: 0.8675373134328358
[2m[36m(func pid=154058)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=154058)[0m f1_macro: 0.30256138514743514
[2m[36m(func pid=154058)[0m f1_weighted: 0.3314903970450313
[2m[36m(func pid=154058)[0m f1_per_class: [0.152, 0.208, 0.786, 0.393, 0.105, 0.368, 0.375, 0.267, 0.179, 0.192]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0119 | Steps: 2 | Val loss: 4.8815 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=146329)[0m top1: 0.29011194029850745
[2m[36m(func pid=146329)[0m top5: 0.804570895522388
[2m[36m(func pid=146329)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=146329)[0m f1_macro: 0.24427320774181896
[2m[36m(func pid=146329)[0m f1_weighted: 0.3115112395689857
[2m[36m(func pid=146329)[0m f1_per_class: [0.211, 0.227, 0.224, 0.382, 0.054, 0.302, 0.338, 0.306, 0.112, 0.288]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4506 | Steps: 2 | Val loss: 1.9770 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0025 | Steps: 2 | Val loss: 13.4143 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 01:53:49 (running for 00:20:10.69)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.822 |      0.244 |                   68 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.476 |      0.348 |                   43 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.012 |      0.341 |                   41 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.012 |      0.303 |                   33 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151878)[0m top1: 0.3628731343283582
[2m[36m(func pid=151878)[0m top5: 0.8722014925373134
[2m[36m(func pid=151878)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=151878)[0m f1_macro: 0.3410675387184307
[2m[36m(func pid=151878)[0m f1_weighted: 0.38099978491206704
[2m[36m(func pid=151878)[0m f1_per_class: [0.4, 0.299, 0.815, 0.347, 0.115, 0.307, 0.547, 0.212, 0.181, 0.187]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m top1: 0.3381529850746269
[2m[36m(func pid=151876)[0m top5: 0.8833955223880597
[2m[36m(func pid=151876)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=151876)[0m f1_macro: 0.3457201616601948
[2m[36m(func pid=151876)[0m f1_weighted: 0.3633711430203117
[2m[36m(func pid=151876)[0m f1_per_class: [0.38, 0.295, 0.759, 0.406, 0.064, 0.406, 0.375, 0.326, 0.194, 0.252]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.6690 | Steps: 2 | Val loss: 2.0095 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=154058)[0m top1: 0.3069029850746269
[2m[36m(func pid=154058)[0m top5: 0.867070895522388
[2m[36m(func pid=154058)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=154058)[0m f1_macro: 0.30106904409535007
[2m[36m(func pid=154058)[0m f1_weighted: 0.334863452373129
[2m[36m(func pid=154058)[0m f1_per_class: [0.151, 0.205, 0.786, 0.395, 0.101, 0.357, 0.392, 0.261, 0.176, 0.187]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0410 | Steps: 2 | Val loss: 5.0006 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=146329)[0m top1: 0.2933768656716418
[2m[36m(func pid=146329)[0m top5: 0.8036380597014925
[2m[36m(func pid=146329)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=146329)[0m f1_macro: 0.24762917681340085
[2m[36m(func pid=146329)[0m f1_weighted: 0.31467789052264583
[2m[36m(func pid=146329)[0m f1_per_class: [0.213, 0.228, 0.237, 0.385, 0.055, 0.314, 0.338, 0.314, 0.112, 0.28]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5969 | Steps: 2 | Val loss: 2.0016 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.1984 | Steps: 2 | Val loss: 13.5759 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 01:53:55 (running for 00:20:16.04)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.669 |      0.248 |                   69 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.451 |      0.346 |                   44 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.041 |      0.344 |                   42 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.002 |      0.301 |                   34 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151878)[0m top1: 0.36240671641791045
[2m[36m(func pid=151878)[0m top5: 0.8722014925373134
[2m[36m(func pid=151878)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=151878)[0m f1_macro: 0.34365794699581453
[2m[36m(func pid=151878)[0m f1_weighted: 0.3805570547292299
[2m[36m(func pid=151878)[0m f1_per_class: [0.406, 0.306, 0.815, 0.346, 0.125, 0.306, 0.542, 0.209, 0.199, 0.183]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.8309 | Steps: 2 | Val loss: 2.0083 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=151876)[0m top1: 0.3423507462686567
[2m[36m(func pid=151876)[0m top5: 0.8810634328358209
[2m[36m(func pid=151876)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=151876)[0m f1_macro: 0.34614067217260347
[2m[36m(func pid=151876)[0m f1_weighted: 0.3683732625562606
[2m[36m(func pid=151876)[0m f1_per_class: [0.376, 0.306, 0.759, 0.403, 0.067, 0.41, 0.389, 0.323, 0.189, 0.24]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.31763059701492535
[2m[36m(func pid=154058)[0m top5: 0.8703358208955224
[2m[36m(func pid=154058)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=154058)[0m f1_macro: 0.3069445460184258
[2m[36m(func pid=154058)[0m f1_weighted: 0.3465395386353589
[2m[36m(func pid=154058)[0m f1_per_class: [0.163, 0.211, 0.786, 0.399, 0.097, 0.357, 0.422, 0.264, 0.179, 0.191]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0131 | Steps: 2 | Val loss: 5.0870 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=146329)[0m top1: 0.2896455223880597
[2m[36m(func pid=146329)[0m top5: 0.8055037313432836
[2m[36m(func pid=146329)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=146329)[0m f1_macro: 0.245660235219684
[2m[36m(func pid=146329)[0m f1_weighted: 0.31168025485434575
[2m[36m(func pid=146329)[0m f1_per_class: [0.209, 0.232, 0.227, 0.37, 0.054, 0.312, 0.341, 0.307, 0.127, 0.277]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5608 | Steps: 2 | Val loss: 2.0126 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2251 | Steps: 2 | Val loss: 13.4989 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 01:54:00 (running for 00:20:21.45)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.831 |      0.246 |                   70 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.597 |      0.346 |                   45 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.013 |      0.343 |                   43 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.198 |      0.307 |                   35 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151878)[0m top1: 0.3619402985074627
[2m[36m(func pid=151878)[0m top5: 0.8717350746268657
[2m[36m(func pid=151878)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=151878)[0m f1_macro: 0.3434134186365281
[2m[36m(func pid=151878)[0m f1_weighted: 0.3804906786344556
[2m[36m(func pid=151878)[0m f1_per_class: [0.409, 0.306, 0.815, 0.351, 0.111, 0.305, 0.536, 0.216, 0.198, 0.188]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m top1: 0.3414179104477612
[2m[36m(func pid=151876)[0m top5: 0.8824626865671642
[2m[36m(func pid=151876)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=151876)[0m f1_macro: 0.3477452863030624
[2m[36m(func pid=151876)[0m f1_weighted: 0.3677813211241204
[2m[36m(func pid=151876)[0m f1_per_class: [0.368, 0.295, 0.786, 0.403, 0.068, 0.411, 0.392, 0.332, 0.182, 0.241]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.7689 | Steps: 2 | Val loss: 1.9992 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=154058)[0m top1: 0.3283582089552239
[2m[36m(func pid=154058)[0m top5: 0.875
[2m[36m(func pid=154058)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=154058)[0m f1_macro: 0.3128696700497225
[2m[36m(func pid=154058)[0m f1_weighted: 0.35777067878973673
[2m[36m(func pid=154058)[0m f1_per_class: [0.174, 0.241, 0.786, 0.4, 0.097, 0.354, 0.441, 0.268, 0.184, 0.184]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0101 | Steps: 2 | Val loss: 5.2293 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=146329)[0m top1: 0.2943097014925373
[2m[36m(func pid=146329)[0m top5: 0.8078358208955224
[2m[36m(func pid=146329)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=146329)[0m f1_macro: 0.24961089829343558
[2m[36m(func pid=146329)[0m f1_weighted: 0.3158597153681223
[2m[36m(func pid=146329)[0m f1_per_class: [0.22, 0.234, 0.232, 0.378, 0.054, 0.309, 0.346, 0.305, 0.128, 0.289]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4227 | Steps: 2 | Val loss: 2.0157 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0011 | Steps: 2 | Val loss: 13.4995 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 01:54:06 (running for 00:20:26.80)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.769 |      0.25  |                   71 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.561 |      0.348 |                   46 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.01  |      0.344 |                   44 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.225 |      0.313 |                   36 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151878)[0m top1: 0.36380597014925375
[2m[36m(func pid=151878)[0m top5: 0.8698694029850746
[2m[36m(func pid=151878)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=151878)[0m f1_macro: 0.3441630261948519
[2m[36m(func pid=151878)[0m f1_weighted: 0.38066410930847083
[2m[36m(func pid=151878)[0m f1_per_class: [0.412, 0.31, 0.815, 0.34, 0.108, 0.302, 0.544, 0.221, 0.2, 0.19]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m top1: 0.34421641791044777
[2m[36m(func pid=151876)[0m top5: 0.882929104477612
[2m[36m(func pid=151876)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=151876)[0m f1_macro: 0.3439187554810361
[2m[36m(func pid=151876)[0m f1_weighted: 0.3699800732549975
[2m[36m(func pid=151876)[0m f1_per_class: [0.347, 0.303, 0.759, 0.403, 0.073, 0.407, 0.398, 0.336, 0.178, 0.236]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.6186 | Steps: 2 | Val loss: 1.9921 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=154058)[0m top1: 0.3362873134328358
[2m[36m(func pid=154058)[0m top5: 0.8852611940298507
[2m[36m(func pid=154058)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=154058)[0m f1_macro: 0.31767963996580145
[2m[36m(func pid=154058)[0m f1_weighted: 0.3642657051534096
[2m[36m(func pid=154058)[0m f1_per_class: [0.208, 0.254, 0.786, 0.396, 0.102, 0.357, 0.459, 0.242, 0.198, 0.174]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0350 | Steps: 2 | Val loss: 5.2938 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=146329)[0m top1: 0.2971082089552239
[2m[36m(func pid=146329)[0m top5: 0.8111007462686567
[2m[36m(func pid=146329)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=146329)[0m f1_macro: 0.24932394169498345
[2m[36m(func pid=146329)[0m f1_weighted: 0.3193335743116358
[2m[36m(func pid=146329)[0m f1_per_class: [0.223, 0.239, 0.234, 0.377, 0.053, 0.306, 0.359, 0.31, 0.113, 0.28]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5750 | Steps: 2 | Val loss: 2.0269 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0011 | Steps: 2 | Val loss: 13.5259 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 01:54:11 (running for 00:20:32.13)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.619 |      0.249 |                   72 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.423 |      0.344 |                   47 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.035 |      0.342 |                   45 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.001 |      0.318 |                   37 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151878)[0m top1: 0.3619402985074627
[2m[36m(func pid=151878)[0m top5: 0.8703358208955224
[2m[36m(func pid=151878)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=151878)[0m f1_macro: 0.34200752068574547
[2m[36m(func pid=151878)[0m f1_weighted: 0.37801178366162996
[2m[36m(func pid=151878)[0m f1_per_class: [0.394, 0.307, 0.815, 0.332, 0.121, 0.303, 0.547, 0.219, 0.187, 0.194]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m top1: 0.3460820895522388
[2m[36m(func pid=151876)[0m top5: 0.8880597014925373
[2m[36m(func pid=151876)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=151876)[0m f1_macro: 0.3464451979208619
[2m[36m(func pid=151876)[0m f1_weighted: 0.3710256612581409
[2m[36m(func pid=151876)[0m f1_per_class: [0.364, 0.304, 0.786, 0.404, 0.078, 0.409, 0.402, 0.312, 0.176, 0.23]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.6029 | Steps: 2 | Val loss: 1.9897 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=154058)[0m top1: 0.3414179104477612
[2m[36m(func pid=154058)[0m top5: 0.8955223880597015
[2m[36m(func pid=154058)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=154058)[0m f1_macro: 0.314894963496745
[2m[36m(func pid=154058)[0m f1_weighted: 0.36754084481929417
[2m[36m(func pid=154058)[0m f1_per_class: [0.207, 0.264, 0.786, 0.407, 0.108, 0.329, 0.472, 0.213, 0.192, 0.173]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.1489 | Steps: 2 | Val loss: 5.4143 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=146329)[0m top1: 0.2947761194029851
[2m[36m(func pid=146329)[0m top5: 0.8120335820895522
[2m[36m(func pid=146329)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=146329)[0m f1_macro: 0.25035066426328206
[2m[36m(func pid=146329)[0m f1_weighted: 0.3172254677619347
[2m[36m(func pid=146329)[0m f1_per_class: [0.225, 0.238, 0.25, 0.376, 0.049, 0.306, 0.352, 0.313, 0.115, 0.28]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3374 | Steps: 2 | Val loss: 2.0388 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0015 | Steps: 2 | Val loss: 13.6683 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=151878)[0m top1: 0.3591417910447761
[2m[36m(func pid=151878)[0m top5: 0.8642723880597015
[2m[36m(func pid=151878)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=151878)[0m f1_macro: 0.33939516395014313
[2m[36m(func pid=151878)[0m f1_weighted: 0.3761754070553677
[2m[36m(func pid=151878)[0m f1_per_class: [0.389, 0.298, 0.815, 0.331, 0.12, 0.3, 0.548, 0.223, 0.183, 0.187]
== Status ==
Current time: 2024-01-07 01:54:17 (running for 00:20:37.72)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.603 |      0.25  |                   73 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.575 |      0.346 |                   48 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.149 |      0.339 |                   46 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.001 |      0.315 |                   38 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m top1: 0.34654850746268656
[2m[36m(func pid=151876)[0m top5: 0.8871268656716418
[2m[36m(func pid=151876)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=151876)[0m f1_macro: 0.3456676643011659
[2m[36m(func pid=151876)[0m f1_weighted: 0.371245272752422
[2m[36m(func pid=151876)[0m f1_per_class: [0.364, 0.307, 0.786, 0.404, 0.07, 0.418, 0.399, 0.307, 0.175, 0.226]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.6705 | Steps: 2 | Val loss: 1.9906 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=154058)[0m top1: 0.3460820895522388
[2m[36m(func pid=154058)[0m top5: 0.8997201492537313
[2m[36m(func pid=154058)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=154058)[0m f1_macro: 0.31795949692734937
[2m[36m(func pid=154058)[0m f1_weighted: 0.3713610250520457
[2m[36m(func pid=154058)[0m f1_per_class: [0.223, 0.277, 0.786, 0.41, 0.102, 0.329, 0.473, 0.211, 0.194, 0.174]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0047 | Steps: 2 | Val loss: 5.5095 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=146329)[0m top1: 0.29011194029850745
[2m[36m(func pid=146329)[0m top5: 0.8106343283582089
[2m[36m(func pid=146329)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=146329)[0m f1_macro: 0.24798078527169967
[2m[36m(func pid=146329)[0m f1_weighted: 0.3129241019435255
[2m[36m(func pid=146329)[0m f1_per_class: [0.224, 0.235, 0.256, 0.372, 0.047, 0.303, 0.345, 0.309, 0.114, 0.275]
[2m[36m(func pid=146329)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3736 | Steps: 2 | Val loss: 2.0443 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.1751 | Steps: 2 | Val loss: 13.9473 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 01:54:22 (running for 00:20:43.06)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00008 | RUNNING    | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.671 |      0.248 |                   74 |
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.337 |      0.346 |                   49 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.005 |      0.339 |                   47 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.001 |      0.318 |                   39 |
| train_66d79_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151878)[0m top1: 0.36007462686567165
[2m[36m(func pid=151878)[0m top5: 0.8624067164179104
[2m[36m(func pid=151878)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=151878)[0m f1_macro: 0.3392819957582355
[2m[36m(func pid=151878)[0m f1_weighted: 0.37687072715156156
[2m[36m(func pid=151878)[0m f1_per_class: [0.384, 0.299, 0.815, 0.327, 0.12, 0.304, 0.552, 0.225, 0.183, 0.184]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m top1: 0.345615671641791
[2m[36m(func pid=151876)[0m top5: 0.8889925373134329
[2m[36m(func pid=151876)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=151876)[0m f1_macro: 0.34408477190790737
[2m[36m(func pid=151876)[0m f1_weighted: 0.37053226333457456
[2m[36m(func pid=151876)[0m f1_per_class: [0.373, 0.302, 0.786, 0.408, 0.071, 0.401, 0.405, 0.293, 0.169, 0.233]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=146329)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.6406 | Steps: 2 | Val loss: 1.9863 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=154058)[0m top1: 0.345615671641791
[2m[36m(func pid=154058)[0m top5: 0.902518656716418
[2m[36m(func pid=154058)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=154058)[0m f1_macro: 0.31953073299593526
[2m[36m(func pid=154058)[0m f1_weighted: 0.3695380532576754
[2m[36m(func pid=154058)[0m f1_per_class: [0.257, 0.29, 0.786, 0.396, 0.097, 0.32, 0.475, 0.209, 0.184, 0.181]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=146329)[0m top1: 0.2905783582089552
[2m[36m(func pid=146329)[0m top5: 0.8125
[2m[36m(func pid=146329)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=146329)[0m f1_macro: 0.24806540987717693
[2m[36m(func pid=146329)[0m f1_weighted: 0.3132301657220295
[2m[36m(func pid=146329)[0m f1_per_class: [0.231, 0.234, 0.244, 0.374, 0.046, 0.304, 0.344, 0.308, 0.114, 0.281]
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0036 | Steps: 2 | Val loss: 5.6026 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3357 | Steps: 2 | Val loss: 2.0635 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0003 | Steps: 2 | Val loss: 14.0158 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=151876)[0m top1: 0.34654850746268656
[2m[36m(func pid=151876)[0m top5: 0.8899253731343284
[2m[36m(func pid=151876)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=151876)[0m f1_macro: 0.34126468846901664
[2m[36m(func pid=151876)[0m f1_weighted: 0.37177066430993255
[2m[36m(func pid=151876)[0m f1_per_class: [0.359, 0.303, 0.786, 0.411, 0.072, 0.402, 0.411, 0.269, 0.169, 0.23]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m top1: 0.3582089552238806
[2m[36m(func pid=151878)[0m top5: 0.8614738805970149
[2m[36m(func pid=151878)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=151878)[0m f1_macro: 0.3380648312648331
[2m[36m(func pid=151878)[0m f1_weighted: 0.37554814036842354
[2m[36m(func pid=151878)[0m f1_per_class: [0.395, 0.297, 0.815, 0.328, 0.118, 0.303, 0.55, 0.217, 0.184, 0.176]
[2m[36m(func pid=154058)[0m top1: 0.3530783582089552
[2m[36m(func pid=154058)[0m top5: 0.9048507462686567
[2m[36m(func pid=154058)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=154058)[0m f1_macro: 0.3241178447464325
[2m[36m(func pid=154058)[0m f1_weighted: 0.37566367625262065
[2m[36m(func pid=154058)[0m f1_per_class: [0.246, 0.303, 0.815, 0.39, 0.104, 0.325, 0.494, 0.19, 0.193, 0.18]
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3537 | Steps: 2 | Val loss: 2.0804 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=151876)[0m top1: 0.34794776119402987
[2m[36m(func pid=151876)[0m top5: 0.8908582089552238
[2m[36m(func pid=151876)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=151876)[0m f1_macro: 0.34539717287863386
[2m[36m(func pid=151876)[0m f1_weighted: 0.372735196710437
[2m[36m(func pid=151876)[0m f1_per_class: [0.355, 0.305, 0.786, 0.407, 0.074, 0.408, 0.408, 0.295, 0.185, 0.23]
== Status ==
Current time: 2024-01-07 01:54:27 (running for 00:20:48.43)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.336 |      0.341 |                   51 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.005 |      0.339 |                   47 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.175 |      0.32  |                   40 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=163416)[0m Dataloader to compute accuracy: val
== Status ==
Current time: 2024-01-07 01:54:34 (running for 00:20:54.92)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.336 |      0.341 |                   51 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.005 |      0.339 |                   47 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0     |      0.324 |                   41 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)

[2m[36m(func pid=163416)[0m 

[2m[36m(func pid=163416)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=163416)[0m Configuration completed!
[2m[36m(func pid=163416)[0m New optimizer parameters:
[2m[36m(func pid=163416)[0m SGD (
[2m[36m(func pid=163416)[0m Parameter Group 0
[2m[36m(func pid=163416)[0m     dampening: 0
[2m[36m(func pid=163416)[0m     differentiable: False
[2m[36m(func pid=163416)[0m     foreach: None
[2m[36m(func pid=163416)[0m     lr: 0.0001
[2m[36m(func pid=163416)[0m     maximize: False
[2m[36m(func pid=163416)[0m     momentum: 0.9
[2m[36m(func pid=163416)[0m     nesterov: False
[2m[36m(func pid=163416)[0m     weight_decay: 0.0001
[2m[36m(func pid=163416)[0m )
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5403 | Steps: 2 | Val loss: 2.1043 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0588 | Steps: 2 | Val loss: 5.6499 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0010 | Steps: 2 | Val loss: 14.2946 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0021 | Steps: 2 | Val loss: 2.3213 | Batch size: 32 | lr: 0.0001 | Duration: 4.71s
== Status ==
Current time: 2024-01-07 01:54:39 (running for 00:20:59.94)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.354 |      0.345 |                   52 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.004 |      0.338 |                   48 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0     |      0.324 |                   41 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151878)[0m top1: 0.3628731343283582
[2m[36m(func pid=151878)[0m top5: 0.8610074626865671
[2m[36m(func pid=151878)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=151878)[0m f1_macro: 0.33913781977239754
[2m[36m(func pid=151878)[0m f1_weighted: 0.3790172449519422
[2m[36m(func pid=151878)[0m f1_per_class: [0.378, 0.297, 0.815, 0.328, 0.12, 0.303, 0.56, 0.219, 0.187, 0.183]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=154058)[0m top1: 0.355410447761194
[2m[36m(func pid=154058)[0m top5: 0.9104477611940298
[2m[36m(func pid=154058)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=154058)[0m f1_macro: 0.3185806442751051
[2m[36m(func pid=154058)[0m f1_weighted: 0.3754050718574608
[2m[36m(func pid=154058)[0m f1_per_class: [0.234, 0.309, 0.815, 0.377, 0.108, 0.305, 0.516, 0.176, 0.169, 0.176]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m top1: 0.35027985074626866
[2m[36m(func pid=151876)[0m top5: 0.8903917910447762
[2m[36m(func pid=151876)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=151876)[0m f1_macro: 0.3473259088565262
[2m[36m(func pid=151876)[0m f1_weighted: 0.37421843649807046
[2m[36m(func pid=151876)[0m f1_per_class: [0.345, 0.319, 0.815, 0.406, 0.076, 0.416, 0.409, 0.26, 0.19, 0.238]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=163416)[0m top1: 0.17537313432835822
[2m[36m(func pid=163416)[0m top5: 0.5307835820895522
[2m[36m(func pid=163416)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=163416)[0m f1_macro: 0.1192226981627041
[2m[36m(func pid=163416)[0m f1_weighted: 0.12357458807643285
[2m[36m(func pid=163416)[0m f1_per_class: [0.324, 0.348, 0.0, 0.09, 0.0, 0.212, 0.018, 0.0, 0.0, 0.2]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3061 | Steps: 2 | Val loss: 2.1137 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0158 | Steps: 2 | Val loss: 5.7693 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0846 | Steps: 2 | Val loss: 14.4860 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9699 | Steps: 2 | Val loss: 2.3249 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 01:54:44 (running for 00:21:05.67)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.54  |      0.347 |                   53 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.016 |      0.333 |                   50 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.001 |      0.319 |                   42 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  3.002 |      0.119 |                    1 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3493470149253731
[2m[36m(func pid=151876)[0m top5: 0.8927238805970149
[2m[36m(func pid=151876)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=151876)[0m f1_macro: 0.3468101622542903
[2m[36m(func pid=151876)[0m f1_weighted: 0.3734301496656195
[2m[36m(func pid=151876)[0m f1_per_class: [0.357, 0.311, 0.815, 0.408, 0.075, 0.414, 0.41, 0.26, 0.184, 0.233]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m top1: 0.3568097014925373
[2m[36m(func pid=151878)[0m top5: 0.8563432835820896
[2m[36m(func pid=151878)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=151878)[0m f1_macro: 0.33332955676781645
[2m[36m(func pid=151878)[0m f1_weighted: 0.37093576689748603
[2m[36m(func pid=151878)[0m f1_per_class: [0.358, 0.29, 0.815, 0.311, 0.115, 0.29, 0.559, 0.226, 0.184, 0.186]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=154058)[0m top1: 0.36380597014925375
[2m[36m(func pid=154058)[0m top5: 0.9118470149253731
[2m[36m(func pid=154058)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=154058)[0m f1_macro: 0.32100948057801393
[2m[36m(func pid=154058)[0m f1_weighted: 0.3801655811641383
[2m[36m(func pid=154058)[0m f1_per_class: [0.242, 0.316, 0.815, 0.369, 0.113, 0.303, 0.536, 0.174, 0.164, 0.177]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=163416)[0m top1: 0.18190298507462688
[2m[36m(func pid=163416)[0m top5: 0.5293843283582089
[2m[36m(func pid=163416)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=163416)[0m f1_macro: 0.10910488468472493
[2m[36m(func pid=163416)[0m f1_weighted: 0.12778838821756247
[2m[36m(func pid=163416)[0m f1_per_class: [0.222, 0.332, 0.0, 0.1, 0.01, 0.28, 0.012, 0.024, 0.0, 0.111]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3178 | Steps: 2 | Val loss: 2.1247 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.1680 | Steps: 2 | Val loss: 5.8642 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0017 | Steps: 2 | Val loss: 14.7643 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9910 | Steps: 2 | Val loss: 2.3341 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 01:54:50 (running for 00:21:11.03)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.318 |      0.341 |                   55 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.016 |      0.333 |                   50 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.085 |      0.321 |                   43 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.97  |      0.109 |                    2 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3498134328358209
[2m[36m(func pid=151876)[0m top5: 0.8903917910447762
[2m[36m(func pid=151876)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=151876)[0m f1_macro: 0.34082706844791016
[2m[36m(func pid=151876)[0m f1_weighted: 0.3740320512601244
[2m[36m(func pid=151876)[0m f1_per_class: [0.357, 0.301, 0.759, 0.414, 0.076, 0.415, 0.413, 0.26, 0.189, 0.225]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.3656716417910448
[2m[36m(func pid=154058)[0m top5: 0.9151119402985075
[2m[36m(func pid=154058)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=154058)[0m f1_macro: 0.3191472719547723
[2m[36m(func pid=154058)[0m f1_weighted: 0.3789088799300223
[2m[36m(func pid=154058)[0m f1_per_class: [0.244, 0.32, 0.815, 0.361, 0.11, 0.296, 0.543, 0.158, 0.17, 0.175]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m top1: 0.35774253731343286
[2m[36m(func pid=151878)[0m top5: 0.8558768656716418
[2m[36m(func pid=151878)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=151878)[0m f1_macro: 0.3338623454211712
[2m[36m(func pid=151878)[0m f1_weighted: 0.3703323494029302
[2m[36m(func pid=151878)[0m f1_per_class: [0.36, 0.288, 0.815, 0.304, 0.118, 0.292, 0.564, 0.223, 0.185, 0.189]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=163416)[0m top1: 0.18050373134328357
[2m[36m(func pid=163416)[0m top5: 0.5121268656716418
[2m[36m(func pid=163416)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=163416)[0m f1_macro: 0.09549595267886571
[2m[36m(func pid=163416)[0m f1_weighted: 0.1262421725422692
[2m[36m(func pid=163416)[0m f1_per_class: [0.11, 0.313, 0.0, 0.107, 0.01, 0.314, 0.009, 0.011, 0.0, 0.082]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3332 | Steps: 2 | Val loss: 2.1510 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0098 | Steps: 2 | Val loss: 15.0858 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0017 | Steps: 2 | Val loss: 5.9304 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9670 | Steps: 2 | Val loss: 2.3402 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 01:54:55 (running for 00:21:16.15)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.333 |      0.339 |                   56 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.168 |      0.334 |                   51 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.002 |      0.319 |                   44 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.991 |      0.095 |                    3 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.34701492537313433
[2m[36m(func pid=151876)[0m top5: 0.8903917910447762
[2m[36m(func pid=151876)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=151876)[0m f1_macro: 0.33898406372989287
[2m[36m(func pid=151876)[0m f1_weighted: 0.37165529999738717
[2m[36m(func pid=151876)[0m f1_per_class: [0.351, 0.3, 0.759, 0.406, 0.074, 0.409, 0.415, 0.264, 0.193, 0.22]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.3619402985074627
[2m[36m(func pid=154058)[0m top5: 0.9104477611940298
[2m[36m(func pid=154058)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=154058)[0m f1_macro: 0.3132917561186107
[2m[36m(func pid=154058)[0m f1_weighted: 0.373059842783712
[2m[36m(func pid=154058)[0m f1_per_class: [0.209, 0.32, 0.815, 0.341, 0.109, 0.294, 0.546, 0.159, 0.159, 0.182]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m top1: 0.3558768656716418
[2m[36m(func pid=151878)[0m top5: 0.8544776119402985
[2m[36m(func pid=151878)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=151878)[0m f1_macro: 0.3314278175685366
[2m[36m(func pid=151878)[0m f1_weighted: 0.3677734834245907
[2m[36m(func pid=151878)[0m f1_per_class: [0.356, 0.288, 0.815, 0.304, 0.114, 0.287, 0.56, 0.213, 0.186, 0.192]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=163416)[0m top1: 0.16930970149253732
[2m[36m(func pid=163416)[0m top5: 0.5009328358208955
[2m[36m(func pid=163416)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=163416)[0m f1_macro: 0.09246343438173485
[2m[36m(func pid=163416)[0m f1_weighted: 0.12080909314830614
[2m[36m(func pid=163416)[0m f1_per_class: [0.113, 0.284, 0.0, 0.109, 0.009, 0.309, 0.006, 0.019, 0.0, 0.075]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2675 | Steps: 2 | Val loss: 2.1531 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0002 | Steps: 2 | Val loss: 15.4980 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2567 | Steps: 2 | Val loss: 6.0195 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 01:55:00 (running for 00:21:21.52)
Memory usage on this node: 24.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.333 |      0.339 |                   56 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.331 |                   52 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0     |      0.31  |                   46 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.967 |      0.092 |                    4 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3512126865671642
[2m[36m(func pid=151876)[0m top5: 0.8903917910447762
[2m[36m(func pid=151876)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=151876)[0m f1_macro: 0.34337226409838373
[2m[36m(func pid=151876)[0m f1_weighted: 0.375449383357803
[2m[36m(func pid=151876)[0m f1_per_class: [0.357, 0.299, 0.786, 0.412, 0.079, 0.411, 0.423, 0.256, 0.19, 0.222]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.36427238805970147
[2m[36m(func pid=154058)[0m top5: 0.9155783582089553
[2m[36m(func pid=154058)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=154058)[0m f1_macro: 0.3095835177744767
[2m[36m(func pid=154058)[0m f1_weighted: 0.37074676400973317
[2m[36m(func pid=154058)[0m f1_per_class: [0.195, 0.322, 0.846, 0.335, 0.099, 0.264, 0.558, 0.149, 0.144, 0.183]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9626 | Steps: 2 | Val loss: 2.3427 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=151878)[0m top1: 0.35401119402985076
[2m[36m(func pid=151878)[0m top5: 0.8507462686567164
[2m[36m(func pid=151878)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=151878)[0m f1_macro: 0.32984758478120463
[2m[36m(func pid=151878)[0m f1_weighted: 0.3667487302726619
[2m[36m(func pid=151878)[0m f1_per_class: [0.343, 0.282, 0.815, 0.301, 0.113, 0.286, 0.561, 0.228, 0.183, 0.186]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=163416)[0m top1: 0.16324626865671643
[2m[36m(func pid=163416)[0m top5: 0.49580223880597013
[2m[36m(func pid=163416)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=163416)[0m f1_macro: 0.08477168617122437
[2m[36m(func pid=163416)[0m f1_weighted: 0.12032629639695425
[2m[36m(func pid=163416)[0m f1_per_class: [0.058, 0.27, 0.0, 0.111, 0.009, 0.304, 0.018, 0.018, 0.0, 0.061]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2565 | Steps: 2 | Val loss: 2.1739 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0726 | Steps: 2 | Val loss: 15.7114 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0031 | Steps: 2 | Val loss: 6.1168 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 01:55:06 (running for 00:21:26.74)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.256 |      0.339 |                   58 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.257 |      0.33  |                   53 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0     |      0.31  |                   46 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.963 |      0.085 |                    5 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.34888059701492535
[2m[36m(func pid=151876)[0m top5: 0.8899253731343284
[2m[36m(func pid=151876)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=151876)[0m f1_macro: 0.3385155250688915
[2m[36m(func pid=151876)[0m f1_weighted: 0.37358077617937635
[2m[36m(func pid=151876)[0m f1_per_class: [0.359, 0.294, 0.759, 0.418, 0.08, 0.407, 0.415, 0.265, 0.179, 0.209]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.36473880597014924
[2m[36m(func pid=154058)[0m top5: 0.9165111940298507
[2m[36m(func pid=154058)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=154058)[0m f1_macro: 0.3077895540775245
[2m[36m(func pid=154058)[0m f1_weighted: 0.36845133834787774
[2m[36m(func pid=154058)[0m f1_per_class: [0.198, 0.321, 0.846, 0.339, 0.114, 0.24, 0.559, 0.137, 0.146, 0.179]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9841 | Steps: 2 | Val loss: 2.3434 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=151878)[0m top1: 0.34794776119402987
[2m[36m(func pid=151878)[0m top5: 0.8446828358208955
[2m[36m(func pid=151878)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=151878)[0m f1_macro: 0.3252245071338366
[2m[36m(func pid=151878)[0m f1_weighted: 0.3620939233485805
[2m[36m(func pid=151878)[0m f1_per_class: [0.328, 0.276, 0.815, 0.292, 0.106, 0.299, 0.557, 0.214, 0.181, 0.185]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=163416)[0m top1: 0.15764925373134328
[2m[36m(func pid=163416)[0m top5: 0.49533582089552236
[2m[36m(func pid=163416)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=163416)[0m f1_macro: 0.08710936544492985
[2m[36m(func pid=163416)[0m f1_weighted: 0.1219689405055201
[2m[36m(func pid=163416)[0m f1_per_class: [0.1, 0.248, 0.0, 0.115, 0.017, 0.297, 0.029, 0.036, 0.0, 0.029]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2936 | Steps: 2 | Val loss: 2.1903 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0066 | Steps: 2 | Val loss: 15.9039 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0033 | Steps: 2 | Val loss: 6.1913 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:55:11 (running for 00:21:32.01)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.294 |      0.336 |                   59 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.003 |      0.325 |                   54 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.073 |      0.308 |                   47 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.984 |      0.087 |                    6 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3474813432835821
[2m[36m(func pid=151876)[0m top5: 0.8894589552238806
[2m[36m(func pid=151876)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=151876)[0m f1_macro: 0.33620340533109183
[2m[36m(func pid=151876)[0m f1_weighted: 0.37278389926469696
[2m[36m(func pid=151876)[0m f1_per_class: [0.368, 0.288, 0.759, 0.419, 0.078, 0.403, 0.419, 0.246, 0.18, 0.201]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.3689365671641791
[2m[36m(func pid=154058)[0m top5: 0.9174440298507462
[2m[36m(func pid=154058)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=154058)[0m f1_macro: 0.306282764914743
[2m[36m(func pid=154058)[0m f1_weighted: 0.3699612632510654
[2m[36m(func pid=154058)[0m f1_per_class: [0.184, 0.323, 0.846, 0.334, 0.115, 0.243, 0.567, 0.139, 0.132, 0.179]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9466 | Steps: 2 | Val loss: 2.3454 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=151878)[0m top1: 0.3493470149253731
[2m[36m(func pid=151878)[0m top5: 0.84375
[2m[36m(func pid=151878)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=151878)[0m f1_macro: 0.3261407692579581
[2m[36m(func pid=151878)[0m f1_weighted: 0.36476599202803933
[2m[36m(func pid=151878)[0m f1_per_class: [0.302, 0.277, 0.815, 0.292, 0.11, 0.315, 0.559, 0.222, 0.184, 0.186]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2083 | Steps: 2 | Val loss: 2.2093 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0545 | Steps: 2 | Val loss: 16.1514 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=163416)[0m top1: 0.14925373134328357
[2m[36m(func pid=163416)[0m top5: 0.4962686567164179
[2m[36m(func pid=163416)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=163416)[0m f1_macro: 0.08274722683891496
[2m[36m(func pid=163416)[0m f1_weighted: 0.12250148128992269
[2m[36m(func pid=163416)[0m f1_per_class: [0.085, 0.215, 0.0, 0.114, 0.016, 0.297, 0.05, 0.05, 0.0, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0209 | Steps: 2 | Val loss: 6.2305 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 01:55:16 (running for 00:21:37.14)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.208 |      0.339 |                   60 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.003 |      0.326 |                   55 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.007 |      0.306 |                   48 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.947 |      0.083 |                    7 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.34888059701492535
[2m[36m(func pid=151876)[0m top5: 0.8894589552238806
[2m[36m(func pid=151876)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=151876)[0m f1_macro: 0.33867592762290194
[2m[36m(func pid=151876)[0m f1_weighted: 0.37446228338704934
[2m[36m(func pid=151876)[0m f1_per_class: [0.377, 0.289, 0.759, 0.421, 0.078, 0.405, 0.419, 0.26, 0.181, 0.198]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.3689365671641791
[2m[36m(func pid=154058)[0m top5: 0.9169776119402985
[2m[36m(func pid=154058)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=154058)[0m f1_macro: 0.3055680118687251
[2m[36m(func pid=154058)[0m f1_weighted: 0.36771095292605277
[2m[36m(func pid=154058)[0m f1_per_class: [0.184, 0.321, 0.846, 0.327, 0.115, 0.243, 0.568, 0.139, 0.136, 0.177]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9648 | Steps: 2 | Val loss: 2.3439 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=151878)[0m top1: 0.3512126865671642
[2m[36m(func pid=151878)[0m top5: 0.8414179104477612
[2m[36m(func pid=151878)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=151878)[0m f1_macro: 0.32890018436419904
[2m[36m(func pid=151878)[0m f1_weighted: 0.3685448728745027
[2m[36m(func pid=151878)[0m f1_per_class: [0.304, 0.274, 0.815, 0.303, 0.112, 0.331, 0.557, 0.223, 0.185, 0.186]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2384 | Steps: 2 | Val loss: 2.2343 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3633 | Steps: 2 | Val loss: 16.2303 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=163416)[0m top1: 0.14738805970149255
[2m[36m(func pid=163416)[0m top5: 0.5093283582089553
[2m[36m(func pid=163416)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=163416)[0m f1_macro: 0.08267707903796913
[2m[36m(func pid=163416)[0m f1_weighted: 0.12474129772715165
[2m[36m(func pid=163416)[0m f1_per_class: [0.056, 0.209, 0.0, 0.12, 0.031, 0.288, 0.058, 0.065, 0.0, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0164 | Steps: 2 | Val loss: 6.3276 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 01:55:21 (running for 00:21:42.36)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.238 |      0.337 |                   61 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.021 |      0.329 |                   56 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.055 |      0.306 |                   49 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.965 |      0.083 |                    8 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.34701492537313433
[2m[36m(func pid=151876)[0m top5: 0.8894589552238806
[2m[36m(func pid=151876)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=151876)[0m f1_macro: 0.337085208240797
[2m[36m(func pid=151876)[0m f1_weighted: 0.3734900229297683
[2m[36m(func pid=151876)[0m f1_per_class: [0.359, 0.288, 0.759, 0.416, 0.078, 0.399, 0.421, 0.275, 0.18, 0.195]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.37453358208955223
[2m[36m(func pid=154058)[0m top5: 0.9165111940298507
[2m[36m(func pid=154058)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=154058)[0m f1_macro: 0.3097933318514109
[2m[36m(func pid=154058)[0m f1_weighted: 0.37265067587342404
[2m[36m(func pid=154058)[0m f1_per_class: [0.208, 0.32, 0.846, 0.338, 0.115, 0.24, 0.573, 0.138, 0.14, 0.18]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9581 | Steps: 2 | Val loss: 2.3388 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=151878)[0m top1: 0.3474813432835821
[2m[36m(func pid=151878)[0m top5: 0.8390858208955224
[2m[36m(func pid=151878)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=151878)[0m f1_macro: 0.3279865259331062
[2m[36m(func pid=151878)[0m f1_weighted: 0.3652361762318596
[2m[36m(func pid=151878)[0m f1_per_class: [0.299, 0.266, 0.815, 0.296, 0.118, 0.344, 0.551, 0.229, 0.185, 0.177]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2816 | Steps: 2 | Val loss: 2.2508 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4187 | Steps: 2 | Val loss: 16.1572 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=163416)[0m top1: 0.14692164179104478
[2m[36m(func pid=163416)[0m top5: 0.5069962686567164
[2m[36m(func pid=163416)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=163416)[0m f1_macro: 0.08322260841512508
[2m[36m(func pid=163416)[0m f1_weighted: 0.12925441943439156
[2m[36m(func pid=163416)[0m f1_per_class: [0.054, 0.189, 0.0, 0.132, 0.022, 0.298, 0.069, 0.069, 0.0, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0066 | Steps: 2 | Val loss: 6.4138 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=151876)[0m top1: 0.3474813432835821
[2m[36m(func pid=151876)[0m top5: 0.8889925373134329
[2m[36m(func pid=151876)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=151876)[0m f1_macro: 0.3392245096692211
[2m[36m(func pid=151876)[0m f1_weighted: 0.37528299758068634
[2m[36m(func pid=151876)[0m f1_per_class: [0.359, 0.288, 0.786, 0.423, 0.078, 0.385, 0.427, 0.268, 0.191, 0.189]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.3829291044776119
[2m[36m(func pid=154058)[0m top5: 0.9146455223880597
[2m[36m(func pid=154058)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=154058)[0m f1_macro: 0.31467882164374206
[2m[36m(func pid=154058)[0m f1_weighted: 0.38212369821655173
[2m[36m(func pid=154058)[0m f1_per_class: [0.22, 0.314, 0.846, 0.367, 0.115, 0.239, 0.58, 0.138, 0.152, 0.175]
== Status ==
Current time: 2024-01-07 01:55:27 (running for 00:21:47.84)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.282 |      0.339 |                   62 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.016 |      0.328 |                   57 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.363 |      0.31  |                   50 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.958 |      0.083 |                    9 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9668 | Steps: 2 | Val loss: 2.3374 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=151878)[0m top1: 0.34654850746268656
[2m[36m(func pid=151878)[0m top5: 0.8386194029850746
[2m[36m(func pid=151878)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=151878)[0m f1_macro: 0.3280362164884898
[2m[36m(func pid=151878)[0m f1_weighted: 0.3643974444544928
[2m[36m(func pid=151878)[0m f1_per_class: [0.294, 0.263, 0.815, 0.299, 0.12, 0.345, 0.547, 0.227, 0.187, 0.183]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2403 | Steps: 2 | Val loss: 2.2568 | Batch size: 32 | lr: 0.001 | Duration: 2.62s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.1374 | Steps: 2 | Val loss: 16.2220 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=163416)[0m top1: 0.14738805970149255
[2m[36m(func pid=163416)[0m top5: 0.5032649253731343
[2m[36m(func pid=163416)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=163416)[0m f1_macro: 0.08359800569433165
[2m[36m(func pid=163416)[0m f1_weighted: 0.13347247584605865
[2m[36m(func pid=163416)[0m f1_per_class: [0.047, 0.177, 0.0, 0.148, 0.021, 0.298, 0.076, 0.07, 0.0, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0059 | Steps: 2 | Val loss: 6.5418 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=151876)[0m top1: 0.34701492537313433
[2m[36m(func pid=151876)[0m top5: 0.8875932835820896
[2m[36m(func pid=151876)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=151876)[0m f1_macro: 0.3390296374014911
[2m[36m(func pid=151876)[0m f1_weighted: 0.37314528392845175
[2m[36m(func pid=151876)[0m f1_per_class: [0.345, 0.293, 0.786, 0.417, 0.08, 0.384, 0.422, 0.264, 0.201, 0.198]
[2m[36m(func pid=151876)[0m 
== Status ==
Current time: 2024-01-07 01:55:32 (running for 00:21:52.92)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.24  |      0.339 |                   63 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.007 |      0.328 |                   58 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.137 |      0.322 |                   52 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.967 |      0.084 |                   10 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=154058)[0m top1: 0.3903917910447761
[2m[36m(func pid=154058)[0m top5: 0.9118470149253731
[2m[36m(func pid=154058)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=154058)[0m f1_macro: 0.32202915748257616
[2m[36m(func pid=154058)[0m f1_weighted: 0.3905121444987191
[2m[36m(func pid=154058)[0m f1_per_class: [0.253, 0.304, 0.846, 0.394, 0.11, 0.234, 0.584, 0.148, 0.162, 0.184]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m top1: 0.33908582089552236
[2m[36m(func pid=151878)[0m top5: 0.8348880597014925
[2m[36m(func pid=151878)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=151878)[0m f1_macro: 0.3235980931079901
[2m[36m(func pid=151878)[0m f1_weighted: 0.3580210082684999
[2m[36m(func pid=151878)[0m f1_per_class: [0.293, 0.26, 0.815, 0.286, 0.113, 0.35, 0.539, 0.221, 0.185, 0.174]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9243 | Steps: 2 | Val loss: 2.3330 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2295 | Steps: 2 | Val loss: 2.2665 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0468 | Steps: 2 | Val loss: 16.3872 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=163416)[0m top1: 0.1478544776119403
[2m[36m(func pid=163416)[0m top5: 0.5144589552238806
[2m[36m(func pid=163416)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=163416)[0m f1_macro: 0.09161776084273654
[2m[36m(func pid=163416)[0m f1_weighted: 0.1367591420874226
[2m[36m(func pid=163416)[0m f1_per_class: [0.044, 0.163, 0.056, 0.155, 0.026, 0.298, 0.083, 0.092, 0.0, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0215 | Steps: 2 | Val loss: 6.6045 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 01:55:37 (running for 00:21:58.18)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.229 |      0.342 |                   64 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.006 |      0.324 |                   59 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.137 |      0.322 |                   52 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.924 |      0.092 |                   11 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.35074626865671643
[2m[36m(func pid=151876)[0m top5: 0.8899253731343284
[2m[36m(func pid=151876)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=151876)[0m f1_macro: 0.34153025565258616
[2m[36m(func pid=151876)[0m f1_weighted: 0.3772068349606196
[2m[36m(func pid=151876)[0m f1_per_class: [0.341, 0.287, 0.786, 0.417, 0.084, 0.381, 0.437, 0.283, 0.198, 0.201]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.39132462686567165
[2m[36m(func pid=154058)[0m top5: 0.9053171641791045
[2m[36m(func pid=154058)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=154058)[0m f1_macro: 0.3318588830837012
[2m[36m(func pid=154058)[0m f1_weighted: 0.39026413320750214
[2m[36m(func pid=154058)[0m f1_per_class: [0.33, 0.279, 0.846, 0.406, 0.12, 0.228, 0.58, 0.157, 0.179, 0.193]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m top1: 0.333955223880597
[2m[36m(func pid=151878)[0m top5: 0.8306902985074627
[2m[36m(func pid=151878)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=151878)[0m f1_macro: 0.32219925921532083
[2m[36m(func pid=151878)[0m f1_weighted: 0.35573254510664565
[2m[36m(func pid=151878)[0m f1_per_class: [0.278, 0.248, 0.815, 0.291, 0.115, 0.353, 0.531, 0.234, 0.186, 0.171]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9318 | Steps: 2 | Val loss: 2.3290 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3256 | Steps: 2 | Val loss: 2.2884 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0567 | Steps: 2 | Val loss: 16.5754 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0155 | Steps: 2 | Val loss: 6.6978 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=163416)[0m top1: 0.14692164179104478
[2m[36m(func pid=163416)[0m top5: 0.5228544776119403
[2m[36m(func pid=163416)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=163416)[0m f1_macro: 0.09183554753938587
[2m[36m(func pid=163416)[0m f1_weighted: 0.13843128034287372
[2m[36m(func pid=163416)[0m f1_per_class: [0.041, 0.157, 0.051, 0.162, 0.025, 0.294, 0.085, 0.103, 0.0, 0.0]
[2m[36m(func pid=163416)[0m 
== Status ==
Current time: 2024-01-07 01:55:42 (running for 00:22:03.51)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.326 |      0.347 |                   65 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.022 |      0.322 |                   60 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.047 |      0.332 |                   53 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.932 |      0.092 |                   12 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3596082089552239
[2m[36m(func pid=151876)[0m top5: 0.8903917910447762
[2m[36m(func pid=151876)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=151876)[0m f1_macro: 0.34725822431735953
[2m[36m(func pid=151876)[0m f1_weighted: 0.3864222195496772
[2m[36m(func pid=151876)[0m f1_per_class: [0.351, 0.295, 0.786, 0.419, 0.085, 0.383, 0.458, 0.289, 0.205, 0.201]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.3880597014925373
[2m[36m(func pid=154058)[0m top5: 0.9039179104477612
[2m[36m(func pid=154058)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=154058)[0m f1_macro: 0.32788871856178936
[2m[36m(func pid=154058)[0m f1_weighted: 0.3883075447218346
[2m[36m(func pid=154058)[0m f1_per_class: [0.32, 0.267, 0.846, 0.414, 0.123, 0.229, 0.575, 0.153, 0.171, 0.18]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m top1: 0.3306902985074627
[2m[36m(func pid=151878)[0m top5: 0.8278917910447762
[2m[36m(func pid=151878)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=151878)[0m f1_macro: 0.3201136980723932
[2m[36m(func pid=151878)[0m f1_weighted: 0.35421468335671447
[2m[36m(func pid=151878)[0m f1_per_class: [0.279, 0.239, 0.815, 0.293, 0.12, 0.356, 0.531, 0.227, 0.176, 0.165]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.9235 | Steps: 2 | Val loss: 2.3249 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2288 | Steps: 2 | Val loss: 2.3254 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.8540 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.1318 | Steps: 2 | Val loss: 6.7667 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=163416)[0m top1: 0.1501865671641791
[2m[36m(func pid=163416)[0m top5: 0.5289179104477612
[2m[36m(func pid=163416)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=163416)[0m f1_macro: 0.09980390506554523
[2m[36m(func pid=163416)[0m f1_weighted: 0.14232172453953268
[2m[36m(func pid=163416)[0m f1_per_class: [0.054, 0.161, 0.098, 0.167, 0.025, 0.299, 0.087, 0.108, 0.0, 0.0]
[2m[36m(func pid=163416)[0m 
== Status ==
Current time: 2024-01-07 01:55:47 (running for 00:22:08.61)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.229 |      0.339 |                   66 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.016 |      0.32  |                   61 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.057 |      0.328 |                   54 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.924 |      0.1   |                   13 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3516791044776119
[2m[36m(func pid=151876)[0m top5: 0.8894589552238806
[2m[36m(func pid=151876)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=151876)[0m f1_macro: 0.33873607511810516
[2m[36m(func pid=151876)[0m f1_weighted: 0.3781942293008256
[2m[36m(func pid=151876)[0m f1_per_class: [0.351, 0.294, 0.759, 0.415, 0.084, 0.381, 0.441, 0.265, 0.201, 0.196]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.3871268656716418
[2m[36m(func pid=154058)[0m top5: 0.8959888059701493
[2m[36m(func pid=154058)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=154058)[0m f1_macro: 0.32938865242454635
[2m[36m(func pid=154058)[0m f1_weighted: 0.39000576026126704
[2m[36m(func pid=154058)[0m f1_per_class: [0.32, 0.257, 0.846, 0.417, 0.12, 0.234, 0.579, 0.173, 0.177, 0.171]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m top1: 0.3292910447761194
[2m[36m(func pid=151878)[0m top5: 0.8264925373134329
[2m[36m(func pid=151878)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=151878)[0m f1_macro: 0.32022860712405354
[2m[36m(func pid=151878)[0m f1_weighted: 0.3542870225045128
[2m[36m(func pid=151878)[0m f1_per_class: [0.271, 0.239, 0.815, 0.3, 0.121, 0.363, 0.523, 0.224, 0.177, 0.169]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.9217 | Steps: 2 | Val loss: 2.3224 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.1735 | Steps: 2 | Val loss: 2.3508 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0011 | Steps: 2 | Val loss: 17.2224 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0185 | Steps: 2 | Val loss: 6.8013 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=163416)[0m top1: 0.14458955223880596
[2m[36m(func pid=163416)[0m top5: 0.5307835820895522
[2m[36m(func pid=163416)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=163416)[0m f1_macro: 0.09613788460619006
[2m[36m(func pid=163416)[0m f1_weighted: 0.1379601879366077
[2m[36m(func pid=163416)[0m f1_per_class: [0.054, 0.144, 0.089, 0.16, 0.024, 0.29, 0.092, 0.108, 0.0, 0.0]
[2m[36m(func pid=163416)[0m 
== Status ==
Current time: 2024-01-07 01:55:53 (running for 00:22:13.72)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.173 |      0.334 |                   67 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.132 |      0.32  |                   62 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0     |      0.329 |                   55 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.922 |      0.096 |                   14 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3451492537313433
[2m[36m(func pid=151876)[0m top5: 0.8847947761194029
[2m[36m(func pid=151876)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=151876)[0m f1_macro: 0.3339110696612164
[2m[36m(func pid=151876)[0m f1_weighted: 0.3729499546010394
[2m[36m(func pid=151876)[0m f1_per_class: [0.341, 0.283, 0.759, 0.409, 0.08, 0.377, 0.439, 0.265, 0.195, 0.19]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.38619402985074625
[2m[36m(func pid=154058)[0m top5: 0.8824626865671642
[2m[36m(func pid=154058)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=154058)[0m f1_macro: 0.33119031663371157
[2m[36m(func pid=154058)[0m f1_weighted: 0.3903673777726275
[2m[36m(func pid=154058)[0m f1_per_class: [0.303, 0.245, 0.846, 0.421, 0.125, 0.241, 0.575, 0.192, 0.194, 0.169]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m top1: 0.33675373134328357
[2m[36m(func pid=151878)[0m top5: 0.8236940298507462
[2m[36m(func pid=151878)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=151878)[0m f1_macro: 0.32618480855475174
[2m[36m(func pid=151878)[0m f1_weighted: 0.36277740392967206
[2m[36m(func pid=151878)[0m f1_per_class: [0.268, 0.24, 0.815, 0.313, 0.129, 0.381, 0.528, 0.235, 0.187, 0.166]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.9452 | Steps: 2 | Val loss: 2.3222 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.1748 | Steps: 2 | Val loss: 2.3822 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0766 | Steps: 2 | Val loss: 17.6141 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 01:55:58 (running for 00:22:18.86)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.175 |      0.336 |                   68 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.018 |      0.326 |                   63 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.001 |      0.331 |                   56 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.922 |      0.096 |                   14 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.35027985074626866
[2m[36m(func pid=151876)[0m top5: 0.8852611940298507
[2m[36m(func pid=151876)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=151876)[0m f1_macro: 0.3363819029508811
[2m[36m(func pid=151876)[0m f1_weighted: 0.37829978709728856
[2m[36m(func pid=151876)[0m f1_per_class: [0.345, 0.291, 0.759, 0.413, 0.082, 0.368, 0.45, 0.269, 0.199, 0.188]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=163416)[0m top1: 0.1455223880597015
[2m[36m(func pid=163416)[0m top5: 0.5345149253731343
[2m[36m(func pid=163416)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=163416)[0m f1_macro: 0.09699624127024012
[2m[36m(func pid=163416)[0m f1_weighted: 0.13831375868347803
[2m[36m(func pid=163416)[0m f1_per_class: [0.056, 0.15, 0.085, 0.164, 0.024, 0.29, 0.085, 0.105, 0.011, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0109 | Steps: 2 | Val loss: 6.8561 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=154058)[0m top1: 0.3810634328358209
[2m[36m(func pid=154058)[0m top5: 0.8782649253731343
[2m[36m(func pid=154058)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=154058)[0m f1_macro: 0.3272797429083901
[2m[36m(func pid=154058)[0m f1_weighted: 0.3852055830005541
[2m[36m(func pid=154058)[0m f1_per_class: [0.299, 0.226, 0.846, 0.427, 0.127, 0.237, 0.566, 0.193, 0.177, 0.174]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.1663 | Steps: 2 | Val loss: 2.4065 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=151878)[0m top1: 0.333955223880597
[2m[36m(func pid=151878)[0m top5: 0.8250932835820896
[2m[36m(func pid=151878)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=151878)[0m f1_macro: 0.3256838620531973
[2m[36m(func pid=151878)[0m f1_weighted: 0.3585869310340677
[2m[36m(func pid=151878)[0m f1_per_class: [0.269, 0.238, 0.815, 0.302, 0.131, 0.386, 0.523, 0.242, 0.185, 0.166]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.9296 | Steps: 2 | Val loss: 2.3171 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0382 | Steps: 2 | Val loss: 17.7640 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:56:03 (running for 00:22:23.93)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.166 |      0.336 |                   69 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.011 |      0.326 |                   64 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.077 |      0.327 |                   57 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.945 |      0.097 |                   15 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3460820895522388
[2m[36m(func pid=151876)[0m top5: 0.8843283582089553
[2m[36m(func pid=151876)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=151876)[0m f1_macro: 0.3362669508691407
[2m[36m(func pid=151876)[0m f1_weighted: 0.37420708912387246
[2m[36m(func pid=151876)[0m f1_per_class: [0.339, 0.292, 0.786, 0.408, 0.08, 0.36, 0.444, 0.268, 0.199, 0.186]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=163416)[0m top1: 0.14505597014925373
[2m[36m(func pid=163416)[0m top5: 0.5424440298507462
[2m[36m(func pid=163416)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=163416)[0m f1_macro: 0.09970428883639755
[2m[36m(func pid=163416)[0m f1_weighted: 0.1378724060549851
[2m[36m(func pid=163416)[0m f1_per_class: [0.053, 0.146, 0.13, 0.165, 0.024, 0.297, 0.085, 0.098, 0.0, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0056 | Steps: 2 | Val loss: 6.9584 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=154058)[0m top1: 0.38013059701492535
[2m[36m(func pid=154058)[0m top5: 0.8698694029850746
[2m[36m(func pid=154058)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=154058)[0m f1_macro: 0.3267803965463628
[2m[36m(func pid=154058)[0m f1_weighted: 0.3856338147734453
[2m[36m(func pid=154058)[0m f1_per_class: [0.268, 0.21, 0.846, 0.439, 0.137, 0.24, 0.563, 0.211, 0.177, 0.177]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.1782 | Steps: 2 | Val loss: 2.4065 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=151878)[0m top1: 0.3278917910447761
[2m[36m(func pid=151878)[0m top5: 0.8208955223880597
[2m[36m(func pid=151878)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=151878)[0m f1_macro: 0.3214326038576445
[2m[36m(func pid=151878)[0m f1_weighted: 0.3531323635081749
[2m[36m(func pid=151878)[0m f1_per_class: [0.265, 0.239, 0.815, 0.289, 0.112, 0.385, 0.517, 0.238, 0.187, 0.167]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8904 | Steps: 2 | Val loss: 2.3141 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0000 | Steps: 2 | Val loss: 18.4540 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 01:56:08 (running for 00:22:29.07)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.178 |      0.338 |                   70 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.006 |      0.321 |                   65 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.038 |      0.327 |                   58 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.93  |      0.1   |                   16 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.34888059701492535
[2m[36m(func pid=151876)[0m top5: 0.8847947761194029
[2m[36m(func pid=151876)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=151876)[0m f1_macro: 0.3384610211890297
[2m[36m(func pid=151876)[0m f1_weighted: 0.37659499059095347
[2m[36m(func pid=151876)[0m f1_per_class: [0.337, 0.291, 0.786, 0.406, 0.082, 0.367, 0.451, 0.271, 0.205, 0.19]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=163416)[0m top1: 0.14692164179104478
[2m[36m(func pid=163416)[0m top5: 0.5443097014925373
[2m[36m(func pid=163416)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=163416)[0m f1_macro: 0.10121191034747268
[2m[36m(func pid=163416)[0m f1_weighted: 0.14018647834968861
[2m[36m(func pid=163416)[0m f1_per_class: [0.05, 0.144, 0.122, 0.167, 0.024, 0.305, 0.087, 0.103, 0.01, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0055 | Steps: 2 | Val loss: 7.0652 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=154058)[0m top1: 0.375
[2m[36m(func pid=154058)[0m top5: 0.8619402985074627
[2m[36m(func pid=154058)[0m f1_micro: 0.375
[2m[36m(func pid=154058)[0m f1_macro: 0.3257419732314821
[2m[36m(func pid=154058)[0m f1_weighted: 0.3842423747102753
[2m[36m(func pid=154058)[0m f1_per_class: [0.243, 0.189, 0.846, 0.437, 0.13, 0.27, 0.561, 0.214, 0.189, 0.179]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.1360 | Steps: 2 | Val loss: 2.4327 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=151878)[0m top1: 0.3208955223880597
[2m[36m(func pid=151878)[0m top5: 0.8157649253731343
[2m[36m(func pid=151878)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=151878)[0m f1_macro: 0.31773778000801745
[2m[36m(func pid=151878)[0m f1_weighted: 0.34586753039491114
[2m[36m(func pid=151878)[0m f1_per_class: [0.261, 0.24, 0.786, 0.28, 0.113, 0.384, 0.499, 0.244, 0.204, 0.167]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.9262 | Steps: 2 | Val loss: 2.3148 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0005 | Steps: 2 | Val loss: 19.2087 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 01:56:13 (running for 00:22:34.23)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.136 |      0.335 |                   71 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.005 |      0.318 |                   66 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0     |      0.326 |                   59 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.89  |      0.101 |                   17 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.34654850746268656
[2m[36m(func pid=151876)[0m top5: 0.8815298507462687
[2m[36m(func pid=151876)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=151876)[0m f1_macro: 0.3351480228049739
[2m[36m(func pid=151876)[0m f1_weighted: 0.3745227896570134
[2m[36m(func pid=151876)[0m f1_per_class: [0.333, 0.29, 0.786, 0.403, 0.08, 0.358, 0.454, 0.257, 0.205, 0.185]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0033 | Steps: 2 | Val loss: 7.1654 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=163416)[0m top1: 0.14972014925373134
[2m[36m(func pid=163416)[0m top5: 0.5405783582089553
[2m[36m(func pid=163416)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=163416)[0m f1_macro: 0.10312771768551325
[2m[36m(func pid=163416)[0m f1_weighted: 0.14440951447312397
[2m[36m(func pid=163416)[0m f1_per_class: [0.05, 0.141, 0.122, 0.179, 0.024, 0.306, 0.089, 0.109, 0.01, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=154058)[0m top1: 0.365205223880597
[2m[36m(func pid=154058)[0m top5: 0.8526119402985075
[2m[36m(func pid=154058)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=154058)[0m f1_macro: 0.3198099963246736
[2m[36m(func pid=154058)[0m f1_weighted: 0.3790047940716241
[2m[36m(func pid=154058)[0m f1_per_class: [0.223, 0.166, 0.815, 0.435, 0.128, 0.288, 0.55, 0.229, 0.194, 0.169]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2134 | Steps: 2 | Val loss: 2.4420 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=151878)[0m top1: 0.3199626865671642
[2m[36m(func pid=151878)[0m top5: 0.8129664179104478
[2m[36m(func pid=151878)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=151878)[0m f1_macro: 0.3170500312680541
[2m[36m(func pid=151878)[0m f1_weighted: 0.3458369547153217
[2m[36m(func pid=151878)[0m f1_per_class: [0.243, 0.237, 0.786, 0.284, 0.113, 0.383, 0.498, 0.24, 0.22, 0.167]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.9202 | Steps: 2 | Val loss: 2.3113 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.5409 | Steps: 2 | Val loss: 19.8132 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:56:18 (running for 00:22:39.49)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.213 |      0.335 |                   72 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.003 |      0.317 |                   67 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0     |      0.32  |                   60 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.926 |      0.103 |                   18 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3474813432835821
[2m[36m(func pid=151876)[0m top5: 0.8857276119402985
[2m[36m(func pid=151876)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=151876)[0m f1_macro: 0.33465621141176005
[2m[36m(func pid=151876)[0m f1_weighted: 0.3767989729226864
[2m[36m(func pid=151876)[0m f1_per_class: [0.331, 0.288, 0.786, 0.409, 0.08, 0.362, 0.457, 0.251, 0.205, 0.177]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.3558768656716418
[2m[36m(func pid=154058)[0m top5: 0.8465485074626866
[2m[36m(func pid=154058)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=154058)[0m f1_macro: 0.3155954135783046
[2m[36m(func pid=154058)[0m f1_weighted: 0.3727886891561419
[2m[36m(func pid=154058)[0m f1_per_class: [0.21, 0.159, 0.815, 0.429, 0.133, 0.281, 0.54, 0.256, 0.176, 0.156]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0020 | Steps: 2 | Val loss: 7.1743 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=163416)[0m top1: 0.15065298507462688
[2m[36m(func pid=163416)[0m top5: 0.5475746268656716
[2m[36m(func pid=163416)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=163416)[0m f1_macro: 0.10459727100411068
[2m[36m(func pid=163416)[0m f1_weighted: 0.14675199923657525
[2m[36m(func pid=163416)[0m f1_per_class: [0.049, 0.136, 0.122, 0.19, 0.024, 0.304, 0.089, 0.11, 0.021, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.1191 | Steps: 2 | Val loss: 2.4643 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=151878)[0m top1: 0.3218283582089552
[2m[36m(func pid=151878)[0m top5: 0.8148320895522388
[2m[36m(func pid=151878)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=151878)[0m f1_macro: 0.3210964095021966
[2m[36m(func pid=151878)[0m f1_weighted: 0.3475494003824311
[2m[36m(func pid=151878)[0m f1_per_class: [0.247, 0.237, 0.815, 0.283, 0.113, 0.381, 0.505, 0.239, 0.219, 0.172]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5544 | Steps: 2 | Val loss: 20.3399 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.9100 | Steps: 2 | Val loss: 2.3075 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 01:56:24 (running for 00:22:44.82)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.119 |      0.335 |                   73 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.321 |                   68 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.541 |      0.316 |                   61 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.92  |      0.105 |                   19 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3474813432835821
[2m[36m(func pid=151876)[0m top5: 0.8847947761194029
[2m[36m(func pid=151876)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=151876)[0m f1_macro: 0.3350853292729038
[2m[36m(func pid=151876)[0m f1_weighted: 0.3769736311914834
[2m[36m(func pid=151876)[0m f1_per_class: [0.33, 0.29, 0.786, 0.411, 0.08, 0.362, 0.454, 0.254, 0.208, 0.176]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.34841417910447764
[2m[36m(func pid=154058)[0m top5: 0.84375
[2m[36m(func pid=154058)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=154058)[0m f1_macro: 0.30892847931437645
[2m[36m(func pid=154058)[0m f1_weighted: 0.36794702751552155
[2m[36m(func pid=154058)[0m f1_per_class: [0.196, 0.151, 0.786, 0.425, 0.134, 0.286, 0.532, 0.253, 0.18, 0.146]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0148 | Steps: 2 | Val loss: 7.2093 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=163416)[0m top1: 0.15438432835820895
[2m[36m(func pid=163416)[0m top5: 0.5517723880597015
[2m[36m(func pid=163416)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=163416)[0m f1_macro: 0.1062898549629416
[2m[36m(func pid=163416)[0m f1_weighted: 0.1521561684488929
[2m[36m(func pid=163416)[0m f1_per_class: [0.047, 0.135, 0.12, 0.201, 0.024, 0.31, 0.096, 0.11, 0.021, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.1640 | Steps: 2 | Val loss: 2.4856 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=151878)[0m top1: 0.3255597014925373
[2m[36m(func pid=151878)[0m top5: 0.816231343283582
[2m[36m(func pid=151878)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=151878)[0m f1_macro: 0.3242995000861517
[2m[36m(func pid=151878)[0m f1_weighted: 0.3504358448972255
[2m[36m(func pid=151878)[0m f1_per_class: [0.25, 0.239, 0.815, 0.285, 0.118, 0.38, 0.509, 0.252, 0.22, 0.176]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0001 | Steps: 2 | Val loss: 20.6707 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.8825 | Steps: 2 | Val loss: 2.3048 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 01:56:29 (running for 00:22:50.13)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.164 |      0.335 |                   74 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.015 |      0.324 |                   69 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.554 |      0.309 |                   62 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.91  |      0.106 |                   20 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3498134328358209
[2m[36m(func pid=151876)[0m top5: 0.8843283582089553
[2m[36m(func pid=151876)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=151876)[0m f1_macro: 0.335462119508902
[2m[36m(func pid=151876)[0m f1_weighted: 0.37979425446983484
[2m[36m(func pid=151876)[0m f1_per_class: [0.322, 0.288, 0.786, 0.412, 0.08, 0.358, 0.464, 0.258, 0.212, 0.174]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.34654850746268656
[2m[36m(func pid=154058)[0m top5: 0.8423507462686567
[2m[36m(func pid=154058)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=154058)[0m f1_macro: 0.31471222980584723
[2m[36m(func pid=154058)[0m f1_weighted: 0.3690692382353917
[2m[36m(func pid=154058)[0m f1_per_class: [0.188, 0.168, 0.815, 0.411, 0.142, 0.316, 0.528, 0.252, 0.18, 0.148]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0021 | Steps: 2 | Val loss: 7.2807 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=163416)[0m top1: 0.15391791044776118
[2m[36m(func pid=163416)[0m top5: 0.5555037313432836
[2m[36m(func pid=163416)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=163416)[0m f1_macro: 0.10610621569409592
[2m[36m(func pid=163416)[0m f1_weighted: 0.15306366561809878
[2m[36m(func pid=163416)[0m f1_per_class: [0.047, 0.138, 0.115, 0.199, 0.023, 0.308, 0.1, 0.11, 0.021, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.1311 | Steps: 2 | Val loss: 2.4987 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=151878)[0m top1: 0.322294776119403
[2m[36m(func pid=151878)[0m top5: 0.8138992537313433
[2m[36m(func pid=151878)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=151878)[0m f1_macro: 0.31849003729074765
[2m[36m(func pid=151878)[0m f1_weighted: 0.3484651375434093
[2m[36m(func pid=151878)[0m f1_per_class: [0.234, 0.24, 0.786, 0.291, 0.122, 0.379, 0.5, 0.241, 0.221, 0.171]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4621 | Steps: 2 | Val loss: 21.0066 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.9048 | Steps: 2 | Val loss: 2.3017 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 01:56:34 (running for 00:22:55.44)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.131 |      0.341 |                   75 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.318 |                   70 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0     |      0.315 |                   63 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.882 |      0.106 |                   21 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.35447761194029853
[2m[36m(func pid=151876)[0m top5: 0.8819962686567164
[2m[36m(func pid=151876)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=151876)[0m f1_macro: 0.34071251245854917
[2m[36m(func pid=151876)[0m f1_weighted: 0.38522627174127116
[2m[36m(func pid=151876)[0m f1_per_class: [0.335, 0.289, 0.786, 0.416, 0.081, 0.361, 0.473, 0.276, 0.213, 0.178]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.341884328358209
[2m[36m(func pid=154058)[0m top5: 0.8432835820895522
[2m[36m(func pid=154058)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=154058)[0m f1_macro: 0.3181596759462524
[2m[36m(func pid=154058)[0m f1_weighted: 0.3693804585998382
[2m[36m(func pid=154058)[0m f1_per_class: [0.179, 0.176, 0.815, 0.404, 0.141, 0.348, 0.513, 0.28, 0.182, 0.144]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0183 | Steps: 2 | Val loss: 7.2901 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=163416)[0m top1: 0.1553171641791045
[2m[36m(func pid=163416)[0m top5: 0.5606343283582089
[2m[36m(func pid=163416)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=163416)[0m f1_macro: 0.10664033685290933
[2m[36m(func pid=163416)[0m f1_weighted: 0.15403354343650355
[2m[36m(func pid=163416)[0m f1_per_class: [0.047, 0.138, 0.109, 0.199, 0.023, 0.312, 0.1, 0.117, 0.021, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.1068 | Steps: 2 | Val loss: 2.5231 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=151878)[0m top1: 0.3180970149253731
[2m[36m(func pid=151878)[0m top5: 0.816231343283582
[2m[36m(func pid=151878)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=151878)[0m f1_macro: 0.3161675797752377
[2m[36m(func pid=151878)[0m f1_weighted: 0.34390213322318736
[2m[36m(func pid=151878)[0m f1_per_class: [0.234, 0.241, 0.786, 0.292, 0.126, 0.378, 0.484, 0.236, 0.216, 0.168]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.1443 | Steps: 2 | Val loss: 21.7243 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.8662 | Steps: 2 | Val loss: 2.3014 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 01:56:39 (running for 00:23:00.46)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.107 |      0.341 |                   76 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.018 |      0.316 |                   71 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.462 |      0.318 |                   64 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.905 |      0.107 |                   22 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.353544776119403
[2m[36m(func pid=151876)[0m top5: 0.8819962686567164
[2m[36m(func pid=151876)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=151876)[0m f1_macro: 0.34056524691555873
[2m[36m(func pid=151876)[0m f1_weighted: 0.3847876808986638
[2m[36m(func pid=151876)[0m f1_per_class: [0.339, 0.287, 0.786, 0.414, 0.079, 0.367, 0.471, 0.278, 0.212, 0.172]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.33115671641791045
[2m[36m(func pid=154058)[0m top5: 0.84375
[2m[36m(func pid=154058)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=154058)[0m f1_macro: 0.31803470122678734
[2m[36m(func pid=154058)[0m f1_weighted: 0.36145333417036435
[2m[36m(func pid=154058)[0m f1_per_class: [0.173, 0.183, 0.815, 0.386, 0.149, 0.366, 0.488, 0.319, 0.165, 0.138]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0099 | Steps: 2 | Val loss: 7.3543 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=163416)[0m top1: 0.15438432835820895
[2m[36m(func pid=163416)[0m top5: 0.5606343283582089
[2m[36m(func pid=163416)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=163416)[0m f1_macro: 0.10582337011505458
[2m[36m(func pid=163416)[0m f1_weighted: 0.15286443768003302
[2m[36m(func pid=163416)[0m f1_per_class: [0.047, 0.142, 0.107, 0.198, 0.024, 0.307, 0.097, 0.114, 0.021, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.1047 | Steps: 2 | Val loss: 2.5338 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0436 | Steps: 2 | Val loss: 22.2814 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=151878)[0m top1: 0.3162313432835821
[2m[36m(func pid=151878)[0m top5: 0.8143656716417911
[2m[36m(func pid=151878)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=151878)[0m f1_macro: 0.3151520720793784
[2m[36m(func pid=151878)[0m f1_weighted: 0.3425465930101552
[2m[36m(func pid=151878)[0m f1_per_class: [0.229, 0.24, 0.786, 0.292, 0.129, 0.381, 0.481, 0.239, 0.21, 0.166]
[2m[36m(func pid=151878)[0m 
== Status ==
Current time: 2024-01-07 01:56:45 (running for 00:23:05.77)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.105 |      0.339 |                   77 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.01  |      0.315 |                   72 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.144 |      0.318 |                   65 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.866 |      0.106 |                   23 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3521455223880597
[2m[36m(func pid=151876)[0m top5: 0.8791977611940298
[2m[36m(func pid=151876)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=151876)[0m f1_macro: 0.33876240884091946
[2m[36m(func pid=151876)[0m f1_weighted: 0.3827285983120237
[2m[36m(func pid=151876)[0m f1_per_class: [0.33, 0.288, 0.786, 0.413, 0.078, 0.353, 0.472, 0.273, 0.215, 0.181]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.8293 | Steps: 2 | Val loss: 2.3004 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=154058)[0m top1: 0.32322761194029853
[2m[36m(func pid=154058)[0m top5: 0.8428171641791045
[2m[36m(func pid=154058)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=154058)[0m f1_macro: 0.31410918303365765
[2m[36m(func pid=154058)[0m f1_weighted: 0.355896279221228
[2m[36m(func pid=154058)[0m f1_per_class: [0.166, 0.197, 0.786, 0.383, 0.144, 0.367, 0.461, 0.34, 0.162, 0.136]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0013 | Steps: 2 | Val loss: 7.3725 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=163416)[0m top1: 0.15671641791044777
[2m[36m(func pid=163416)[0m top5: 0.566231343283582
[2m[36m(func pid=163416)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=163416)[0m f1_macro: 0.10807501126881203
[2m[36m(func pid=163416)[0m f1_weighted: 0.15509428535043757
[2m[36m(func pid=163416)[0m f1_per_class: [0.046, 0.141, 0.105, 0.202, 0.025, 0.314, 0.097, 0.12, 0.032, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0986 | Steps: 2 | Val loss: 2.5581 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0002 | Steps: 2 | Val loss: 22.6602 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=151878)[0m top1: 0.3148320895522388
[2m[36m(func pid=151878)[0m top5: 0.8176305970149254
[2m[36m(func pid=151878)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=151878)[0m f1_macro: 0.3155099820440008
[2m[36m(func pid=151878)[0m f1_weighted: 0.3408929486568777
[2m[36m(func pid=151878)[0m f1_per_class: [0.236, 0.241, 0.786, 0.292, 0.128, 0.376, 0.474, 0.248, 0.213, 0.162]
[2m[36m(func pid=151878)[0m 
== Status ==
Current time: 2024-01-07 01:56:50 (running for 00:23:10.96)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.099 |      0.336 |                   78 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.001 |      0.316 |                   73 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.044 |      0.314 |                   66 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.829 |      0.108 |                   24 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.34841417910447764
[2m[36m(func pid=151876)[0m top5: 0.8782649253731343
[2m[36m(func pid=151876)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=151876)[0m f1_macro: 0.33626467042362623
[2m[36m(func pid=151876)[0m f1_weighted: 0.3793335400326847
[2m[36m(func pid=151876)[0m f1_per_class: [0.328, 0.289, 0.786, 0.411, 0.076, 0.35, 0.463, 0.273, 0.214, 0.173]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.8571 | Steps: 2 | Val loss: 2.2998 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=154058)[0m top1: 0.32369402985074625
[2m[36m(func pid=154058)[0m top5: 0.8432835820895522
[2m[36m(func pid=154058)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=154058)[0m f1_macro: 0.3198751705493351
[2m[36m(func pid=154058)[0m f1_weighted: 0.3570459846436233
[2m[36m(func pid=154058)[0m f1_per_class: [0.167, 0.207, 0.786, 0.385, 0.16, 0.397, 0.443, 0.345, 0.176, 0.132]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0019 | Steps: 2 | Val loss: 7.5115 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.1022 | Steps: 2 | Val loss: 2.5701 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=163416)[0m top1: 0.15345149253731344
[2m[36m(func pid=163416)[0m top5: 0.5676305970149254
[2m[36m(func pid=163416)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=163416)[0m f1_macro: 0.10550560442958741
[2m[36m(func pid=163416)[0m f1_weighted: 0.1522787333527106
[2m[36m(func pid=163416)[0m f1_per_class: [0.046, 0.142, 0.1, 0.194, 0.024, 0.308, 0.097, 0.123, 0.021, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0004 | Steps: 2 | Val loss: 23.3357 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 01:56:55 (running for 00:23:15.98)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.099 |      0.336 |                   78 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.313 |                   74 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0     |      0.32  |                   67 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.857 |      0.106 |                   25 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151878)[0m top1: 0.3111007462686567
[2m[36m(func pid=151878)[0m top5: 0.8111007462686567
[2m[36m(func pid=151878)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=151878)[0m f1_macro: 0.3125776877299304
[2m[36m(func pid=151878)[0m f1_weighted: 0.3372735605047383
[2m[36m(func pid=151878)[0m f1_per_class: [0.226, 0.239, 0.786, 0.286, 0.122, 0.379, 0.47, 0.241, 0.212, 0.167]
[2m[36m(func pid=151878)[0m 
[2m[36m(func pid=151876)[0m top1: 0.3493470149253731
[2m[36m(func pid=151876)[0m top5: 0.8777985074626866
[2m[36m(func pid=151876)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=151876)[0m f1_macro: 0.3355627539518346
[2m[36m(func pid=151876)[0m f1_weighted: 0.380598140704621
[2m[36m(func pid=151876)[0m f1_per_class: [0.33, 0.289, 0.786, 0.41, 0.075, 0.344, 0.472, 0.269, 0.205, 0.175]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.8694 | Steps: 2 | Val loss: 2.2992 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=154058)[0m top1: 0.30970149253731344
[2m[36m(func pid=154058)[0m top5: 0.8404850746268657
[2m[36m(func pid=154058)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=154058)[0m f1_macro: 0.31504213820548743
[2m[36m(func pid=154058)[0m f1_weighted: 0.34322244618161085
[2m[36m(func pid=154058)[0m f1_per_class: [0.162, 0.196, 0.786, 0.37, 0.167, 0.392, 0.418, 0.344, 0.19, 0.126]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0826 | Steps: 2 | Val loss: 2.5879 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=151878)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0021 | Steps: 2 | Val loss: 7.5896 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=163416)[0m top1: 0.15391791044776118
[2m[36m(func pid=163416)[0m top5: 0.5690298507462687
[2m[36m(func pid=163416)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=163416)[0m f1_macro: 0.1047880823041537
[2m[36m(func pid=163416)[0m f1_weighted: 0.1530104658860125
[2m[36m(func pid=163416)[0m f1_per_class: [0.045, 0.139, 0.095, 0.196, 0.025, 0.313, 0.1, 0.114, 0.021, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0002 | Steps: 2 | Val loss: 24.2604 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 01:57:00 (running for 00:23:21.42)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.083 |      0.336 |                   80 |
| train_66d79_00010 | RUNNING    | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.313 |                   74 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0     |      0.315 |                   68 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.869 |      0.105 |                   26 |
| train_66d79_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3498134328358209
[2m[36m(func pid=151876)[0m top5: 0.8763992537313433
[2m[36m(func pid=151876)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=151876)[0m f1_macro: 0.33559391350250045
[2m[36m(func pid=151876)[0m f1_weighted: 0.38104489032793554
[2m[36m(func pid=151876)[0m f1_per_class: [0.328, 0.288, 0.786, 0.415, 0.075, 0.345, 0.47, 0.26, 0.214, 0.175]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=151878)[0m top1: 0.31203358208955223
[2m[36m(func pid=151878)[0m top5: 0.8115671641791045
[2m[36m(func pid=151878)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=151878)[0m f1_macro: 0.31518134183535673
[2m[36m(func pid=151878)[0m f1_weighted: 0.3375128382170729
[2m[36m(func pid=151878)[0m f1_per_class: [0.225, 0.243, 0.786, 0.286, 0.133, 0.377, 0.464, 0.262, 0.213, 0.163]
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.8572 | Steps: 2 | Val loss: 2.2984 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=154058)[0m top1: 0.30597014925373134
[2m[36m(func pid=154058)[0m top5: 0.8367537313432836
[2m[36m(func pid=154058)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=154058)[0m f1_macro: 0.31496140369453757
[2m[36m(func pid=154058)[0m f1_weighted: 0.33921833055380457
[2m[36m(func pid=154058)[0m f1_per_class: [0.161, 0.196, 0.786, 0.376, 0.17, 0.388, 0.397, 0.367, 0.189, 0.121]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.1370 | Steps: 2 | Val loss: 2.6071 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=163416)[0m top1: 0.15485074626865672
[2m[36m(func pid=163416)[0m top5: 0.5732276119402985
[2m[36m(func pid=163416)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=163416)[0m f1_macro: 0.10638189627476144
[2m[36m(func pid=163416)[0m f1_weighted: 0.15468425998950858
[2m[36m(func pid=163416)[0m f1_per_class: [0.055, 0.138, 0.097, 0.205, 0.025, 0.31, 0.097, 0.116, 0.022, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0110 | Steps: 2 | Val loss: 25.0668 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=151876)[0m top1: 0.34375
[2m[36m(func pid=151876)[0m top5: 0.8763992537313433
[2m[36m(func pid=151876)[0m f1_micro: 0.34375
[2m[36m(func pid=151876)[0m f1_macro: 0.3297352436867675
[2m[36m(func pid=151876)[0m f1_weighted: 0.3745845978775622
[2m[36m(func pid=151876)[0m f1_per_class: [0.326, 0.283, 0.786, 0.407, 0.074, 0.338, 0.466, 0.246, 0.197, 0.175]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.8235 | Steps: 2 | Val loss: 2.2923 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=154058)[0m top1: 0.29524253731343286
[2m[36m(func pid=154058)[0m top5: 0.8325559701492538
[2m[36m(func pid=154058)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=154058)[0m f1_macro: 0.30557422924541877
[2m[36m(func pid=154058)[0m f1_weighted: 0.32658419742343936
[2m[36m(func pid=154058)[0m f1_per_class: [0.156, 0.193, 0.759, 0.358, 0.133, 0.407, 0.365, 0.373, 0.19, 0.12]
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.1731 | Steps: 2 | Val loss: 2.6468 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=163416)[0m top1: 0.15904850746268656
[2m[36m(func pid=163416)[0m top5: 0.5844216417910447
[2m[36m(func pid=163416)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=163416)[0m f1_macro: 0.1085527920645986
[2m[36m(func pid=163416)[0m f1_weighted: 0.16119721385694896
[2m[36m(func pid=163416)[0m f1_per_class: [0.054, 0.135, 0.09, 0.221, 0.031, 0.314, 0.104, 0.115, 0.021, 0.0]
== Status ==
Current time: 2024-01-07 01:57:05 (running for 00:23:26.65)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.137 |      0.33  |                   81 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0     |      0.315 |                   69 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.857 |      0.106 |                   27 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=169716)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=169716)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=169716)[0m Configuration completed!
[2m[36m(func pid=169716)[0m New optimizer parameters:
[2m[36m(func pid=169716)[0m SGD (
[2m[36m(func pid=169716)[0m Parameter Group 0
[2m[36m(func pid=169716)[0m     dampening: 0
[2m[36m(func pid=169716)[0m     differentiable: False
[2m[36m(func pid=169716)[0m     foreach: None
[2m[36m(func pid=169716)[0m     lr: 0.001
[2m[36m(func pid=169716)[0m     maximize: False
[2m[36m(func pid=169716)[0m     momentum: 0.9
[2m[36m(func pid=169716)[0m     nesterov: False
[2m[36m(func pid=169716)[0m     weight_decay: 0.0001
[2m[36m(func pid=169716)[0m )
[2m[36m(func pid=169716)[0m 
== Status ==
Current time: 2024-01-07 01:57:11 (running for 00:23:31.91)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.173 |      0.332 |                   82 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.011 |      0.306 |                   70 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.824 |      0.109 |                   28 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.34281716417910446
[2m[36m(func pid=151876)[0m top5: 0.8731343283582089
[2m[36m(func pid=151876)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=151876)[0m f1_macro: 0.3315077013430351
[2m[36m(func pid=151876)[0m f1_weighted: 0.37388341807758957
[2m[36m(func pid=151876)[0m f1_per_class: [0.312, 0.282, 0.786, 0.402, 0.075, 0.349, 0.46, 0.275, 0.201, 0.173]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2451 | Steps: 2 | Val loss: 25.4647 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.8415 | Steps: 2 | Val loss: 2.2917 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.1061 | Steps: 2 | Val loss: 2.6453 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9735 | Steps: 2 | Val loss: 2.3218 | Batch size: 32 | lr: 0.001 | Duration: 4.72s
[2m[36m(func pid=154058)[0m top1: 0.29151119402985076
[2m[36m(func pid=154058)[0m top5: 0.8348880597014925
[2m[36m(func pid=154058)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=154058)[0m f1_macro: 0.306455364564275
[2m[36m(func pid=154058)[0m f1_weighted: 0.3199897156646046
[2m[36m(func pid=154058)[0m f1_per_class: [0.161, 0.194, 0.759, 0.363, 0.141, 0.398, 0.336, 0.389, 0.204, 0.121]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=163416)[0m top1: 0.15811567164179105
[2m[36m(func pid=163416)[0m top5: 0.5867537313432836
[2m[36m(func pid=163416)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=163416)[0m f1_macro: 0.10716841896662883
[2m[36m(func pid=163416)[0m f1_weighted: 0.16032160202600615
[2m[36m(func pid=163416)[0m f1_per_class: [0.053, 0.137, 0.088, 0.216, 0.019, 0.318, 0.104, 0.115, 0.021, 0.0]
[2m[36m(func pid=163416)[0m 
== Status ==
Current time: 2024-01-07 01:57:16 (running for 00:23:36.99)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.106 |      0.335 |                   83 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.245 |      0.306 |                   71 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.841 |      0.107 |                   29 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.34888059701492535
[2m[36m(func pid=151876)[0m top5: 0.875
[2m[36m(func pid=151876)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=151876)[0m f1_macro: 0.33454250133304025
[2m[36m(func pid=151876)[0m f1_weighted: 0.3807010291541788
[2m[36m(func pid=151876)[0m f1_per_class: [0.324, 0.286, 0.786, 0.423, 0.074, 0.344, 0.463, 0.269, 0.207, 0.17]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=169716)[0m top1: 0.17537313432835822
[2m[36m(func pid=169716)[0m top5: 0.5293843283582089
[2m[36m(func pid=169716)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=169716)[0m f1_macro: 0.12080132645644828
[2m[36m(func pid=169716)[0m f1_weighted: 0.1228742653973807
[2m[36m(func pid=169716)[0m f1_per_class: [0.346, 0.35, 0.0, 0.086, 0.0, 0.208, 0.018, 0.0, 0.0, 0.2]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0032 | Steps: 2 | Val loss: 26.1089 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.8710 | Steps: 2 | Val loss: 2.2897 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0780 | Steps: 2 | Val loss: 2.6431 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9904 | Steps: 2 | Val loss: 2.3220 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=163416)[0m top1: 0.15764925373134328
[2m[36m(func pid=163416)[0m top5: 0.5895522388059702
[2m[36m(func pid=163416)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=163416)[0m f1_macro: 0.1100458567737925
[2m[36m(func pid=163416)[0m f1_weighted: 0.16076451567050404
[2m[36m(func pid=163416)[0m f1_per_class: [0.054, 0.134, 0.113, 0.219, 0.025, 0.318, 0.104, 0.114, 0.02, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=154058)[0m top1: 0.2826492537313433
[2m[36m(func pid=154058)[0m top5: 0.8283582089552238
[2m[36m(func pid=154058)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=154058)[0m f1_macro: 0.30443954698026543
[2m[36m(func pid=154058)[0m f1_weighted: 0.30778174995389423
[2m[36m(func pid=154058)[0m f1_per_class: [0.162, 0.196, 0.786, 0.359, 0.143, 0.393, 0.3, 0.385, 0.203, 0.117]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=151876)[0m top1: 0.3558768656716418
[2m[36m(func pid=151876)[0m top5: 0.875
[2m[36m(func pid=151876)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=151876)[0m f1_macro: 0.3410229887099562
[2m[36m(func pid=151876)[0m f1_weighted: 0.3867453139861523
[2m[36m(func pid=151876)[0m f1_per_class: [0.342, 0.289, 0.786, 0.429, 0.077, 0.36, 0.466, 0.266, 0.225, 0.17]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=169716)[0m top1: 0.1875
[2m[36m(func pid=169716)[0m top5: 0.5298507462686567
[2m[36m(func pid=169716)[0m f1_micro: 0.1875
[2m[36m(func pid=169716)[0m f1_macro: 0.11944273564588057
[2m[36m(func pid=169716)[0m f1_weighted: 0.1323690734790712
[2m[36m(func pid=169716)[0m f1_per_class: [0.31, 0.331, 0.0, 0.101, 0.011, 0.296, 0.015, 0.024, 0.0, 0.107]
== Status ==
Current time: 2024-01-07 01:57:22 (running for 00:23:42.73)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.078 |      0.341 |                   84 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.003 |      0.304 |                   72 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.871 |      0.11  |                   30 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.99  |      0.119 |                    2 |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.8277 | Steps: 2 | Val loss: 2.2897 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0277 | Steps: 2 | Val loss: 26.5555 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0883 | Steps: 2 | Val loss: 2.6637 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9443 | Steps: 2 | Val loss: 2.3277 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=154058)[0m top1: 0.2798507462686567
[2m[36m(func pid=154058)[0m top5: 0.8213619402985075
[2m[36m(func pid=154058)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=154058)[0m f1_macro: 0.30147083537618263
[2m[36m(func pid=154058)[0m f1_weighted: 0.30232967590160426
[2m[36m(func pid=154058)[0m f1_per_class: [0.167, 0.207, 0.786, 0.363, 0.125, 0.393, 0.273, 0.382, 0.202, 0.117]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=163416)[0m top1: 0.15671641791044777
[2m[36m(func pid=163416)[0m top5: 0.5914179104477612
[2m[36m(func pid=163416)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=163416)[0m f1_macro: 0.10923986288109529
[2m[36m(func pid=163416)[0m f1_weighted: 0.15956371696640986
[2m[36m(func pid=163416)[0m f1_per_class: [0.054, 0.138, 0.108, 0.212, 0.025, 0.32, 0.104, 0.111, 0.02, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m top1: 0.35074626865671643
[2m[36m(func pid=151876)[0m top5: 0.875
[2m[36m(func pid=151876)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=151876)[0m f1_macro: 0.33820961693092066
[2m[36m(func pid=151876)[0m f1_weighted: 0.3822227727776631
[2m[36m(func pid=151876)[0m f1_per_class: [0.346, 0.27, 0.786, 0.427, 0.082, 0.349, 0.467, 0.278, 0.209, 0.167]
[2m[36m(func pid=151876)[0m 
== Status ==
Current time: 2024-01-07 01:57:27 (running for 00:23:48.17)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.088 |      0.338 |                   85 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.028 |      0.301 |                   73 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.828 |      0.109 |                   31 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.944 |      0.114 |                    3 |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.18563432835820895
[2m[36m(func pid=169716)[0m top5: 0.5135261194029851
[2m[36m(func pid=169716)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=169716)[0m f1_macro: 0.11353664750435108
[2m[36m(func pid=169716)[0m f1_weighted: 0.13366645606895577
[2m[36m(func pid=169716)[0m f1_per_class: [0.272, 0.311, 0.0, 0.115, 0.011, 0.322, 0.012, 0.022, 0.0, 0.071]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0774 | Steps: 2 | Val loss: 2.6783 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4748 | Steps: 2 | Val loss: 26.9576 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.8312 | Steps: 2 | Val loss: 2.2891 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9199 | Steps: 2 | Val loss: 2.3274 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=151876)[0m top1: 0.3474813432835821
[2m[36m(func pid=151876)[0m top5: 0.8736007462686567
[2m[36m(func pid=151876)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=151876)[0m f1_macro: 0.3342556852510786
[2m[36m(func pid=151876)[0m f1_weighted: 0.37925699889489106
[2m[36m(func pid=151876)[0m f1_per_class: [0.328, 0.268, 0.786, 0.425, 0.081, 0.348, 0.464, 0.27, 0.207, 0.167]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.2756529850746269
[2m[36m(func pid=154058)[0m top5: 0.8176305970149254
[2m[36m(func pid=154058)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=154058)[0m f1_macro: 0.30025697839129806
[2m[36m(func pid=154058)[0m f1_weighted: 0.2972162986135573
[2m[36m(func pid=154058)[0m f1_per_class: [0.163, 0.209, 0.786, 0.358, 0.128, 0.393, 0.259, 0.384, 0.208, 0.116]
[2m[36m(func pid=154058)[0m 
[2m[36m(func pid=163416)[0m top1: 0.15951492537313433
[2m[36m(func pid=163416)[0m top5: 0.5909514925373134
[2m[36m(func pid=163416)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=163416)[0m f1_macro: 0.1106241863590958
[2m[36m(func pid=163416)[0m f1_weighted: 0.1628431177976642
[2m[36m(func pid=163416)[0m f1_per_class: [0.054, 0.139, 0.105, 0.215, 0.026, 0.322, 0.11, 0.115, 0.02, 0.0]
[2m[36m(func pid=163416)[0m 
== Status ==
Current time: 2024-01-07 01:57:32 (running for 00:23:53.52)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.077 |      0.334 |                   86 |
| train_66d79_00011 | RUNNING    | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.475 |      0.3   |                   74 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.831 |      0.111 |                   32 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.92  |      0.116 |                    4 |
| train_66d79_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.18050373134328357
[2m[36m(func pid=169716)[0m top5: 0.5153917910447762
[2m[36m(func pid=169716)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=169716)[0m f1_macro: 0.11587699656389565
[2m[36m(func pid=169716)[0m f1_weighted: 0.13491939416595233
[2m[36m(func pid=169716)[0m f1_per_class: [0.302, 0.304, 0.0, 0.115, 0.01, 0.324, 0.018, 0.019, 0.0, 0.067]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0719 | Steps: 2 | Val loss: 2.6860 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.8346 | Steps: 2 | Val loss: 2.2872 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=154058)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0973 | Steps: 2 | Val loss: 27.1130 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8916 | Steps: 2 | Val loss: 2.3278 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=151876)[0m top1: 0.3474813432835821
[2m[36m(func pid=151876)[0m top5: 0.8759328358208955
[2m[36m(func pid=151876)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=151876)[0m f1_macro: 0.333450199295295
[2m[36m(func pid=151876)[0m f1_weighted: 0.37896697420806913
[2m[36m(func pid=151876)[0m f1_per_class: [0.33, 0.263, 0.786, 0.425, 0.082, 0.353, 0.467, 0.256, 0.207, 0.167]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=154058)[0m top1: 0.2751865671641791
[2m[36m(func pid=154058)[0m top5: 0.8092350746268657
[2m[36m(func pid=154058)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=154058)[0m f1_macro: 0.296142150189885
[2m[36m(func pid=154058)[0m f1_weighted: 0.30042181739834395
[2m[36m(func pid=154058)[0m f1_per_class: [0.155, 0.211, 0.759, 0.351, 0.127, 0.387, 0.279, 0.386, 0.193, 0.114]
[2m[36m(func pid=163416)[0m top1: 0.1609141791044776
[2m[36m(func pid=163416)[0m top5: 0.5932835820895522
[2m[36m(func pid=163416)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=163416)[0m f1_macro: 0.11351071570216095
[2m[36m(func pid=163416)[0m f1_weighted: 0.1664384498398705
[2m[36m(func pid=163416)[0m f1_per_class: [0.063, 0.139, 0.103, 0.224, 0.024, 0.325, 0.111, 0.117, 0.029, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=169716)[0m top1: 0.18003731343283583
[2m[36m(func pid=169716)[0m top5: 0.5074626865671642
[2m[36m(func pid=169716)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=169716)[0m f1_macro: 0.12397351248331293
[2m[36m(func pid=169716)[0m f1_weighted: 0.14278615602713762
[2m[36m(func pid=169716)[0m f1_per_class: [0.292, 0.29, 0.047, 0.125, 0.01, 0.334, 0.035, 0.034, 0.018, 0.056]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0980 | Steps: 2 | Val loss: 2.6898 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.8157 | Steps: 2 | Val loss: 2.2847 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8514 | Steps: 2 | Val loss: 2.3235 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=151876)[0m top1: 0.3530783582089552
[2m[36m(func pid=151876)[0m top5: 0.8759328358208955
[2m[36m(func pid=151876)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=151876)[0m f1_macro: 0.3353480977705806
[2m[36m(func pid=151876)[0m f1_weighted: 0.38528532207528887
[2m[36m(func pid=151876)[0m f1_per_class: [0.326, 0.264, 0.786, 0.432, 0.083, 0.352, 0.481, 0.259, 0.205, 0.167]
[2m[36m(func pid=163416)[0m top1: 0.16044776119402984
[2m[36m(func pid=163416)[0m top5: 0.5960820895522388
[2m[36m(func pid=163416)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=163416)[0m f1_macro: 0.11309689369143194
[2m[36m(func pid=163416)[0m f1_weighted: 0.16788843604071718
[2m[36m(func pid=163416)[0m f1_per_class: [0.056, 0.15, 0.098, 0.22, 0.024, 0.319, 0.115, 0.129, 0.02, 0.0]
[2m[36m(func pid=169716)[0m top1: 0.17723880597014927
[2m[36m(func pid=169716)[0m top5: 0.5251865671641791
[2m[36m(func pid=169716)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=169716)[0m f1_macro: 0.13099073859692986
[2m[36m(func pid=169716)[0m f1_weighted: 0.14989871104365354
[2m[36m(func pid=169716)[0m f1_per_class: [0.246, 0.274, 0.097, 0.12, 0.009, 0.354, 0.061, 0.062, 0.015, 0.072]
== Status ==
Current time: 2024-01-07 01:57:38 (running for 00:23:58.86)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.072 |      0.333 |                   87 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.835 |      0.114 |                   33 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.892 |      0.124 |                    5 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


== Status ==
Current time: 2024-01-07 01:57:45 (running for 00:24:06.27)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.072 |      0.333 |                   87 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.835 |      0.114 |                   33 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.851 |      0.131 |                    6 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=171628)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=171628)[0m Configuration completed!
[2m[36m(func pid=171628)[0m New optimizer parameters:
[2m[36m(func pid=171628)[0m SGD (
[2m[36m(func pid=171628)[0m Parameter Group 0
[2m[36m(func pid=171628)[0m     dampening: 0
[2m[36m(func pid=171628)[0m     differentiable: False
[2m[36m(func pid=171628)[0m     foreach: None
[2m[36m(func pid=171628)[0m     lr: 0.01
[2m[36m(func pid=171628)[0m     maximize: False
[2m[36m(func pid=171628)[0m     momentum: 0.9
[2m[36m(func pid=171628)[0m     nesterov: False
[2m[36m(func pid=171628)[0m     weight_decay: 0.0001
[2m[36m(func pid=171628)[0m )
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0593 | Steps: 2 | Val loss: 2.7046 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.8247 | Steps: 2 | Val loss: 2.2821 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8006 | Steps: 2 | Val loss: 2.3155 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9754 | Steps: 2 | Val loss: 2.3108 | Batch size: 32 | lr: 0.01 | Duration: 4.48s
== Status ==
Current time: 2024-01-07 01:57:50 (running for 00:24:11.29)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.098 |      0.335 |                   88 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.816 |      0.113 |                   34 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.851 |      0.131 |                    6 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3498134328358209
[2m[36m(func pid=151876)[0m top5: 0.8736007462686567
[2m[36m(func pid=151876)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=151876)[0m f1_macro: 0.3338715989529762
[2m[36m(func pid=151876)[0m f1_weighted: 0.3813241684869548
[2m[36m(func pid=151876)[0m f1_per_class: [0.326, 0.264, 0.786, 0.429, 0.082, 0.343, 0.473, 0.266, 0.205, 0.166]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=163416)[0m top1: 0.166044776119403
[2m[36m(func pid=163416)[0m top5: 0.5984141791044776
[2m[36m(func pid=163416)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=163416)[0m f1_macro: 0.11693615470942134
[2m[36m(func pid=163416)[0m f1_weighted: 0.17431091296915677
[2m[36m(func pid=163416)[0m f1_per_class: [0.056, 0.157, 0.095, 0.226, 0.025, 0.33, 0.12, 0.14, 0.02, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=169716)[0m top1: 0.17350746268656717
[2m[36m(func pid=169716)[0m top5: 0.539179104477612
[2m[36m(func pid=169716)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=169716)[0m f1_macro: 0.13473277156924163
[2m[36m(func pid=169716)[0m f1_weighted: 0.1564899822865062
[2m[36m(func pid=169716)[0m f1_per_class: [0.204, 0.248, 0.087, 0.133, 0.024, 0.361, 0.076, 0.114, 0.013, 0.087]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m top1: 0.18563432835820895
[2m[36m(func pid=171628)[0m top5: 0.5387126865671642
[2m[36m(func pid=171628)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=171628)[0m f1_macro: 0.13287624510747714
[2m[36m(func pid=171628)[0m f1_weighted: 0.13675780142437996
[2m[36m(func pid=171628)[0m f1_per_class: [0.4, 0.346, 0.0, 0.12, 0.0, 0.207, 0.027, 0.026, 0.0, 0.203]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.1036 | Steps: 2 | Val loss: 2.7252 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7920 | Steps: 2 | Val loss: 2.3049 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.8236 | Steps: 2 | Val loss: 2.2782 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8667 | Steps: 2 | Val loss: 2.2997 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 01:57:56 (running for 00:24:16.75)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.104 |      0.333 |                   90 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.825 |      0.117 |                   35 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.801 |      0.135 |                    7 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  2.975 |      0.133 |                    1 |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3474813432835821
[2m[36m(func pid=151876)[0m top5: 0.8717350746268657
[2m[36m(func pid=151876)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=151876)[0m f1_macro: 0.3332045350098411
[2m[36m(func pid=151876)[0m f1_weighted: 0.3792187707020518
[2m[36m(func pid=151876)[0m f1_per_class: [0.324, 0.272, 0.786, 0.426, 0.081, 0.341, 0.464, 0.267, 0.203, 0.167]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=169716)[0m top1: 0.17583955223880596
[2m[36m(func pid=169716)[0m top5: 0.5569029850746269
[2m[36m(func pid=169716)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=169716)[0m f1_macro: 0.14376619851518674
[2m[36m(func pid=169716)[0m f1_weighted: 0.1637420713708622
[2m[36m(func pid=169716)[0m f1_per_class: [0.205, 0.236, 0.132, 0.157, 0.025, 0.353, 0.078, 0.157, 0.012, 0.082]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m top1: 0.16884328358208955
[2m[36m(func pid=163416)[0m top5: 0.6040111940298507
[2m[36m(func pid=163416)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=163416)[0m f1_macro: 0.12048859521621294
[2m[36m(func pid=163416)[0m f1_weighted: 0.17586867960315208
[2m[36m(func pid=163416)[0m f1_per_class: [0.057, 0.152, 0.094, 0.225, 0.031, 0.336, 0.123, 0.147, 0.04, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=171628)[0m top1: 0.2042910447761194
[2m[36m(func pid=171628)[0m top5: 0.5382462686567164
[2m[36m(func pid=171628)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=171628)[0m f1_macro: 0.16187815038416997
[2m[36m(func pid=171628)[0m f1_weighted: 0.16468494997503103
[2m[36m(func pid=171628)[0m f1_per_class: [0.325, 0.337, 0.12, 0.165, 0.014, 0.295, 0.024, 0.184, 0.0, 0.156]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0742 | Steps: 2 | Val loss: 2.7287 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7856 | Steps: 2 | Val loss: 2.2946 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.8405 | Steps: 2 | Val loss: 2.2836 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.7115 | Steps: 2 | Val loss: 2.2711 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 01:58:01 (running for 00:24:21.94)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.074 |      0.329 |                   91 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.824 |      0.12  |                   36 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.792 |      0.144 |                    8 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  2.867 |      0.162 |                    2 |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.34468283582089554
[2m[36m(func pid=151876)[0m top5: 0.8736007462686567
[2m[36m(func pid=151876)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=151876)[0m f1_macro: 0.3291619667872302
[2m[36m(func pid=151876)[0m f1_weighted: 0.3760084668241709
[2m[36m(func pid=151876)[0m f1_per_class: [0.323, 0.269, 0.786, 0.424, 0.076, 0.331, 0.464, 0.252, 0.203, 0.163]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=169716)[0m top1: 0.17723880597014927
[2m[36m(func pid=169716)[0m top5: 0.5690298507462687
[2m[36m(func pid=169716)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=169716)[0m f1_macro: 0.15139306449340295
[2m[36m(func pid=169716)[0m f1_weighted: 0.17145030944353284
[2m[36m(func pid=169716)[0m f1_per_class: [0.222, 0.222, 0.108, 0.17, 0.022, 0.357, 0.09, 0.189, 0.011, 0.122]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m top1: 0.17024253731343283
[2m[36m(func pid=163416)[0m top5: 0.5956156716417911
[2m[36m(func pid=163416)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=163416)[0m f1_macro: 0.12196830405945888
[2m[36m(func pid=163416)[0m f1_weighted: 0.17539886604384824
[2m[36m(func pid=163416)[0m f1_per_class: [0.069, 0.175, 0.093, 0.215, 0.032, 0.335, 0.117, 0.145, 0.039, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=171628)[0m top1: 0.1921641791044776
[2m[36m(func pid=171628)[0m top5: 0.5788246268656716
[2m[36m(func pid=171628)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=171628)[0m f1_macro: 0.1639003085818453
[2m[36m(func pid=171628)[0m f1_weighted: 0.16868383448835472
[2m[36m(func pid=171628)[0m f1_per_class: [0.215, 0.301, 0.186, 0.186, 0.034, 0.221, 0.055, 0.256, 0.022, 0.162]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0497 | Steps: 2 | Val loss: 2.7695 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7191 | Steps: 2 | Val loss: 2.2867 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.7977 | Steps: 2 | Val loss: 2.2814 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.4718 | Steps: 2 | Val loss: 2.2491 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 01:58:06 (running for 00:24:27.19)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.329 |                   92 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.841 |      0.122 |                   37 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.786 |      0.151 |                    9 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  2.711 |      0.164 |                    3 |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.34375
[2m[36m(func pid=151876)[0m top5: 0.871268656716418
[2m[36m(func pid=151876)[0m f1_micro: 0.34375
[2m[36m(func pid=151876)[0m f1_macro: 0.329384489133569
[2m[36m(func pid=151876)[0m f1_weighted: 0.3754035072825866
[2m[36m(func pid=151876)[0m f1_per_class: [0.323, 0.271, 0.786, 0.422, 0.076, 0.332, 0.462, 0.258, 0.205, 0.161]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=169716)[0m top1: 0.1828358208955224
[2m[36m(func pid=169716)[0m top5: 0.5918843283582089
[2m[36m(func pid=169716)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=169716)[0m f1_macro: 0.15954585159765206
[2m[36m(func pid=169716)[0m f1_weighted: 0.18281225035532656
[2m[36m(func pid=169716)[0m f1_per_class: [0.206, 0.215, 0.139, 0.188, 0.029, 0.361, 0.111, 0.209, 0.01, 0.128]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m top1: 0.1707089552238806
[2m[36m(func pid=163416)[0m top5: 0.5988805970149254
[2m[36m(func pid=163416)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=163416)[0m f1_macro: 0.12177199649895953
[2m[36m(func pid=163416)[0m f1_weighted: 0.1782809387263907
[2m[36m(func pid=163416)[0m f1_per_class: [0.07, 0.161, 0.096, 0.23, 0.031, 0.331, 0.124, 0.146, 0.029, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=171628)[0m top1: 0.17257462686567165
[2m[36m(func pid=171628)[0m top5: 0.6161380597014925
[2m[36m(func pid=171628)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=171628)[0m f1_macro: 0.15621743716847783
[2m[36m(func pid=171628)[0m f1_weighted: 0.163105638563472
[2m[36m(func pid=171628)[0m f1_per_class: [0.156, 0.226, 0.132, 0.206, 0.06, 0.217, 0.06, 0.27, 0.063, 0.173]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0995 | Steps: 2 | Val loss: 2.7652 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.6517 | Steps: 2 | Val loss: 2.2695 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.7777 | Steps: 2 | Val loss: 2.2789 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.2371 | Steps: 2 | Val loss: 2.2100 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 01:58:11 (running for 00:24:32.26)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.1   |      0.334 |                   93 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.798 |      0.122 |                   38 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.719 |      0.16  |                   10 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  2.472 |      0.156 |                    4 |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.35027985074626866
[2m[36m(func pid=151876)[0m top5: 0.8740671641791045
[2m[36m(func pid=151876)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=151876)[0m f1_macro: 0.3339728297863725
[2m[36m(func pid=151876)[0m f1_weighted: 0.3809162988650737
[2m[36m(func pid=151876)[0m f1_per_class: [0.331, 0.272, 0.786, 0.431, 0.081, 0.339, 0.468, 0.253, 0.212, 0.167]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=169716)[0m top1: 0.18563432835820895
[2m[36m(func pid=169716)[0m top5: 0.6175373134328358
[2m[36m(func pid=169716)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=169716)[0m f1_macro: 0.16526071151128904
[2m[36m(func pid=169716)[0m f1_weighted: 0.18760746494050975
[2m[36m(func pid=169716)[0m f1_per_class: [0.216, 0.201, 0.141, 0.202, 0.036, 0.368, 0.116, 0.207, 0.02, 0.144]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m top1: 0.1707089552238806
[2m[36m(func pid=163416)[0m top5: 0.601679104477612
[2m[36m(func pid=163416)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=163416)[0m f1_macro: 0.12162260010577212
[2m[36m(func pid=163416)[0m f1_weighted: 0.17869236395761198
[2m[36m(func pid=163416)[0m f1_per_class: [0.068, 0.154, 0.092, 0.23, 0.031, 0.335, 0.128, 0.149, 0.03, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=171628)[0m top1: 0.1814365671641791
[2m[36m(func pid=171628)[0m top5: 0.6571828358208955
[2m[36m(func pid=171628)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=171628)[0m f1_macro: 0.17079068790629087
[2m[36m(func pid=171628)[0m f1_weighted: 0.18056915425391576
[2m[36m(func pid=171628)[0m f1_per_class: [0.144, 0.153, 0.122, 0.24, 0.058, 0.238, 0.105, 0.328, 0.091, 0.229]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.1511 | Steps: 2 | Val loss: 2.7815 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.6017 | Steps: 2 | Val loss: 2.2563 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.0013 | Steps: 2 | Val loss: 2.1483 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.7924 | Steps: 2 | Val loss: 2.2778 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=151876)[0m top1: 0.3474813432835821
[2m[36m(func pid=151876)[0m top5: 0.8726679104477612
[2m[36m(func pid=151876)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=151876)[0m f1_macro: 0.33083584878554617
[2m[36m(func pid=151876)[0m f1_weighted: 0.3776833847361326
[2m[36m(func pid=151876)[0m f1_per_class: [0.319, 0.274, 0.786, 0.428, 0.081, 0.33, 0.464, 0.253, 0.205, 0.17]
[2m[36m(func pid=151876)[0m 
== Status ==
Current time: 2024-01-07 01:58:16 (running for 00:24:37.36)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.151 |      0.331 |                   94 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.778 |      0.122 |                   39 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.652 |      0.165 |                   11 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  2.237 |      0.171 |                    5 |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.18889925373134328
[2m[36m(func pid=169716)[0m top5: 0.6338619402985075
[2m[36m(func pid=169716)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=169716)[0m f1_macro: 0.1688740395097557
[2m[36m(func pid=169716)[0m f1_weighted: 0.1942344778492311
[2m[36m(func pid=169716)[0m f1_per_class: [0.216, 0.188, 0.137, 0.225, 0.038, 0.355, 0.126, 0.217, 0.028, 0.157]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m top1: 0.21361940298507462
[2m[36m(func pid=171628)[0m top5: 0.7168843283582089
[2m[36m(func pid=171628)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=171628)[0m f1_macro: 0.19955979696279455
[2m[36m(func pid=171628)[0m f1_weighted: 0.21475289076596885
[2m[36m(func pid=171628)[0m f1_per_class: [0.145, 0.167, 0.164, 0.305, 0.069, 0.272, 0.129, 0.341, 0.113, 0.291]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=163416)[0m top1: 0.17164179104477612
[2m[36m(func pid=163416)[0m top5: 0.6082089552238806
[2m[36m(func pid=163416)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=163416)[0m f1_macro: 0.12228954431341958
[2m[36m(func pid=163416)[0m f1_weighted: 0.17973306695015534
[2m[36m(func pid=163416)[0m f1_per_class: [0.068, 0.152, 0.096, 0.226, 0.032, 0.338, 0.136, 0.146, 0.03, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0540 | Steps: 2 | Val loss: 2.8121 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.6109 | Steps: 2 | Val loss: 2.2408 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.8275 | Steps: 2 | Val loss: 2.0903 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.8052 | Steps: 2 | Val loss: 2.2764 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 01:58:21 (running for 00:24:42.40)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.054 |      0.331 |                   95 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.792 |      0.122 |                   40 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.602 |      0.169 |                   12 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  2.001 |      0.2   |                    6 |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.3460820895522388
[2m[36m(func pid=151876)[0m top5: 0.8722014925373134
[2m[36m(func pid=151876)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=151876)[0m f1_macro: 0.33104621404285317
[2m[36m(func pid=151876)[0m f1_weighted: 0.37682735493435776
[2m[36m(func pid=151876)[0m f1_per_class: [0.321, 0.275, 0.786, 0.423, 0.08, 0.337, 0.462, 0.253, 0.206, 0.169]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=169716)[0m top1: 0.19309701492537312
[2m[36m(func pid=169716)[0m top5: 0.6539179104477612
[2m[36m(func pid=169716)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=169716)[0m f1_macro: 0.17718452299682055
[2m[36m(func pid=169716)[0m f1_weighted: 0.19889756149406554
[2m[36m(func pid=169716)[0m f1_per_class: [0.253, 0.197, 0.123, 0.23, 0.039, 0.343, 0.131, 0.23, 0.028, 0.198]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m top1: 0.24580223880597016
[2m[36m(func pid=171628)[0m top5: 0.7453358208955224
[2m[36m(func pid=171628)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=171628)[0m f1_macro: 0.2277520059260425
[2m[36m(func pid=171628)[0m f1_weighted: 0.24756231255191213
[2m[36m(func pid=171628)[0m f1_per_class: [0.157, 0.159, 0.187, 0.35, 0.079, 0.37, 0.154, 0.371, 0.127, 0.323]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=163416)[0m top1: 0.17210820895522388
[2m[36m(func pid=163416)[0m top5: 0.6119402985074627
[2m[36m(func pid=163416)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=163416)[0m f1_macro: 0.12227880906659676
[2m[36m(func pid=163416)[0m f1_weighted: 0.1806348434495019
[2m[36m(func pid=163416)[0m f1_per_class: [0.077, 0.152, 0.096, 0.231, 0.031, 0.337, 0.136, 0.132, 0.031, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.1330 | Steps: 2 | Val loss: 2.8392 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.5942 | Steps: 2 | Val loss: 1.9982 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5216 | Steps: 2 | Val loss: 2.2276 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.7954 | Steps: 2 | Val loss: 2.2752 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=151876)[0m top1: 0.34421641791044777
[2m[36m(func pid=151876)[0m top5: 0.8708022388059702
[2m[36m(func pid=151876)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=151876)[0m f1_macro: 0.33155481796624137
[2m[36m(func pid=151876)[0m f1_weighted: 0.3746369084562055
[2m[36m(func pid=151876)[0m f1_per_class: [0.328, 0.272, 0.786, 0.424, 0.078, 0.332, 0.457, 0.25, 0.211, 0.179]
[2m[36m(func pid=151876)[0m 
== Status ==
Current time: 2024-01-07 01:58:26 (running for 00:24:47.52)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.133 |      0.332 |                   96 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.805 |      0.122 |                   41 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.611 |      0.177 |                   13 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  1.827 |      0.228 |                    7 |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.27611940298507465
[2m[36m(func pid=171628)[0m top5: 0.7938432835820896
[2m[36m(func pid=171628)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=171628)[0m f1_macro: 0.2425801564804365
[2m[36m(func pid=171628)[0m f1_weighted: 0.27635397078473184
[2m[36m(func pid=171628)[0m f1_per_class: [0.193, 0.176, 0.247, 0.404, 0.079, 0.381, 0.188, 0.367, 0.1, 0.292]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m top1: 0.20055970149253732
[2m[36m(func pid=169716)[0m top5: 0.6725746268656716
[2m[36m(func pid=169716)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=169716)[0m f1_macro: 0.18240469919502636
[2m[36m(func pid=169716)[0m f1_weighted: 0.20857559720717755
[2m[36m(func pid=169716)[0m f1_per_class: [0.258, 0.191, 0.119, 0.255, 0.04, 0.345, 0.14, 0.239, 0.028, 0.21]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m top1: 0.17210820895522388
[2m[36m(func pid=163416)[0m top5: 0.6152052238805971
[2m[36m(func pid=163416)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=163416)[0m f1_macro: 0.12345993268433084
[2m[36m(func pid=163416)[0m f1_weighted: 0.18115404567053126
[2m[36m(func pid=163416)[0m f1_per_class: [0.076, 0.152, 0.095, 0.231, 0.031, 0.337, 0.135, 0.147, 0.03, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.1502 | Steps: 2 | Val loss: 2.8714 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.3712 | Steps: 2 | Val loss: 1.9340 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.5092 | Steps: 2 | Val loss: 2.2145 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:58:31 (running for 00:24:52.66)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.15  |      0.325 |                   97 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.795 |      0.123 |                   42 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.522 |      0.182 |                   14 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  1.594 |      0.243 |                    8 |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.33861940298507465
[2m[36m(func pid=151876)[0m top5: 0.8656716417910447
[2m[36m(func pid=151876)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=151876)[0m f1_macro: 0.32503092325410926
[2m[36m(func pid=151876)[0m f1_weighted: 0.3697041513200777
[2m[36m(func pid=151876)[0m f1_per_class: [0.312, 0.275, 0.759, 0.407, 0.076, 0.343, 0.452, 0.251, 0.203, 0.171]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.7707 | Steps: 2 | Val loss: 2.2728 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=171628)[0m top1: 0.2933768656716418
[2m[36m(func pid=171628)[0m top5: 0.8264925373134329
[2m[36m(func pid=171628)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=171628)[0m f1_macro: 0.2612074597043942
[2m[36m(func pid=171628)[0m f1_weighted: 0.2975962126736669
[2m[36m(func pid=171628)[0m f1_per_class: [0.209, 0.204, 0.353, 0.412, 0.076, 0.404, 0.227, 0.356, 0.079, 0.292]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m top1: 0.21175373134328357
[2m[36m(func pid=169716)[0m top5: 0.6907649253731343
[2m[36m(func pid=169716)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=169716)[0m f1_macro: 0.19172506781092863
[2m[36m(func pid=169716)[0m f1_weighted: 0.22047399334772333
[2m[36m(func pid=169716)[0m f1_per_class: [0.267, 0.193, 0.126, 0.279, 0.041, 0.346, 0.151, 0.255, 0.029, 0.231]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m top1: 0.17257462686567165
[2m[36m(func pid=163416)[0m top5: 0.6142723880597015
[2m[36m(func pid=163416)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=163416)[0m f1_macro: 0.12357635330509094
[2m[36m(func pid=163416)[0m f1_weighted: 0.18186694771942682
[2m[36m(func pid=163416)[0m f1_per_class: [0.076, 0.149, 0.09, 0.227, 0.032, 0.342, 0.14, 0.149, 0.03, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0932 | Steps: 2 | Val loss: 2.8753 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.2225 | Steps: 2 | Val loss: 1.8887 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.4331 | Steps: 2 | Val loss: 2.1970 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=151876)[0m top1: 0.3376865671641791
[2m[36m(func pid=151876)[0m top5: 0.8666044776119403
[2m[36m(func pid=151876)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=151876)[0m f1_macro: 0.32730457151675985
[2m[36m(func pid=151876)[0m f1_weighted: 0.36877987974306625
[2m[36m(func pid=151876)[0m f1_per_class: [0.317, 0.273, 0.786, 0.409, 0.077, 0.339, 0.45, 0.252, 0.205, 0.167]
[2m[36m(func pid=151876)[0m 
== Status ==
Current time: 2024-01-07 01:58:37 (running for 00:24:57.80)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.093 |      0.327 |                   98 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.771 |      0.124 |                   43 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.509 |      0.192 |                   15 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  1.371 |      0.261 |                    9 |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.7739 | Steps: 2 | Val loss: 2.2741 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=171628)[0m top1: 0.29757462686567165
[2m[36m(func pid=171628)[0m top5: 0.8502798507462687
[2m[36m(func pid=171628)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=171628)[0m f1_macro: 0.27916654733311913
[2m[36m(func pid=171628)[0m f1_weighted: 0.2985057892532499
[2m[36m(func pid=171628)[0m f1_per_class: [0.243, 0.236, 0.462, 0.395, 0.08, 0.408, 0.219, 0.367, 0.082, 0.299]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m top1: 0.2248134328358209
[2m[36m(func pid=169716)[0m top5: 0.7047574626865671
[2m[36m(func pid=169716)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=169716)[0m f1_macro: 0.19821662495121095
[2m[36m(func pid=169716)[0m f1_weighted: 0.23377924185924914
[2m[36m(func pid=169716)[0m f1_per_class: [0.269, 0.185, 0.129, 0.317, 0.035, 0.353, 0.159, 0.269, 0.029, 0.236]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0651 | Steps: 2 | Val loss: 2.8895 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=163416)[0m top1: 0.17350746268656717
[2m[36m(func pid=163416)[0m top5: 0.6147388059701493
[2m[36m(func pid=163416)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=163416)[0m f1_macro: 0.12548530305900785
[2m[36m(func pid=163416)[0m f1_weighted: 0.18375895223525782
[2m[36m(func pid=163416)[0m f1_per_class: [0.072, 0.149, 0.111, 0.231, 0.032, 0.342, 0.143, 0.145, 0.028, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.1764 | Steps: 2 | Val loss: 1.8428 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.3977 | Steps: 2 | Val loss: 2.1804 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:58:42 (running for 00:25:02.93)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00009 | RUNNING    | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.065 |      0.332 |                   99 |
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.774 |      0.125 |                   44 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.433 |      0.198 |                   16 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  1.223 |      0.279 |                   10 |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.34328358208955223
[2m[36m(func pid=151876)[0m top5: 0.8680037313432836
[2m[36m(func pid=151876)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=151876)[0m f1_macro: 0.33176493668627277
[2m[36m(func pid=151876)[0m f1_weighted: 0.3743042833188194
[2m[36m(func pid=151876)[0m f1_per_class: [0.324, 0.274, 0.786, 0.419, 0.08, 0.35, 0.451, 0.258, 0.213, 0.163]
[2m[36m(func pid=151876)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.7831 | Steps: 2 | Val loss: 2.2718 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=171628)[0m top1: 0.3125
[2m[36m(func pid=171628)[0m top5: 0.8661380597014925
[2m[36m(func pid=171628)[0m f1_micro: 0.3125
[2m[36m(func pid=171628)[0m f1_macro: 0.30294923856516853
[2m[36m(func pid=171628)[0m f1_weighted: 0.31808037207194734
[2m[36m(func pid=171628)[0m f1_per_class: [0.278, 0.258, 0.615, 0.396, 0.072, 0.411, 0.265, 0.373, 0.062, 0.298]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m top1: 0.23367537313432835
[2m[36m(func pid=169716)[0m top5: 0.7206156716417911
[2m[36m(func pid=169716)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=169716)[0m f1_macro: 0.20393701430906025
[2m[36m(func pid=169716)[0m f1_weighted: 0.24312809082419057
[2m[36m(func pid=169716)[0m f1_per_class: [0.28, 0.181, 0.138, 0.339, 0.036, 0.364, 0.167, 0.265, 0.03, 0.241]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=151876)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0577 | Steps: 2 | Val loss: 2.9033 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=163416)[0m top1: 0.1791044776119403
[2m[36m(func pid=163416)[0m top5: 0.6203358208955224
[2m[36m(func pid=163416)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=163416)[0m f1_macro: 0.12981691383037067
[2m[36m(func pid=163416)[0m f1_weighted: 0.1890671050760873
[2m[36m(func pid=163416)[0m f1_per_class: [0.082, 0.16, 0.112, 0.231, 0.033, 0.345, 0.152, 0.155, 0.029, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.9642 | Steps: 2 | Val loss: 1.8301 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4110 | Steps: 2 | Val loss: 2.1632 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:58:47 (running for 00:25:08.08)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 3 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.783 |      0.13  |                   45 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.398 |      0.204 |                   17 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  1.176 |      0.303 |                   11 |
| train_66d79_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 TERMINATED)


[2m[36m(func pid=151876)[0m top1: 0.34421641791044777
[2m[36m(func pid=151876)[0m top5: 0.8689365671641791
[2m[36m(func pid=151876)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=151876)[0m f1_macro: 0.333653585325771
[2m[36m(func pid=151876)[0m f1_weighted: 0.3755201921732547
[2m[36m(func pid=151876)[0m f1_per_class: [0.333, 0.276, 0.786, 0.415, 0.077, 0.35, 0.457, 0.256, 0.216, 0.171]
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.7777 | Steps: 2 | Val loss: 2.2713 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=171628)[0m top1: 0.3180970149253731
[2m[36m(func pid=171628)[0m top5: 0.8726679104477612
[2m[36m(func pid=171628)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=171628)[0m f1_macro: 0.31304129551203747
[2m[36m(func pid=171628)[0m f1_weighted: 0.32830653156154743
[2m[36m(func pid=171628)[0m f1_per_class: [0.259, 0.258, 0.688, 0.399, 0.079, 0.412, 0.295, 0.357, 0.101, 0.282]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m top1: 0.24300373134328357
[2m[36m(func pid=169716)[0m top5: 0.7294776119402985
[2m[36m(func pid=169716)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=169716)[0m f1_macro: 0.20895401735344218
[2m[36m(func pid=169716)[0m f1_weighted: 0.2543436053648118
[2m[36m(func pid=169716)[0m f1_per_class: [0.28, 0.176, 0.141, 0.359, 0.036, 0.36, 0.19, 0.266, 0.033, 0.25]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m top1: 0.1767723880597015
[2m[36m(func pid=163416)[0m top5: 0.617070895522388
[2m[36m(func pid=163416)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=163416)[0m f1_macro: 0.1294147830539875
[2m[36m(func pid=163416)[0m f1_weighted: 0.18620564678718782
[2m[36m(func pid=163416)[0m f1_per_class: [0.09, 0.155, 0.114, 0.224, 0.033, 0.351, 0.149, 0.15, 0.028, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8887 | Steps: 2 | Val loss: 1.8321 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.3124 | Steps: 2 | Val loss: 2.1502 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.7472 | Steps: 2 | Val loss: 2.2710 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=171628)[0m top1: 0.31902985074626866
[2m[36m(func pid=171628)[0m top5: 0.8698694029850746
[2m[36m(func pid=171628)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=171628)[0m f1_macro: 0.31458868202549606
[2m[36m(func pid=171628)[0m f1_weighted: 0.33306867524417805
[2m[36m(func pid=171628)[0m f1_per_class: [0.246, 0.265, 0.71, 0.392, 0.071, 0.399, 0.319, 0.356, 0.115, 0.273]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m top1: 0.25279850746268656
[2m[36m(func pid=169716)[0m top5: 0.7392723880597015
[2m[36m(func pid=169716)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=169716)[0m f1_macro: 0.2175982231514249
[2m[36m(func pid=169716)[0m f1_weighted: 0.2655143569945972
[2m[36m(func pid=169716)[0m f1_per_class: [0.281, 0.179, 0.142, 0.37, 0.044, 0.366, 0.209, 0.27, 0.056, 0.259]
== Status ==
Current time: 2024-01-07 01:58:54 (running for 00:25:15.08)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.778 |      0.129 |                   46 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.411 |      0.209 |                   18 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.889 |      0.315 |                   13 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=174617)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=174617)[0m Configuration completed!
[2m[36m(func pid=174617)[0m New optimizer parameters:
[2m[36m(func pid=174617)[0m SGD (
[2m[36m(func pid=174617)[0m Parameter Group 0
[2m[36m(func pid=174617)[0m     dampening: 0
[2m[36m(func pid=174617)[0m     differentiable: False
[2m[36m(func pid=174617)[0m     foreach: None
[2m[36m(func pid=174617)[0m     lr: 0.1
[2m[36m(func pid=174617)[0m     maximize: False
[2m[36m(func pid=174617)[0m     momentum: 0.9
[2m[36m(func pid=174617)[0m     nesterov: False
[2m[36m(func pid=174617)[0m     weight_decay: 0.0001
[2m[36m(func pid=174617)[0m )
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=163416)[0m top1: 0.17257462686567165
[2m[36m(func pid=163416)[0m top5: 0.6236007462686567
[2m[36m(func pid=163416)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=163416)[0m f1_macro: 0.1260892113250546
[2m[36m(func pid=163416)[0m f1_weighted: 0.18266412869464066
[2m[36m(func pid=163416)[0m f1_per_class: [0.09, 0.145, 0.11, 0.225, 0.027, 0.34, 0.147, 0.15, 0.027, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8641 | Steps: 2 | Val loss: 1.8392 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.3104 | Steps: 2 | Val loss: 2.1387 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:58:59 (running for 00:25:20.32)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.747 |      0.126 |                   47 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.312 |      0.218 |                   19 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.864 |      0.322 |                   14 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.3278917910447761
[2m[36m(func pid=171628)[0m top5: 0.8656716417910447
[2m[36m(func pid=171628)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=171628)[0m f1_macro: 0.3216036408353678
[2m[36m(func pid=171628)[0m f1_weighted: 0.34730318403202187
[2m[36m(func pid=171628)[0m f1_per_class: [0.251, 0.255, 0.71, 0.399, 0.078, 0.397, 0.364, 0.35, 0.151, 0.261]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.7614 | Steps: 2 | Val loss: 2.2688 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0229 | Steps: 2 | Val loss: 2.3533 | Batch size: 32 | lr: 0.1 | Duration: 4.33s
[2m[36m(func pid=169716)[0m top1: 0.25326492537313433
[2m[36m(func pid=169716)[0m top5: 0.746268656716418
[2m[36m(func pid=169716)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=169716)[0m f1_macro: 0.2181550920772905
[2m[36m(func pid=169716)[0m f1_weighted: 0.26509225013890114
[2m[36m(func pid=169716)[0m f1_per_class: [0.271, 0.18, 0.155, 0.37, 0.043, 0.357, 0.21, 0.27, 0.059, 0.267]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m top1: 0.17630597014925373
[2m[36m(func pid=163416)[0m top5: 0.6236007462686567
[2m[36m(func pid=163416)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=163416)[0m f1_macro: 0.13228897752849778
[2m[36m(func pid=163416)[0m f1_weighted: 0.18596938361357856
[2m[36m(func pid=163416)[0m f1_per_class: [0.079, 0.163, 0.156, 0.22, 0.033, 0.339, 0.151, 0.152, 0.03, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8912 | Steps: 2 | Val loss: 1.8491 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=174617)[0m top1: 0.11847014925373134
[2m[36m(func pid=174617)[0m top5: 0.5163246268656716
[2m[36m(func pid=174617)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=174617)[0m f1_macro: 0.12183911894414681
[2m[36m(func pid=174617)[0m f1_weighted: 0.10741035918288236
[2m[36m(func pid=174617)[0m f1_per_class: [0.081, 0.224, 0.198, 0.146, 0.034, 0.023, 0.009, 0.287, 0.019, 0.195]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.3016 | Steps: 2 | Val loss: 2.1332 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 01:59:04 (running for 00:25:25.41)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.761 |      0.132 |                   48 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.31  |      0.218 |                   20 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.891 |      0.325 |                   15 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  3.023 |      0.122 |                    1 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.33302238805970147
[2m[36m(func pid=171628)[0m top5: 0.8675373134328358
[2m[36m(func pid=171628)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=171628)[0m f1_macro: 0.32496900256499756
[2m[36m(func pid=171628)[0m f1_weighted: 0.3566887038633906
[2m[36m(func pid=171628)[0m f1_per_class: [0.252, 0.255, 0.71, 0.401, 0.083, 0.394, 0.395, 0.336, 0.169, 0.255]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.7531 | Steps: 2 | Val loss: 2.2656 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.0795 | Steps: 2 | Val loss: 2.4729 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=169716)[0m top1: 0.2593283582089552
[2m[36m(func pid=169716)[0m top5: 0.7495335820895522
[2m[36m(func pid=169716)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=169716)[0m f1_macro: 0.22516762327577083
[2m[36m(func pid=169716)[0m f1_weighted: 0.2718142500915846
[2m[36m(func pid=169716)[0m f1_per_class: [0.262, 0.18, 0.157, 0.374, 0.058, 0.362, 0.225, 0.275, 0.063, 0.296]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7138 | Steps: 2 | Val loss: 1.8586 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=163416)[0m top1: 0.1767723880597015
[2m[36m(func pid=163416)[0m top5: 0.628731343283582
[2m[36m(func pid=163416)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=163416)[0m f1_macro: 0.13273832171863698
[2m[36m(func pid=163416)[0m f1_weighted: 0.18591304327889333
[2m[36m(func pid=163416)[0m f1_per_class: [0.088, 0.156, 0.151, 0.224, 0.033, 0.343, 0.149, 0.153, 0.029, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=174617)[0m top1: 0.12686567164179105
[2m[36m(func pid=174617)[0m top5: 0.5153917910447762
[2m[36m(func pid=174617)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=174617)[0m f1_macro: 0.16827610453919833
[2m[36m(func pid=174617)[0m f1_weighted: 0.12272206968263086
[2m[36m(func pid=174617)[0m f1_per_class: [0.073, 0.07, 0.293, 0.157, 0.072, 0.314, 0.0, 0.393, 0.033, 0.276]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.2150 | Steps: 2 | Val loss: 2.1261 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:59:10 (running for 00:25:30.71)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.753 |      0.133 |                   49 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.302 |      0.225 |                   21 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.714 |      0.324 |                   16 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  2.08  |      0.168 |                    2 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.33488805970149255
[2m[36m(func pid=171628)[0m top5: 0.8661380597014925
[2m[36m(func pid=171628)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=171628)[0m f1_macro: 0.3238444899446251
[2m[36m(func pid=171628)[0m f1_weighted: 0.3591728600410407
[2m[36m(func pid=171628)[0m f1_per_class: [0.272, 0.252, 0.71, 0.413, 0.075, 0.365, 0.405, 0.328, 0.178, 0.241]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.7530 | Steps: 2 | Val loss: 2.2631 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.2832 | Steps: 2 | Val loss: 2.1852 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=169716)[0m top1: 0.2574626865671642
[2m[36m(func pid=169716)[0m top5: 0.7509328358208955
[2m[36m(func pid=169716)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=169716)[0m f1_macro: 0.22549091540783128
[2m[36m(func pid=169716)[0m f1_weighted: 0.26993531274160376
[2m[36m(func pid=169716)[0m f1_per_class: [0.258, 0.173, 0.164, 0.375, 0.062, 0.351, 0.226, 0.267, 0.077, 0.303]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6974 | Steps: 2 | Val loss: 1.8621 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=163416)[0m top1: 0.1791044776119403
[2m[36m(func pid=163416)[0m top5: 0.6282649253731343
[2m[36m(func pid=163416)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=163416)[0m f1_macro: 0.13116911320761765
[2m[36m(func pid=163416)[0m f1_weighted: 0.18947441817041216
[2m[36m(func pid=163416)[0m f1_per_class: [0.095, 0.171, 0.112, 0.223, 0.033, 0.345, 0.154, 0.149, 0.03, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=174617)[0m top1: 0.22294776119402984
[2m[36m(func pid=174617)[0m top5: 0.784981343283582
[2m[36m(func pid=174617)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=174617)[0m f1_macro: 0.2244568581282617
[2m[36m(func pid=174617)[0m f1_weighted: 0.19841236632600648
[2m[36m(func pid=174617)[0m f1_per_class: [0.13, 0.123, 0.247, 0.311, 0.133, 0.372, 0.036, 0.422, 0.094, 0.376]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.1786 | Steps: 2 | Val loss: 2.1154 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 01:59:15 (running for 00:25:36.19)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.753 |      0.131 |                   50 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.215 |      0.225 |                   22 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.697 |      0.323 |                   17 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  1.283 |      0.224 |                    3 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.34048507462686567
[2m[36m(func pid=171628)[0m top5: 0.8661380597014925
[2m[36m(func pid=171628)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=171628)[0m f1_macro: 0.3231535270123763
[2m[36m(func pid=171628)[0m f1_weighted: 0.36558972706768017
[2m[36m(func pid=171628)[0m f1_per_class: [0.272, 0.255, 0.71, 0.415, 0.078, 0.353, 0.433, 0.3, 0.181, 0.235]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.7754 | Steps: 2 | Val loss: 1.9352 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.7384 | Steps: 2 | Val loss: 2.2587 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=169716)[0m top1: 0.2667910447761194
[2m[36m(func pid=169716)[0m top5: 0.7593283582089553
[2m[36m(func pid=169716)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=169716)[0m f1_macro: 0.2318383933019273
[2m[36m(func pid=169716)[0m f1_weighted: 0.2802359035793272
[2m[36m(func pid=169716)[0m f1_per_class: [0.261, 0.181, 0.188, 0.391, 0.059, 0.349, 0.24, 0.276, 0.068, 0.305]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6418 | Steps: 2 | Val loss: 1.8739 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=174617)[0m top1: 0.30783582089552236
[2m[36m(func pid=174617)[0m top5: 0.894589552238806
[2m[36m(func pid=174617)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=174617)[0m f1_macro: 0.2714550776984207
[2m[36m(func pid=174617)[0m f1_weighted: 0.3255137680196428
[2m[36m(func pid=174617)[0m f1_per_class: [0.198, 0.187, 0.444, 0.351, 0.116, 0.402, 0.422, 0.162, 0.082, 0.35]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=163416)[0m top1: 0.18563432835820895
[2m[36m(func pid=163416)[0m top5: 0.6389925373134329
[2m[36m(func pid=163416)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=163416)[0m f1_macro: 0.13888105338174767
[2m[36m(func pid=163416)[0m f1_weighted: 0.19694016888484187
[2m[36m(func pid=163416)[0m f1_per_class: [0.097, 0.172, 0.152, 0.229, 0.034, 0.345, 0.171, 0.15, 0.039, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.1540 | Steps: 2 | Val loss: 2.1041 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 01:59:20 (running for 00:25:41.43)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.738 |      0.139 |                   51 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.179 |      0.232 |                   23 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.642 |      0.325 |                   18 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.775 |      0.271 |                    4 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.34281716417910446
[2m[36m(func pid=171628)[0m top5: 0.8666044776119403
[2m[36m(func pid=171628)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=171628)[0m f1_macro: 0.3252646792444964
[2m[36m(func pid=171628)[0m f1_weighted: 0.36914751299208165
[2m[36m(func pid=171628)[0m f1_per_class: [0.292, 0.257, 0.71, 0.42, 0.073, 0.357, 0.438, 0.283, 0.189, 0.233]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5876 | Steps: 2 | Val loss: 2.2306 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.7391 | Steps: 2 | Val loss: 2.2569 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=169716)[0m top1: 0.271455223880597
[2m[36m(func pid=169716)[0m top5: 0.7681902985074627
[2m[36m(func pid=169716)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=169716)[0m f1_macro: 0.2348297262385442
[2m[36m(func pid=169716)[0m f1_weighted: 0.2869768480406943
[2m[36m(func pid=169716)[0m f1_per_class: [0.257, 0.172, 0.195, 0.392, 0.059, 0.349, 0.264, 0.279, 0.083, 0.298]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.5207 | Steps: 2 | Val loss: 1.8822 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=174617)[0m top1: 0.283115671641791
[2m[36m(func pid=174617)[0m top5: 0.8861940298507462
[2m[36m(func pid=174617)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=174617)[0m f1_macro: 0.29787135633304557
[2m[36m(func pid=174617)[0m f1_weighted: 0.3148392778585467
[2m[36m(func pid=174617)[0m f1_per_class: [0.23, 0.216, 0.558, 0.307, 0.127, 0.403, 0.367, 0.361, 0.082, 0.326]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=163416)[0m top1: 0.18703358208955223
[2m[36m(func pid=163416)[0m top5: 0.6371268656716418
[2m[36m(func pid=163416)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=163416)[0m f1_macro: 0.13922925312393666
[2m[36m(func pid=163416)[0m f1_weighted: 0.19912638089944362
[2m[36m(func pid=163416)[0m f1_per_class: [0.095, 0.162, 0.149, 0.246, 0.034, 0.353, 0.165, 0.151, 0.038, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.1389 | Steps: 2 | Val loss: 2.0970 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:59:25 (running for 00:25:46.54)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.739 |      0.139 |                   52 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.154 |      0.235 |                   24 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.521 |      0.32  |                   19 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.588 |      0.298 |                    5 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.33955223880597013
[2m[36m(func pid=171628)[0m top5: 0.8661380597014925
[2m[36m(func pid=171628)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=171628)[0m f1_macro: 0.3199774024130943
[2m[36m(func pid=171628)[0m f1_weighted: 0.36564900640383163
[2m[36m(func pid=171628)[0m f1_per_class: [0.31, 0.252, 0.667, 0.422, 0.089, 0.348, 0.43, 0.289, 0.192, 0.2]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.7369 | Steps: 2 | Val loss: 2.2566 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4544 | Steps: 2 | Val loss: 2.6287 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=169716)[0m top1: 0.27098880597014924
[2m[36m(func pid=169716)[0m top5: 0.7723880597014925
[2m[36m(func pid=169716)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=169716)[0m f1_macro: 0.2352198728234091
[2m[36m(func pid=169716)[0m f1_weighted: 0.28752768237441206
[2m[36m(func pid=169716)[0m f1_per_class: [0.265, 0.181, 0.196, 0.382, 0.057, 0.343, 0.273, 0.278, 0.072, 0.304]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6471 | Steps: 2 | Val loss: 1.8857 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=163416)[0m top1: 0.1837686567164179
[2m[36m(func pid=163416)[0m top5: 0.6385261194029851
[2m[36m(func pid=163416)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=163416)[0m f1_macro: 0.13726500215991425
[2m[36m(func pid=163416)[0m f1_weighted: 0.195565137146602
[2m[36m(func pid=163416)[0m f1_per_class: [0.106, 0.163, 0.149, 0.238, 0.034, 0.341, 0.165, 0.148, 0.028, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=174617)[0m top1: 0.259794776119403
[2m[36m(func pid=174617)[0m top5: 0.8404850746268657
[2m[36m(func pid=174617)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=174617)[0m f1_macro: 0.2849409349966114
[2m[36m(func pid=174617)[0m f1_weighted: 0.2707369308127811
[2m[36m(func pid=174617)[0m f1_per_class: [0.167, 0.214, 0.615, 0.308, 0.143, 0.402, 0.219, 0.367, 0.113, 0.301]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.0969 | Steps: 2 | Val loss: 2.0882 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=171628)[0m top1: 0.341884328358209
[2m[36m(func pid=171628)[0m top5: 0.8708022388059702
[2m[36m(func pid=171628)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=171628)[0m f1_macro: 0.32113953621414576
[2m[36m(func pid=171628)[0m f1_weighted: 0.3680751971200895
[2m[36m(func pid=171628)[0m f1_per_class: [0.325, 0.257, 0.647, 0.427, 0.091, 0.366, 0.425, 0.281, 0.185, 0.206]
== Status ==
Current time: 2024-01-07 01:59:30 (running for 00:25:51.60)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.737 |      0.137 |                   53 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.139 |      0.235 |                   25 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.647 |      0.321 |                   20 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.454 |      0.285 |                    6 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.7570 | Steps: 2 | Val loss: 2.2554 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.3455 | Steps: 2 | Val loss: 2.8504 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=169716)[0m top1: 0.271455223880597
[2m[36m(func pid=169716)[0m top5: 0.7765858208955224
[2m[36m(func pid=169716)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=169716)[0m f1_macro: 0.23651845449232564
[2m[36m(func pid=169716)[0m f1_weighted: 0.2885438271133567
[2m[36m(func pid=169716)[0m f1_per_class: [0.254, 0.181, 0.206, 0.382, 0.056, 0.338, 0.278, 0.275, 0.089, 0.306]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5204 | Steps: 2 | Val loss: 1.8967 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=163416)[0m top1: 0.1875
[2m[36m(func pid=163416)[0m top5: 0.6385261194029851
[2m[36m(func pid=163416)[0m f1_micro: 0.1875
[2m[36m(func pid=163416)[0m f1_macro: 0.13928020387197423
[2m[36m(func pid=163416)[0m f1_weighted: 0.19925817098955317
[2m[36m(func pid=163416)[0m f1_per_class: [0.102, 0.164, 0.149, 0.25, 0.028, 0.353, 0.16, 0.159, 0.028, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=174617)[0m top1: 0.261660447761194
[2m[36m(func pid=174617)[0m top5: 0.8367537313432836
[2m[36m(func pid=174617)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=174617)[0m f1_macro: 0.29072591133452186
[2m[36m(func pid=174617)[0m f1_weighted: 0.2651681993684835
[2m[36m(func pid=174617)[0m f1_per_class: [0.158, 0.207, 0.727, 0.327, 0.134, 0.41, 0.183, 0.357, 0.141, 0.263]
[2m[36m(func pid=174617)[0m 
== Status ==
Current time: 2024-01-07 01:59:36 (running for 00:25:56.76)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.757 |      0.139 |                   54 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.097 |      0.237 |                   26 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.52  |      0.321 |                   21 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.346 |      0.291 |                    7 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.33908582089552236
[2m[36m(func pid=171628)[0m top5: 0.8717350746268657
[2m[36m(func pid=171628)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=171628)[0m f1_macro: 0.3209564060349688
[2m[36m(func pid=171628)[0m f1_weighted: 0.36538936710101794
[2m[36m(func pid=171628)[0m f1_per_class: [0.344, 0.27, 0.647, 0.421, 0.082, 0.377, 0.412, 0.268, 0.19, 0.198]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.0717 | Steps: 2 | Val loss: 2.0787 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.7497 | Steps: 2 | Val loss: 2.2544 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.1817 | Steps: 2 | Val loss: 2.9047 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=169716)[0m top1: 0.2756529850746269
[2m[36m(func pid=169716)[0m top5: 0.7803171641791045
[2m[36m(func pid=169716)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=169716)[0m f1_macro: 0.24650324627092174
[2m[36m(func pid=169716)[0m f1_weighted: 0.29138783124076
[2m[36m(func pid=169716)[0m f1_per_class: [0.263, 0.185, 0.229, 0.385, 0.061, 0.329, 0.28, 0.28, 0.111, 0.341]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5525 | Steps: 2 | Val loss: 1.9272 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=163416)[0m top1: 0.1865671641791045
[2m[36m(func pid=163416)[0m top5: 0.6380597014925373
[2m[36m(func pid=163416)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=163416)[0m f1_macro: 0.13797351457421517
[2m[36m(func pid=163416)[0m f1_weighted: 0.1988703115060294
[2m[36m(func pid=163416)[0m f1_per_class: [0.093, 0.162, 0.14, 0.253, 0.034, 0.353, 0.158, 0.15, 0.037, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=174617)[0m top1: 0.28591417910447764
[2m[36m(func pid=174617)[0m top5: 0.8362873134328358
[2m[36m(func pid=174617)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=174617)[0m f1_macro: 0.3021641984021873
[2m[36m(func pid=174617)[0m f1_weighted: 0.30167653186096727
[2m[36m(func pid=174617)[0m f1_per_class: [0.16, 0.178, 0.774, 0.363, 0.149, 0.403, 0.3, 0.309, 0.145, 0.242]
[2m[36m(func pid=174617)[0m 
== Status ==
Current time: 2024-01-07 01:59:41 (running for 00:26:01.87)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.75  |      0.138 |                   55 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.072 |      0.247 |                   27 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.552 |      0.322 |                   22 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.182 |      0.302 |                    8 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.33861940298507465
[2m[36m(func pid=171628)[0m top5: 0.8745335820895522
[2m[36m(func pid=171628)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=171628)[0m f1_macro: 0.32201031710589534
[2m[36m(func pid=171628)[0m f1_weighted: 0.36538167237894054
[2m[36m(func pid=171628)[0m f1_per_class: [0.33, 0.278, 0.647, 0.411, 0.073, 0.386, 0.41, 0.271, 0.219, 0.195]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.0575 | Steps: 2 | Val loss: 2.0712 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.6783 | Steps: 2 | Val loss: 2.2542 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.1752 | Steps: 2 | Val loss: 2.9768 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=169716)[0m top1: 0.2789179104477612
[2m[36m(func pid=169716)[0m top5: 0.7840485074626866
[2m[36m(func pid=169716)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=169716)[0m f1_macro: 0.24783424794767245
[2m[36m(func pid=169716)[0m f1_weighted: 0.29561823848164326
[2m[36m(func pid=169716)[0m f1_per_class: [0.262, 0.171, 0.234, 0.397, 0.06, 0.344, 0.286, 0.274, 0.117, 0.333]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3970 | Steps: 2 | Val loss: 1.9552 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=163416)[0m top1: 0.18563432835820895
[2m[36m(func pid=163416)[0m top5: 0.6450559701492538
[2m[36m(func pid=163416)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=163416)[0m f1_macro: 0.13692207271432974
[2m[36m(func pid=163416)[0m f1_weighted: 0.19799241384084976
[2m[36m(func pid=163416)[0m f1_per_class: [0.1, 0.161, 0.13, 0.253, 0.034, 0.349, 0.157, 0.157, 0.029, 0.0]
[2m[36m(func pid=163416)[0m 
== Status ==
Current time: 2024-01-07 01:59:46 (running for 00:26:06.93)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.678 |      0.137 |                   56 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.058 |      0.248 |                   28 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.397 |      0.325 |                   23 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.182 |      0.302 |                    8 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.3423507462686567
[2m[36m(func pid=171628)[0m top5: 0.8717350746268657
[2m[36m(func pid=171628)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=171628)[0m f1_macro: 0.32512295365581634
[2m[36m(func pid=171628)[0m f1_weighted: 0.37037702139573675
[2m[36m(func pid=171628)[0m f1_per_class: [0.328, 0.284, 0.647, 0.401, 0.073, 0.399, 0.427, 0.275, 0.221, 0.195]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=174617)[0m top1: 0.3162313432835821
[2m[36m(func pid=174617)[0m top5: 0.8418843283582089
[2m[36m(func pid=174617)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=174617)[0m f1_macro: 0.3049711648337659
[2m[36m(func pid=174617)[0m f1_weighted: 0.33234431843026396
[2m[36m(func pid=174617)[0m f1_per_class: [0.18, 0.176, 0.774, 0.41, 0.172, 0.408, 0.382, 0.174, 0.15, 0.224]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.0303 | Steps: 2 | Val loss: 2.0646 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4824 | Steps: 2 | Val loss: 1.9790 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.7206 | Steps: 2 | Val loss: 2.2547 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=169716)[0m top1: 0.2775186567164179
[2m[36m(func pid=169716)[0m top5: 0.789179104477612
[2m[36m(func pid=169716)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=169716)[0m f1_macro: 0.24680672327259656
[2m[36m(func pid=169716)[0m f1_weighted: 0.2959784880070239
[2m[36m(func pid=169716)[0m f1_per_class: [0.26, 0.185, 0.253, 0.387, 0.052, 0.338, 0.293, 0.275, 0.103, 0.322]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.0781 | Steps: 2 | Val loss: 3.0859 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 01:59:51 (running for 00:26:12.11)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.678 |      0.137 |                   56 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.03  |      0.247 |                   29 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.482 |      0.322 |                   24 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.175 |      0.305 |                    9 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.33861940298507465
[2m[36m(func pid=171628)[0m top5: 0.871268656716418
[2m[36m(func pid=171628)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=171628)[0m f1_macro: 0.3218450090478676
[2m[36m(func pid=171628)[0m f1_weighted: 0.3657503756543475
[2m[36m(func pid=171628)[0m f1_per_class: [0.332, 0.289, 0.647, 0.387, 0.076, 0.404, 0.423, 0.276, 0.2, 0.185]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=163416)[0m top1: 0.1837686567164179
[2m[36m(func pid=163416)[0m top5: 0.6436567164179104
[2m[36m(func pid=163416)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=163416)[0m f1_macro: 0.13663002230577617
[2m[36m(func pid=163416)[0m f1_weighted: 0.19585335860820427
[2m[36m(func pid=163416)[0m f1_per_class: [0.099, 0.166, 0.136, 0.23, 0.035, 0.355, 0.167, 0.15, 0.028, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=174617)[0m top1: 0.314365671641791
[2m[36m(func pid=174617)[0m top5: 0.8442164179104478
[2m[36m(func pid=174617)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=174617)[0m f1_macro: 0.2993773928826312
[2m[36m(func pid=174617)[0m f1_weighted: 0.33047744917050076
[2m[36m(func pid=174617)[0m f1_per_class: [0.202, 0.178, 0.8, 0.418, 0.139, 0.373, 0.384, 0.164, 0.133, 0.203]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.9967 | Steps: 2 | Val loss: 2.0552 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3964 | Steps: 2 | Val loss: 1.9844 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=169716)[0m top1: 0.2826492537313433
[2m[36m(func pid=169716)[0m top5: 0.7924440298507462
[2m[36m(func pid=169716)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=169716)[0m f1_macro: 0.24932915494605035
[2m[36m(func pid=169716)[0m f1_weighted: 0.30380957373620693
[2m[36m(func pid=169716)[0m f1_per_class: [0.26, 0.206, 0.253, 0.384, 0.047, 0.327, 0.313, 0.27, 0.123, 0.311]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.7191 | Steps: 2 | Val loss: 2.2516 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.2256 | Steps: 2 | Val loss: 3.1663 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 01:59:56 (running for 00:26:17.21)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.721 |      0.137 |                   57 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.997 |      0.249 |                   30 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.396 |      0.318 |                   25 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.078 |      0.299 |                   10 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.33675373134328357
[2m[36m(func pid=171628)[0m top5: 0.8689365671641791
[2m[36m(func pid=171628)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=171628)[0m f1_macro: 0.3177696631687827
[2m[36m(func pid=171628)[0m f1_weighted: 0.3638523216470762
[2m[36m(func pid=171628)[0m f1_per_class: [0.321, 0.284, 0.629, 0.402, 0.078, 0.398, 0.408, 0.267, 0.215, 0.175]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=163416)[0m top1: 0.18796641791044777
[2m[36m(func pid=163416)[0m top5: 0.6436567164179104
[2m[36m(func pid=163416)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=163416)[0m f1_macro: 0.14094789720986653
[2m[36m(func pid=163416)[0m f1_weighted: 0.19956459833978984
[2m[36m(func pid=163416)[0m f1_per_class: [0.111, 0.172, 0.139, 0.241, 0.034, 0.363, 0.159, 0.162, 0.028, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.0132 | Steps: 2 | Val loss: 2.0433 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=174617)[0m top1: 0.314365671641791
[2m[36m(func pid=174617)[0m top5: 0.8530783582089553
[2m[36m(func pid=174617)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=174617)[0m f1_macro: 0.3041127357011386
[2m[36m(func pid=174617)[0m f1_weighted: 0.330125677155675
[2m[36m(func pid=174617)[0m f1_per_class: [0.298, 0.217, 0.774, 0.422, 0.113, 0.347, 0.36, 0.154, 0.16, 0.196]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3852 | Steps: 2 | Val loss: 2.0013 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=169716)[0m top1: 0.2835820895522388
[2m[36m(func pid=169716)[0m top5: 0.7938432835820896
[2m[36m(func pid=169716)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=169716)[0m f1_macro: 0.2519957218021524
[2m[36m(func pid=169716)[0m f1_weighted: 0.30403862336733056
[2m[36m(func pid=169716)[0m f1_per_class: [0.258, 0.208, 0.25, 0.377, 0.047, 0.339, 0.314, 0.274, 0.127, 0.326]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.7192 | Steps: 2 | Val loss: 2.2511 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.0537 | Steps: 2 | Val loss: 3.2486 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 02:00:01 (running for 00:26:22.42)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.719 |      0.141 |                   58 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  2.013 |      0.252 |                   31 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.385 |      0.319 |                   26 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.226 |      0.304 |                   11 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.3376865671641791
[2m[36m(func pid=171628)[0m top5: 0.8619402985074627
[2m[36m(func pid=171628)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=171628)[0m f1_macro: 0.31908669931720357
[2m[36m(func pid=171628)[0m f1_weighted: 0.36514392874358614
[2m[36m(func pid=171628)[0m f1_per_class: [0.33, 0.277, 0.629, 0.407, 0.079, 0.404, 0.41, 0.271, 0.209, 0.176]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=163416)[0m top1: 0.18936567164179105
[2m[36m(func pid=163416)[0m top5: 0.6473880597014925
[2m[36m(func pid=163416)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=163416)[0m f1_macro: 0.14201353775654352
[2m[36m(func pid=163416)[0m f1_weighted: 0.20023713463138051
[2m[36m(func pid=163416)[0m f1_per_class: [0.11, 0.173, 0.143, 0.244, 0.035, 0.366, 0.158, 0.164, 0.029, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.9872 | Steps: 2 | Val loss: 2.0380 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=174617)[0m top1: 0.3204291044776119
[2m[36m(func pid=174617)[0m top5: 0.8521455223880597
[2m[36m(func pid=174617)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=174617)[0m f1_macro: 0.3187541938283375
[2m[36m(func pid=174617)[0m f1_weighted: 0.34411171096880255
[2m[36m(func pid=174617)[0m f1_per_class: [0.368, 0.25, 0.774, 0.418, 0.115, 0.322, 0.388, 0.186, 0.19, 0.176]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5459 | Steps: 2 | Val loss: 1.9863 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=169716)[0m top1: 0.2873134328358209
[2m[36m(func pid=169716)[0m top5: 0.7952425373134329
[2m[36m(func pid=169716)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=169716)[0m f1_macro: 0.2548762785245774
[2m[36m(func pid=169716)[0m f1_weighted: 0.30791454159922693
[2m[36m(func pid=169716)[0m f1_per_class: [0.259, 0.208, 0.25, 0.382, 0.047, 0.353, 0.314, 0.28, 0.128, 0.326]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.7125 | Steps: 2 | Val loss: 2.2480 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0700 | Steps: 2 | Val loss: 3.4390 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 02:00:06 (running for 00:26:27.48)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.719 |      0.142 |                   59 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.987 |      0.255 |                   32 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.546 |      0.323 |                   27 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.054 |      0.319 |                   12 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.33722014925373134
[2m[36m(func pid=171628)[0m top5: 0.8703358208955224
[2m[36m(func pid=171628)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=171628)[0m f1_macro: 0.3233631110526421
[2m[36m(func pid=171628)[0m f1_weighted: 0.3643039920991378
[2m[36m(func pid=171628)[0m f1_per_class: [0.333, 0.274, 0.688, 0.414, 0.082, 0.391, 0.407, 0.26, 0.21, 0.174]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=163416)[0m top1: 0.18936567164179105
[2m[36m(func pid=163416)[0m top5: 0.6539179104477612
[2m[36m(func pid=163416)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=163416)[0m f1_macro: 0.14233368252700096
[2m[36m(func pid=163416)[0m f1_weighted: 0.20149844063299605
[2m[36m(func pid=163416)[0m f1_per_class: [0.119, 0.166, 0.149, 0.243, 0.034, 0.363, 0.169, 0.152, 0.029, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.8880 | Steps: 2 | Val loss: 2.0261 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=174617)[0m top1: 0.3064365671641791
[2m[36m(func pid=174617)[0m top5: 0.8470149253731343
[2m[36m(func pid=174617)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=174617)[0m f1_macro: 0.31522818110326606
[2m[36m(func pid=174617)[0m f1_weighted: 0.3337176512764219
[2m[36m(func pid=174617)[0m f1_per_class: [0.43, 0.26, 0.75, 0.39, 0.101, 0.278, 0.386, 0.185, 0.186, 0.185]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3398 | Steps: 2 | Val loss: 1.9782 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=169716)[0m top1: 0.29011194029850745
[2m[36m(func pid=169716)[0m top5: 0.8027052238805971
[2m[36m(func pid=169716)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=169716)[0m f1_macro: 0.25569985268790696
[2m[36m(func pid=169716)[0m f1_weighted: 0.31017643146591406
[2m[36m(func pid=169716)[0m f1_per_class: [0.249, 0.219, 0.268, 0.387, 0.049, 0.355, 0.313, 0.283, 0.113, 0.322]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.6938 | Steps: 2 | Val loss: 2.2505 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0331 | Steps: 2 | Val loss: 3.5849 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 02:00:12 (running for 00:26:32.73)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.712 |      0.142 |                   60 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.888 |      0.256 |                   33 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.34  |      0.325 |                   28 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.07  |      0.315 |                   13 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.341884328358209
[2m[36m(func pid=171628)[0m top5: 0.8731343283582089
[2m[36m(func pid=171628)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=171628)[0m f1_macro: 0.3246565999360436
[2m[36m(func pid=171628)[0m f1_weighted: 0.36964146091239747
[2m[36m(func pid=171628)[0m f1_per_class: [0.34, 0.285, 0.688, 0.419, 0.085, 0.381, 0.418, 0.26, 0.21, 0.162]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.9196 | Steps: 2 | Val loss: 2.0195 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=163416)[0m top1: 0.1884328358208955
[2m[36m(func pid=163416)[0m top5: 0.6534514925373134
[2m[36m(func pid=163416)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=163416)[0m f1_macro: 0.14600122052550826
[2m[36m(func pid=163416)[0m f1_weighted: 0.19978374875271318
[2m[36m(func pid=163416)[0m f1_per_class: [0.144, 0.172, 0.167, 0.23, 0.034, 0.362, 0.17, 0.154, 0.028, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=174617)[0m top1: 0.29617537313432835
[2m[36m(func pid=174617)[0m top5: 0.8493470149253731
[2m[36m(func pid=174617)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=174617)[0m f1_macro: 0.3083941034437001
[2m[36m(func pid=174617)[0m f1_weighted: 0.32540176091939454
[2m[36m(func pid=174617)[0m f1_per_class: [0.388, 0.262, 0.727, 0.359, 0.096, 0.306, 0.378, 0.181, 0.209, 0.178]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3547 | Steps: 2 | Val loss: 1.9905 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=169716)[0m top1: 0.29197761194029853
[2m[36m(func pid=169716)[0m top5: 0.8027052238805971
[2m[36m(func pid=169716)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=169716)[0m f1_macro: 0.2574862980905102
[2m[36m(func pid=169716)[0m f1_weighted: 0.31251361293551055
[2m[36m(func pid=169716)[0m f1_per_class: [0.241, 0.219, 0.297, 0.393, 0.05, 0.347, 0.317, 0.292, 0.109, 0.311]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.6880 | Steps: 2 | Val loss: 2.2483 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0938 | Steps: 2 | Val loss: 3.5855 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 02:00:17 (running for 00:26:38.05)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.694 |      0.146 |                   61 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.92  |      0.257 |                   34 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.355 |      0.325 |                   29 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.033 |      0.308 |                   14 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.34281716417910446
[2m[36m(func pid=171628)[0m top5: 0.8740671641791045
[2m[36m(func pid=171628)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=171628)[0m f1_macro: 0.32453224697322386
[2m[36m(func pid=171628)[0m f1_weighted: 0.3710883664786516
[2m[36m(func pid=171628)[0m f1_per_class: [0.344, 0.29, 0.71, 0.415, 0.083, 0.372, 0.432, 0.24, 0.198, 0.163]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.8046 | Steps: 2 | Val loss: 2.0112 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=163416)[0m top1: 0.18983208955223882
[2m[36m(func pid=163416)[0m top5: 0.6557835820895522
[2m[36m(func pid=163416)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=163416)[0m f1_macro: 0.14704677569512073
[2m[36m(func pid=163416)[0m f1_weighted: 0.20072646526859497
[2m[36m(func pid=163416)[0m f1_per_class: [0.152, 0.169, 0.163, 0.236, 0.034, 0.361, 0.168, 0.158, 0.029, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=174617)[0m top1: 0.30597014925373134
[2m[36m(func pid=174617)[0m top5: 0.8600746268656716
[2m[36m(func pid=174617)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=174617)[0m f1_macro: 0.3073203316473523
[2m[36m(func pid=174617)[0m f1_weighted: 0.33638742775938785
[2m[36m(func pid=174617)[0m f1_per_class: [0.403, 0.27, 0.649, 0.367, 0.094, 0.341, 0.391, 0.174, 0.211, 0.173]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3393 | Steps: 2 | Val loss: 2.0042 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=169716)[0m top1: 0.29384328358208955
[2m[36m(func pid=169716)[0m top5: 0.8073694029850746
[2m[36m(func pid=169716)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=169716)[0m f1_macro: 0.26361143720047414
[2m[36m(func pid=169716)[0m f1_weighted: 0.3126033553468669
[2m[36m(func pid=169716)[0m f1_per_class: [0.243, 0.211, 0.328, 0.398, 0.062, 0.356, 0.31, 0.304, 0.108, 0.316]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.6811 | Steps: 2 | Val loss: 2.2450 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0376 | Steps: 2 | Val loss: 3.5491 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 02:00:22 (running for 00:26:43.27)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.688 |      0.147 |                   62 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.805 |      0.264 |                   35 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.339 |      0.324 |                   30 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.094 |      0.307 |                   15 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.3414179104477612
[2m[36m(func pid=171628)[0m top5: 0.8754664179104478
[2m[36m(func pid=171628)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=171628)[0m f1_macro: 0.32393959089181684
[2m[36m(func pid=171628)[0m f1_weighted: 0.3702108746075261
[2m[36m(func pid=171628)[0m f1_per_class: [0.339, 0.292, 0.71, 0.407, 0.083, 0.35, 0.441, 0.255, 0.194, 0.169]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.7630 | Steps: 2 | Val loss: 2.0060 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=163416)[0m top1: 0.19029850746268656
[2m[36m(func pid=163416)[0m top5: 0.6632462686567164
[2m[36m(func pid=163416)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=163416)[0m f1_macro: 0.14630708617962562
[2m[36m(func pid=163416)[0m f1_weighted: 0.20225602734547732
[2m[36m(func pid=163416)[0m f1_per_class: [0.143, 0.163, 0.163, 0.241, 0.036, 0.362, 0.173, 0.153, 0.029, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=174617)[0m top1: 0.3199626865671642
[2m[36m(func pid=174617)[0m top5: 0.8642723880597015
[2m[36m(func pid=174617)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=174617)[0m f1_macro: 0.30796983810253364
[2m[36m(func pid=174617)[0m f1_weighted: 0.3457032649462774
[2m[36m(func pid=174617)[0m f1_per_class: [0.392, 0.261, 0.6, 0.403, 0.113, 0.359, 0.385, 0.189, 0.208, 0.168]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4868 | Steps: 2 | Val loss: 2.0261 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=169716)[0m top1: 0.29617537313432835
[2m[36m(func pid=169716)[0m top5: 0.8073694029850746
[2m[36m(func pid=169716)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=169716)[0m f1_macro: 0.26556741405947215
[2m[36m(func pid=169716)[0m f1_weighted: 0.315498316190133
[2m[36m(func pid=169716)[0m f1_per_class: [0.244, 0.214, 0.338, 0.403, 0.061, 0.358, 0.312, 0.307, 0.107, 0.312]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.6810 | Steps: 2 | Val loss: 2.2447 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0663 | Steps: 2 | Val loss: 3.5576 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 02:00:27 (running for 00:26:48.27)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.681 |      0.146 |                   63 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.763 |      0.266 |                   36 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.487 |      0.321 |                   31 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.038 |      0.308 |                   16 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.34048507462686567
[2m[36m(func pid=171628)[0m top5: 0.8708022388059702
[2m[36m(func pid=171628)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=171628)[0m f1_macro: 0.3210094186422101
[2m[36m(func pid=171628)[0m f1_weighted: 0.3711975072805568
[2m[36m(func pid=171628)[0m f1_per_class: [0.318, 0.287, 0.71, 0.408, 0.078, 0.344, 0.451, 0.248, 0.188, 0.177]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.7046 | Steps: 2 | Val loss: 1.9961 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=163416)[0m top1: 0.1921641791044776
[2m[36m(func pid=163416)[0m top5: 0.6623134328358209
[2m[36m(func pid=163416)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=163416)[0m f1_macro: 0.14871658055450976
[2m[36m(func pid=163416)[0m f1_weighted: 0.20395366001879398
[2m[36m(func pid=163416)[0m f1_per_class: [0.151, 0.167, 0.176, 0.239, 0.036, 0.358, 0.18, 0.151, 0.029, 0.0]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=174617)[0m top1: 0.3218283582089552
[2m[36m(func pid=174617)[0m top5: 0.8582089552238806
[2m[36m(func pid=174617)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=174617)[0m f1_macro: 0.30402431871418467
[2m[36m(func pid=174617)[0m f1_weighted: 0.34331890331440035
[2m[36m(func pid=174617)[0m f1_per_class: [0.386, 0.257, 0.585, 0.415, 0.112, 0.357, 0.371, 0.197, 0.198, 0.163]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3031 | Steps: 2 | Val loss: 2.0426 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=169716)[0m top1: 0.29757462686567165
[2m[36m(func pid=169716)[0m top5: 0.8115671641791045
[2m[36m(func pid=169716)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=169716)[0m f1_macro: 0.27032046708168844
[2m[36m(func pid=169716)[0m f1_weighted: 0.31557006953734107
[2m[36m(func pid=169716)[0m f1_per_class: [0.255, 0.211, 0.367, 0.4, 0.061, 0.36, 0.313, 0.31, 0.118, 0.31]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6698 | Steps: 2 | Val loss: 2.2447 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=171628)[0m top1: 0.3362873134328358
[2m[36m(func pid=171628)[0m top5: 0.8694029850746269
[2m[36m(func pid=171628)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=171628)[0m f1_macro: 0.31895924590779534
[2m[36m(func pid=171628)[0m f1_weighted: 0.36656801944827244
[2m[36m(func pid=171628)[0m f1_per_class: [0.308, 0.284, 0.71, 0.4, 0.076, 0.327, 0.45, 0.263, 0.187, 0.186]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0377 | Steps: 2 | Val loss: 3.5577 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.7900 | Steps: 2 | Val loss: 1.9875 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 02:00:34 (running for 00:26:55.56)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.67  |      0.153 |                   65 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.705 |      0.27  |                   37 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.303 |      0.319 |                   32 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.066 |      0.304 |                   17 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163416)[0m top1: 0.19263059701492538
[2m[36m(func pid=163416)[0m top5: 0.6609141791044776
[2m[36m(func pid=163416)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=163416)[0m f1_macro: 0.15256195367523914
[2m[36m(func pid=163416)[0m f1_weighted: 0.20540846543074562
[2m[36m(func pid=163416)[0m f1_per_class: [0.148, 0.153, 0.18, 0.25, 0.035, 0.361, 0.177, 0.166, 0.029, 0.026]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=174617)[0m top1: 0.3269589552238806
[2m[36m(func pid=174617)[0m top5: 0.8568097014925373
[2m[36m(func pid=174617)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=174617)[0m f1_macro: 0.3025101735546953
[2m[36m(func pid=174617)[0m f1_weighted: 0.34742214330716625
[2m[36m(func pid=174617)[0m f1_per_class: [0.365, 0.253, 0.545, 0.436, 0.123, 0.358, 0.366, 0.201, 0.218, 0.16]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2522 | Steps: 2 | Val loss: 2.0592 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=169716)[0m top1: 0.30363805970149255
[2m[36m(func pid=169716)[0m top5: 0.8152985074626866
[2m[36m(func pid=169716)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=169716)[0m f1_macro: 0.27702997945079916
[2m[36m(func pid=169716)[0m f1_weighted: 0.32147227408965257
[2m[36m(func pid=169716)[0m f1_per_class: [0.274, 0.221, 0.386, 0.413, 0.054, 0.373, 0.308, 0.306, 0.119, 0.316]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m top1: 0.33908582089552236
[2m[36m(func pid=171628)[0m top5: 0.8652052238805971
[2m[36m(func pid=171628)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=171628)[0m f1_macro: 0.32126678583727597
[2m[36m(func pid=171628)[0m f1_weighted: 0.3693301763524974
[2m[36m(func pid=171628)[0m f1_per_class: [0.299, 0.29, 0.71, 0.4, 0.078, 0.319, 0.453, 0.294, 0.186, 0.184]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.7093 | Steps: 2 | Val loss: 2.2443 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0219 | Steps: 2 | Val loss: 3.5595 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.7601 | Steps: 2 | Val loss: 1.9824 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2615 | Steps: 2 | Val loss: 2.0874 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 02:00:40 (running for 00:27:01.12)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.709 |      0.154 |                   66 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.79  |      0.277 |                   38 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.252 |      0.321 |                   33 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.038 |      0.303 |                   18 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163416)[0m top1: 0.19496268656716417
[2m[36m(func pid=163416)[0m top5: 0.6581156716417911
[2m[36m(func pid=163416)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=163416)[0m f1_macro: 0.15397436928687652
[2m[36m(func pid=163416)[0m f1_weighted: 0.20696732380794047
[2m[36m(func pid=163416)[0m f1_per_class: [0.149, 0.158, 0.175, 0.246, 0.035, 0.368, 0.179, 0.173, 0.03, 0.027]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=174617)[0m top1: 0.3260261194029851
[2m[36m(func pid=174617)[0m top5: 0.855410447761194
[2m[36m(func pid=174617)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=174617)[0m f1_macro: 0.30104031737561365
[2m[36m(func pid=174617)[0m f1_weighted: 0.3442620308274019
[2m[36m(func pid=174617)[0m f1_per_class: [0.373, 0.255, 0.545, 0.439, 0.124, 0.349, 0.356, 0.195, 0.208, 0.166]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m top1: 0.30177238805970147
[2m[36m(func pid=169716)[0m top5: 0.8157649253731343
[2m[36m(func pid=169716)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=169716)[0m f1_macro: 0.27397886123295684
[2m[36m(func pid=169716)[0m f1_weighted: 0.32022220762559434
[2m[36m(func pid=169716)[0m f1_per_class: [0.262, 0.218, 0.373, 0.408, 0.06, 0.38, 0.31, 0.3, 0.117, 0.312]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m top1: 0.33255597014925375
[2m[36m(func pid=171628)[0m top5: 0.8568097014925373
[2m[36m(func pid=171628)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=171628)[0m f1_macro: 0.3182799669331594
[2m[36m(func pid=171628)[0m f1_weighted: 0.3629909720964876
[2m[36m(func pid=171628)[0m f1_per_class: [0.284, 0.278, 0.733, 0.395, 0.078, 0.303, 0.45, 0.287, 0.194, 0.179]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0521 | Steps: 2 | Val loss: 3.5756 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.6614 | Steps: 2 | Val loss: 2.2473 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.6854 | Steps: 2 | Val loss: 1.9733 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2559 | Steps: 2 | Val loss: 2.0908 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=174617)[0m top1: 0.3292910447761194
[2m[36m(func pid=174617)[0m top5: 0.8568097014925373
[2m[36m(func pid=174617)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=174617)[0m f1_macro: 0.29519477809657046
[2m[36m(func pid=174617)[0m f1_weighted: 0.34994505361538536
[2m[36m(func pid=174617)[0m f1_per_class: [0.329, 0.264, 0.511, 0.44, 0.128, 0.347, 0.373, 0.208, 0.19, 0.163]
[2m[36m(func pid=174617)[0m 
== Status ==
Current time: 2024-01-07 02:00:45 (running for 00:27:06.52)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.709 |      0.154 |                   66 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.76  |      0.274 |                   39 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.261 |      0.318 |                   34 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.052 |      0.295 |                   20 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163416)[0m top1: 0.1935634328358209
[2m[36m(func pid=163416)[0m top5: 0.6581156716417911
[2m[36m(func pid=163416)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=163416)[0m f1_macro: 0.1566610788934283
[2m[36m(func pid=163416)[0m f1_weighted: 0.20635674274989116
[2m[36m(func pid=163416)[0m f1_per_class: [0.152, 0.163, 0.17, 0.238, 0.035, 0.365, 0.184, 0.157, 0.029, 0.074]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=169716)[0m top1: 0.3031716417910448
[2m[36m(func pid=169716)[0m top5: 0.8218283582089553
[2m[36m(func pid=169716)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=169716)[0m f1_macro: 0.27812724206519346
[2m[36m(func pid=169716)[0m f1_weighted: 0.3215991347252025
[2m[36m(func pid=169716)[0m f1_per_class: [0.274, 0.229, 0.4, 0.41, 0.053, 0.363, 0.31, 0.307, 0.117, 0.318]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m top1: 0.33348880597014924
[2m[36m(func pid=171628)[0m top5: 0.855410447761194
[2m[36m(func pid=171628)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=171628)[0m f1_macro: 0.31586920512813677
[2m[36m(func pid=171628)[0m f1_weighted: 0.3648622974144322
[2m[36m(func pid=171628)[0m f1_per_class: [0.287, 0.273, 0.71, 0.407, 0.078, 0.308, 0.448, 0.283, 0.2, 0.166]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.1491 | Steps: 2 | Val loss: 3.5960 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.6657 | Steps: 2 | Val loss: 2.2450 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.5823 | Steps: 2 | Val loss: 1.9710 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2890 | Steps: 2 | Val loss: 2.0854 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 02:00:51 (running for 00:27:11.82)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.661 |      0.157 |                   67 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.685 |      0.278 |                   40 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.256 |      0.316 |                   35 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.149 |      0.295 |                   21 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174617)[0m top1: 0.33348880597014924
[2m[36m(func pid=174617)[0m top5: 0.8628731343283582
[2m[36m(func pid=174617)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=174617)[0m f1_macro: 0.2954164394011474
[2m[36m(func pid=174617)[0m f1_weighted: 0.3575641106052185
[2m[36m(func pid=174617)[0m f1_per_class: [0.331, 0.264, 0.5, 0.429, 0.122, 0.345, 0.412, 0.192, 0.197, 0.163]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=163416)[0m top1: 0.19542910447761194
[2m[36m(func pid=163416)[0m top5: 0.659981343283582
[2m[36m(func pid=163416)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=163416)[0m f1_macro: 0.1582305966873777
[2m[36m(func pid=163416)[0m f1_weighted: 0.20852363571801374
[2m[36m(func pid=163416)[0m f1_per_class: [0.154, 0.161, 0.167, 0.247, 0.035, 0.362, 0.183, 0.166, 0.029, 0.078]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=169716)[0m top1: 0.3045708955223881
[2m[36m(func pid=169716)[0m top5: 0.8250932835820896
[2m[36m(func pid=169716)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=169716)[0m f1_macro: 0.2826516154041717
[2m[36m(func pid=169716)[0m f1_weighted: 0.3236272972045429
[2m[36m(func pid=169716)[0m f1_per_class: [0.29, 0.236, 0.415, 0.41, 0.051, 0.377, 0.307, 0.307, 0.119, 0.315]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m top1: 0.33675373134328357
[2m[36m(func pid=171628)[0m top5: 0.8568097014925373
[2m[36m(func pid=171628)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=171628)[0m f1_macro: 0.32132721045514256
[2m[36m(func pid=171628)[0m f1_weighted: 0.3673551679852661
[2m[36m(func pid=171628)[0m f1_per_class: [0.287, 0.273, 0.733, 0.398, 0.08, 0.328, 0.457, 0.281, 0.2, 0.177]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0083 | Steps: 2 | Val loss: 3.6155 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.6267 | Steps: 2 | Val loss: 2.2461 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.6552 | Steps: 2 | Val loss: 1.9647 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2745 | Steps: 2 | Val loss: 2.0973 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 02:00:56 (running for 00:27:17.45)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.627 |      0.157 |                   69 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.582 |      0.283 |                   41 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.289 |      0.321 |                   36 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.149 |      0.295 |                   21 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163416)[0m top1: 0.19309701492537312
[2m[36m(func pid=163416)[0m top5: 0.6543843283582089
[2m[36m(func pid=163416)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=163416)[0m f1_macro: 0.1574132310111887
[2m[36m(func pid=163416)[0m f1_weighted: 0.20694804024984764
[2m[36m(func pid=163416)[0m f1_per_class: [0.152, 0.158, 0.161, 0.239, 0.041, 0.366, 0.187, 0.163, 0.029, 0.079]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=174617)[0m top1: 0.33302238805970147
[2m[36m(func pid=174617)[0m top5: 0.8614738805970149
[2m[36m(func pid=174617)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=174617)[0m f1_macro: 0.2908095386671789
[2m[36m(func pid=174617)[0m f1_weighted: 0.3588314666438795
[2m[36m(func pid=174617)[0m f1_per_class: [0.301, 0.269, 0.49, 0.4, 0.123, 0.344, 0.445, 0.182, 0.189, 0.165]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m top1: 0.30783582089552236
[2m[36m(func pid=169716)[0m top5: 0.8260261194029851
[2m[36m(func pid=169716)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=169716)[0m f1_macro: 0.28316510714895166
[2m[36m(func pid=169716)[0m f1_weighted: 0.3275897615422139
[2m[36m(func pid=169716)[0m f1_per_class: [0.294, 0.235, 0.407, 0.415, 0.051, 0.38, 0.316, 0.301, 0.119, 0.313]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m top1: 0.33675373134328357
[2m[36m(func pid=171628)[0m top5: 0.855410447761194
[2m[36m(func pid=171628)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=171628)[0m f1_macro: 0.31904952571441475
[2m[36m(func pid=171628)[0m f1_weighted: 0.3676762312026736
[2m[36m(func pid=171628)[0m f1_per_class: [0.282, 0.271, 0.71, 0.402, 0.081, 0.329, 0.456, 0.276, 0.205, 0.179]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.6484 | Steps: 2 | Val loss: 2.2449 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0794 | Steps: 2 | Val loss: 3.6283 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.6779 | Steps: 2 | Val loss: 1.9565 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2351 | Steps: 2 | Val loss: 2.1164 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 02:01:02 (running for 00:27:22.87)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.648 |      0.157 |                   70 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.655 |      0.283 |                   42 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.274 |      0.319 |                   37 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.008 |      0.291 |                   22 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163416)[0m top1: 0.1921641791044776
[2m[36m(func pid=163416)[0m top5: 0.6567164179104478
[2m[36m(func pid=163416)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=163416)[0m f1_macro: 0.15669993785736303
[2m[36m(func pid=163416)[0m f1_weighted: 0.20639567500313472
[2m[36m(func pid=163416)[0m f1_per_class: [0.148, 0.154, 0.167, 0.245, 0.04, 0.359, 0.183, 0.167, 0.029, 0.073]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=174617)[0m top1: 0.33722014925373134
[2m[36m(func pid=174617)[0m top5: 0.8530783582089553
[2m[36m(func pid=174617)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=174617)[0m f1_macro: 0.29288835812170533
[2m[36m(func pid=174617)[0m f1_weighted: 0.36191558612668057
[2m[36m(func pid=174617)[0m f1_per_class: [0.298, 0.267, 0.522, 0.378, 0.126, 0.357, 0.478, 0.17, 0.171, 0.164]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m top1: 0.31203358208955223
[2m[36m(func pid=169716)[0m top5: 0.8292910447761194
[2m[36m(func pid=169716)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=169716)[0m f1_macro: 0.2910769886516107
[2m[36m(func pid=169716)[0m f1_weighted: 0.3306666926951419
[2m[36m(func pid=169716)[0m f1_per_class: [0.31, 0.235, 0.431, 0.416, 0.058, 0.384, 0.319, 0.31, 0.121, 0.326]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m top1: 0.33255597014925375
[2m[36m(func pid=171628)[0m top5: 0.8530783582089553
[2m[36m(func pid=171628)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=171628)[0m f1_macro: 0.317216012292466
[2m[36m(func pid=171628)[0m f1_weighted: 0.36306901258145935
[2m[36m(func pid=171628)[0m f1_per_class: [0.274, 0.268, 0.71, 0.394, 0.08, 0.337, 0.448, 0.273, 0.201, 0.188]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.6602 | Steps: 2 | Val loss: 2.2437 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0161 | Steps: 2 | Val loss: 3.6446 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.6157 | Steps: 2 | Val loss: 1.9532 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2395 | Steps: 2 | Val loss: 2.1234 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=163416)[0m top1: 0.19309701492537312
[2m[36m(func pid=163416)[0m top5: 0.6553171641791045
[2m[36m(func pid=163416)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=163416)[0m f1_macro: 0.15725316865101144
[2m[36m(func pid=163416)[0m f1_weighted: 0.2077357354406307
[2m[36m(func pid=163416)[0m f1_per_class: [0.148, 0.149, 0.171, 0.249, 0.034, 0.361, 0.187, 0.167, 0.029, 0.078]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=174617)[0m top1: 0.33908582089552236
[2m[36m(func pid=174617)[0m top5: 0.8465485074626866
[2m[36m(func pid=174617)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=174617)[0m f1_macro: 0.297540939601299
[2m[36m(func pid=174617)[0m f1_weighted: 0.3653154189729804
[2m[36m(func pid=174617)[0m f1_per_class: [0.275, 0.249, 0.585, 0.402, 0.125, 0.348, 0.479, 0.169, 0.186, 0.157]
== Status ==
Current time: 2024-01-07 02:01:07 (running for 00:27:28.31)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.66  |      0.157 |                   71 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.678 |      0.291 |                   43 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.235 |      0.317 |                   38 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.079 |      0.293 |                   23 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m top1: 0.310634328358209
[2m[36m(func pid=169716)[0m top5: 0.8288246268656716
[2m[36m(func pid=169716)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=169716)[0m f1_macro: 0.2893563907894373
[2m[36m(func pid=169716)[0m f1_weighted: 0.33030808375938503
[2m[36m(func pid=169716)[0m f1_per_class: [0.312, 0.24, 0.415, 0.412, 0.056, 0.382, 0.319, 0.315, 0.121, 0.321]
[2m[36m(func pid=171628)[0m top1: 0.3362873134328358
[2m[36m(func pid=171628)[0m top5: 0.8502798507462687
[2m[36m(func pid=171628)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=171628)[0m f1_macro: 0.3209631327740723
[2m[36m(func pid=171628)[0m f1_weighted: 0.36767019392041506
[2m[36m(func pid=171628)[0m f1_per_class: [0.274, 0.266, 0.733, 0.396, 0.082, 0.348, 0.459, 0.267, 0.201, 0.183]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0074 | Steps: 2 | Val loss: 3.6403 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.6559 | Steps: 2 | Val loss: 2.2413 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2158 | Steps: 2 | Val loss: 2.1036 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.5726 | Steps: 2 | Val loss: 1.9504 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 02:01:12 (running for 00:27:33.67)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.66  |      0.157 |                   71 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.616 |      0.289 |                   44 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.24  |      0.321 |                   39 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.007 |      0.296 |                   25 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174617)[0m top1: 0.33675373134328357
[2m[36m(func pid=174617)[0m top5: 0.8465485074626866
[2m[36m(func pid=174617)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=174617)[0m f1_macro: 0.29629398289796577
[2m[36m(func pid=174617)[0m f1_weighted: 0.3622568856703708
[2m[36m(func pid=174617)[0m f1_per_class: [0.275, 0.216, 0.6, 0.429, 0.124, 0.344, 0.464, 0.171, 0.183, 0.158]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=163416)[0m top1: 0.19542910447761194
[2m[36m(func pid=163416)[0m top5: 0.6581156716417911
[2m[36m(func pid=163416)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=163416)[0m f1_macro: 0.16158865392566296
[2m[36m(func pid=163416)[0m f1_weighted: 0.21029965208112786
[2m[36m(func pid=163416)[0m f1_per_class: [0.15, 0.155, 0.174, 0.247, 0.034, 0.366, 0.189, 0.169, 0.03, 0.101]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=171628)[0m top1: 0.345615671641791
[2m[36m(func pid=171628)[0m top5: 0.8577425373134329
[2m[36m(func pid=171628)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=171628)[0m f1_macro: 0.3230863377892428
[2m[36m(func pid=171628)[0m f1_weighted: 0.37561067297300416
[2m[36m(func pid=171628)[0m f1_per_class: [0.279, 0.268, 0.733, 0.398, 0.085, 0.356, 0.483, 0.244, 0.197, 0.186]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m top1: 0.3101679104477612
[2m[36m(func pid=169716)[0m top5: 0.8316231343283582
[2m[36m(func pid=169716)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=169716)[0m f1_macro: 0.2913765360568698
[2m[36m(func pid=169716)[0m f1_weighted: 0.32994120245849845
[2m[36m(func pid=169716)[0m f1_per_class: [0.324, 0.237, 0.431, 0.41, 0.054, 0.385, 0.319, 0.317, 0.119, 0.317]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.1163 | Steps: 2 | Val loss: 3.6773 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.6260 | Steps: 2 | Val loss: 2.2387 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2179 | Steps: 2 | Val loss: 2.1078 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.5898 | Steps: 2 | Val loss: 1.9459 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 02:01:18 (running for 00:27:38.98)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.656 |      0.162 |                   72 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.573 |      0.291 |                   45 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.216 |      0.323 |                   40 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.116 |      0.3   |                   26 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174617)[0m top1: 0.3423507462686567
[2m[36m(func pid=174617)[0m top5: 0.8423507462686567
[2m[36m(func pid=174617)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=174617)[0m f1_macro: 0.2995313584694096
[2m[36m(func pid=174617)[0m f1_weighted: 0.36958671058781895
[2m[36m(func pid=174617)[0m f1_per_class: [0.279, 0.211, 0.595, 0.443, 0.119, 0.346, 0.474, 0.18, 0.193, 0.154]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=163416)[0m top1: 0.20149253731343283
[2m[36m(func pid=163416)[0m top5: 0.664179104477612
[2m[36m(func pid=163416)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=163416)[0m f1_macro: 0.16545379584653785
[2m[36m(func pid=163416)[0m f1_weighted: 0.21776261476506625
[2m[36m(func pid=163416)[0m f1_per_class: [0.146, 0.159, 0.185, 0.258, 0.035, 0.36, 0.203, 0.171, 0.04, 0.098]
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=171628)[0m top1: 0.3460820895522388
[2m[36m(func pid=171628)[0m top5: 0.8568097014925373
[2m[36m(func pid=171628)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=171628)[0m f1_macro: 0.32187938811290084
[2m[36m(func pid=171628)[0m f1_weighted: 0.37632171343977483
[2m[36m(func pid=171628)[0m f1_per_class: [0.276, 0.27, 0.733, 0.407, 0.086, 0.359, 0.479, 0.231, 0.2, 0.178]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m top1: 0.310634328358209
[2m[36m(func pid=169716)[0m top5: 0.8325559701492538
[2m[36m(func pid=169716)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=169716)[0m f1_macro: 0.29088844057975255
[2m[36m(func pid=169716)[0m f1_weighted: 0.3293586285794808
[2m[36m(func pid=169716)[0m f1_per_class: [0.321, 0.235, 0.423, 0.409, 0.055, 0.399, 0.315, 0.315, 0.119, 0.319]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0375 | Steps: 2 | Val loss: 3.8256 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.6189 | Steps: 2 | Val loss: 2.2352 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2089 | Steps: 2 | Val loss: 2.1185 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.5924 | Steps: 2 | Val loss: 1.9348 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 02:01:23 (running for 00:27:44.55)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.626 |      0.165 |                   73 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.59  |      0.291 |                   46 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.218 |      0.322 |                   41 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.037 |      0.293 |                   27 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163416)[0m top1: 0.1958955223880597
[2m[36m(func pid=163416)[0m top5: 0.6707089552238806
[2m[36m(func pid=163416)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=163416)[0m f1_macro: 0.16142973144950667
[2m[36m(func pid=163416)[0m f1_weighted: 0.21054905034661778
[2m[36m(func pid=163416)[0m f1_per_class: [0.143, 0.155, 0.183, 0.244, 0.035, 0.361, 0.195, 0.173, 0.03, 0.094]
[2m[36m(func pid=174617)[0m top1: 0.3208955223880597
[2m[36m(func pid=174617)[0m top5: 0.8348880597014925
[2m[36m(func pid=174617)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=174617)[0m f1_macro: 0.29295084629911156
[2m[36m(func pid=174617)[0m f1_weighted: 0.3537277703344267
[2m[36m(func pid=174617)[0m f1_per_class: [0.278, 0.211, 0.629, 0.409, 0.09, 0.311, 0.464, 0.193, 0.196, 0.15]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=163416)[0m 
[2m[36m(func pid=171628)[0m top1: 0.3451492537313433
[2m[36m(func pid=171628)[0m top5: 0.8596082089552238
[2m[36m(func pid=171628)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=171628)[0m f1_macro: 0.32141377449591113
[2m[36m(func pid=171628)[0m f1_weighted: 0.37585734416800715
[2m[36m(func pid=171628)[0m f1_per_class: [0.274, 0.269, 0.733, 0.41, 0.085, 0.365, 0.472, 0.231, 0.2, 0.174]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m top1: 0.31763059701492535
[2m[36m(func pid=169716)[0m top5: 0.8344216417910447
[2m[36m(func pid=169716)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=169716)[0m f1_macro: 0.2968447813735361
[2m[36m(func pid=169716)[0m f1_weighted: 0.3358499540982946
[2m[36m(func pid=169716)[0m f1_per_class: [0.333, 0.24, 0.423, 0.417, 0.058, 0.407, 0.319, 0.33, 0.115, 0.326]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0328 | Steps: 2 | Val loss: 3.9627 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=163416)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.6361 | Steps: 2 | Val loss: 2.2357 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3061 | Steps: 2 | Val loss: 2.1153 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.5713 | Steps: 2 | Val loss: 1.9289 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 02:01:29 (running for 00:27:49.96)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00012 | RUNNING    | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.619 |      0.161 |                   74 |
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.592 |      0.297 |                   47 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.209 |      0.321 |                   42 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.033 |      0.286 |                   28 |
| train_66d79_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=163416)[0m top1: 0.19542910447761194
[2m[36m(func pid=163416)[0m top5: 0.6716417910447762
[2m[36m(func pid=163416)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=163416)[0m f1_macro: 0.16016361376143196
[2m[36m(func pid=163416)[0m f1_weighted: 0.2099051919705303
[2m[36m(func pid=163416)[0m f1_per_class: [0.136, 0.168, 0.175, 0.234, 0.042, 0.362, 0.196, 0.164, 0.032, 0.092]
[2m[36m(func pid=174617)[0m top1: 0.30550373134328357
[2m[36m(func pid=174617)[0m top5: 0.8292910447761194
[2m[36m(func pid=174617)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=174617)[0m f1_macro: 0.28586825670438465
[2m[36m(func pid=174617)[0m f1_weighted: 0.3395782771970909
[2m[36m(func pid=174617)[0m f1_per_class: [0.271, 0.217, 0.629, 0.377, 0.087, 0.278, 0.456, 0.195, 0.192, 0.158]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m top1: 0.34328358208955223
[2m[36m(func pid=171628)[0m top5: 0.8628731343283582
[2m[36m(func pid=171628)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=171628)[0m f1_macro: 0.32278352766407636
[2m[36m(func pid=171628)[0m f1_weighted: 0.37372032476862294
[2m[36m(func pid=171628)[0m f1_per_class: [0.288, 0.269, 0.733, 0.423, 0.085, 0.358, 0.451, 0.25, 0.201, 0.169]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m top1: 0.3185634328358209
[2m[36m(func pid=169716)[0m top5: 0.8339552238805971
[2m[36m(func pid=169716)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=169716)[0m f1_macro: 0.29907454769902786
[2m[36m(func pid=169716)[0m f1_weighted: 0.3376476680567059
[2m[36m(func pid=169716)[0m f1_per_class: [0.352, 0.244, 0.415, 0.411, 0.057, 0.402, 0.328, 0.328, 0.132, 0.321]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0103 | Steps: 2 | Val loss: 4.0576 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.1802 | Steps: 2 | Val loss: 2.1161 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.5322 | Steps: 2 | Val loss: 1.9261 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=171628)[0m top1: 0.34095149253731344
[2m[36m(func pid=171628)[0m top5: 0.8624067164179104
[2m[36m(func pid=171628)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=171628)[0m f1_macro: 0.3233346213343285
[2m[36m(func pid=171628)[0m f1_weighted: 0.3703389754191359
[2m[36m(func pid=171628)[0m f1_per_class: [0.295, 0.269, 0.733, 0.423, 0.083, 0.361, 0.437, 0.25, 0.211, 0.171]
[2m[36m(func pid=174617)[0m top1: 0.302705223880597
[2m[36m(func pid=174617)[0m top5: 0.832089552238806
[2m[36m(func pid=174617)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=174617)[0m f1_macro: 0.2884370930992951
[2m[36m(func pid=174617)[0m f1_weighted: 0.33696535813680173
[2m[36m(func pid=174617)[0m f1_per_class: [0.287, 0.24, 0.667, 0.351, 0.086, 0.243, 0.47, 0.197, 0.192, 0.153]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m top1: 0.3162313432835821
[2m[36m(func pid=169716)[0m top5: 0.8367537313432836
[2m[36m(func pid=169716)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=169716)[0m f1_macro: 0.29868028078190584
[2m[36m(func pid=169716)[0m f1_weighted: 0.3344393104239822
[2m[36m(func pid=169716)[0m f1_per_class: [0.347, 0.247, 0.415, 0.412, 0.052, 0.408, 0.312, 0.322, 0.143, 0.328]
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0162 | Steps: 2 | Val loss: 4.0536 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 02:01:34 (running for 00:27:55.54)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.571 |      0.299 |                   48 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.306 |      0.323 |                   43 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.01  |      0.288 |                   29 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=181342)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=181342)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=181342)[0m Configuration completed!
[2m[36m(func pid=181342)[0m New optimizer parameters:
[2m[36m(func pid=181342)[0m SGD (
[2m[36m(func pid=181342)[0m Parameter Group 0
[2m[36m(func pid=181342)[0m     dampening: 0
[2m[36m(func pid=181342)[0m     differentiable: False
[2m[36m(func pid=181342)[0m     foreach: None
[2m[36m(func pid=181342)[0m     lr: 0.0001
[2m[36m(func pid=181342)[0m     maximize: False
[2m[36m(func pid=181342)[0m     momentum: 0.99
[2m[36m(func pid=181342)[0m     nesterov: False
[2m[36m(func pid=181342)[0m     weight_decay: 1e-05
[2m[36m(func pid=181342)[0m )
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m top1: 0.30177238805970147
[2m[36m(func pid=174617)[0m top5: 0.8404850746268657
[2m[36m(func pid=174617)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=174617)[0m f1_macro: 0.29408956807479847
[2m[36m(func pid=174617)[0m f1_weighted: 0.3347674062820577
[2m[36m(func pid=174617)[0m f1_per_class: [0.302, 0.246, 0.71, 0.346, 0.08, 0.232, 0.462, 0.219, 0.182, 0.162]
[2m[36m(func pid=174617)[0m 
== Status ==
Current time: 2024-01-07 02:01:40 (running for 00:28:00.78)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.532 |      0.299 |                   49 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.18  |      0.323 |                   44 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.016 |      0.294 |                   30 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.4425 | Steps: 2 | Val loss: 1.9235 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2010 | Steps: 2 | Val loss: 2.1237 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9796 | Steps: 2 | Val loss: 2.3205 | Batch size: 32 | lr: 0.0001 | Duration: 4.80s
[2m[36m(func pid=171628)[0m top1: 0.3381529850746269
[2m[36m(func pid=171628)[0m top5: 0.8656716417910447
[2m[36m(func pid=171628)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=171628)[0m f1_macro: 0.32237402262842874
[2m[36m(func pid=171628)[0m f1_weighted: 0.3671272612606599
[2m[36m(func pid=171628)[0m f1_per_class: [0.295, 0.271, 0.733, 0.421, 0.081, 0.361, 0.426, 0.257, 0.211, 0.168]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m top1: 0.31763059701492535
[2m[36m(func pid=169716)[0m top5: 0.8372201492537313
[2m[36m(func pid=169716)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=169716)[0m f1_macro: 0.29895966919634825
[2m[36m(func pid=169716)[0m f1_weighted: 0.3366967895217892
[2m[36m(func pid=169716)[0m f1_per_class: [0.338, 0.253, 0.415, 0.406, 0.058, 0.413, 0.32, 0.328, 0.143, 0.314]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2019 | Steps: 2 | Val loss: 3.9484 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 02:01:45 (running for 00:28:05.78)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.442 |      0.299 |                   50 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.201 |      0.322 |                   45 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.016 |      0.294 |                   30 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.98  |      0.119 |                    1 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.17583955223880596
[2m[36m(func pid=181342)[0m top5: 0.5326492537313433
[2m[36m(func pid=181342)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=181342)[0m f1_macro: 0.11894548873142137
[2m[36m(func pid=181342)[0m f1_weighted: 0.12476186533850354
[2m[36m(func pid=181342)[0m f1_per_class: [0.317, 0.349, 0.0, 0.091, 0.0, 0.211, 0.021, 0.0, 0.0, 0.2]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2542 | Steps: 2 | Val loss: 2.1308 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=174617)[0m top1: 0.30550373134328357
[2m[36m(func pid=174617)[0m top5: 0.8367537313432836
[2m[36m(func pid=174617)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=174617)[0m f1_macro: 0.29985590555763963
[2m[36m(func pid=174617)[0m f1_weighted: 0.33844379514753975
[2m[36m(func pid=174617)[0m f1_per_class: [0.265, 0.247, 0.75, 0.35, 0.082, 0.251, 0.462, 0.218, 0.208, 0.166]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.5798 | Steps: 2 | Val loss: 1.9188 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9733 | Steps: 2 | Val loss: 2.3235 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=169716)[0m top1: 0.31529850746268656
[2m[36m(func pid=169716)[0m top5: 0.8330223880597015
[2m[36m(func pid=169716)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=169716)[0m f1_macro: 0.301475063447158
[2m[36m(func pid=169716)[0m f1_weighted: 0.33324102766278946
[2m[36m(func pid=169716)[0m f1_per_class: [0.368, 0.243, 0.431, 0.406, 0.051, 0.405, 0.315, 0.335, 0.136, 0.326]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m top1: 0.3362873134328358
[2m[36m(func pid=171628)[0m top5: 0.8638059701492538
[2m[36m(func pid=171628)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=171628)[0m f1_macro: 0.3220055665452117
[2m[36m(func pid=171628)[0m f1_weighted: 0.36361016786711753
[2m[36m(func pid=171628)[0m f1_per_class: [0.303, 0.279, 0.733, 0.418, 0.08, 0.359, 0.414, 0.249, 0.211, 0.174]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0135 | Steps: 2 | Val loss: 3.8693 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 02:01:50 (running for 00:28:11.35)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.58  |      0.301 |                   51 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.254 |      0.322 |                   46 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.202 |      0.3   |                   31 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.973 |      0.107 |                    2 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.1828358208955224
[2m[36m(func pid=181342)[0m top5: 0.5293843283582089
[2m[36m(func pid=181342)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=181342)[0m f1_macro: 0.10736307600350817
[2m[36m(func pid=181342)[0m f1_weighted: 0.1278782857475408
[2m[36m(func pid=181342)[0m f1_per_class: [0.192, 0.339, 0.0, 0.094, 0.01, 0.28, 0.015, 0.024, 0.0, 0.12]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.1755 | Steps: 2 | Val loss: 2.1559 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.4752 | Steps: 2 | Val loss: 1.9147 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=174617)[0m top1: 0.31529850746268656
[2m[36m(func pid=174617)[0m top5: 0.8334888059701493
[2m[36m(func pid=174617)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=174617)[0m f1_macro: 0.295124611211034
[2m[36m(func pid=174617)[0m f1_weighted: 0.3508444568335527
[2m[36m(func pid=174617)[0m f1_per_class: [0.224, 0.22, 0.688, 0.401, 0.079, 0.275, 0.464, 0.228, 0.213, 0.16]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m top1: 0.3362873134328358
[2m[36m(func pid=171628)[0m top5: 0.8582089552238806
[2m[36m(func pid=171628)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=171628)[0m f1_macro: 0.3252905106701492
[2m[36m(func pid=171628)[0m f1_weighted: 0.36438321496657144
[2m[36m(func pid=171628)[0m f1_per_class: [0.315, 0.275, 0.733, 0.419, 0.08, 0.359, 0.41, 0.286, 0.211, 0.165]
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9837 | Steps: 2 | Val loss: 2.3286 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m top1: 0.31902985074626866
[2m[36m(func pid=169716)[0m top5: 0.835820895522388
[2m[36m(func pid=169716)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=169716)[0m f1_macro: 0.30053589452631446
[2m[36m(func pid=169716)[0m f1_weighted: 0.3376352584146909
[2m[36m(func pid=169716)[0m f1_per_class: [0.358, 0.254, 0.423, 0.409, 0.053, 0.401, 0.323, 0.324, 0.143, 0.317]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0114 | Steps: 2 | Val loss: 3.8495 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 02:01:55 (running for 00:28:16.71)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.475 |      0.301 |                   52 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.175 |      0.325 |                   47 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.014 |      0.295 |                   32 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.984 |      0.096 |                    3 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.17583955223880596
[2m[36m(func pid=181342)[0m top5: 0.5209888059701493
[2m[36m(func pid=181342)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=181342)[0m f1_macro: 0.09637668181803848
[2m[36m(func pid=181342)[0m f1_weighted: 0.12472464082851457
[2m[36m(func pid=181342)[0m f1_per_class: [0.135, 0.307, 0.0, 0.105, 0.009, 0.303, 0.012, 0.011, 0.0, 0.082]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.1788 | Steps: 2 | Val loss: 2.1761 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.3916 | Steps: 2 | Val loss: 1.9099 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=174617)[0m top1: 0.31949626865671643
[2m[36m(func pid=174617)[0m top5: 0.8264925373134329
[2m[36m(func pid=174617)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=174617)[0m f1_macro: 0.29722233548478216
[2m[36m(func pid=174617)[0m f1_weighted: 0.35355450474129024
[2m[36m(func pid=174617)[0m f1_per_class: [0.201, 0.195, 0.688, 0.429, 0.094, 0.3, 0.45, 0.24, 0.218, 0.157]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m top1: 0.3292910447761194
[2m[36m(func pid=171628)[0m top5: 0.8582089552238806
[2m[36m(func pid=171628)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=171628)[0m f1_macro: 0.31873635491618213
[2m[36m(func pid=171628)[0m f1_weighted: 0.3578279053590905
[2m[36m(func pid=171628)[0m f1_per_class: [0.312, 0.257, 0.71, 0.423, 0.075, 0.355, 0.397, 0.287, 0.209, 0.162]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9989 | Steps: 2 | Val loss: 2.3346 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=169716)[0m top1: 0.32369402985074625
[2m[36m(func pid=169716)[0m top5: 0.8372201492537313
[2m[36m(func pid=169716)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=169716)[0m f1_macro: 0.3020963574183228
[2m[36m(func pid=169716)[0m f1_weighted: 0.3420716297899683
[2m[36m(func pid=169716)[0m f1_per_class: [0.363, 0.257, 0.423, 0.413, 0.055, 0.412, 0.33, 0.32, 0.142, 0.307]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0121 | Steps: 2 | Val loss: 3.8375 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=181342)[0m top1: 0.17024253731343283
[2m[36m(func pid=181342)[0m top5: 0.5093283582089553
[2m[36m(func pid=181342)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=181342)[0m f1_macro: 0.09253492813593558
[2m[36m(func pid=181342)[0m f1_weighted: 0.12310142360806536
[2m[36m(func pid=181342)[0m f1_per_class: [0.114, 0.293, 0.0, 0.11, 0.009, 0.299, 0.012, 0.019, 0.0, 0.069]
== Status ==
Current time: 2024-01-07 02:02:01 (running for 00:28:21.89)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.392 |      0.302 |                   53 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.179 |      0.319 |                   48 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.011 |      0.297 |                   33 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.999 |      0.093 |                    4 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.1554 | Steps: 2 | Val loss: 2.2009 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.3884 | Steps: 2 | Val loss: 1.9060 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=174617)[0m top1: 0.31296641791044777
[2m[36m(func pid=174617)[0m top5: 0.8218283582089553
[2m[36m(func pid=174617)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=174617)[0m f1_macro: 0.29171185210664835
[2m[36m(func pid=174617)[0m f1_weighted: 0.3425504287840974
[2m[36m(func pid=174617)[0m f1_per_class: [0.178, 0.149, 0.688, 0.431, 0.115, 0.305, 0.437, 0.24, 0.215, 0.159]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m top1: 0.32882462686567165
[2m[36m(func pid=171628)[0m top5: 0.8521455223880597
[2m[36m(func pid=171628)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=171628)[0m f1_macro: 0.31762997240112656
[2m[36m(func pid=171628)[0m f1_weighted: 0.35776783458161354
[2m[36m(func pid=171628)[0m f1_per_class: [0.302, 0.256, 0.71, 0.427, 0.075, 0.355, 0.396, 0.282, 0.205, 0.169]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9470 | Steps: 2 | Val loss: 2.3393 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=169716)[0m top1: 0.322294776119403
[2m[36m(func pid=169716)[0m top5: 0.8418843283582089
[2m[36m(func pid=169716)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=169716)[0m f1_macro: 0.29585310447880225
[2m[36m(func pid=169716)[0m f1_weighted: 0.34090454216243277
[2m[36m(func pid=169716)[0m f1_per_class: [0.345, 0.246, 0.423, 0.423, 0.057, 0.388, 0.338, 0.297, 0.145, 0.297]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0569 | Steps: 2 | Val loss: 3.7950 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 02:02:06 (running for 00:28:27.26)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.388 |      0.296 |                   54 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.155 |      0.318 |                   49 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.012 |      0.292 |                   34 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.947 |      0.087 |                    5 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.1623134328358209
[2m[36m(func pid=181342)[0m top5: 0.5
[2m[36m(func pid=181342)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=181342)[0m f1_macro: 0.08716408626710861
[2m[36m(func pid=181342)[0m f1_weighted: 0.12240782083387546
[2m[36m(func pid=181342)[0m f1_per_class: [0.111, 0.268, 0.0, 0.116, 0.009, 0.299, 0.021, 0.018, 0.0, 0.031]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2351 | Steps: 2 | Val loss: 2.2254 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.4344 | Steps: 2 | Val loss: 1.8981 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=174617)[0m top1: 0.31716417910447764
[2m[36m(func pid=174617)[0m top5: 0.8255597014925373
[2m[36m(func pid=174617)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=174617)[0m f1_macro: 0.29343829824824574
[2m[36m(func pid=174617)[0m f1_weighted: 0.34595641924020826
[2m[36m(func pid=174617)[0m f1_per_class: [0.176, 0.153, 0.706, 0.437, 0.128, 0.314, 0.443, 0.215, 0.202, 0.16]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m top1: 0.31902985074626866
[2m[36m(func pid=171628)[0m top5: 0.8456156716417911
[2m[36m(func pid=171628)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=171628)[0m f1_macro: 0.3155205752346338
[2m[36m(func pid=171628)[0m f1_weighted: 0.3469439075345799
[2m[36m(func pid=171628)[0m f1_per_class: [0.308, 0.254, 0.733, 0.406, 0.072, 0.344, 0.384, 0.274, 0.205, 0.173]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9616 | Steps: 2 | Val loss: 2.3387 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=169716)[0m top1: 0.3246268656716418
[2m[36m(func pid=169716)[0m top5: 0.84375
[2m[36m(func pid=169716)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=169716)[0m f1_macro: 0.2980671227535135
[2m[36m(func pid=169716)[0m f1_weighted: 0.3427457820560376
[2m[36m(func pid=169716)[0m f1_per_class: [0.335, 0.253, 0.431, 0.42, 0.053, 0.391, 0.339, 0.313, 0.15, 0.297]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0562 | Steps: 2 | Val loss: 3.6573 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=181342)[0m top1: 0.15671641791044777
[2m[36m(func pid=181342)[0m top5: 0.49953358208955223
[2m[36m(func pid=181342)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=181342)[0m f1_macro: 0.08658818816104638
[2m[36m(func pid=181342)[0m f1_weighted: 0.12238196085001328
[2m[36m(func pid=181342)[0m f1_per_class: [0.098, 0.252, 0.0, 0.114, 0.016, 0.303, 0.029, 0.026, 0.0, 0.028]
[2m[36m(func pid=181342)[0m 
== Status ==
Current time: 2024-01-07 02:02:12 (running for 00:28:32.71)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.434 |      0.298 |                   55 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.235 |      0.316 |                   50 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.057 |      0.293 |                   35 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.962 |      0.087 |                    6 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2385 | Steps: 2 | Val loss: 2.2379 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.3290 | Steps: 2 | Val loss: 1.8901 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=174617)[0m top1: 0.32742537313432835
[2m[36m(func pid=174617)[0m top5: 0.8418843283582089
[2m[36m(func pid=174617)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=174617)[0m f1_macro: 0.2977851555812263
[2m[36m(func pid=174617)[0m f1_weighted: 0.3601600997523776
[2m[36m(func pid=174617)[0m f1_per_class: [0.179, 0.215, 0.71, 0.439, 0.114, 0.329, 0.453, 0.195, 0.184, 0.16]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m top1: 0.3204291044776119
[2m[36m(func pid=171628)[0m top5: 0.8484141791044776
[2m[36m(func pid=171628)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=171628)[0m f1_macro: 0.31708989765256695
[2m[36m(func pid=171628)[0m f1_weighted: 0.3486094334294197
[2m[36m(func pid=171628)[0m f1_per_class: [0.321, 0.26, 0.71, 0.401, 0.07, 0.349, 0.386, 0.289, 0.213, 0.173]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9377 | Steps: 2 | Val loss: 2.3376 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=169716)[0m top1: 0.32509328358208955
[2m[36m(func pid=169716)[0m top5: 0.8442164179104478
[2m[36m(func pid=169716)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=169716)[0m f1_macro: 0.2986139386864391
[2m[36m(func pid=169716)[0m f1_weighted: 0.34300096739373576
[2m[36m(func pid=169716)[0m f1_per_class: [0.338, 0.252, 0.44, 0.424, 0.053, 0.381, 0.341, 0.311, 0.15, 0.297]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0100 | Steps: 2 | Val loss: 3.5815 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.1755 | Steps: 2 | Val loss: 2.2330 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 02:02:17 (running for 00:28:38.01)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.329 |      0.299 |                   56 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.239 |      0.317 |                   51 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.056 |      0.298 |                   36 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.938 |      0.085 |                    7 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.1501865671641791
[2m[36m(func pid=181342)[0m top5: 0.511660447761194
[2m[36m(func pid=181342)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=181342)[0m f1_macro: 0.08478479952884106
[2m[36m(func pid=181342)[0m f1_weighted: 0.12451579797237573
[2m[36m(func pid=181342)[0m f1_per_class: [0.09, 0.223, 0.0, 0.118, 0.023, 0.297, 0.048, 0.048, 0.0, 0.0]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.4013 | Steps: 2 | Val loss: 1.8926 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=174617)[0m top1: 0.32649253731343286
[2m[36m(func pid=174617)[0m top5: 0.855410447761194
[2m[36m(func pid=174617)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=174617)[0m f1_macro: 0.299217793220295
[2m[36m(func pid=174617)[0m f1_weighted: 0.36007500526587505
[2m[36m(func pid=174617)[0m f1_per_class: [0.19, 0.251, 0.733, 0.424, 0.094, 0.331, 0.445, 0.198, 0.173, 0.151]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m top1: 0.32276119402985076
[2m[36m(func pid=171628)[0m top5: 0.8484141791044776
[2m[36m(func pid=171628)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=171628)[0m f1_macro: 0.31497244438713756
[2m[36m(func pid=171628)[0m f1_weighted: 0.3526363142437862
[2m[36m(func pid=171628)[0m f1_per_class: [0.298, 0.254, 0.71, 0.412, 0.071, 0.353, 0.395, 0.276, 0.212, 0.169]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9590 | Steps: 2 | Val loss: 2.3384 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=169716)[0m top1: 0.32369402985074625
[2m[36m(func pid=169716)[0m top5: 0.8442164179104478
[2m[36m(func pid=169716)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=169716)[0m f1_macro: 0.2968775206078048
[2m[36m(func pid=169716)[0m f1_weighted: 0.3423143787932474
[2m[36m(func pid=169716)[0m f1_per_class: [0.318, 0.25, 0.431, 0.42, 0.053, 0.376, 0.344, 0.321, 0.151, 0.305]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0176 | Steps: 2 | Val loss: 3.5413 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2149 | Steps: 2 | Val loss: 2.2258 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 02:02:22 (running for 00:28:43.37)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.401 |      0.297 |                   57 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.175 |      0.315 |                   52 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.01  |      0.299 |                   37 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.959 |      0.087 |                    8 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.15065298507462688
[2m[36m(func pid=181342)[0m top5: 0.5167910447761194
[2m[36m(func pid=181342)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=181342)[0m f1_macro: 0.08672737235756653
[2m[36m(func pid=181342)[0m f1_weighted: 0.12993789297061537
[2m[36m(func pid=181342)[0m f1_per_class: [0.075, 0.215, 0.0, 0.129, 0.029, 0.299, 0.058, 0.062, 0.0, 0.0]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.4275 | Steps: 2 | Val loss: 1.8892 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=174617)[0m top1: 0.3358208955223881
[2m[36m(func pid=174617)[0m top5: 0.8703358208955224
[2m[36m(func pid=174617)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=174617)[0m f1_macro: 0.3087587808562022
[2m[36m(func pid=174617)[0m f1_weighted: 0.3661942240090049
[2m[36m(func pid=174617)[0m f1_per_class: [0.229, 0.298, 0.759, 0.418, 0.102, 0.339, 0.44, 0.203, 0.143, 0.157]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m top1: 0.32742537313432835
[2m[36m(func pid=171628)[0m top5: 0.8488805970149254
[2m[36m(func pid=171628)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=171628)[0m f1_macro: 0.31741624627467846
[2m[36m(func pid=171628)[0m f1_weighted: 0.3593512171522097
[2m[36m(func pid=171628)[0m f1_per_class: [0.3, 0.252, 0.71, 0.409, 0.073, 0.352, 0.421, 0.288, 0.205, 0.165]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9449 | Steps: 2 | Val loss: 2.3353 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=169716)[0m top1: 0.32369402985074625
[2m[36m(func pid=169716)[0m top5: 0.8456156716417911
[2m[36m(func pid=169716)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=169716)[0m f1_macro: 0.3002521267298063
[2m[36m(func pid=169716)[0m f1_weighted: 0.34272138502659877
[2m[36m(func pid=169716)[0m f1_per_class: [0.318, 0.247, 0.423, 0.417, 0.052, 0.38, 0.344, 0.326, 0.18, 0.315]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0120 | Steps: 2 | Val loss: 3.5423 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.1983 | Steps: 2 | Val loss: 2.2417 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 02:02:27 (running for 00:28:48.68)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.428 |      0.3   |                   58 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.215 |      0.317 |                   53 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.018 |      0.309 |                   38 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.945 |      0.087 |                    9 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.14972014925373134
[2m[36m(func pid=181342)[0m top5: 0.5163246268656716
[2m[36m(func pid=181342)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=181342)[0m f1_macro: 0.08709036847879059
[2m[36m(func pid=181342)[0m f1_weighted: 0.1344674632665248
[2m[36m(func pid=181342)[0m f1_per_class: [0.058, 0.196, 0.0, 0.143, 0.021, 0.303, 0.068, 0.082, 0.0, 0.0]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.5610 | Steps: 2 | Val loss: 1.8883 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=174617)[0m top1: 0.33908582089552236
[2m[36m(func pid=174617)[0m top5: 0.8773320895522388
[2m[36m(func pid=174617)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=174617)[0m f1_macro: 0.30913812531819324
[2m[36m(func pid=174617)[0m f1_weighted: 0.36873050537547114
[2m[36m(func pid=174617)[0m f1_per_class: [0.224, 0.314, 0.759, 0.401, 0.088, 0.346, 0.453, 0.201, 0.148, 0.156]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m top1: 0.3269589552238806
[2m[36m(func pid=171628)[0m top5: 0.8465485074626866
[2m[36m(func pid=171628)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=171628)[0m f1_macro: 0.31268796815198224
[2m[36m(func pid=171628)[0m f1_weighted: 0.35890955377586053
[2m[36m(func pid=171628)[0m f1_per_class: [0.301, 0.263, 0.667, 0.402, 0.074, 0.35, 0.423, 0.278, 0.204, 0.165]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9234 | Steps: 2 | Val loss: 2.3288 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=169716)[0m top1: 0.3204291044776119
[2m[36m(func pid=169716)[0m top5: 0.8432835820895522
[2m[36m(func pid=169716)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=169716)[0m f1_macro: 0.2968054465352173
[2m[36m(func pid=169716)[0m f1_weighted: 0.3398183413369076
[2m[36m(func pid=169716)[0m f1_per_class: [0.309, 0.254, 0.415, 0.418, 0.051, 0.374, 0.334, 0.321, 0.171, 0.322]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.1531 | Steps: 2 | Val loss: 2.2417 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0387 | Steps: 2 | Val loss: 3.5445 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 02:02:33 (running for 00:28:54.10)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.561 |      0.297 |                   59 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.198 |      0.313 |                   54 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.012 |      0.309 |                   39 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.923 |      0.094 |                   10 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.14878731343283583
[2m[36m(func pid=181342)[0m top5: 0.5200559701492538
[2m[36m(func pid=181342)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=181342)[0m f1_macro: 0.09356397341280984
[2m[36m(func pid=181342)[0m f1_weighted: 0.13792864339488686
[2m[36m(func pid=181342)[0m f1_per_class: [0.052, 0.18, 0.05, 0.155, 0.02, 0.306, 0.073, 0.089, 0.011, 0.0]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.3865 | Steps: 2 | Val loss: 1.8768 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=171628)[0m top1: 0.3283582089552239
[2m[36m(func pid=171628)[0m top5: 0.8470149253731343
[2m[36m(func pid=171628)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=171628)[0m f1_macro: 0.3112971379479582
[2m[36m(func pid=171628)[0m f1_weighted: 0.3600783816025127
[2m[36m(func pid=171628)[0m f1_per_class: [0.297, 0.267, 0.667, 0.398, 0.075, 0.347, 0.434, 0.258, 0.204, 0.168]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=174617)[0m top1: 0.33955223880597013
[2m[36m(func pid=174617)[0m top5: 0.8833955223880597
[2m[36m(func pid=174617)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=174617)[0m f1_macro: 0.3141768300719278
[2m[36m(func pid=174617)[0m f1_weighted: 0.36687994194765666
[2m[36m(func pid=174617)[0m f1_per_class: [0.257, 0.319, 0.786, 0.394, 0.088, 0.338, 0.451, 0.204, 0.144, 0.161]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9156 | Steps: 2 | Val loss: 2.3204 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=169716)[0m top1: 0.322294776119403
[2m[36m(func pid=169716)[0m top5: 0.8498134328358209
[2m[36m(func pid=169716)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=169716)[0m f1_macro: 0.30029466924718845
[2m[36m(func pid=169716)[0m f1_weighted: 0.34191968880768775
[2m[36m(func pid=169716)[0m f1_per_class: [0.318, 0.263, 0.449, 0.409, 0.051, 0.372, 0.345, 0.315, 0.175, 0.306]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.1569 | Steps: 2 | Val loss: 2.2473 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0294 | Steps: 2 | Val loss: 3.5606 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 02:02:38 (running for 00:28:59.51)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.387 |      0.3   |                   60 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.153 |      0.311 |                   55 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.039 |      0.314 |                   40 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.916 |      0.097 |                   11 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.15345149253731344
[2m[36m(func pid=181342)[0m top5: 0.5345149253731343
[2m[36m(func pid=181342)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=181342)[0m f1_macro: 0.09662642142313708
[2m[36m(func pid=181342)[0m f1_weighted: 0.14575934960296
[2m[36m(func pid=181342)[0m f1_per_class: [0.046, 0.18, 0.053, 0.164, 0.02, 0.316, 0.087, 0.09, 0.011, 0.0]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.2572 | Steps: 2 | Val loss: 1.8693 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=171628)[0m top1: 0.32742537313432835
[2m[36m(func pid=171628)[0m top5: 0.8470149253731343
[2m[36m(func pid=171628)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=171628)[0m f1_macro: 0.3085295905214688
[2m[36m(func pid=171628)[0m f1_weighted: 0.3590964900265529
[2m[36m(func pid=171628)[0m f1_per_class: [0.302, 0.27, 0.647, 0.392, 0.077, 0.338, 0.438, 0.254, 0.2, 0.167]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=174617)[0m top1: 0.3344216417910448
[2m[36m(func pid=174617)[0m top5: 0.8871268656716418
[2m[36m(func pid=174617)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=174617)[0m f1_macro: 0.3109926823731585
[2m[36m(func pid=174617)[0m f1_weighted: 0.3599577894625109
[2m[36m(func pid=174617)[0m f1_per_class: [0.269, 0.318, 0.786, 0.381, 0.092, 0.312, 0.452, 0.197, 0.129, 0.175]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9242 | Steps: 2 | Val loss: 2.3172 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=169716)[0m top1: 0.32742537313432835
[2m[36m(func pid=169716)[0m top5: 0.8484141791044776
[2m[36m(func pid=169716)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=169716)[0m f1_macro: 0.30216572498055183
[2m[36m(func pid=169716)[0m f1_weighted: 0.34616605883963897
[2m[36m(func pid=169716)[0m f1_per_class: [0.329, 0.258, 0.449, 0.422, 0.054, 0.379, 0.347, 0.312, 0.177, 0.295]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.1749 | Steps: 2 | Val loss: 2.2480 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0662 | Steps: 2 | Val loss: 3.5781 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 02:02:44 (running for 00:29:04.89)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.257 |      0.302 |                   61 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.157 |      0.309 |                   56 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.029 |      0.311 |                   41 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.924 |      0.102 |                   12 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.1515858208955224
[2m[36m(func pid=181342)[0m top5: 0.5443097014925373
[2m[36m(func pid=181342)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=181342)[0m f1_macro: 0.10162690548880218
[2m[36m(func pid=181342)[0m f1_weighted: 0.14593410833583811
[2m[36m(func pid=181342)[0m f1_per_class: [0.046, 0.172, 0.103, 0.171, 0.026, 0.307, 0.087, 0.095, 0.01, 0.0]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.2445 | Steps: 2 | Val loss: 1.8673 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=171628)[0m top1: 0.3278917910447761
[2m[36m(func pid=171628)[0m top5: 0.8498134328358209
[2m[36m(func pid=171628)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=171628)[0m f1_macro: 0.3120705273813373
[2m[36m(func pid=171628)[0m f1_weighted: 0.3590210856875077
[2m[36m(func pid=171628)[0m f1_per_class: [0.297, 0.273, 0.688, 0.386, 0.079, 0.343, 0.441, 0.247, 0.198, 0.17]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=174617)[0m top1: 0.33722014925373134
[2m[36m(func pid=174617)[0m top5: 0.8875932835820896
[2m[36m(func pid=174617)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=174617)[0m f1_macro: 0.31181192566208515
[2m[36m(func pid=174617)[0m f1_weighted: 0.36403234076914803
[2m[36m(func pid=174617)[0m f1_per_class: [0.263, 0.308, 0.786, 0.383, 0.094, 0.314, 0.469, 0.203, 0.119, 0.179]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.9176 | Steps: 2 | Val loss: 2.3100 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=169716)[0m top1: 0.32975746268656714
[2m[36m(func pid=169716)[0m top5: 0.8488805970149254
[2m[36m(func pid=169716)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=169716)[0m f1_macro: 0.3045080142601585
[2m[36m(func pid=169716)[0m f1_weighted: 0.3491079843697818
[2m[36m(func pid=169716)[0m f1_per_class: [0.342, 0.258, 0.458, 0.424, 0.053, 0.379, 0.354, 0.318, 0.169, 0.289]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.1611 | Steps: 2 | Val loss: 2.2290 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0294 | Steps: 2 | Val loss: 3.6282 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 02:02:49 (running for 00:29:10.07)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.245 |      0.305 |                   62 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.175 |      0.312 |                   57 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.066 |      0.312 |                   42 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.918 |      0.104 |                   13 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.15391791044776118
[2m[36m(func pid=181342)[0m top5: 0.5555037313432836
[2m[36m(func pid=181342)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=181342)[0m f1_macro: 0.10418388005237836
[2m[36m(func pid=181342)[0m f1_weighted: 0.1511742168202255
[2m[36m(func pid=181342)[0m f1_per_class: [0.047, 0.168, 0.095, 0.176, 0.025, 0.306, 0.098, 0.116, 0.01, 0.0]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=171628)[0m top1: 0.3353544776119403
[2m[36m(func pid=171628)[0m top5: 0.851679104477612
[2m[36m(func pid=171628)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=171628)[0m f1_macro: 0.316959614344522
[2m[36m(func pid=171628)[0m f1_weighted: 0.3661334689012229
[2m[36m(func pid=171628)[0m f1_per_class: [0.301, 0.275, 0.688, 0.4, 0.08, 0.344, 0.448, 0.25, 0.209, 0.175]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.2328 | Steps: 2 | Val loss: 1.8632 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=174617)[0m top1: 0.3414179104477612
[2m[36m(func pid=174617)[0m top5: 0.8796641791044776
[2m[36m(func pid=174617)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=174617)[0m f1_macro: 0.3137467826853302
[2m[36m(func pid=174617)[0m f1_weighted: 0.37003020088718613
[2m[36m(func pid=174617)[0m f1_per_class: [0.278, 0.306, 0.786, 0.4, 0.083, 0.295, 0.478, 0.212, 0.133, 0.167]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.9040 | Steps: 2 | Val loss: 2.3031 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=169716)[0m top1: 0.3306902985074627
[2m[36m(func pid=169716)[0m top5: 0.851679104477612
[2m[36m(func pid=169716)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=169716)[0m f1_macro: 0.3055631650284169
[2m[36m(func pid=169716)[0m f1_weighted: 0.3512192727269069
[2m[36m(func pid=169716)[0m f1_per_class: [0.342, 0.265, 0.458, 0.418, 0.053, 0.379, 0.362, 0.322, 0.168, 0.288]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.1287 | Steps: 2 | Val loss: 2.2172 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0067 | Steps: 2 | Val loss: 3.6646 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 02:02:55 (running for 00:29:15.73)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.233 |      0.306 |                   63 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.161 |      0.317 |                   58 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.029 |      0.314 |                   43 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.904 |      0.111 |                   14 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.15578358208955223
[2m[36m(func pid=181342)[0m top5: 0.5676305970149254
[2m[36m(func pid=181342)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=181342)[0m f1_macro: 0.11073329050718742
[2m[36m(func pid=181342)[0m f1_weighted: 0.15477811690355825
[2m[36m(func pid=181342)[0m f1_per_class: [0.056, 0.165, 0.136, 0.181, 0.025, 0.307, 0.105, 0.121, 0.01, 0.0]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=171628)[0m top1: 0.3381529850746269
[2m[36m(func pid=171628)[0m top5: 0.8549440298507462
[2m[36m(func pid=171628)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=171628)[0m f1_macro: 0.31759445118292645
[2m[36m(func pid=171628)[0m f1_weighted: 0.3677824584496005
[2m[36m(func pid=171628)[0m f1_per_class: [0.301, 0.274, 0.688, 0.407, 0.086, 0.346, 0.448, 0.249, 0.209, 0.169]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.3024 | Steps: 2 | Val loss: 1.8615 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
[2m[36m(func pid=174617)[0m top1: 0.3376865671641791
[2m[36m(func pid=174617)[0m top5: 0.8652052238805971
[2m[36m(func pid=174617)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=174617)[0m f1_macro: 0.3059136425584893
[2m[36m(func pid=174617)[0m f1_weighted: 0.36879754656754976
[2m[36m(func pid=174617)[0m f1_per_class: [0.258, 0.287, 0.733, 0.407, 0.074, 0.295, 0.48, 0.208, 0.159, 0.159]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8610 | Steps: 2 | Val loss: 2.2969 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.1988 | Steps: 2 | Val loss: 2.2189 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=169716)[0m top1: 0.3292910447761194
[2m[36m(func pid=169716)[0m top5: 0.8498134328358209
[2m[36m(func pid=169716)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=169716)[0m f1_macro: 0.3021415069465554
[2m[36m(func pid=169716)[0m f1_weighted: 0.3501193800847272
[2m[36m(func pid=169716)[0m f1_per_class: [0.335, 0.264, 0.478, 0.415, 0.055, 0.367, 0.373, 0.298, 0.164, 0.273]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0943 | Steps: 2 | Val loss: 3.7260 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 02:03:00 (running for 00:29:21.01)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.302 |      0.302 |                   64 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.129 |      0.318 |                   59 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.007 |      0.306 |                   44 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.861 |      0.113 |                   15 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.15858208955223882
[2m[36m(func pid=181342)[0m top5: 0.5802238805970149
[2m[36m(func pid=181342)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=181342)[0m f1_macro: 0.11344832465952667
[2m[36m(func pid=181342)[0m f1_weighted: 0.16039066483472392
[2m[36m(func pid=181342)[0m f1_per_class: [0.054, 0.164, 0.128, 0.19, 0.025, 0.316, 0.11, 0.127, 0.02, 0.0]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=171628)[0m top1: 0.3381529850746269
[2m[36m(func pid=171628)[0m top5: 0.8572761194029851
[2m[36m(func pid=171628)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=171628)[0m f1_macro: 0.31729471134952936
[2m[36m(func pid=171628)[0m f1_weighted: 0.36783637301784927
[2m[36m(func pid=171628)[0m f1_per_class: [0.308, 0.27, 0.688, 0.413, 0.085, 0.348, 0.444, 0.241, 0.22, 0.157]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.2424 | Steps: 2 | Val loss: 1.8577 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=174617)[0m top1: 0.3376865671641791
[2m[36m(func pid=174617)[0m top5: 0.8563432835820896
[2m[36m(func pid=174617)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=174617)[0m f1_macro: 0.30456331768715456
[2m[36m(func pid=174617)[0m f1_weighted: 0.37027248665778106
[2m[36m(func pid=174617)[0m f1_per_class: [0.251, 0.249, 0.733, 0.413, 0.076, 0.302, 0.498, 0.204, 0.168, 0.151]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8860 | Steps: 2 | Val loss: 2.2917 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.1419 | Steps: 2 | Val loss: 2.2111 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=169716)[0m top1: 0.333955223880597
[2m[36m(func pid=169716)[0m top5: 0.8521455223880597
[2m[36m(func pid=169716)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=169716)[0m f1_macro: 0.30914873861769865
[2m[36m(func pid=169716)[0m f1_weighted: 0.35462306377456454
[2m[36m(func pid=169716)[0m f1_per_class: [0.327, 0.268, 0.5, 0.415, 0.057, 0.381, 0.375, 0.322, 0.165, 0.282]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0425 | Steps: 2 | Val loss: 3.8887 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=181342)[0m top1: 0.16324626865671643
[2m[36m(func pid=181342)[0m top5: 0.5928171641791045
[2m[36m(func pid=181342)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=181342)[0m f1_macro: 0.1170456100924129
[2m[36m(func pid=181342)[0m f1_weighted: 0.1690992275407515
[2m[36m(func pid=181342)[0m f1_per_class: [0.05, 0.165, 0.125, 0.202, 0.025, 0.323, 0.123, 0.138, 0.019, 0.0]
[2m[36m(func pid=181342)[0m 
== Status ==
Current time: 2024-01-07 02:03:05 (running for 00:29:26.38)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.242 |      0.309 |                   65 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.199 |      0.317 |                   60 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.094 |      0.305 |                   45 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.886 |      0.117 |                   16 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.3400186567164179
[2m[36m(func pid=171628)[0m top5: 0.8596082089552238
[2m[36m(func pid=171628)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=171628)[0m f1_macro: 0.3170051704732201
[2m[36m(func pid=171628)[0m f1_weighted: 0.3692997435848399
[2m[36m(func pid=171628)[0m f1_per_class: [0.29, 0.273, 0.71, 0.417, 0.088, 0.346, 0.448, 0.236, 0.205, 0.159]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.2128 | Steps: 2 | Val loss: 1.8553 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=174617)[0m top1: 0.3269589552238806
[2m[36m(func pid=174617)[0m top5: 0.8456156716417911
[2m[36m(func pid=174617)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=174617)[0m f1_macro: 0.29431866065577755
[2m[36m(func pid=174617)[0m f1_weighted: 0.35975935401846765
[2m[36m(func pid=174617)[0m f1_per_class: [0.218, 0.225, 0.733, 0.396, 0.08, 0.279, 0.51, 0.168, 0.172, 0.163]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8390 | Steps: 2 | Val loss: 2.2857 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.1581 | Steps: 2 | Val loss: 2.2031 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=169716)[0m top1: 0.33722014925373134
[2m[36m(func pid=169716)[0m top5: 0.8544776119402985
[2m[36m(func pid=169716)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=169716)[0m f1_macro: 0.31449501366942445
[2m[36m(func pid=169716)[0m f1_weighted: 0.3587519530925282
[2m[36m(func pid=169716)[0m f1_per_class: [0.333, 0.26, 0.537, 0.42, 0.064, 0.377, 0.387, 0.331, 0.165, 0.271]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0327 | Steps: 2 | Val loss: 4.1331 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 02:03:11 (running for 00:29:31.85)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.213 |      0.314 |                   66 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.142 |      0.317 |                   61 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.042 |      0.294 |                   46 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.839 |      0.119 |                   17 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.16744402985074627
[2m[36m(func pid=181342)[0m top5: 0.6058768656716418
[2m[36m(func pid=181342)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=181342)[0m f1_macro: 0.11939646850702694
[2m[36m(func pid=181342)[0m f1_weighted: 0.17650719171241197
[2m[36m(func pid=181342)[0m f1_per_class: [0.048, 0.167, 0.109, 0.203, 0.026, 0.33, 0.143, 0.141, 0.027, 0.0]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=171628)[0m top1: 0.34421641791044777
[2m[36m(func pid=171628)[0m top5: 0.8610074626865671
[2m[36m(func pid=171628)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=171628)[0m f1_macro: 0.3191096240314343
[2m[36m(func pid=171628)[0m f1_weighted: 0.37365857753963677
[2m[36m(func pid=171628)[0m f1_per_class: [0.298, 0.271, 0.71, 0.42, 0.088, 0.345, 0.46, 0.234, 0.207, 0.158]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.2445 | Steps: 2 | Val loss: 1.8554 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=174617)[0m top1: 0.30923507462686567
[2m[36m(func pid=174617)[0m top5: 0.8264925373134329
[2m[36m(func pid=174617)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=174617)[0m f1_macro: 0.2831356115863789
[2m[36m(func pid=174617)[0m f1_weighted: 0.3452288665089649
[2m[36m(func pid=174617)[0m f1_per_class: [0.181, 0.202, 0.733, 0.378, 0.078, 0.258, 0.504, 0.153, 0.178, 0.166]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8327 | Steps: 2 | Val loss: 2.2815 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.1073 | Steps: 2 | Val loss: 2.2032 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=169716)[0m top1: 0.33488805970149255
[2m[36m(func pid=169716)[0m top5: 0.8544776119402985
[2m[36m(func pid=169716)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=169716)[0m f1_macro: 0.31403879032761506
[2m[36m(func pid=169716)[0m f1_weighted: 0.3561158105949269
[2m[36m(func pid=169716)[0m f1_per_class: [0.342, 0.269, 0.537, 0.419, 0.055, 0.374, 0.374, 0.335, 0.165, 0.271]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0111 | Steps: 2 | Val loss: 4.3716 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=181342)[0m top1: 0.1669776119402985
[2m[36m(func pid=181342)[0m top5: 0.6077425373134329
[2m[36m(func pid=181342)[0m f1_micro: 0.1669776119402985
[2m[36m(func pid=181342)[0m f1_macro: 0.11893229192592152
[2m[36m(func pid=181342)[0m f1_weighted: 0.17903841834703033
[2m[36m(func pid=181342)[0m f1_per_class: [0.049, 0.173, 0.1, 0.218, 0.025, 0.324, 0.137, 0.138, 0.026, 0.0]
[2m[36m(func pid=181342)[0m 
== Status ==
Current time: 2024-01-07 02:03:16 (running for 00:29:37.30)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.244 |      0.314 |                   67 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.158 |      0.319 |                   62 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.033 |      0.283 |                   47 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.833 |      0.119 |                   18 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.34281716417910446
[2m[36m(func pid=171628)[0m top5: 0.8666044776119403
[2m[36m(func pid=171628)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=171628)[0m f1_macro: 0.31227936132967044
[2m[36m(func pid=171628)[0m f1_weighted: 0.3722094728519559
[2m[36m(func pid=171628)[0m f1_per_class: [0.3, 0.27, 0.667, 0.413, 0.09, 0.349, 0.465, 0.229, 0.186, 0.153]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.2784 | Steps: 2 | Val loss: 1.8573 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=174617)[0m top1: 0.2826492537313433
[2m[36m(func pid=174617)[0m top5: 0.8106343283582089
[2m[36m(func pid=174617)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=174617)[0m f1_macro: 0.2675816852839559
[2m[36m(func pid=174617)[0m f1_weighted: 0.32244444228019803
[2m[36m(func pid=174617)[0m f1_per_class: [0.147, 0.178, 0.71, 0.338, 0.066, 0.262, 0.48, 0.154, 0.17, 0.171]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7979 | Steps: 2 | Val loss: 2.2769 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.1531 | Steps: 2 | Val loss: 2.2083 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=169716)[0m top1: 0.3316231343283582
[2m[36m(func pid=169716)[0m top5: 0.8535447761194029
[2m[36m(func pid=169716)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=169716)[0m f1_macro: 0.3111952034844873
[2m[36m(func pid=169716)[0m f1_weighted: 0.35346850635879234
[2m[36m(func pid=169716)[0m f1_per_class: [0.32, 0.264, 0.537, 0.415, 0.056, 0.372, 0.373, 0.339, 0.172, 0.266]
[2m[36m(func pid=169716)[0m 
== Status ==
Current time: 2024-01-07 02:03:21 (running for 00:29:42.51)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.278 |      0.311 |                   68 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.107 |      0.312 |                   63 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.011 |      0.268 |                   48 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.798 |      0.121 |                   19 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.16651119402985073
[2m[36m(func pid=181342)[0m top5: 0.6175373134328358
[2m[36m(func pid=181342)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=181342)[0m f1_macro: 0.12118540025374744
[2m[36m(func pid=181342)[0m f1_weighted: 0.18056523583188763
[2m[36m(func pid=181342)[0m f1_per_class: [0.048, 0.154, 0.116, 0.228, 0.031, 0.322, 0.142, 0.144, 0.026, 0.0]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=171628)[0m top1: 0.3460820895522388
[2m[36m(func pid=171628)[0m top5: 0.867070895522388
[2m[36m(func pid=171628)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=171628)[0m f1_macro: 0.31499385272807157
[2m[36m(func pid=171628)[0m f1_weighted: 0.37489897448718923
[2m[36m(func pid=171628)[0m f1_per_class: [0.316, 0.283, 0.667, 0.403, 0.094, 0.351, 0.473, 0.24, 0.171, 0.151]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0302 | Steps: 2 | Val loss: 4.5760 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.2221 | Steps: 2 | Val loss: 1.8565 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.1066 | Steps: 2 | Val loss: 2.2247 | Batch size: 32 | lr: 0.01 | Duration: 2.62s
[2m[36m(func pid=174617)[0m top1: 0.26492537313432835
[2m[36m(func pid=174617)[0m top5: 0.7975746268656716
[2m[36m(func pid=174617)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=174617)[0m f1_macro: 0.2634368932070437
[2m[36m(func pid=174617)[0m f1_weighted: 0.3083924655382004
[2m[36m(func pid=174617)[0m f1_per_class: [0.129, 0.174, 0.71, 0.311, 0.067, 0.253, 0.459, 0.18, 0.178, 0.173]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7933 | Steps: 2 | Val loss: 2.2710 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=169716)[0m top1: 0.33255597014925375
[2m[36m(func pid=169716)[0m top5: 0.851679104477612
[2m[36m(func pid=169716)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=169716)[0m f1_macro: 0.3118010939818202
[2m[36m(func pid=169716)[0m f1_weighted: 0.3537904797496386
[2m[36m(func pid=169716)[0m f1_per_class: [0.32, 0.267, 0.55, 0.421, 0.071, 0.372, 0.371, 0.322, 0.157, 0.267]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m top1: 0.341884328358209
[2m[36m(func pid=171628)[0m top5: 0.8642723880597015
[2m[36m(func pid=171628)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=171628)[0m f1_macro: 0.31419021468645225
[2m[36m(func pid=171628)[0m f1_weighted: 0.371094245292148
[2m[36m(func pid=171628)[0m f1_per_class: [0.294, 0.283, 0.688, 0.397, 0.094, 0.35, 0.467, 0.239, 0.174, 0.155]
[2m[36m(func pid=171628)[0m 
== Status ==
Current time: 2024-01-07 02:03:26 (running for 00:29:47.67)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.222 |      0.312 |                   69 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.107 |      0.314 |                   65 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.03  |      0.263 |                   49 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.798 |      0.121 |                   19 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0382 | Steps: 2 | Val loss: 4.6908 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=181342)[0m top1: 0.17397388059701493
[2m[36m(func pid=181342)[0m top5: 0.6208022388059702
[2m[36m(func pid=181342)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=181342)[0m f1_macro: 0.12603054592154558
[2m[36m(func pid=181342)[0m f1_weighted: 0.19040206832816864
[2m[36m(func pid=181342)[0m f1_per_class: [0.068, 0.157, 0.105, 0.24, 0.032, 0.33, 0.159, 0.143, 0.026, 0.0]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.3241 | Steps: 2 | Val loss: 1.8430 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.1246 | Steps: 2 | Val loss: 2.2295 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=174617)[0m top1: 0.2560634328358209
[2m[36m(func pid=174617)[0m top5: 0.7831156716417911
[2m[36m(func pid=174617)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=174617)[0m f1_macro: 0.26284694734567365
[2m[36m(func pid=174617)[0m f1_weighted: 0.3011707372048363
[2m[36m(func pid=174617)[0m f1_per_class: [0.125, 0.173, 0.706, 0.3, 0.059, 0.265, 0.438, 0.189, 0.201, 0.172]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7826 | Steps: 2 | Val loss: 2.2663 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=169716)[0m top1: 0.33861940298507465
[2m[36m(func pid=169716)[0m top5: 0.8549440298507462
[2m[36m(func pid=169716)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=169716)[0m f1_macro: 0.3161380004952356
[2m[36m(func pid=169716)[0m f1_weighted: 0.3598621056265709
[2m[36m(func pid=169716)[0m f1_per_class: [0.339, 0.261, 0.537, 0.427, 0.075, 0.382, 0.382, 0.315, 0.188, 0.256]
[2m[36m(func pid=169716)[0m 
== Status ==
Current time: 2024-01-07 02:03:32 (running for 00:29:52.81)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.324 |      0.316 |                   70 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.125 |      0.312 |                   66 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.038 |      0.263 |                   50 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.793 |      0.126 |                   20 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.33675373134328357
[2m[36m(func pid=171628)[0m top5: 0.8656716417910447
[2m[36m(func pid=171628)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=171628)[0m f1_macro: 0.3124736396657279
[2m[36m(func pid=171628)[0m f1_weighted: 0.36590101009773596
[2m[36m(func pid=171628)[0m f1_per_class: [0.294, 0.277, 0.688, 0.397, 0.092, 0.351, 0.453, 0.241, 0.178, 0.156]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0328 | Steps: 2 | Val loss: 4.7243 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=181342)[0m top1: 0.17444029850746268
[2m[36m(func pid=181342)[0m top5: 0.6277985074626866
[2m[36m(func pid=181342)[0m f1_micro: 0.17444029850746268
[2m[36m(func pid=181342)[0m f1_macro: 0.1265083771331868
[2m[36m(func pid=181342)[0m f1_weighted: 0.19222989529507228
[2m[36m(func pid=181342)[0m f1_per_class: [0.066, 0.155, 0.104, 0.243, 0.04, 0.329, 0.165, 0.138, 0.026, 0.0]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.2607 | Steps: 2 | Val loss: 1.8414 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.1319 | Steps: 2 | Val loss: 2.2560 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=174617)[0m top1: 0.25279850746268656
[2m[36m(func pid=174617)[0m top5: 0.7761194029850746
[2m[36m(func pid=174617)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=174617)[0m f1_macro: 0.262556953675405
[2m[36m(func pid=174617)[0m f1_weighted: 0.2985151062735821
[2m[36m(func pid=174617)[0m f1_per_class: [0.124, 0.174, 0.706, 0.303, 0.067, 0.267, 0.423, 0.199, 0.192, 0.17]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7510 | Steps: 2 | Val loss: 2.2600 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=169716)[0m top1: 0.33861940298507465
[2m[36m(func pid=169716)[0m top5: 0.8544776119402985
[2m[36m(func pid=169716)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=169716)[0m f1_macro: 0.31775246481808417
[2m[36m(func pid=169716)[0m f1_weighted: 0.36054291874924854
[2m[36m(func pid=169716)[0m f1_per_class: [0.348, 0.259, 0.537, 0.425, 0.066, 0.391, 0.383, 0.318, 0.198, 0.253]
[2m[36m(func pid=169716)[0m 
== Status ==
Current time: 2024-01-07 02:03:37 (running for 00:29:58.17)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.261 |      0.318 |                   71 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.132 |      0.311 |                   67 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.033 |      0.263 |                   51 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.783 |      0.127 |                   21 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.33302238805970147
[2m[36m(func pid=171628)[0m top5: 0.8619402985074627
[2m[36m(func pid=171628)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=171628)[0m f1_macro: 0.31058619468289306
[2m[36m(func pid=171628)[0m f1_weighted: 0.36194868528144974
[2m[36m(func pid=171628)[0m f1_per_class: [0.292, 0.281, 0.688, 0.393, 0.091, 0.349, 0.441, 0.237, 0.183, 0.15]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0218 | Steps: 2 | Val loss: 4.6649 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=181342)[0m top1: 0.17630597014925373
[2m[36m(func pid=181342)[0m top5: 0.6394589552238806
[2m[36m(func pid=181342)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=181342)[0m f1_macro: 0.1266535788405828
[2m[36m(func pid=181342)[0m f1_weighted: 0.19599836854904537
[2m[36m(func pid=181342)[0m f1_per_class: [0.067, 0.152, 0.1, 0.244, 0.034, 0.325, 0.18, 0.138, 0.026, 0.0]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.1848 | Steps: 2 | Val loss: 1.8420 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.1694 | Steps: 2 | Val loss: 2.2895 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=174617)[0m top1: 0.25279850746268656
[2m[36m(func pid=174617)[0m top5: 0.777518656716418
[2m[36m(func pid=174617)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=174617)[0m f1_macro: 0.26717122899230483
[2m[36m(func pid=174617)[0m f1_weighted: 0.2966520342303612
[2m[36m(func pid=174617)[0m f1_per_class: [0.134, 0.173, 0.727, 0.304, 0.062, 0.265, 0.415, 0.197, 0.209, 0.185]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7693 | Steps: 2 | Val loss: 2.2551 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=169716)[0m top1: 0.3414179104477612
[2m[36m(func pid=169716)[0m top5: 0.8549440298507462
[2m[36m(func pid=169716)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=169716)[0m f1_macro: 0.3260098546715609
[2m[36m(func pid=169716)[0m f1_weighted: 0.3639674056192102
[2m[36m(func pid=169716)[0m f1_per_class: [0.348, 0.266, 0.595, 0.425, 0.065, 0.408, 0.382, 0.317, 0.202, 0.253]
[2m[36m(func pid=169716)[0m 
== Status ==
Current time: 2024-01-07 02:03:42 (running for 00:30:03.35)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.185 |      0.326 |                   72 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.169 |      0.312 |                   68 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.022 |      0.267 |                   52 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.751 |      0.127 |                   22 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.32882462686567165
[2m[36m(func pid=171628)[0m top5: 0.8586753731343284
[2m[36m(func pid=171628)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=171628)[0m f1_macro: 0.3124873501533517
[2m[36m(func pid=171628)[0m f1_weighted: 0.3574494136973143
[2m[36m(func pid=171628)[0m f1_per_class: [0.288, 0.281, 0.71, 0.385, 0.087, 0.348, 0.431, 0.263, 0.171, 0.161]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=181342)[0m top1: 0.17863805970149255
[2m[36m(func pid=181342)[0m top5: 0.6427238805970149
[2m[36m(func pid=181342)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=181342)[0m f1_macro: 0.13296381757779183
[2m[36m(func pid=181342)[0m f1_weighted: 0.19889372671527625
[2m[36m(func pid=181342)[0m f1_per_class: [0.09, 0.15, 0.116, 0.248, 0.035, 0.324, 0.185, 0.136, 0.026, 0.019]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0178 | Steps: 2 | Val loss: 4.5563 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.3421 | Steps: 2 | Val loss: 1.8367 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.1443 | Steps: 2 | Val loss: 2.3012 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7460 | Steps: 2 | Val loss: 2.2515 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=174617)[0m top1: 0.25419776119402987
[2m[36m(func pid=174617)[0m top5: 0.7817164179104478
[2m[36m(func pid=174617)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=174617)[0m f1_macro: 0.2675984718915803
[2m[36m(func pid=174617)[0m f1_weighted: 0.29558608979575146
[2m[36m(func pid=174617)[0m f1_per_class: [0.14, 0.176, 0.706, 0.305, 0.062, 0.264, 0.408, 0.199, 0.217, 0.2]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m top1: 0.34095149253731344
[2m[36m(func pid=169716)[0m top5: 0.8568097014925373
[2m[36m(func pid=169716)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=169716)[0m f1_macro: 0.32043916479125084
[2m[36m(func pid=169716)[0m f1_weighted: 0.3633503583769664
[2m[36m(func pid=169716)[0m f1_per_class: [0.339, 0.271, 0.564, 0.427, 0.066, 0.388, 0.388, 0.299, 0.197, 0.266]
[2m[36m(func pid=169716)[0m 
== Status ==
Current time: 2024-01-07 02:03:47 (running for 00:30:08.65)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.342 |      0.32  |                   73 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.144 |      0.314 |                   69 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.018 |      0.268 |                   53 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.769 |      0.133 |                   23 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.3260261194029851
[2m[36m(func pid=171628)[0m top5: 0.8540111940298507
[2m[36m(func pid=171628)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=171628)[0m f1_macro: 0.3144289052683392
[2m[36m(func pid=171628)[0m f1_weighted: 0.3553552470068022
[2m[36m(func pid=171628)[0m f1_per_class: [0.281, 0.276, 0.71, 0.385, 0.086, 0.351, 0.42, 0.278, 0.201, 0.157]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=181342)[0m top1: 0.1837686567164179
[2m[36m(func pid=181342)[0m top5: 0.6515858208955224
[2m[36m(func pid=181342)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=181342)[0m f1_macro: 0.1431307181386973
[2m[36m(func pid=181342)[0m f1_weighted: 0.20461165812444623
[2m[36m(func pid=181342)[0m f1_per_class: [0.097, 0.153, 0.13, 0.248, 0.036, 0.328, 0.195, 0.148, 0.042, 0.056]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0195 | Steps: 2 | Val loss: 4.4821 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.1686 | Steps: 2 | Val loss: 1.8386 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.1326 | Steps: 2 | Val loss: 2.3097 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7100 | Steps: 2 | Val loss: 2.2469 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=174617)[0m top1: 0.2579291044776119
[2m[36m(func pid=174617)[0m top5: 0.784981343283582
[2m[36m(func pid=174617)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=174617)[0m f1_macro: 0.2687396601634663
[2m[36m(func pid=174617)[0m f1_weighted: 0.298875732372798
[2m[36m(func pid=174617)[0m f1_per_class: [0.158, 0.185, 0.688, 0.317, 0.065, 0.262, 0.398, 0.227, 0.216, 0.171]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m top1: 0.33955223880597013
[2m[36m(func pid=169716)[0m top5: 0.8549440298507462
[2m[36m(func pid=169716)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=169716)[0m f1_macro: 0.32430737058423936
[2m[36m(func pid=169716)[0m f1_weighted: 0.3610916004515877
[2m[36m(func pid=169716)[0m f1_per_class: [0.333, 0.267, 0.611, 0.429, 0.073, 0.386, 0.38, 0.311, 0.18, 0.272]
[2m[36m(func pid=169716)[0m 
== Status ==
Current time: 2024-01-07 02:03:53 (running for 00:30:13.82)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.169 |      0.324 |                   74 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.133 |      0.314 |                   70 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.02  |      0.269 |                   54 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.746 |      0.143 |                   24 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.3260261194029851
[2m[36m(func pid=171628)[0m top5: 0.8535447761194029
[2m[36m(func pid=171628)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=171628)[0m f1_macro: 0.314231760669998
[2m[36m(func pid=171628)[0m f1_weighted: 0.3554558310003063
[2m[36m(func pid=171628)[0m f1_per_class: [0.284, 0.272, 0.71, 0.391, 0.085, 0.351, 0.418, 0.271, 0.204, 0.157]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=181342)[0m top1: 0.19076492537313433
[2m[36m(func pid=181342)[0m top5: 0.659981343283582
[2m[36m(func pid=181342)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=181342)[0m f1_macro: 0.1566852832550651
[2m[36m(func pid=181342)[0m f1_weighted: 0.21088994970584715
[2m[36m(func pid=181342)[0m f1_per_class: [0.115, 0.161, 0.163, 0.249, 0.036, 0.337, 0.201, 0.157, 0.043, 0.104]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0365 | Steps: 2 | Val loss: 4.3628 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.1754 | Steps: 2 | Val loss: 1.8371 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.1939 | Steps: 2 | Val loss: 2.3169 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.6932 | Steps: 2 | Val loss: 2.2410 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=174617)[0m top1: 0.2691231343283582
[2m[36m(func pid=174617)[0m top5: 0.7924440298507462
[2m[36m(func pid=174617)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=174617)[0m f1_macro: 0.2745937154168203
[2m[36m(func pid=174617)[0m f1_weighted: 0.3100742630650778
[2m[36m(func pid=174617)[0m f1_per_class: [0.174, 0.21, 0.688, 0.332, 0.062, 0.268, 0.402, 0.238, 0.206, 0.164]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m top1: 0.3400186567164179
[2m[36m(func pid=169716)[0m top5: 0.8544776119402985
[2m[36m(func pid=169716)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=169716)[0m f1_macro: 0.32774479522559075
[2m[36m(func pid=169716)[0m f1_weighted: 0.36150006508231103
[2m[36m(func pid=169716)[0m f1_per_class: [0.333, 0.271, 0.611, 0.415, 0.073, 0.408, 0.38, 0.324, 0.183, 0.279]
[2m[36m(func pid=169716)[0m 
== Status ==
Current time: 2024-01-07 02:03:58 (running for 00:30:19.12)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.175 |      0.328 |                   75 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.194 |      0.316 |                   71 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.036 |      0.275 |                   55 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.71  |      0.157 |                   25 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171628)[0m top1: 0.3269589552238806
[2m[36m(func pid=171628)[0m top5: 0.851679104477612
[2m[36m(func pid=171628)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=171628)[0m f1_macro: 0.31581011449310553
[2m[36m(func pid=171628)[0m f1_weighted: 0.35561101101099113
[2m[36m(func pid=171628)[0m f1_per_class: [0.293, 0.273, 0.71, 0.396, 0.084, 0.357, 0.409, 0.277, 0.204, 0.155]
[2m[36m(func pid=171628)[0m 
[2m[36m(func pid=181342)[0m top1: 0.19869402985074627
[2m[36m(func pid=181342)[0m top5: 0.6632462686567164
[2m[36m(func pid=181342)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=181342)[0m f1_macro: 0.16313141394016734
[2m[36m(func pid=181342)[0m f1_weighted: 0.21990713554406993
[2m[36m(func pid=181342)[0m f1_per_class: [0.146, 0.162, 0.158, 0.266, 0.037, 0.337, 0.211, 0.163, 0.043, 0.108]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 1.1160 | Steps: 2 | Val loss: 1.8394 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0249 | Steps: 2 | Val loss: 4.2771 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.1029 | Steps: 2 | Val loss: 2.3158 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.6737 | Steps: 2 | Val loss: 2.2361 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=169716)[0m top1: 0.3381529850746269
[2m[36m(func pid=169716)[0m top5: 0.851679104477612
[2m[36m(func pid=169716)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=169716)[0m f1_macro: 0.32405840659156393
[2m[36m(func pid=169716)[0m f1_weighted: 0.3596290008957694
[2m[36m(func pid=169716)[0m f1_per_class: [0.332, 0.276, 0.595, 0.413, 0.073, 0.397, 0.378, 0.32, 0.178, 0.279]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m top1: 0.28125
[2m[36m(func pid=174617)[0m top5: 0.8069029850746269
[2m[36m(func pid=174617)[0m f1_micro: 0.28125
[2m[36m(func pid=174617)[0m f1_macro: 0.28226083507754046
[2m[36m(func pid=174617)[0m f1_weighted: 0.321529649308285
[2m[36m(func pid=174617)[0m f1_per_class: [0.19, 0.24, 0.727, 0.342, 0.054, 0.267, 0.417, 0.225, 0.193, 0.168]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m top1: 0.32509328358208955
[2m[36m(func pid=171628)[0m top5: 0.8502798507462687
[2m[36m(func pid=171628)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=171628)[0m f1_macro: 0.3164897558240243
[2m[36m(func pid=171628)[0m f1_weighted: 0.3539167018845637
[2m[36m(func pid=171628)[0m f1_per_class: [0.297, 0.255, 0.71, 0.4, 0.081, 0.358, 0.407, 0.276, 0.224, 0.157]
[2m[36m(func pid=171628)[0m 
== Status ==
Current time: 2024-01-07 02:04:04 (running for 00:30:25.37)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.116 |      0.324 |                   76 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.103 |      0.316 |                   72 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.025 |      0.282 |                   56 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.674 |      0.172 |                   27 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.20522388059701493
[2m[36m(func pid=181342)[0m top5: 0.6711753731343284
[2m[36m(func pid=181342)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=181342)[0m f1_macro: 0.17173702932340434
[2m[36m(func pid=181342)[0m f1_weighted: 0.22781746423454027
[2m[36m(func pid=181342)[0m f1_per_class: [0.154, 0.164, 0.164, 0.273, 0.037, 0.341, 0.222, 0.189, 0.043, 0.131]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0052 | Steps: 2 | Val loss: 4.1555 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.0987 | Steps: 2 | Val loss: 1.8468 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.1369 | Steps: 2 | Val loss: 2.3224 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6546 | Steps: 2 | Val loss: 2.2313 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=169716)[0m top1: 0.33675373134328357
[2m[36m(func pid=169716)[0m top5: 0.8507462686567164
[2m[36m(func pid=169716)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=169716)[0m f1_macro: 0.32302568215935223
[2m[36m(func pid=169716)[0m f1_weighted: 0.3590432057088343
[2m[36m(func pid=169716)[0m f1_per_class: [0.328, 0.275, 0.579, 0.409, 0.07, 0.397, 0.38, 0.321, 0.194, 0.277]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m top1: 0.2905783582089552
[2m[36m(func pid=174617)[0m top5: 0.8232276119402985
[2m[36m(func pid=174617)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=174617)[0m f1_macro: 0.2852615213194082
[2m[36m(func pid=174617)[0m f1_weighted: 0.3278374132749556
[2m[36m(func pid=174617)[0m f1_per_class: [0.2, 0.262, 0.71, 0.338, 0.058, 0.285, 0.424, 0.212, 0.197, 0.167]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=171628)[0m top1: 0.324160447761194
[2m[36m(func pid=171628)[0m top5: 0.851679104477612
[2m[36m(func pid=171628)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=171628)[0m f1_macro: 0.3150390735291388
[2m[36m(func pid=171628)[0m f1_weighted: 0.35387131431382207
[2m[36m(func pid=171628)[0m f1_per_class: [0.297, 0.258, 0.71, 0.399, 0.078, 0.356, 0.409, 0.267, 0.221, 0.156]
[2m[36m(func pid=171628)[0m 
== Status ==
Current time: 2024-01-07 02:04:10 (running for 00:30:30.79)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.099 |      0.323 |                   77 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.137 |      0.315 |                   73 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.285 |                   57 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.655 |      0.171 |                   28 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.20615671641791045
[2m[36m(func pid=181342)[0m top5: 0.6767723880597015
[2m[36m(func pid=181342)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=181342)[0m f1_macro: 0.17130089176250304
[2m[36m(func pid=181342)[0m f1_weighted: 0.22898320082226087
[2m[36m(func pid=181342)[0m f1_per_class: [0.157, 0.159, 0.153, 0.278, 0.038, 0.341, 0.224, 0.189, 0.043, 0.131]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0079 | Steps: 2 | Val loss: 4.0577 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.2521 | Steps: 2 | Val loss: 1.8434 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.1135 | Steps: 2 | Val loss: 2.3252 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6118 | Steps: 2 | Val loss: 2.2250 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=174617)[0m top1: 0.300839552238806
[2m[36m(func pid=174617)[0m top5: 0.8339552238805971
[2m[36m(func pid=174617)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=174617)[0m f1_macro: 0.29153159200646644
[2m[36m(func pid=174617)[0m f1_weighted: 0.3353733928678719
[2m[36m(func pid=174617)[0m f1_per_class: [0.225, 0.275, 0.733, 0.331, 0.058, 0.296, 0.445, 0.203, 0.181, 0.168]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m top1: 0.3376865671641791
[2m[36m(func pid=169716)[0m top5: 0.8488805970149254
[2m[36m(func pid=169716)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=169716)[0m f1_macro: 0.32193227572301114
[2m[36m(func pid=169716)[0m f1_weighted: 0.3610546595623288
[2m[36m(func pid=169716)[0m f1_per_class: [0.318, 0.278, 0.595, 0.406, 0.073, 0.396, 0.393, 0.302, 0.19, 0.269]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=171628)[0m top1: 0.324160447761194
[2m[36m(func pid=171628)[0m top5: 0.8502798507462687
[2m[36m(func pid=171628)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=171628)[0m f1_macro: 0.31449629769551035
[2m[36m(func pid=171628)[0m f1_weighted: 0.35398354643647706
[2m[36m(func pid=171628)[0m f1_per_class: [0.295, 0.254, 0.71, 0.4, 0.08, 0.352, 0.412, 0.275, 0.215, 0.153]
[2m[36m(func pid=171628)[0m 
== Status ==
Current time: 2024-01-07 02:04:15 (running for 00:30:36.39)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.252 |      0.322 |                   78 |
| train_66d79_00014 | RUNNING    | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.114 |      0.314 |                   74 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.008 |      0.292 |                   58 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.612 |      0.175 |                   29 |
| train_66d79_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.20942164179104478
[2m[36m(func pid=181342)[0m top5: 0.6823694029850746
[2m[36m(func pid=181342)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=181342)[0m f1_macro: 0.17541107590058386
[2m[36m(func pid=181342)[0m f1_weighted: 0.23183260332531627
[2m[36m(func pid=181342)[0m f1_per_class: [0.162, 0.162, 0.157, 0.282, 0.039, 0.346, 0.226, 0.189, 0.035, 0.155]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=171628)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.1214 | Steps: 2 | Val loss: 2.3303 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 1.1058 | Steps: 2 | Val loss: 1.8425 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0442 | Steps: 2 | Val loss: 4.0159 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6091 | Steps: 2 | Val loss: 2.2225 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=171628)[0m top1: 0.3260261194029851
[2m[36m(func pid=171628)[0m top5: 0.8479477611940298
[2m[36m(func pid=171628)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=171628)[0m f1_macro: 0.31505684978297693
[2m[36m(func pid=171628)[0m f1_weighted: 0.3564736501899329
[2m[36m(func pid=171628)[0m f1_per_class: [0.295, 0.25, 0.71, 0.413, 0.075, 0.362, 0.408, 0.269, 0.212, 0.157]
[2m[36m(func pid=169716)[0m top1: 0.3376865671641791
[2m[36m(func pid=169716)[0m top5: 0.8498134328358209
[2m[36m(func pid=169716)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=169716)[0m f1_macro: 0.32280433203901615
[2m[36m(func pid=169716)[0m f1_weighted: 0.36096258352722377
[2m[36m(func pid=169716)[0m f1_per_class: [0.327, 0.275, 0.595, 0.406, 0.074, 0.386, 0.395, 0.311, 0.2, 0.26]
[2m[36m(func pid=174617)[0m top1: 0.3050373134328358
[2m[36m(func pid=174617)[0m top5: 0.84375
[2m[36m(func pid=174617)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=174617)[0m f1_macro: 0.2937771837877619
[2m[36m(func pid=174617)[0m f1_weighted: 0.3362211221079181
[2m[36m(func pid=174617)[0m f1_per_class: [0.239, 0.285, 0.733, 0.313, 0.062, 0.31, 0.454, 0.207, 0.163, 0.171]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=181342)[0m top1: 0.2126865671641791
[2m[36m(func pid=181342)[0m top5: 0.6944962686567164
[2m[36m(func pid=181342)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=181342)[0m f1_macro: 0.17739422043720016
[2m[36m(func pid=181342)[0m f1_weighted: 0.23620619970291162
[2m[36m(func pid=181342)[0m f1_per_class: [0.169, 0.156, 0.144, 0.295, 0.031, 0.34, 0.233, 0.19, 0.037, 0.179]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 1.1482 | Steps: 2 | Val loss: 1.8471 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0119 | Steps: 2 | Val loss: 4.0193 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.5896 | Steps: 2 | Val loss: 2.2183 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=174617)[0m top1: 0.30736940298507465
[2m[36m(func pid=174617)[0m top5: 0.8460820895522388
[2m[36m(func pid=174617)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=174617)[0m f1_macro: 0.2914301517989858
[2m[36m(func pid=174617)[0m f1_weighted: 0.33836569221213453
[2m[36m(func pid=174617)[0m f1_per_class: [0.226, 0.289, 0.733, 0.32, 0.065, 0.302, 0.46, 0.194, 0.147, 0.178]
[2m[36m(func pid=169716)[0m top1: 0.3381529850746269
[2m[36m(func pid=169716)[0m top5: 0.8488805970149254
[2m[36m(func pid=169716)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=169716)[0m f1_macro: 0.3210867268032791
[2m[36m(func pid=169716)[0m f1_weighted: 0.36152068282675665
[2m[36m(func pid=169716)[0m f1_per_class: [0.317, 0.271, 0.579, 0.409, 0.073, 0.403, 0.391, 0.319, 0.186, 0.263]
[2m[36m(func pid=181342)[0m top1: 0.21688432835820895
[2m[36m(func pid=181342)[0m top5: 0.6949626865671642
[2m[36m(func pid=181342)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=181342)[0m f1_macro: 0.18043551087473658
[2m[36m(func pid=181342)[0m f1_weighted: 0.2407395929052281
[2m[36m(func pid=181342)[0m f1_per_class: [0.174, 0.16, 0.135, 0.301, 0.04, 0.337, 0.238, 0.204, 0.037, 0.178]
== Status ==
Current time: 2024-01-07 02:04:21 (running for 00:30:41.96)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.106 |      0.323 |                   79 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.044 |      0.294 |                   59 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.609 |      0.177 |                   30 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 02:04:26 (running for 00:30:47.56)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.106 |      0.323 |                   79 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.044 |      0.294 |                   59 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.59  |      0.18  |                   31 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=188206)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=188206)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=188206)[0m Configuration completed!
[2m[36m(func pid=188206)[0m New optimizer parameters:
[2m[36m(func pid=188206)[0m SGD (
[2m[36m(func pid=188206)[0m Parameter Group 0
[2m[36m(func pid=188206)[0m     dampening: 0
[2m[36m(func pid=188206)[0m     differentiable: False
[2m[36m(func pid=188206)[0m     foreach: None
[2m[36m(func pid=188206)[0m     lr: 0.001
[2m[36m(func pid=188206)[0m     maximize: False
[2m[36m(func pid=188206)[0m     momentum: 0.99
[2m[36m(func pid=188206)[0m     nesterov: False
[2m[36m(func pid=188206)[0m     weight_decay: 1e-05
[2m[36m(func pid=188206)[0m )
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0114 | Steps: 2 | Val loss: 3.9827 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.5371 | Steps: 2 | Val loss: 2.2102 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.1121 | Steps: 2 | Val loss: 1.8412 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9959 | Steps: 2 | Val loss: 2.3204 | Batch size: 32 | lr: 0.001 | Duration: 4.30s
== Status ==
Current time: 2024-01-07 02:04:31 (running for 00:30:52.59)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.148 |      0.321 |                   80 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.012 |      0.291 |                   60 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.59  |      0.18  |                   31 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.3400186567164179
[2m[36m(func pid=169716)[0m top5: 0.851679104477612
[2m[36m(func pid=169716)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=169716)[0m f1_macro: 0.31619410592010105
[2m[36m(func pid=169716)[0m f1_weighted: 0.36372621690842416
[2m[36m(func pid=169716)[0m f1_per_class: [0.313, 0.276, 0.564, 0.414, 0.066, 0.405, 0.396, 0.313, 0.154, 0.261]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=181342)[0m top1: 0.22667910447761194
[2m[36m(func pid=181342)[0m top5: 0.7014925373134329
[2m[36m(func pid=181342)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=181342)[0m f1_macro: 0.18549666963993028
[2m[36m(func pid=181342)[0m f1_weighted: 0.2512911476816593
[2m[36m(func pid=181342)[0m f1_per_class: [0.185, 0.154, 0.137, 0.318, 0.033, 0.348, 0.255, 0.211, 0.039, 0.176]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m top1: 0.3138992537313433
[2m[36m(func pid=174617)[0m top5: 0.8530783582089553
[2m[36m(func pid=174617)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=174617)[0m f1_macro: 0.29281579021585846
[2m[36m(func pid=174617)[0m f1_weighted: 0.344706247681873
[2m[36m(func pid=174617)[0m f1_per_class: [0.211, 0.304, 0.733, 0.336, 0.063, 0.304, 0.458, 0.197, 0.151, 0.172]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=188206)[0m top1: 0.17723880597014927
[2m[36m(func pid=188206)[0m top5: 0.53125
[2m[36m(func pid=188206)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=188206)[0m f1_macro: 0.12126645583033509
[2m[36m(func pid=188206)[0m f1_weighted: 0.1270346313162938
[2m[36m(func pid=188206)[0m f1_per_class: [0.329, 0.35, 0.0, 0.093, 0.0, 0.209, 0.024, 0.014, 0.0, 0.194]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 1.1082 | Steps: 2 | Val loss: 1.8429 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6036 | Steps: 2 | Val loss: 2.2050 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0353 | Steps: 2 | Val loss: 3.9471 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9819 | Steps: 2 | Val loss: 2.3222 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 02:04:37 (running for 00:30:58.46)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.112 |      0.316 |                   81 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.011 |      0.293 |                   61 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.604 |      0.184 |                   33 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  2.996 |      0.121 |                    1 |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.22621268656716417
[2m[36m(func pid=181342)[0m top5: 0.7052238805970149
[2m[36m(func pid=181342)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=181342)[0m f1_macro: 0.18393980427636739
[2m[36m(func pid=181342)[0m f1_weighted: 0.25100527282015495
[2m[36m(func pid=181342)[0m f1_per_class: [0.186, 0.159, 0.132, 0.317, 0.034, 0.333, 0.26, 0.193, 0.041, 0.183]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=169716)[0m top1: 0.33722014925373134
[2m[36m(func pid=169716)[0m top5: 0.8498134328358209
[2m[36m(func pid=169716)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=169716)[0m f1_macro: 0.31448475762804295
[2m[36m(func pid=169716)[0m f1_weighted: 0.36111802767949497
[2m[36m(func pid=169716)[0m f1_per_class: [0.3, 0.272, 0.579, 0.404, 0.075, 0.404, 0.4, 0.315, 0.138, 0.257]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=174617)[0m top1: 0.3138992537313433
[2m[36m(func pid=174617)[0m top5: 0.851679104477612
[2m[36m(func pid=174617)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=174617)[0m f1_macro: 0.2935575004858543
[2m[36m(func pid=174617)[0m f1_weighted: 0.3456580211695318
[2m[36m(func pid=174617)[0m f1_per_class: [0.224, 0.293, 0.733, 0.35, 0.066, 0.303, 0.452, 0.201, 0.146, 0.165]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=188206)[0m top1: 0.18423507462686567
[2m[36m(func pid=188206)[0m top5: 0.5284514925373134
[2m[36m(func pid=188206)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=188206)[0m f1_macro: 0.11928581440965665
[2m[36m(func pid=188206)[0m f1_weighted: 0.1316027446748814
[2m[36m(func pid=188206)[0m f1_per_class: [0.308, 0.329, 0.0, 0.1, 0.011, 0.288, 0.015, 0.036, 0.0, 0.107]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 1.0289 | Steps: 2 | Val loss: 1.8416 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.5322 | Steps: 2 | Val loss: 2.2000 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0031 | Steps: 2 | Val loss: 3.9102 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9269 | Steps: 2 | Val loss: 2.3260 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=169716)[0m top1: 0.33955223880597013
[2m[36m(func pid=169716)[0m top5: 0.8512126865671642
[2m[36m(func pid=169716)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=169716)[0m f1_macro: 0.32046145147949684
[2m[36m(func pid=169716)[0m f1_weighted: 0.36428234138031257
[2m[36m(func pid=169716)[0m f1_per_class: [0.304, 0.276, 0.611, 0.401, 0.073, 0.409, 0.408, 0.313, 0.152, 0.258]
[2m[36m(func pid=169716)[0m 
== Status ==
Current time: 2024-01-07 02:04:43 (running for 00:31:03.82)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.029 |      0.32  |                   83 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.035 |      0.294 |                   62 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.604 |      0.184 |                   33 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  2.982 |      0.119 |                    2 |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.22621268656716417
[2m[36m(func pid=181342)[0m top5: 0.7112873134328358
[2m[36m(func pid=181342)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=181342)[0m f1_macro: 0.18231634026062055
[2m[36m(func pid=181342)[0m f1_weighted: 0.2516734448818965
[2m[36m(func pid=181342)[0m f1_per_class: [0.188, 0.159, 0.124, 0.31, 0.035, 0.33, 0.271, 0.196, 0.032, 0.178]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m top1: 0.3199626865671642
[2m[36m(func pid=174617)[0m top5: 0.8521455223880597
[2m[36m(func pid=174617)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=174617)[0m f1_macro: 0.30506181039080477
[2m[36m(func pid=174617)[0m f1_weighted: 0.35079757571933107
[2m[36m(func pid=174617)[0m f1_per_class: [0.327, 0.288, 0.733, 0.362, 0.068, 0.305, 0.453, 0.204, 0.147, 0.162]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=188206)[0m top1: 0.1837686567164179
[2m[36m(func pid=188206)[0m top5: 0.5186567164179104
[2m[36m(func pid=188206)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=188206)[0m f1_macro: 0.12051833438554238
[2m[36m(func pid=188206)[0m f1_weighted: 0.1362951344550086
[2m[36m(func pid=188206)[0m f1_per_class: [0.292, 0.31, 0.0, 0.114, 0.01, 0.329, 0.015, 0.032, 0.0, 0.103]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 1.0885 | Steps: 2 | Val loss: 1.8396 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5979 | Steps: 2 | Val loss: 2.1954 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0426 | Steps: 2 | Val loss: 3.8266 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9359 | Steps: 2 | Val loss: 2.3254 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 02:04:48 (running for 00:31:09.19)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.088 |      0.324 |                   84 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.003 |      0.305 |                   63 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.532 |      0.182 |                   34 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  2.927 |      0.121 |                    3 |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.34095149253731344
[2m[36m(func pid=169716)[0m top5: 0.851679104477612
[2m[36m(func pid=169716)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=169716)[0m f1_macro: 0.3238385951778907
[2m[36m(func pid=169716)[0m f1_weighted: 0.3655185791142196
[2m[36m(func pid=169716)[0m f1_per_class: [0.316, 0.273, 0.611, 0.406, 0.072, 0.407, 0.406, 0.311, 0.18, 0.256]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=181342)[0m top1: 0.22901119402985073
[2m[36m(func pid=181342)[0m top5: 0.7173507462686567
[2m[36m(func pid=181342)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=181342)[0m f1_macro: 0.1852058842910649
[2m[36m(func pid=181342)[0m f1_weighted: 0.2536842437656213
[2m[36m(func pid=181342)[0m f1_per_class: [0.187, 0.162, 0.131, 0.313, 0.036, 0.332, 0.272, 0.195, 0.043, 0.183]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m top1: 0.32975746268656714
[2m[36m(func pid=174617)[0m top5: 0.8572761194029851
[2m[36m(func pid=174617)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=174617)[0m f1_macro: 0.3106574072970873
[2m[36m(func pid=174617)[0m f1_weighted: 0.362527980290996
[2m[36m(func pid=174617)[0m f1_per_class: [0.304, 0.263, 0.759, 0.4, 0.073, 0.318, 0.466, 0.208, 0.161, 0.154]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=188206)[0m top1: 0.17583955223880596
[2m[36m(func pid=188206)[0m top5: 0.5107276119402985
[2m[36m(func pid=188206)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=188206)[0m f1_macro: 0.11943331055395087
[2m[36m(func pid=188206)[0m f1_weighted: 0.13541970417432944
[2m[36m(func pid=188206)[0m f1_per_class: [0.286, 0.293, 0.059, 0.12, 0.01, 0.326, 0.021, 0.019, 0.0, 0.062]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.0031 | Steps: 2 | Val loss: 1.8391 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.4813 | Steps: 2 | Val loss: 2.1870 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8536 | Steps: 2 | Val loss: 2.3196 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0081 | Steps: 2 | Val loss: 3.8285 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 02:04:53 (running for 00:31:14.61)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.088 |      0.324 |                   84 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.043 |      0.311 |                   64 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.481 |      0.188 |                   36 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  2.936 |      0.119 |                    4 |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.23600746268656717
[2m[36m(func pid=181342)[0m top5: 0.7280783582089553
[2m[36m(func pid=181342)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=181342)[0m f1_macro: 0.18818817181497025
[2m[36m(func pid=181342)[0m f1_weighted: 0.2601905947666279
[2m[36m(func pid=181342)[0m f1_per_class: [0.198, 0.159, 0.133, 0.33, 0.037, 0.329, 0.28, 0.191, 0.045, 0.179]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=169716)[0m top1: 0.34095149253731344
[2m[36m(func pid=169716)[0m top5: 0.8521455223880597
[2m[36m(func pid=169716)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=169716)[0m f1_macro: 0.3251226679181961
[2m[36m(func pid=169716)[0m f1_weighted: 0.36561375041710636
[2m[36m(func pid=169716)[0m f1_per_class: [0.317, 0.275, 0.611, 0.405, 0.072, 0.404, 0.405, 0.325, 0.176, 0.26]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=188206)[0m top1: 0.1730410447761194
[2m[36m(func pid=188206)[0m top5: 0.5237873134328358
[2m[36m(func pid=188206)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=188206)[0m f1_macro: 0.12346794006842635
[2m[36m(func pid=188206)[0m f1_weighted: 0.1463148492668175
[2m[36m(func pid=188206)[0m f1_per_class: [0.242, 0.275, 0.067, 0.141, 0.01, 0.323, 0.045, 0.049, 0.0, 0.082]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=174617)[0m top1: 0.3362873134328358
[2m[36m(func pid=174617)[0m top5: 0.8549440298507462
[2m[36m(func pid=174617)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=174617)[0m f1_macro: 0.3097349510030807
[2m[36m(func pid=174617)[0m f1_weighted: 0.3687740354277447
[2m[36m(func pid=174617)[0m f1_per_class: [0.282, 0.252, 0.733, 0.407, 0.074, 0.313, 0.488, 0.21, 0.184, 0.154]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.1738 | Steps: 2 | Val loss: 1.8498 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8410 | Steps: 2 | Val loss: 2.3114 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.5129 | Steps: 2 | Val loss: 2.1827 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0063 | Steps: 2 | Val loss: 3.7883 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 02:04:59 (running for 00:31:19.87)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.003 |      0.325 |                   85 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.008 |      0.31  |                   65 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.481 |      0.188 |                   36 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  2.841 |      0.136 |                    6 |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.17397388059701493
[2m[36m(func pid=188206)[0m top5: 0.5345149253731343
[2m[36m(func pid=188206)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=188206)[0m f1_macro: 0.1355658006910287
[2m[36m(func pid=188206)[0m f1_weighted: 0.1591785490524655
[2m[36m(func pid=188206)[0m f1_per_class: [0.242, 0.263, 0.082, 0.147, 0.01, 0.348, 0.071, 0.095, 0.0, 0.099]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=169716)[0m top1: 0.3292910447761194
[2m[36m(func pid=169716)[0m top5: 0.8493470149253731
[2m[36m(func pid=169716)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=169716)[0m f1_macro: 0.31381997878605566
[2m[36m(func pid=169716)[0m f1_weighted: 0.3550698469092987
[2m[36m(func pid=169716)[0m f1_per_class: [0.295, 0.272, 0.611, 0.398, 0.064, 0.37, 0.396, 0.323, 0.154, 0.256]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=181342)[0m top1: 0.23274253731343283
[2m[36m(func pid=181342)[0m top5: 0.7276119402985075
[2m[36m(func pid=181342)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=181342)[0m f1_macro: 0.18368192799369143
[2m[36m(func pid=181342)[0m f1_weighted: 0.2575748720344464
[2m[36m(func pid=181342)[0m f1_per_class: [0.19, 0.146, 0.124, 0.326, 0.037, 0.323, 0.287, 0.184, 0.047, 0.174]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m top1: 0.34328358208955223
[2m[36m(func pid=174617)[0m top5: 0.851679104477612
[2m[36m(func pid=174617)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=174617)[0m f1_macro: 0.31123461540071296
[2m[36m(func pid=174617)[0m f1_weighted: 0.37371968160163926
[2m[36m(func pid=174617)[0m f1_per_class: [0.274, 0.256, 0.733, 0.417, 0.081, 0.302, 0.496, 0.217, 0.182, 0.153]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7566 | Steps: 2 | Val loss: 2.3002 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 1.0975 | Steps: 2 | Val loss: 1.8505 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.4744 | Steps: 2 | Val loss: 2.1802 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0024 | Steps: 2 | Val loss: 3.7705 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 02:05:04 (running for 00:31:24.91)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.174 |      0.314 |                   86 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.006 |      0.311 |                   66 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.513 |      0.184 |                   37 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  2.757 |      0.148 |                    7 |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.17723880597014927
[2m[36m(func pid=188206)[0m top5: 0.550839552238806
[2m[36m(func pid=188206)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=188206)[0m f1_macro: 0.14758290688500925
[2m[36m(func pid=188206)[0m f1_weighted: 0.17145266011218527
[2m[36m(func pid=188206)[0m f1_per_class: [0.237, 0.253, 0.093, 0.179, 0.02, 0.337, 0.077, 0.16, 0.0, 0.119]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=169716)[0m top1: 0.32509328358208955
[2m[36m(func pid=169716)[0m top5: 0.8507462686567164
[2m[36m(func pid=169716)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=169716)[0m f1_macro: 0.307460069502412
[2m[36m(func pid=169716)[0m f1_weighted: 0.35000509600128676
[2m[36m(func pid=169716)[0m f1_per_class: [0.287, 0.27, 0.595, 0.396, 0.064, 0.381, 0.383, 0.317, 0.129, 0.253]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=181342)[0m top1: 0.23647388059701493
[2m[36m(func pid=181342)[0m top5: 0.7252798507462687
[2m[36m(func pid=181342)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=181342)[0m f1_macro: 0.18841234685641858
[2m[36m(func pid=181342)[0m f1_weighted: 0.2615798232855103
[2m[36m(func pid=181342)[0m f1_per_class: [0.188, 0.151, 0.12, 0.325, 0.037, 0.331, 0.291, 0.204, 0.047, 0.19]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m top1: 0.3474813432835821
[2m[36m(func pid=174617)[0m top5: 0.8530783582089553
[2m[36m(func pid=174617)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=174617)[0m f1_macro: 0.30754031023903133
[2m[36m(func pid=174617)[0m f1_weighted: 0.37700244642117864
[2m[36m(func pid=174617)[0m f1_per_class: [0.262, 0.255, 0.71, 0.424, 0.081, 0.292, 0.508, 0.207, 0.181, 0.156]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7556 | Steps: 2 | Val loss: 2.2900 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 1.1146 | Steps: 2 | Val loss: 1.8528 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.4596 | Steps: 2 | Val loss: 2.1770 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0044 | Steps: 2 | Val loss: 3.7848 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 02:05:09 (running for 00:31:30.03)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.097 |      0.307 |                   87 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.002 |      0.308 |                   67 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.474 |      0.188 |                   38 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  2.756 |      0.156 |                    8 |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.17863805970149255
[2m[36m(func pid=188206)[0m top5: 0.5760261194029851
[2m[36m(func pid=188206)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=188206)[0m f1_macro: 0.15627358200917163
[2m[36m(func pid=188206)[0m f1_weighted: 0.18079427402867837
[2m[36m(func pid=188206)[0m f1_per_class: [0.233, 0.222, 0.089, 0.204, 0.029, 0.331, 0.094, 0.21, 0.02, 0.13]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=169716)[0m top1: 0.32742537313432835
[2m[36m(func pid=169716)[0m top5: 0.8526119402985075
[2m[36m(func pid=169716)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=169716)[0m f1_macro: 0.30638742212567893
[2m[36m(func pid=169716)[0m f1_weighted: 0.3533178980490373
[2m[36m(func pid=169716)[0m f1_per_class: [0.292, 0.273, 0.564, 0.397, 0.064, 0.389, 0.388, 0.316, 0.133, 0.247]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=181342)[0m top1: 0.240205223880597
[2m[36m(func pid=181342)[0m top5: 0.7248134328358209
[2m[36m(func pid=181342)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=181342)[0m f1_macro: 0.19284473709673683
[2m[36m(func pid=181342)[0m f1_weighted: 0.26375318657317004
[2m[36m(func pid=181342)[0m f1_per_class: [0.202, 0.157, 0.109, 0.336, 0.039, 0.328, 0.28, 0.213, 0.061, 0.202]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m top1: 0.3474813432835821
[2m[36m(func pid=174617)[0m top5: 0.8484141791044776
[2m[36m(func pid=174617)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=174617)[0m f1_macro: 0.30848874256521674
[2m[36m(func pid=174617)[0m f1_weighted: 0.3772383472794146
[2m[36m(func pid=174617)[0m f1_per_class: [0.251, 0.245, 0.733, 0.433, 0.08, 0.284, 0.51, 0.197, 0.201, 0.151]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6025 | Steps: 2 | Val loss: 2.2702 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.9990 | Steps: 2 | Val loss: 1.8450 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.3810 | Steps: 2 | Val loss: 2.1694 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0046 | Steps: 2 | Val loss: 3.8002 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=188206)[0m top1: 0.18516791044776118
[2m[36m(func pid=188206)[0m top5: 0.6072761194029851
[2m[36m(func pid=188206)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=188206)[0m f1_macro: 0.16546888598894835
[2m[36m(func pid=188206)[0m f1_weighted: 0.1934532389534574
[2m[36m(func pid=188206)[0m f1_per_class: [0.23, 0.215, 0.07, 0.223, 0.039, 0.338, 0.114, 0.227, 0.032, 0.165]
[2m[36m(func pid=188206)[0m 
== Status ==
Current time: 2024-01-07 02:05:14 (running for 00:31:35.10)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.115 |      0.306 |                   88 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.004 |      0.308 |                   68 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.46  |      0.193 |                   39 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  2.603 |      0.165 |                    9 |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.3344216417910448
[2m[36m(func pid=169716)[0m top5: 0.8549440298507462
[2m[36m(func pid=169716)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=169716)[0m f1_macro: 0.3200564288537275
[2m[36m(func pid=169716)[0m f1_weighted: 0.3590868673160931
[2m[36m(func pid=169716)[0m f1_per_class: [0.305, 0.278, 0.611, 0.401, 0.066, 0.393, 0.393, 0.306, 0.189, 0.258]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=181342)[0m top1: 0.24440298507462688
[2m[36m(func pid=181342)[0m top5: 0.7332089552238806
[2m[36m(func pid=181342)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=181342)[0m f1_macro: 0.1952663679897762
[2m[36m(func pid=181342)[0m f1_weighted: 0.26810800043580774
[2m[36m(func pid=181342)[0m f1_per_class: [0.202, 0.151, 0.11, 0.351, 0.038, 0.327, 0.285, 0.213, 0.065, 0.212]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m top1: 0.3460820895522388
[2m[36m(func pid=174617)[0m top5: 0.8465485074626866
[2m[36m(func pid=174617)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=174617)[0m f1_macro: 0.30784200873217127
[2m[36m(func pid=174617)[0m f1_weighted: 0.375326410050964
[2m[36m(func pid=174617)[0m f1_per_class: [0.243, 0.242, 0.733, 0.429, 0.078, 0.283, 0.509, 0.197, 0.211, 0.154]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.5757 | Steps: 2 | Val loss: 2.2422 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 1.0029 | Steps: 2 | Val loss: 1.8513 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.4123 | Steps: 2 | Val loss: 2.1624 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0179 | Steps: 2 | Val loss: 3.8157 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=188206)[0m top1: 0.19496268656716417
[2m[36m(func pid=188206)[0m top5: 0.6529850746268657
[2m[36m(func pid=188206)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=188206)[0m f1_macro: 0.1751939866240191
[2m[36m(func pid=188206)[0m f1_weighted: 0.20917938929447855
[2m[36m(func pid=188206)[0m f1_per_class: [0.25, 0.179, 0.067, 0.255, 0.042, 0.342, 0.152, 0.23, 0.034, 0.2]
[2m[36m(func pid=188206)[0m 
== Status ==
Current time: 2024-01-07 02:05:20 (running for 00:31:41.42)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.003 |      0.317 |                   90 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.308 |                   69 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.381 |      0.195 |                   40 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  2.576 |      0.175 |                   10 |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.3306902985074627
[2m[36m(func pid=169716)[0m top5: 0.8521455223880597
[2m[36m(func pid=169716)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=169716)[0m f1_macro: 0.31698036567173804
[2m[36m(func pid=169716)[0m f1_weighted: 0.3557486470675071
[2m[36m(func pid=169716)[0m f1_per_class: [0.302, 0.273, 0.611, 0.396, 0.065, 0.393, 0.389, 0.316, 0.178, 0.247]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=181342)[0m top1: 0.24440298507462688
[2m[36m(func pid=181342)[0m top5: 0.7402052238805971
[2m[36m(func pid=181342)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=181342)[0m f1_macro: 0.19482118864050796
[2m[36m(func pid=181342)[0m f1_weighted: 0.2672090740807251
[2m[36m(func pid=181342)[0m f1_per_class: [0.204, 0.149, 0.106, 0.351, 0.04, 0.315, 0.287, 0.213, 0.067, 0.216]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m top1: 0.34375
[2m[36m(func pid=174617)[0m top5: 0.8460820895522388
[2m[36m(func pid=174617)[0m f1_micro: 0.34375
[2m[36m(func pid=174617)[0m f1_macro: 0.30358583572812037
[2m[36m(func pid=174617)[0m f1_weighted: 0.3730464472046314
[2m[36m(func pid=174617)[0m f1_per_class: [0.23, 0.24, 0.71, 0.423, 0.08, 0.283, 0.509, 0.201, 0.206, 0.155]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.4641 | Steps: 2 | Val loss: 2.2168 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.9740 | Steps: 2 | Val loss: 1.8436 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.3344 | Steps: 2 | Val loss: 2.1575 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=188206)[0m top1: 0.20009328358208955
[2m[36m(func pid=188206)[0m top5: 0.6833022388059702
[2m[36m(func pid=188206)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=188206)[0m f1_macro: 0.1796878364261474
[2m[36m(func pid=188206)[0m f1_weighted: 0.21546525941783648
[2m[36m(func pid=188206)[0m f1_per_class: [0.24, 0.154, 0.061, 0.274, 0.043, 0.34, 0.167, 0.251, 0.026, 0.241]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0023 | Steps: 2 | Val loss: 3.8591 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 02:05:26 (running for 00:31:46.88)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.003 |      0.317 |                   90 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.018 |      0.304 |                   70 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.334 |      0.2   |                   42 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  2.464 |      0.18  |                   11 |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.33488805970149255
[2m[36m(func pid=169716)[0m top5: 0.8544776119402985
[2m[36m(func pid=169716)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=169716)[0m f1_macro: 0.319847021541618
[2m[36m(func pid=169716)[0m f1_weighted: 0.3590903756608785
[2m[36m(func pid=169716)[0m f1_per_class: [0.307, 0.282, 0.611, 0.402, 0.067, 0.39, 0.39, 0.316, 0.178, 0.256]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=181342)[0m top1: 0.24813432835820895
[2m[36m(func pid=181342)[0m top5: 0.7411380597014925
[2m[36m(func pid=181342)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=181342)[0m f1_macro: 0.20005642338091936
[2m[36m(func pid=181342)[0m f1_weighted: 0.27062980587571317
[2m[36m(func pid=181342)[0m f1_per_class: [0.22, 0.154, 0.11, 0.349, 0.039, 0.312, 0.294, 0.219, 0.092, 0.213]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m top1: 0.33908582089552236
[2m[36m(func pid=174617)[0m top5: 0.8442164179104478
[2m[36m(func pid=174617)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=174617)[0m f1_macro: 0.30134162095212136
[2m[36m(func pid=174617)[0m f1_weighted: 0.36894705012032225
[2m[36m(func pid=174617)[0m f1_per_class: [0.237, 0.239, 0.71, 0.404, 0.08, 0.287, 0.513, 0.203, 0.186, 0.155]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.4224 | Steps: 2 | Val loss: 2.1951 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.3010 | Steps: 2 | Val loss: 2.1514 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 1.0203 | Steps: 2 | Val loss: 1.8384 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=188206)[0m top1: 0.20569029850746268
[2m[36m(func pid=188206)[0m top5: 0.7098880597014925
[2m[36m(func pid=188206)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=188206)[0m f1_macro: 0.18574020639869188
[2m[36m(func pid=188206)[0m f1_weighted: 0.22139357467244067
[2m[36m(func pid=188206)[0m f1_per_class: [0.244, 0.14, 0.058, 0.286, 0.055, 0.314, 0.189, 0.263, 0.029, 0.28]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0049 | Steps: 2 | Val loss: 3.8988 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 02:05:31 (running for 00:31:52.29)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.974 |      0.32  |                   91 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.002 |      0.301 |                   71 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.301 |      0.198 |                   43 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  2.422 |      0.186 |                   12 |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.34095149253731344
[2m[36m(func pid=169716)[0m top5: 0.8568097014925373
[2m[36m(func pid=169716)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=169716)[0m f1_macro: 0.32146118541393054
[2m[36m(func pid=169716)[0m f1_weighted: 0.3656670357104756
[2m[36m(func pid=169716)[0m f1_per_class: [0.318, 0.282, 0.611, 0.413, 0.069, 0.397, 0.401, 0.309, 0.164, 0.25]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=181342)[0m top1: 0.24813432835820895
[2m[36m(func pid=181342)[0m top5: 0.7444029850746269
[2m[36m(func pid=181342)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=181342)[0m f1_macro: 0.19759253018842457
[2m[36m(func pid=181342)[0m f1_weighted: 0.27040542765072195
[2m[36m(func pid=181342)[0m f1_per_class: [0.229, 0.154, 0.107, 0.353, 0.04, 0.314, 0.29, 0.231, 0.057, 0.201]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.2968 | Steps: 2 | Val loss: 2.1692 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=174617)[0m top1: 0.33722014925373134
[2m[36m(func pid=174617)[0m top5: 0.8414179104477612
[2m[36m(func pid=174617)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=174617)[0m f1_macro: 0.3030861206244385
[2m[36m(func pid=174617)[0m f1_weighted: 0.36756375693625404
[2m[36m(func pid=174617)[0m f1_per_class: [0.237, 0.239, 0.733, 0.395, 0.079, 0.29, 0.514, 0.207, 0.183, 0.153]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.9606 | Steps: 2 | Val loss: 1.8374 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.2815 | Steps: 2 | Val loss: 2.1464 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=188206)[0m top1: 0.21921641791044777
[2m[36m(func pid=188206)[0m top5: 0.7332089552238806
[2m[36m(func pid=188206)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=188206)[0m f1_macro: 0.19452699624854902
[2m[36m(func pid=188206)[0m f1_weighted: 0.2350734814636373
[2m[36m(func pid=188206)[0m f1_per_class: [0.241, 0.14, 0.06, 0.313, 0.055, 0.319, 0.203, 0.278, 0.033, 0.302]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0086 | Steps: 2 | Val loss: 3.9102 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 02:05:36 (running for 00:31:57.69)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.02  |      0.321 |                   92 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.303 |                   72 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.282 |      0.201 |                   44 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  2.297 |      0.195 |                   13 |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.341884328358209
[2m[36m(func pid=169716)[0m top5: 0.8577425373134329
[2m[36m(func pid=169716)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=169716)[0m f1_macro: 0.3265121251304829
[2m[36m(func pid=169716)[0m f1_weighted: 0.3667385696090923
[2m[36m(func pid=169716)[0m f1_per_class: [0.323, 0.278, 0.629, 0.412, 0.068, 0.394, 0.405, 0.307, 0.2, 0.25]
[2m[36m(func pid=181342)[0m top1: 0.25326492537313433
[2m[36m(func pid=181342)[0m top5: 0.7453358208955224
[2m[36m(func pid=181342)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=181342)[0m f1_macro: 0.2012872251942826
[2m[36m(func pid=181342)[0m f1_weighted: 0.2750131067004723
[2m[36m(func pid=181342)[0m f1_per_class: [0.219, 0.156, 0.108, 0.352, 0.052, 0.312, 0.304, 0.236, 0.06, 0.212]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.2029 | Steps: 2 | Val loss: 2.1389 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=174617)[0m top1: 0.33675373134328357
[2m[36m(func pid=174617)[0m top5: 0.8376865671641791
[2m[36m(func pid=174617)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=174617)[0m f1_macro: 0.301945275853827
[2m[36m(func pid=174617)[0m f1_weighted: 0.3668639911318667
[2m[36m(func pid=174617)[0m f1_per_class: [0.236, 0.25, 0.71, 0.389, 0.08, 0.286, 0.512, 0.204, 0.197, 0.156]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.9955 | Steps: 2 | Val loss: 1.8362 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=188206)[0m top1: 0.23600746268656717
[2m[36m(func pid=188206)[0m top5: 0.7560634328358209
[2m[36m(func pid=188206)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=188206)[0m f1_macro: 0.19941393621508494
[2m[36m(func pid=188206)[0m f1_weighted: 0.250984526010065
[2m[36m(func pid=188206)[0m f1_per_class: [0.234, 0.15, 0.068, 0.347, 0.056, 0.322, 0.22, 0.28, 0.019, 0.298]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.2743 | Steps: 2 | Val loss: 2.1395 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0191 | Steps: 2 | Val loss: 3.9596 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 02:05:42 (running for 00:32:03.19)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.996 |      0.327 |                   94 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.009 |      0.302 |                   73 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.282 |      0.201 |                   44 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  2.203 |      0.199 |                   14 |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.3423507462686567
[2m[36m(func pid=169716)[0m top5: 0.8577425373134329
[2m[36m(func pid=169716)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=169716)[0m f1_macro: 0.32692403831837835
[2m[36m(func pid=169716)[0m f1_weighted: 0.36685675184261546
[2m[36m(func pid=169716)[0m f1_per_class: [0.332, 0.276, 0.629, 0.415, 0.068, 0.396, 0.403, 0.307, 0.197, 0.247]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=181342)[0m top1: 0.25699626865671643
[2m[36m(func pid=181342)[0m top5: 0.7486007462686567
[2m[36m(func pid=181342)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=181342)[0m f1_macro: 0.20728563379531045
[2m[36m(func pid=181342)[0m f1_weighted: 0.2761339502364443
[2m[36m(func pid=181342)[0m f1_per_class: [0.227, 0.153, 0.112, 0.359, 0.06, 0.307, 0.299, 0.252, 0.063, 0.24]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.1775 | Steps: 2 | Val loss: 2.1107 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=174617)[0m top1: 0.32882462686567165
[2m[36m(func pid=174617)[0m top5: 0.8353544776119403
[2m[36m(func pid=174617)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=174617)[0m f1_macro: 0.29805546317789056
[2m[36m(func pid=174617)[0m f1_weighted: 0.3595471029848379
[2m[36m(func pid=174617)[0m f1_per_class: [0.236, 0.247, 0.71, 0.373, 0.079, 0.289, 0.506, 0.2, 0.187, 0.154]
[2m[36m(func pid=174617)[0m 
[2m[36m(func pid=188206)[0m top1: 0.25279850746268656
[2m[36m(func pid=188206)[0m top5: 0.7709888059701493
[2m[36m(func pid=188206)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=188206)[0m f1_macro: 0.21441598854020202
[2m[36m(func pid=188206)[0m f1_weighted: 0.26605848406030824
[2m[36m(func pid=188206)[0m f1_per_class: [0.253, 0.163, 0.072, 0.363, 0.055, 0.332, 0.236, 0.302, 0.022, 0.344]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.9030 | Steps: 2 | Val loss: 1.8379 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.2574 | Steps: 2 | Val loss: 2.1310 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=174617)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0054 | Steps: 2 | Val loss: 3.9799 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.1300 | Steps: 2 | Val loss: 2.0846 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 02:05:47 (running for 00:32:08.69)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.318
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.903 |      0.328 |                   95 |
| train_66d79_00015 | RUNNING    | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.019 |      0.298 |                   74 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.274 |      0.207 |                   45 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  2.177 |      0.214 |                   15 |
| train_66d79_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.34095149253731344
[2m[36m(func pid=169716)[0m top5: 0.8572761194029851
[2m[36m(func pid=169716)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=169716)[0m f1_macro: 0.32791242211695043
[2m[36m(func pid=169716)[0m f1_weighted: 0.36546905533436097
[2m[36m(func pid=169716)[0m f1_per_class: [0.33, 0.276, 0.629, 0.413, 0.065, 0.399, 0.397, 0.307, 0.215, 0.249]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=181342)[0m top1: 0.26119402985074625
[2m[36m(func pid=181342)[0m top5: 0.7565298507462687
[2m[36m(func pid=181342)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=181342)[0m f1_macro: 0.2088113376957897
[2m[36m(func pid=181342)[0m f1_weighted: 0.28007967339095086
[2m[36m(func pid=181342)[0m f1_per_class: [0.229, 0.148, 0.114, 0.369, 0.061, 0.307, 0.306, 0.256, 0.064, 0.235]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=174617)[0m top1: 0.32509328358208955
[2m[36m(func pid=174617)[0m top5: 0.8353544776119403
[2m[36m(func pid=174617)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=174617)[0m f1_macro: 0.2963521249306019
[2m[36m(func pid=174617)[0m f1_weighted: 0.35576293081358834
[2m[36m(func pid=174617)[0m f1_per_class: [0.235, 0.248, 0.688, 0.361, 0.079, 0.299, 0.498, 0.207, 0.192, 0.158]
[2m[36m(func pid=188206)[0m top1: 0.259794776119403
[2m[36m(func pid=188206)[0m top5: 0.7821828358208955
[2m[36m(func pid=188206)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=188206)[0m f1_macro: 0.21647618971417887
[2m[36m(func pid=188206)[0m f1_weighted: 0.27234912916766696
[2m[36m(func pid=188206)[0m f1_per_class: [0.253, 0.156, 0.086, 0.371, 0.052, 0.326, 0.256, 0.309, 0.024, 0.333]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 1.0358 | Steps: 2 | Val loss: 1.8375 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.2566 | Steps: 2 | Val loss: 2.1236 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.9421 | Steps: 2 | Val loss: 2.0576 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=169716)[0m top1: 0.3362873134328358
[2m[36m(func pid=169716)[0m top5: 0.8582089552238806
[2m[36m(func pid=169716)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=169716)[0m f1_macro: 0.32762288874156054
[2m[36m(func pid=169716)[0m f1_weighted: 0.36078716274460554
[2m[36m(func pid=169716)[0m f1_per_class: [0.33, 0.266, 0.647, 0.408, 0.062, 0.403, 0.391, 0.303, 0.218, 0.249]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=181342)[0m top1: 0.26492537313432835
[2m[36m(func pid=181342)[0m top5: 0.7593283582089553
[2m[36m(func pid=181342)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=181342)[0m f1_macro: 0.21137429570868854
[2m[36m(func pid=181342)[0m f1_weighted: 0.28362355753683316
[2m[36m(func pid=181342)[0m f1_per_class: [0.227, 0.149, 0.117, 0.377, 0.061, 0.302, 0.31, 0.256, 0.081, 0.235]
[2m[36m(func pid=188206)[0m top1: 0.2667910447761194
[2m[36m(func pid=188206)[0m top5: 0.792910447761194
[2m[36m(func pid=188206)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=188206)[0m f1_macro: 0.22071366942441104
[2m[36m(func pid=188206)[0m f1_weighted: 0.28085760443881086
[2m[36m(func pid=188206)[0m f1_per_class: [0.242, 0.166, 0.1, 0.369, 0.061, 0.321, 0.283, 0.306, 0.024, 0.336]
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.9611 | Steps: 2 | Val loss: 1.8414 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 02:05:53 (running for 00:32:14.21)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  1.036 |      0.328 |                   96 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.257 |      0.209 |                   46 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  2.13  |      0.216 |                   16 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=4309)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=4309)[0m Configuration completed!
[2m[36m(func pid=4309)[0m New optimizer parameters:
[2m[36m(func pid=4309)[0m SGD (
[2m[36m(func pid=4309)[0m Parameter Group 0
[2m[36m(func pid=4309)[0m     dampening: 0
[2m[36m(func pid=4309)[0m     differentiable: False
[2m[36m(func pid=4309)[0m     foreach: None
[2m[36m(func pid=4309)[0m     lr: 0.01
[2m[36m(func pid=4309)[0m     maximize: False
[2m[36m(func pid=4309)[0m     momentum: 0.99
[2m[36m(func pid=4309)[0m     nesterov: False
[2m[36m(func pid=4309)[0m     weight_decay: 1e-05
[2m[36m(func pid=4309)[0m )
[2m[36m(func pid=4309)[0m 
== Status ==
Current time: 2024-01-07 02:05:58 (running for 00:32:19.49)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.961 |      0.324 |                   97 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.257 |      0.211 |                   47 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  1.942 |      0.221 |                   17 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.33722014925373134
[2m[36m(func pid=169716)[0m top5: 0.8563432835820896
[2m[36m(func pid=169716)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=169716)[0m f1_macro: 0.3235038878545654
[2m[36m(func pid=169716)[0m f1_weighted: 0.3629816121703384
[2m[36m(func pid=169716)[0m f1_per_class: [0.33, 0.263, 0.611, 0.41, 0.063, 0.393, 0.402, 0.309, 0.212, 0.242]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.8517 | Steps: 2 | Val loss: 2.0232 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.1927 | Steps: 2 | Val loss: 2.1175 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.9618 | Steps: 2 | Val loss: 1.8414 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0029 | Steps: 2 | Val loss: 2.3154 | Batch size: 32 | lr: 0.01 | Duration: 4.53s
[2m[36m(func pid=188206)[0m top1: 0.2896455223880597
[2m[36m(func pid=188206)[0m top5: 0.8092350746268657
[2m[36m(func pid=188206)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=188206)[0m f1_macro: 0.2371696204543936
[2m[36m(func pid=188206)[0m f1_weighted: 0.30601942244031305
[2m[36m(func pid=188206)[0m f1_per_class: [0.239, 0.182, 0.129, 0.389, 0.063, 0.335, 0.325, 0.327, 0.072, 0.311]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=181342)[0m top1: 0.26865671641791045
[2m[36m(func pid=181342)[0m top5: 0.7635261194029851
[2m[36m(func pid=181342)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=181342)[0m f1_macro: 0.21097693555439329
[2m[36m(func pid=181342)[0m f1_weighted: 0.28667035903023885
[2m[36m(func pid=181342)[0m f1_per_class: [0.23, 0.15, 0.116, 0.384, 0.062, 0.304, 0.315, 0.259, 0.051, 0.239]
[2m[36m(func pid=181342)[0m 
== Status ==
Current time: 2024-01-07 02:06:04 (running for 00:32:24.82)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.962 |      0.321 |                   98 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.193 |      0.211 |                   48 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  1.852 |      0.237 |                   18 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.3381529850746269
[2m[36m(func pid=169716)[0m top5: 0.8577425373134329
[2m[36m(func pid=169716)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=169716)[0m f1_macro: 0.32109763385535117
[2m[36m(func pid=169716)[0m f1_weighted: 0.3636589501186353
[2m[36m(func pid=169716)[0m f1_per_class: [0.332, 0.268, 0.595, 0.412, 0.065, 0.388, 0.404, 0.297, 0.211, 0.241]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=4309)[0m top1: 0.1837686567164179
[2m[36m(func pid=4309)[0m top5: 0.5335820895522388
[2m[36m(func pid=4309)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=4309)[0m f1_macro: 0.13023098763118415
[2m[36m(func pid=4309)[0m f1_weighted: 0.13400696828866152
[2m[36m(func pid=4309)[0m f1_per_class: [0.378, 0.349, 0.0, 0.11, 0.0, 0.206, 0.027, 0.027, 0.0, 0.205]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.8429 | Steps: 2 | Val loss: 1.9957 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.1882 | Steps: 2 | Val loss: 2.1071 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.9648 | Steps: 2 | Val loss: 1.8375 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8357 | Steps: 2 | Val loss: 2.2961 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=188206)[0m top1: 0.29990671641791045
[2m[36m(func pid=188206)[0m top5: 0.8166977611940298
[2m[36m(func pid=188206)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=188206)[0m f1_macro: 0.2465646290128522
[2m[36m(func pid=188206)[0m f1_weighted: 0.3181271068588656
[2m[36m(func pid=188206)[0m f1_per_class: [0.241, 0.212, 0.147, 0.381, 0.063, 0.332, 0.353, 0.335, 0.092, 0.311]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=181342)[0m top1: 0.28031716417910446
[2m[36m(func pid=181342)[0m top5: 0.769589552238806
[2m[36m(func pid=181342)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=181342)[0m f1_macro: 0.2207125790080609
[2m[36m(func pid=181342)[0m f1_weighted: 0.298023686696963
[2m[36m(func pid=181342)[0m f1_per_class: [0.244, 0.164, 0.122, 0.397, 0.061, 0.298, 0.329, 0.266, 0.072, 0.254]
[2m[36m(func pid=181342)[0m 
== Status ==
Current time: 2024-01-07 02:06:09 (running for 00:32:30.01)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00013 | RUNNING    | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.965 |      0.323 |                   99 |
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.188 |      0.221 |                   49 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  1.843 |      0.247 |                   19 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  3.003 |      0.13  |                    1 |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.33955223880597013
[2m[36m(func pid=169716)[0m top5: 0.8577425373134329
[2m[36m(func pid=169716)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=169716)[0m f1_macro: 0.3231324310262986
[2m[36m(func pid=169716)[0m f1_weighted: 0.3643624830293369
[2m[36m(func pid=169716)[0m f1_per_class: [0.341, 0.269, 0.595, 0.413, 0.065, 0.388, 0.404, 0.297, 0.213, 0.247]
[2m[36m(func pid=169716)[0m 
[2m[36m(func pid=4309)[0m top1: 0.20289179104477612
[2m[36m(func pid=4309)[0m top5: 0.5447761194029851
[2m[36m(func pid=4309)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=4309)[0m f1_macro: 0.1696854373781565
[2m[36m(func pid=4309)[0m f1_weighted: 0.16708514949668424
[2m[36m(func pid=4309)[0m f1_per_class: [0.37, 0.332, 0.11, 0.166, 0.015, 0.287, 0.029, 0.199, 0.0, 0.189]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.7212 | Steps: 2 | Val loss: 1.9674 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.1442 | Steps: 2 | Val loss: 2.1059 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=169716)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.8851 | Steps: 2 | Val loss: 1.8444 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=188206)[0m top1: 0.3138992537313433
[2m[36m(func pid=188206)[0m top5: 0.8264925373134329
[2m[36m(func pid=188206)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=188206)[0m f1_macro: 0.25433934264474295
[2m[36m(func pid=188206)[0m f1_weighted: 0.33232702263655
[2m[36m(func pid=188206)[0m f1_per_class: [0.237, 0.222, 0.177, 0.392, 0.066, 0.337, 0.385, 0.319, 0.095, 0.313]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.6945 | Steps: 2 | Val loss: 2.2548 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=181342)[0m top1: 0.28031716417910446
[2m[36m(func pid=181342)[0m top5: 0.769589552238806
[2m[36m(func pid=181342)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=181342)[0m f1_macro: 0.22281886248971583
[2m[36m(func pid=181342)[0m f1_weighted: 0.29899990489581163
[2m[36m(func pid=181342)[0m f1_per_class: [0.238, 0.165, 0.124, 0.396, 0.06, 0.304, 0.329, 0.265, 0.089, 0.259]
[2m[36m(func pid=181342)[0m 
== Status ==
Current time: 2024-01-07 02:06:14 (running for 00:32:35.34)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 3 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.144 |      0.223 |                   50 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  1.721 |      0.254 |                   20 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  2.836 |      0.17  |                    2 |
| train_66d79_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=169716)[0m top1: 0.33722014925373134
[2m[36m(func pid=169716)[0m top5: 0.8558768656716418
[2m[36m(func pid=169716)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=169716)[0m f1_macro: 0.32621330354333083
[2m[36m(func pid=169716)[0m f1_weighted: 0.36276473274933624
[2m[36m(func pid=169716)[0m f1_per_class: [0.343, 0.266, 0.629, 0.411, 0.062, 0.389, 0.398, 0.309, 0.211, 0.245]
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.5778 | Steps: 2 | Val loss: 1.9383 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=4309)[0m top1: 0.197294776119403
[2m[36m(func pid=4309)[0m top5: 0.6030783582089553
[2m[36m(func pid=4309)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=4309)[0m f1_macro: 0.1772934362029733
[2m[36m(func pid=4309)[0m f1_weighted: 0.18428117624187448
[2m[36m(func pid=4309)[0m f1_per_class: [0.304, 0.275, 0.081, 0.206, 0.042, 0.313, 0.06, 0.274, 0.025, 0.194]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.1267 | Steps: 2 | Val loss: 2.1039 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=188206)[0m top1: 0.32276119402985076
[2m[36m(func pid=188206)[0m top5: 0.8334888059701493
[2m[36m(func pid=188206)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=188206)[0m f1_macro: 0.264995006168354
[2m[36m(func pid=188206)[0m f1_weighted: 0.3414733201934467
[2m[36m(func pid=188206)[0m f1_per_class: [0.25, 0.245, 0.21, 0.385, 0.065, 0.36, 0.394, 0.342, 0.095, 0.305]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=181342)[0m top1: 0.27798507462686567
[2m[36m(func pid=181342)[0m top5: 0.7691231343283582
[2m[36m(func pid=181342)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=181342)[0m f1_macro: 0.22378433932115555
[2m[36m(func pid=181342)[0m f1_weighted: 0.2969854563155119
[2m[36m(func pid=181342)[0m f1_per_class: [0.238, 0.16, 0.128, 0.392, 0.056, 0.301, 0.328, 0.267, 0.106, 0.263]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.3857 | Steps: 2 | Val loss: 2.1950 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.4812 | Steps: 2 | Val loss: 1.9080 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=4309)[0m top1: 0.20569029850746268
[2m[36m(func pid=4309)[0m top5: 0.6777052238805971
[2m[36m(func pid=4309)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=4309)[0m f1_macro: 0.18457069507499244
[2m[36m(func pid=4309)[0m f1_weighted: 0.19985889381700558
[2m[36m(func pid=4309)[0m f1_per_class: [0.234, 0.153, 0.065, 0.283, 0.092, 0.326, 0.094, 0.32, 0.078, 0.201]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.1254 | Steps: 2 | Val loss: 2.0982 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 02:06:21 (running for 00:32:41.91)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.127 |      0.224 |                   51 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  1.578 |      0.265 |                   21 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  2.386 |      0.185 |                    4 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.3283582089552239
[2m[36m(func pid=188206)[0m top5: 0.8414179104477612
[2m[36m(func pid=188206)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=188206)[0m f1_macro: 0.2740489978296696
[2m[36m(func pid=188206)[0m f1_weighted: 0.34594108947281765
[2m[36m(func pid=188206)[0m f1_per_class: [0.263, 0.252, 0.262, 0.38, 0.066, 0.355, 0.409, 0.332, 0.112, 0.309]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=5547)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=5547)[0m Configuration completed!
[2m[36m(func pid=5547)[0m New optimizer parameters:
[2m[36m(func pid=5547)[0m SGD (
[2m[36m(func pid=5547)[0m Parameter Group 0
[2m[36m(func pid=5547)[0m     dampening: 0
[2m[36m(func pid=5547)[0m     differentiable: False
[2m[36m(func pid=5547)[0m     foreach: None
[2m[36m(func pid=5547)[0m     lr: 0.1
[2m[36m(func pid=5547)[0m     maximize: False
[2m[36m(func pid=5547)[0m     momentum: 0.99
[2m[36m(func pid=5547)[0m     nesterov: False
[2m[36m(func pid=5547)[0m     weight_decay: 1e-05
[2m[36m(func pid=5547)[0m )
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=181342)[0m top1: 0.2826492537313433
[2m[36m(func pid=181342)[0m top5: 0.7728544776119403
[2m[36m(func pid=181342)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=181342)[0m f1_macro: 0.2273385366659008
[2m[36m(func pid=181342)[0m f1_weighted: 0.3021115211930903
[2m[36m(func pid=181342)[0m f1_per_class: [0.238, 0.161, 0.129, 0.396, 0.056, 0.311, 0.335, 0.275, 0.108, 0.265]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.0461 | Steps: 2 | Val loss: 2.1080 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.4064 | Steps: 2 | Val loss: 1.8831 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 02:06:26 (running for 00:32:47.46)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.125 |      0.227 |                   52 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  1.481 |      0.274 |                   22 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  2.046 |      0.207 |                    5 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.0467 | Steps: 2 | Val loss: 2.0907 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=4309)[0m top1: 0.2490671641791045
[2m[36m(func pid=4309)[0m top5: 0.7364738805970149
[2m[36m(func pid=4309)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=4309)[0m f1_macro: 0.20749514982239053
[2m[36m(func pid=4309)[0m f1_weighted: 0.23170880753850806
[2m[36m(func pid=4309)[0m f1_per_class: [0.224, 0.067, 0.095, 0.377, 0.079, 0.335, 0.148, 0.348, 0.086, 0.313]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0731 | Steps: 2 | Val loss: 2.2962 | Batch size: 32 | lr: 0.1 | Duration: 4.35s
[2m[36m(func pid=188206)[0m top1: 0.33255597014925375
[2m[36m(func pid=188206)[0m top5: 0.8493470149253731
[2m[36m(func pid=188206)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=188206)[0m f1_macro: 0.28006398508471053
[2m[36m(func pid=188206)[0m f1_weighted: 0.3501986713392984
[2m[36m(func pid=188206)[0m f1_per_class: [0.285, 0.253, 0.306, 0.375, 0.063, 0.362, 0.425, 0.327, 0.108, 0.299]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=181342)[0m top1: 0.28544776119402987
[2m[36m(func pid=181342)[0m top5: 0.7770522388059702
[2m[36m(func pid=181342)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=181342)[0m f1_macro: 0.22816473730859443
[2m[36m(func pid=181342)[0m f1_weighted: 0.3051396917179196
[2m[36m(func pid=181342)[0m f1_per_class: [0.238, 0.163, 0.134, 0.404, 0.054, 0.307, 0.339, 0.269, 0.107, 0.267]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.8001 | Steps: 2 | Val loss: 2.0035 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=5547)[0m top1: 0.13013059701492538
[2m[36m(func pid=5547)[0m top5: 0.5694962686567164
[2m[36m(func pid=5547)[0m f1_micro: 0.13013059701492538
[2m[36m(func pid=5547)[0m f1_macro: 0.10987259308583992
[2m[36m(func pid=5547)[0m f1_weighted: 0.11382298427157377
[2m[36m(func pid=5547)[0m f1_per_class: [0.11, 0.199, 0.092, 0.174, 0.0, 0.046, 0.024, 0.226, 0.0, 0.227]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.3907 | Steps: 2 | Val loss: 1.8674 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.0389 | Steps: 2 | Val loss: 2.0857 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 02:06:32 (running for 00:32:53.02)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.047 |      0.228 |                   53 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  1.406 |      0.28  |                   23 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  1.8   |      0.232 |                    6 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  3.073 |      0.11  |                    1 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=4309)[0m top1: 0.29151119402985076
[2m[36m(func pid=4309)[0m top5: 0.7779850746268657
[2m[36m(func pid=4309)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=4309)[0m f1_macro: 0.23199744469777883
[2m[36m(func pid=4309)[0m f1_weighted: 0.2763364960915849
[2m[36m(func pid=4309)[0m f1_per_class: [0.252, 0.046, 0.164, 0.437, 0.074, 0.366, 0.243, 0.321, 0.105, 0.313]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.2682 | Steps: 2 | Val loss: 2.3009 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=188206)[0m top1: 0.33302238805970147
[2m[36m(func pid=188206)[0m top5: 0.8535447761194029
[2m[36m(func pid=188206)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=188206)[0m f1_macro: 0.2841762545946815
[2m[36m(func pid=188206)[0m f1_weighted: 0.3508649454952528
[2m[36m(func pid=188206)[0m f1_per_class: [0.282, 0.253, 0.344, 0.365, 0.061, 0.387, 0.428, 0.315, 0.106, 0.301]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=181342)[0m top1: 0.28544776119402987
[2m[36m(func pid=181342)[0m top5: 0.7789179104477612
[2m[36m(func pid=181342)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=181342)[0m f1_macro: 0.2272669441139238
[2m[36m(func pid=181342)[0m f1_weighted: 0.30269910796606164
[2m[36m(func pid=181342)[0m f1_per_class: [0.237, 0.143, 0.142, 0.41, 0.057, 0.309, 0.336, 0.272, 0.107, 0.261]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.4742 | Steps: 2 | Val loss: 1.8974 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=5547)[0m top1: 0.1609141791044776
[2m[36m(func pid=5547)[0m top5: 0.6478544776119403
[2m[36m(func pid=5547)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=5547)[0m f1_macro: 0.1336263873329334
[2m[36m(func pid=5547)[0m f1_weighted: 0.11524963340358588
[2m[36m(func pid=5547)[0m f1_per_class: [0.101, 0.0, 0.153, 0.267, 0.062, 0.099, 0.003, 0.371, 0.019, 0.261]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.2999 | Steps: 2 | Val loss: 1.8444 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.0456 | Steps: 2 | Val loss: 2.0841 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 02:06:37 (running for 00:32:58.58)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.039 |      0.227 |                   54 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  1.391 |      0.284 |                   24 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  1.474 |      0.266 |                    7 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  2.268 |      0.134 |                    2 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=4309)[0m top1: 0.3260261194029851
[2m[36m(func pid=4309)[0m top5: 0.8171641791044776
[2m[36m(func pid=4309)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=4309)[0m f1_macro: 0.2663768063868182
[2m[36m(func pid=4309)[0m f1_weighted: 0.3156346199324464
[2m[36m(func pid=4309)[0m f1_per_class: [0.304, 0.113, 0.279, 0.484, 0.082, 0.388, 0.274, 0.339, 0.111, 0.29]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m top1: 0.3400186567164179
[2m[36m(func pid=188206)[0m top5: 0.8577425373134329
[2m[36m(func pid=188206)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=188206)[0m f1_macro: 0.2941192866259993
[2m[36m(func pid=188206)[0m f1_weighted: 0.3571208811764353
[2m[36m(func pid=188206)[0m f1_per_class: [0.297, 0.262, 0.4, 0.361, 0.072, 0.398, 0.441, 0.322, 0.101, 0.288]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.3287 | Steps: 2 | Val loss: 2.2740 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=181342)[0m top1: 0.28031716417910446
[2m[36m(func pid=181342)[0m top5: 0.7784514925373134
[2m[36m(func pid=181342)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=181342)[0m f1_macro: 0.22671914711427016
[2m[36m(func pid=181342)[0m f1_weighted: 0.29844858813575287
[2m[36m(func pid=181342)[0m f1_per_class: [0.232, 0.156, 0.144, 0.4, 0.051, 0.304, 0.324, 0.286, 0.093, 0.277]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=5547)[0m top1: 0.2126865671641791
[2m[36m(func pid=5547)[0m top5: 0.8055037313432836
[2m[36m(func pid=5547)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=5547)[0m f1_macro: 0.24860164343909158
[2m[36m(func pid=5547)[0m f1_weighted: 0.19734544805239512
[2m[36m(func pid=5547)[0m f1_per_class: [0.174, 0.128, 0.558, 0.302, 0.137, 0.333, 0.062, 0.333, 0.078, 0.38]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.1970 | Steps: 2 | Val loss: 1.8271 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.1640 | Steps: 2 | Val loss: 1.8374 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.0975 | Steps: 2 | Val loss: 2.0761 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 02:06:43 (running for 00:33:03.77)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.046 |      0.227 |                   55 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  1.197 |      0.304 |                   26 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  1.474 |      0.266 |                    7 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  1.329 |      0.249 |                    3 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.34328358208955223
[2m[36m(func pid=188206)[0m top5: 0.8638059701492538
[2m[36m(func pid=188206)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=188206)[0m f1_macro: 0.3036836469970687
[2m[36m(func pid=188206)[0m f1_weighted: 0.36052669798217934
[2m[36m(func pid=188206)[0m f1_per_class: [0.299, 0.264, 0.468, 0.37, 0.082, 0.399, 0.441, 0.31, 0.117, 0.288]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m top1: 0.3358208955223881
[2m[36m(func pid=4309)[0m top5: 0.8507462686567164
[2m[36m(func pid=4309)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=4309)[0m f1_macro: 0.2982019516245105
[2m[36m(func pid=4309)[0m f1_weighted: 0.3443443747896205
[2m[36m(func pid=4309)[0m f1_per_class: [0.302, 0.188, 0.436, 0.474, 0.073, 0.389, 0.326, 0.351, 0.146, 0.296]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6485 | Steps: 2 | Val loss: 2.6188 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=181342)[0m top1: 0.28404850746268656
[2m[36m(func pid=181342)[0m top5: 0.7807835820895522
[2m[36m(func pid=181342)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=181342)[0m f1_macro: 0.22754665325401402
[2m[36m(func pid=181342)[0m f1_weighted: 0.30206711641552125
[2m[36m(func pid=181342)[0m f1_per_class: [0.245, 0.156, 0.142, 0.406, 0.051, 0.297, 0.332, 0.285, 0.099, 0.262]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.1340 | Steps: 2 | Val loss: 1.8180 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=5547)[0m top1: 0.19542910447761194
[2m[36m(func pid=5547)[0m top5: 0.8316231343283582
[2m[36m(func pid=5547)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=5547)[0m f1_macro: 0.2566431811003225
[2m[36m(func pid=5547)[0m f1_weighted: 0.20115083197241787
[2m[36m(func pid=5547)[0m f1_per_class: [0.217, 0.212, 0.75, 0.179, 0.138, 0.384, 0.158, 0.129, 0.072, 0.327]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.0540 | Steps: 2 | Val loss: 1.8264 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.0092 | Steps: 2 | Val loss: 2.0691 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 02:06:48 (running for 00:33:09.00)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.098 |      0.228 |                   56 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  1.134 |      0.314 |                   27 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  1.164 |      0.298 |                    8 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.648 |      0.257 |                    4 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.34375
[2m[36m(func pid=188206)[0m top5: 0.8680037313432836
[2m[36m(func pid=188206)[0m f1_micro: 0.34375
[2m[36m(func pid=188206)[0m f1_macro: 0.31448556668608046
[2m[36m(func pid=188206)[0m f1_weighted: 0.3616251323186645
[2m[36m(func pid=188206)[0m f1_per_class: [0.304, 0.267, 0.55, 0.37, 0.086, 0.397, 0.439, 0.33, 0.117, 0.286]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m top1: 0.3358208955223881
[2m[36m(func pid=4309)[0m top5: 0.8726679104477612
[2m[36m(func pid=4309)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=4309)[0m f1_macro: 0.3083563351851792
[2m[36m(func pid=4309)[0m f1_weighted: 0.35955728171164186
[2m[36m(func pid=4309)[0m f1_per_class: [0.28, 0.236, 0.558, 0.46, 0.062, 0.37, 0.374, 0.336, 0.141, 0.266]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5126 | Steps: 2 | Val loss: 2.9434 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=181342)[0m top1: 0.2868470149253731
[2m[36m(func pid=181342)[0m top5: 0.7835820895522388
[2m[36m(func pid=181342)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=181342)[0m f1_macro: 0.22866181417941273
[2m[36m(func pid=181342)[0m f1_weighted: 0.3054752462940336
[2m[36m(func pid=181342)[0m f1_per_class: [0.244, 0.156, 0.144, 0.412, 0.051, 0.303, 0.337, 0.28, 0.099, 0.261]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.0392 | Steps: 2 | Val loss: 1.8103 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=5547)[0m top1: 0.23507462686567165
[2m[36m(func pid=5547)[0m top5: 0.820429104477612
[2m[36m(func pid=5547)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=5547)[0m f1_macro: 0.2781588417468721
[2m[36m(func pid=5547)[0m f1_weighted: 0.2544931992770526
[2m[36m(func pid=5547)[0m f1_per_class: [0.186, 0.204, 0.8, 0.223, 0.11, 0.395, 0.279, 0.229, 0.097, 0.259]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.9543 | Steps: 2 | Val loss: 1.8371 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 02:06:53 (running for 00:33:14.10)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  2.009 |      0.229 |                   57 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  1.039 |      0.318 |                   28 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  1.054 |      0.308 |                    9 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.513 |      0.278 |                    5 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.33908582089552236
[2m[36m(func pid=188206)[0m top5: 0.8689365671641791
[2m[36m(func pid=188206)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=188206)[0m f1_macro: 0.3179778229853138
[2m[36m(func pid=188206)[0m f1_weighted: 0.3574365015376092
[2m[36m(func pid=188206)[0m f1_per_class: [0.307, 0.267, 0.595, 0.363, 0.083, 0.404, 0.427, 0.325, 0.13, 0.28]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.9858 | Steps: 2 | Val loss: 2.0625 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=4309)[0m top1: 0.33861940298507465
[2m[36m(func pid=4309)[0m top5: 0.8833955223880597
[2m[36m(func pid=4309)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=4309)[0m f1_macro: 0.32331499442161593
[2m[36m(func pid=4309)[0m f1_weighted: 0.36726528181735
[2m[36m(func pid=4309)[0m f1_per_class: [0.261, 0.258, 0.706, 0.451, 0.059, 0.361, 0.397, 0.339, 0.141, 0.26]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.2836 | Steps: 2 | Val loss: 3.3162 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=181342)[0m top1: 0.2891791044776119
[2m[36m(func pid=181342)[0m top5: 0.7859141791044776
[2m[36m(func pid=181342)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=181342)[0m f1_macro: 0.23056746454064353
[2m[36m(func pid=181342)[0m f1_weighted: 0.30817291609050423
[2m[36m(func pid=181342)[0m f1_per_class: [0.25, 0.148, 0.147, 0.411, 0.052, 0.314, 0.346, 0.285, 0.096, 0.257]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.9594 | Steps: 2 | Val loss: 1.7994 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=5547)[0m top1: 0.28031716417910446
[2m[36m(func pid=5547)[0m top5: 0.7971082089552238
[2m[36m(func pid=5547)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=5547)[0m f1_macro: 0.29475754280761124
[2m[36m(func pid=5547)[0m f1_weighted: 0.30299939095737477
[2m[36m(func pid=5547)[0m f1_per_class: [0.166, 0.174, 0.774, 0.355, 0.111, 0.407, 0.318, 0.296, 0.126, 0.221]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8009 | Steps: 2 | Val loss: 1.8711 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 02:06:58 (running for 00:33:19.15)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.986 |      0.231 |                   58 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.959 |      0.322 |                   29 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.954 |      0.323 |                   10 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.284 |      0.295 |                    6 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.3381529850746269
[2m[36m(func pid=188206)[0m top5: 0.8763992537313433
[2m[36m(func pid=188206)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=188206)[0m f1_macro: 0.3217859544786513
[2m[36m(func pid=188206)[0m f1_weighted: 0.3567464042587172
[2m[36m(func pid=188206)[0m f1_per_class: [0.311, 0.272, 0.629, 0.363, 0.08, 0.403, 0.423, 0.306, 0.136, 0.295]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.9430 | Steps: 2 | Val loss: 2.0559 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=4309)[0m top1: 0.33861940298507465
[2m[36m(func pid=4309)[0m top5: 0.8889925373134329
[2m[36m(func pid=4309)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=4309)[0m f1_macro: 0.3297269487159481
[2m[36m(func pid=4309)[0m f1_weighted: 0.3678055653310698
[2m[36m(func pid=4309)[0m f1_per_class: [0.26, 0.282, 0.75, 0.425, 0.066, 0.383, 0.401, 0.338, 0.146, 0.247]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.1462 | Steps: 2 | Val loss: 3.9377 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.9618 | Steps: 2 | Val loss: 1.7938 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=181342)[0m top1: 0.29011194029850745
[2m[36m(func pid=181342)[0m top5: 0.7882462686567164
[2m[36m(func pid=181342)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=181342)[0m f1_macro: 0.2303341194057258
[2m[36m(func pid=181342)[0m f1_weighted: 0.3086275844731893
[2m[36m(func pid=181342)[0m f1_per_class: [0.243, 0.151, 0.155, 0.411, 0.054, 0.305, 0.349, 0.289, 0.096, 0.25]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=5547)[0m top1: 0.2849813432835821
[2m[36m(func pid=5547)[0m top5: 0.7765858208955224
[2m[36m(func pid=5547)[0m f1_micro: 0.2849813432835821
[2m[36m(func pid=5547)[0m f1_macro: 0.2922460638073479
[2m[36m(func pid=5547)[0m f1_weighted: 0.3001075207866236
[2m[36m(func pid=5547)[0m f1_per_class: [0.16, 0.149, 0.727, 0.39, 0.109, 0.395, 0.281, 0.363, 0.145, 0.203]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5088 | Steps: 2 | Val loss: 1.9268 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 02:07:03 (running for 00:33:24.25)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.943 |      0.23  |                   59 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.962 |      0.328 |                   30 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.801 |      0.33  |                   11 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.146 |      0.292 |                    7 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.34375
[2m[36m(func pid=188206)[0m top5: 0.8810634328358209
[2m[36m(func pid=188206)[0m f1_micro: 0.34375
[2m[36m(func pid=188206)[0m f1_macro: 0.3280527074630293
[2m[36m(func pid=188206)[0m f1_weighted: 0.36296503809116765
[2m[36m(func pid=188206)[0m f1_per_class: [0.322, 0.278, 0.629, 0.372, 0.08, 0.407, 0.427, 0.305, 0.16, 0.301]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.9499 | Steps: 2 | Val loss: 2.0511 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3109 | Steps: 2 | Val loss: 4.3155 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=4309)[0m top1: 0.345615671641791
[2m[36m(func pid=4309)[0m top5: 0.8917910447761194
[2m[36m(func pid=4309)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=4309)[0m f1_macro: 0.3384675517964505
[2m[36m(func pid=4309)[0m f1_weighted: 0.37176339810851206
[2m[36m(func pid=4309)[0m f1_per_class: [0.277, 0.319, 0.774, 0.414, 0.065, 0.385, 0.399, 0.332, 0.168, 0.253]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.8714 | Steps: 2 | Val loss: 1.7976 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=181342)[0m top1: 0.29011194029850745
[2m[36m(func pid=181342)[0m top5: 0.7901119402985075
[2m[36m(func pid=181342)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=181342)[0m f1_macro: 0.2347484478217073
[2m[36m(func pid=181342)[0m f1_weighted: 0.3083488292334203
[2m[36m(func pid=181342)[0m f1_per_class: [0.252, 0.166, 0.152, 0.408, 0.054, 0.305, 0.337, 0.296, 0.118, 0.259]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=5547)[0m top1: 0.29757462686567165
[2m[36m(func pid=5547)[0m top5: 0.7905783582089553
[2m[36m(func pid=5547)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=5547)[0m f1_macro: 0.2976646574403973
[2m[36m(func pid=5547)[0m f1_weighted: 0.3167448974782144
[2m[36m(func pid=5547)[0m f1_per_class: [0.181, 0.149, 0.75, 0.406, 0.106, 0.359, 0.338, 0.345, 0.152, 0.191]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5717 | Steps: 2 | Val loss: 1.9947 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 02:07:08 (running for 00:33:29.31)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.95  |      0.235 |                   60 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.871 |      0.339 |                   31 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.509 |      0.338 |                   12 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.311 |      0.298 |                    8 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.3451492537313433
[2m[36m(func pid=188206)[0m top5: 0.8889925373134329
[2m[36m(func pid=188206)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=188206)[0m f1_macro: 0.33901516836486534
[2m[36m(func pid=188206)[0m f1_weighted: 0.3646256867098803
[2m[36m(func pid=188206)[0m f1_per_class: [0.337, 0.284, 0.71, 0.371, 0.079, 0.411, 0.426, 0.305, 0.172, 0.297]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.8521 | Steps: 2 | Val loss: 2.0455 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.0665 | Steps: 2 | Val loss: 4.5185 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=4309)[0m top1: 0.34888059701492535
[2m[36m(func pid=4309)[0m top5: 0.8922574626865671
[2m[36m(func pid=4309)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=4309)[0m f1_macro: 0.33395273907656225
[2m[36m(func pid=4309)[0m f1_weighted: 0.37571837583771744
[2m[36m(func pid=4309)[0m f1_per_class: [0.278, 0.325, 0.733, 0.412, 0.065, 0.384, 0.415, 0.316, 0.169, 0.243]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.8438 | Steps: 2 | Val loss: 1.8026 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=181342)[0m top1: 0.292910447761194
[2m[36m(func pid=181342)[0m top5: 0.7933768656716418
[2m[36m(func pid=181342)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=181342)[0m f1_macro: 0.23776978664958975
[2m[36m(func pid=181342)[0m f1_weighted: 0.30952167321893703
[2m[36m(func pid=181342)[0m f1_per_class: [0.257, 0.167, 0.161, 0.412, 0.054, 0.299, 0.336, 0.306, 0.121, 0.265]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=5547)[0m top1: 0.33675373134328357
[2m[36m(func pid=5547)[0m top5: 0.8115671641791045
[2m[36m(func pid=5547)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=5547)[0m f1_macro: 0.31864185862412797
[2m[36m(func pid=5547)[0m f1_weighted: 0.36324244093540803
[2m[36m(func pid=5547)[0m f1_per_class: [0.262, 0.175, 0.774, 0.438, 0.1, 0.356, 0.454, 0.292, 0.144, 0.19]
[2m[36m(func pid=5547)[0m 
== Status ==
Current time: 2024-01-07 02:07:13 (running for 00:33:34.50)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.852 |      0.238 |                   61 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.844 |      0.338 |                   32 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.572 |      0.334 |                   13 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.066 |      0.319 |                    9 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.34281716417910446
[2m[36m(func pid=188206)[0m top5: 0.8889925373134329
[2m[36m(func pid=188206)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=188206)[0m f1_macro: 0.33756722576855347
[2m[36m(func pid=188206)[0m f1_weighted: 0.36285319827353657
[2m[36m(func pid=188206)[0m f1_per_class: [0.337, 0.286, 0.71, 0.368, 0.077, 0.414, 0.421, 0.304, 0.167, 0.293]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4681 | Steps: 2 | Val loss: 2.0783 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.9738 | Steps: 2 | Val loss: 2.0414 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.1776 | Steps: 2 | Val loss: 4.8702 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=4309)[0m top1: 0.35634328358208955
[2m[36m(func pid=4309)[0m top5: 0.8903917910447762
[2m[36m(func pid=4309)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=4309)[0m f1_macro: 0.34095581719767815
[2m[36m(func pid=4309)[0m f1_weighted: 0.3821975347383672
[2m[36m(func pid=4309)[0m f1_per_class: [0.282, 0.333, 0.759, 0.411, 0.074, 0.399, 0.426, 0.319, 0.169, 0.239]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7898 | Steps: 2 | Val loss: 1.8119 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=181342)[0m top1: 0.292910447761194
[2m[36m(func pid=181342)[0m top5: 0.7961753731343284
[2m[36m(func pid=181342)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=181342)[0m f1_macro: 0.23948569518868762
[2m[36m(func pid=181342)[0m f1_weighted: 0.3101904600782982
[2m[36m(func pid=181342)[0m f1_per_class: [0.257, 0.169, 0.175, 0.414, 0.052, 0.318, 0.331, 0.297, 0.119, 0.265]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=5547)[0m top1: 0.34888059701492535
[2m[36m(func pid=5547)[0m top5: 0.832089552238806
[2m[36m(func pid=5547)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=5547)[0m f1_macro: 0.3255036176272847
[2m[36m(func pid=5547)[0m f1_weighted: 0.37351356968591876
[2m[36m(func pid=5547)[0m f1_per_class: [0.326, 0.199, 0.8, 0.418, 0.112, 0.324, 0.515, 0.217, 0.138, 0.206]
[2m[36m(func pid=5547)[0m 
== Status ==
Current time: 2024-01-07 02:07:18 (running for 00:33:39.65)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.974 |      0.239 |                   62 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.79  |      0.339 |                   33 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.468 |      0.341 |                   14 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.178 |      0.326 |                   10 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.34421641791044777
[2m[36m(func pid=188206)[0m top5: 0.8913246268656716
[2m[36m(func pid=188206)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=188206)[0m f1_macro: 0.3394768712772293
[2m[36m(func pid=188206)[0m f1_weighted: 0.36620010620267107
[2m[36m(func pid=188206)[0m f1_per_class: [0.348, 0.288, 0.71, 0.376, 0.072, 0.413, 0.424, 0.294, 0.186, 0.286]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5207 | Steps: 2 | Val loss: 2.1851 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.9454 | Steps: 2 | Val loss: 2.0370 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.0582 | Steps: 2 | Val loss: 5.3873 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.7866 | Steps: 2 | Val loss: 1.8144 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=4309)[0m top1: 0.34841417910447764
[2m[36m(func pid=4309)[0m top5: 0.8955223880597015
[2m[36m(func pid=4309)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=4309)[0m f1_macro: 0.3342771402887488
[2m[36m(func pid=4309)[0m f1_weighted: 0.37456808362265165
[2m[36m(func pid=4309)[0m f1_per_class: [0.272, 0.325, 0.759, 0.4, 0.067, 0.405, 0.418, 0.302, 0.168, 0.228]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=181342)[0m top1: 0.292910447761194
[2m[36m(func pid=181342)[0m top5: 0.7966417910447762
[2m[36m(func pid=181342)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=181342)[0m f1_macro: 0.24023616353081695
[2m[36m(func pid=181342)[0m f1_weighted: 0.3116155358090372
[2m[36m(func pid=181342)[0m f1_per_class: [0.254, 0.181, 0.177, 0.41, 0.05, 0.322, 0.33, 0.297, 0.117, 0.264]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=5547)[0m top1: 0.36007462686567165
[2m[36m(func pid=5547)[0m top5: 0.8367537313432836
[2m[36m(func pid=5547)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=5547)[0m f1_macro: 0.3204432199064434
[2m[36m(func pid=5547)[0m f1_weighted: 0.38009983389591523
[2m[36m(func pid=5547)[0m f1_per_class: [0.286, 0.226, 0.8, 0.394, 0.093, 0.287, 0.564, 0.203, 0.151, 0.201]
[2m[36m(func pid=5547)[0m 
== Status ==
Current time: 2024-01-07 02:07:24 (running for 00:33:44.84)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.945 |      0.24  |                   63 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.787 |      0.338 |                   34 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.521 |      0.334 |                   15 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.058 |      0.32  |                   11 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.3498134328358209
[2m[36m(func pid=188206)[0m top5: 0.8936567164179104
[2m[36m(func pid=188206)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=188206)[0m f1_macro: 0.33831239505171157
[2m[36m(func pid=188206)[0m f1_weighted: 0.37233134993685035
[2m[36m(func pid=188206)[0m f1_per_class: [0.36, 0.294, 0.667, 0.384, 0.073, 0.415, 0.43, 0.309, 0.177, 0.273]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3193 | Steps: 2 | Val loss: 2.2821 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.8191 | Steps: 2 | Val loss: 2.0323 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.2724 | Steps: 2 | Val loss: 5.7939 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.7461 | Steps: 2 | Val loss: 1.8284 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=4309)[0m top1: 0.3451492537313433
[2m[36m(func pid=4309)[0m top5: 0.8941231343283582
[2m[36m(func pid=4309)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=4309)[0m f1_macro: 0.33457426426134174
[2m[36m(func pid=4309)[0m f1_weighted: 0.3720486085541316
[2m[36m(func pid=4309)[0m f1_per_class: [0.287, 0.302, 0.759, 0.406, 0.08, 0.403, 0.416, 0.301, 0.188, 0.206]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=181342)[0m top1: 0.29197761194029853
[2m[36m(func pid=181342)[0m top5: 0.7975746268656716
[2m[36m(func pid=181342)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=181342)[0m f1_macro: 0.2405727106903794
[2m[36m(func pid=181342)[0m f1_weighted: 0.30947670764952123
[2m[36m(func pid=181342)[0m f1_per_class: [0.257, 0.185, 0.179, 0.411, 0.05, 0.318, 0.32, 0.298, 0.12, 0.267]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=5547)[0m top1: 0.37033582089552236
[2m[36m(func pid=5547)[0m top5: 0.840018656716418
[2m[36m(func pid=5547)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=5547)[0m f1_macro: 0.32322822971985604
[2m[36m(func pid=5547)[0m f1_weighted: 0.3823661711552085
[2m[36m(func pid=5547)[0m f1_per_class: [0.303, 0.238, 0.8, 0.369, 0.097, 0.278, 0.59, 0.195, 0.17, 0.192]
[2m[36m(func pid=5547)[0m 
== Status ==
Current time: 2024-01-07 02:07:29 (running for 00:33:50.06)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.819 |      0.241 |                   64 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.746 |      0.338 |                   35 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.319 |      0.335 |                   16 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.272 |      0.323 |                   12 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.3474813432835821
[2m[36m(func pid=188206)[0m top5: 0.894589552238806
[2m[36m(func pid=188206)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=188206)[0m f1_macro: 0.33777361216534396
[2m[36m(func pid=188206)[0m f1_weighted: 0.3706637750825291
[2m[36m(func pid=188206)[0m f1_per_class: [0.36, 0.295, 0.688, 0.379, 0.073, 0.414, 0.432, 0.301, 0.169, 0.268]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.2185 | Steps: 2 | Val loss: 2.3751 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.8439 | Steps: 2 | Val loss: 2.0279 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0254 | Steps: 2 | Val loss: 6.2255 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.7640 | Steps: 2 | Val loss: 1.8353 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=4309)[0m top1: 0.3414179104477612
[2m[36m(func pid=4309)[0m top5: 0.8908582089552238
[2m[36m(func pid=4309)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=4309)[0m f1_macro: 0.33248097266299914
[2m[36m(func pid=4309)[0m f1_weighted: 0.3688126824535813
[2m[36m(func pid=4309)[0m f1_per_class: [0.292, 0.278, 0.786, 0.416, 0.071, 0.404, 0.411, 0.288, 0.178, 0.201]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=181342)[0m top1: 0.292910447761194
[2m[36m(func pid=181342)[0m top5: 0.7985074626865671
[2m[36m(func pid=181342)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=181342)[0m f1_macro: 0.24098135194336892
[2m[36m(func pid=181342)[0m f1_weighted: 0.3110313370823128
[2m[36m(func pid=181342)[0m f1_per_class: [0.256, 0.185, 0.179, 0.41, 0.05, 0.307, 0.33, 0.311, 0.118, 0.265]
[2m[36m(func pid=181342)[0m 
== Status ==
Current time: 2024-01-07 02:07:34 (running for 00:33:55.08)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.844 |      0.241 |                   65 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.764 |      0.34  |                   36 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.219 |      0.332 |                   17 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.272 |      0.323 |                   12 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.34841417910447764
[2m[36m(func pid=188206)[0m top5: 0.8936567164179104
[2m[36m(func pid=188206)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=188206)[0m f1_macro: 0.3399207383659363
[2m[36m(func pid=188206)[0m f1_weighted: 0.37268614082827506
[2m[36m(func pid=188206)[0m f1_per_class: [0.361, 0.297, 0.71, 0.382, 0.073, 0.408, 0.436, 0.3, 0.173, 0.259]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3810634328358209
[2m[36m(func pid=5547)[0m top5: 0.84375
[2m[36m(func pid=5547)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=5547)[0m f1_macro: 0.3295707782036739
[2m[36m(func pid=5547)[0m f1_weighted: 0.3895689460070769
[2m[36m(func pid=5547)[0m f1_per_class: [0.326, 0.247, 0.786, 0.379, 0.097, 0.28, 0.596, 0.192, 0.185, 0.209]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2400 | Steps: 2 | Val loss: 2.4697 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.7556 | Steps: 2 | Val loss: 2.0214 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6629 | Steps: 2 | Val loss: 1.8436 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.2128 | Steps: 2 | Val loss: 6.7079 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=4309)[0m top1: 0.34421641791044777
[2m[36m(func pid=4309)[0m top5: 0.8861940298507462
[2m[36m(func pid=4309)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=4309)[0m f1_macro: 0.3353181888952406
[2m[36m(func pid=4309)[0m f1_weighted: 0.37187974169562293
[2m[36m(func pid=4309)[0m f1_per_class: [0.297, 0.275, 0.786, 0.43, 0.082, 0.402, 0.408, 0.295, 0.189, 0.189]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=181342)[0m top1: 0.29757462686567165
[2m[36m(func pid=181342)[0m top5: 0.8022388059701493
[2m[36m(func pid=181342)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=181342)[0m f1_macro: 0.24699765665881293
[2m[36m(func pid=181342)[0m f1_weighted: 0.3160636893451661
[2m[36m(func pid=181342)[0m f1_per_class: [0.253, 0.194, 0.193, 0.414, 0.058, 0.314, 0.332, 0.317, 0.133, 0.262]
[2m[36m(func pid=181342)[0m 
== Status ==
Current time: 2024-01-07 02:07:39 (running for 00:34:00.12)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.756 |      0.247 |                   66 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.663 |      0.341 |                   37 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.24  |      0.335 |                   18 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.025 |      0.33  |                   13 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.34841417910447764
[2m[36m(func pid=188206)[0m top5: 0.894589552238806
[2m[36m(func pid=188206)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=188206)[0m f1_macro: 0.34098496230276
[2m[36m(func pid=188206)[0m f1_weighted: 0.3729824887937635
[2m[36m(func pid=188206)[0m f1_per_class: [0.364, 0.294, 0.71, 0.397, 0.073, 0.408, 0.425, 0.291, 0.189, 0.259]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3666044776119403
[2m[36m(func pid=5547)[0m top5: 0.8470149253731343
[2m[36m(func pid=5547)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=5547)[0m f1_macro: 0.3280167080811465
[2m[36m(func pid=5547)[0m f1_weighted: 0.3805516490095724
[2m[36m(func pid=5547)[0m f1_per_class: [0.293, 0.245, 0.828, 0.372, 0.088, 0.294, 0.569, 0.188, 0.198, 0.207]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4028 | Steps: 2 | Val loss: 2.5514 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.8805 | Steps: 2 | Val loss: 2.0204 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.7842 | Steps: 2 | Val loss: 1.8663 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.1248 | Steps: 2 | Val loss: 7.2445 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=4309)[0m top1: 0.3451492537313433
[2m[36m(func pid=4309)[0m top5: 0.8805970149253731
[2m[36m(func pid=4309)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=4309)[0m f1_macro: 0.3325838998517889
[2m[36m(func pid=4309)[0m f1_weighted: 0.3746768327663585
[2m[36m(func pid=4309)[0m f1_per_class: [0.292, 0.268, 0.786, 0.446, 0.083, 0.395, 0.414, 0.281, 0.184, 0.178]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=181342)[0m top1: 0.29757462686567165
[2m[36m(func pid=181342)[0m top5: 0.7999067164179104
[2m[36m(func pid=181342)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=181342)[0m f1_macro: 0.24639331904122744
[2m[36m(func pid=181342)[0m f1_weighted: 0.31749736221458297
[2m[36m(func pid=181342)[0m f1_per_class: [0.246, 0.204, 0.198, 0.41, 0.058, 0.309, 0.339, 0.307, 0.13, 0.264]
[2m[36m(func pid=181342)[0m 
== Status ==
Current time: 2024-01-07 02:07:44 (running for 00:34:05.25)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.88  |      0.246 |                   67 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.784 |      0.337 |                   38 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.403 |      0.333 |                   19 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.213 |      0.328 |                   14 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.3423507462686567
[2m[36m(func pid=188206)[0m top5: 0.8894589552238806
[2m[36m(func pid=188206)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=188206)[0m f1_macro: 0.3373062119259038
[2m[36m(func pid=188206)[0m f1_weighted: 0.36801443869823963
[2m[36m(func pid=188206)[0m f1_per_class: [0.366, 0.297, 0.71, 0.399, 0.07, 0.39, 0.412, 0.284, 0.201, 0.244]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3619402985074627
[2m[36m(func pid=5547)[0m top5: 0.8470149253731343
[2m[36m(func pid=5547)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=5547)[0m f1_macro: 0.3357991337120995
[2m[36m(func pid=5547)[0m f1_weighted: 0.38078275012028445
[2m[36m(func pid=5547)[0m f1_per_class: [0.281, 0.248, 0.828, 0.362, 0.088, 0.33, 0.549, 0.258, 0.213, 0.203]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.1276 | Steps: 2 | Val loss: 2.6606 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.7016 | Steps: 2 | Val loss: 2.0139 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6224 | Steps: 2 | Val loss: 1.8882 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0022 | Steps: 2 | Val loss: 7.7852 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 02:07:49 (running for 00:34:10.47)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.88  |      0.246 |                   67 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.784 |      0.337 |                   38 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.128 |      0.334 |                   20 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.125 |      0.336 |                   15 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.34328358208955223
[2m[36m(func pid=188206)[0m top5: 0.8894589552238806
[2m[36m(func pid=188206)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=188206)[0m f1_macro: 0.3382801110263177
[2m[36m(func pid=188206)[0m f1_weighted: 0.36949549715696456
[2m[36m(func pid=188206)[0m f1_per_class: [0.364, 0.299, 0.71, 0.401, 0.069, 0.397, 0.411, 0.284, 0.204, 0.244]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m top1: 0.34701492537313433
[2m[36m(func pid=4309)[0m top5: 0.8805970149253731
[2m[36m(func pid=4309)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=4309)[0m f1_macro: 0.33380526019523515
[2m[36m(func pid=4309)[0m f1_weighted: 0.37742920514718675
[2m[36m(func pid=4309)[0m f1_per_class: [0.292, 0.262, 0.786, 0.444, 0.085, 0.402, 0.425, 0.282, 0.191, 0.169]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=181342)[0m top1: 0.2994402985074627
[2m[36m(func pid=181342)[0m top5: 0.8017723880597015
[2m[36m(func pid=181342)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=181342)[0m f1_macro: 0.24768209493058113
[2m[36m(func pid=181342)[0m f1_weighted: 0.3186873416179781
[2m[36m(func pid=181342)[0m f1_per_class: [0.247, 0.201, 0.204, 0.417, 0.05, 0.313, 0.334, 0.318, 0.133, 0.259]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3568097014925373
[2m[36m(func pid=5547)[0m top5: 0.84375
[2m[36m(func pid=5547)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=5547)[0m f1_macro: 0.33701334754067147
[2m[36m(func pid=5547)[0m f1_weighted: 0.37871402919678204
[2m[36m(func pid=5547)[0m f1_per_class: [0.259, 0.24, 0.828, 0.371, 0.102, 0.338, 0.534, 0.269, 0.215, 0.216]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.5071 | Steps: 2 | Val loss: 1.8989 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.1165 | Steps: 2 | Val loss: 2.7624 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.7132 | Steps: 2 | Val loss: 2.0097 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0564 | Steps: 2 | Val loss: 8.3436 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 02:07:55 (running for 00:34:15.79)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.702 |      0.248 |                   68 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.507 |      0.339 |                   40 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.128 |      0.334 |                   20 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.002 |      0.337 |                   16 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.3414179104477612
[2m[36m(func pid=188206)[0m top5: 0.8917910447761194
[2m[36m(func pid=188206)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=188206)[0m f1_macro: 0.3394403724314524
[2m[36m(func pid=188206)[0m f1_weighted: 0.36843856509164724
[2m[36m(func pid=188206)[0m f1_per_class: [0.367, 0.293, 0.733, 0.402, 0.068, 0.396, 0.411, 0.284, 0.199, 0.241]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=181342)[0m top1: 0.2994402985074627
[2m[36m(func pid=181342)[0m top5: 0.8027052238805971
[2m[36m(func pid=181342)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=181342)[0m f1_macro: 0.2502316969186326
[2m[36m(func pid=181342)[0m f1_weighted: 0.31917654319193356
[2m[36m(func pid=181342)[0m f1_per_class: [0.246, 0.211, 0.229, 0.414, 0.049, 0.312, 0.333, 0.316, 0.133, 0.258]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=4309)[0m top1: 0.34701492537313433
[2m[36m(func pid=4309)[0m top5: 0.8754664179104478
[2m[36m(func pid=4309)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=4309)[0m f1_macro: 0.33547240702379727
[2m[36m(func pid=4309)[0m f1_weighted: 0.37560319836076594
[2m[36m(func pid=4309)[0m f1_per_class: [0.302, 0.259, 0.786, 0.444, 0.097, 0.394, 0.423, 0.268, 0.211, 0.172]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.34421641791044777
[2m[36m(func pid=5547)[0m top5: 0.8390858208955224
[2m[36m(func pid=5547)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=5547)[0m f1_macro: 0.32939209122820046
[2m[36m(func pid=5547)[0m f1_weighted: 0.36883449027729154
[2m[36m(func pid=5547)[0m f1_per_class: [0.24, 0.223, 0.828, 0.367, 0.095, 0.348, 0.512, 0.281, 0.195, 0.206]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.5654 | Steps: 2 | Val loss: 1.9090 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.7046 | Steps: 2 | Val loss: 2.0028 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0715 | Steps: 2 | Val loss: 2.8663 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.1216 | Steps: 2 | Val loss: 8.6392 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 02:08:00 (running for 00:34:20.96)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.713 |      0.25  |                   69 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.565 |      0.34  |                   41 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.117 |      0.335 |                   21 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.056 |      0.329 |                   17 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.341884328358209
[2m[36m(func pid=188206)[0m top5: 0.8941231343283582
[2m[36m(func pid=188206)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=188206)[0m f1_macro: 0.34023770009173726
[2m[36m(func pid=188206)[0m f1_weighted: 0.36866871143128344
[2m[36m(func pid=188206)[0m f1_per_class: [0.365, 0.301, 0.733, 0.399, 0.069, 0.394, 0.409, 0.29, 0.197, 0.245]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m top1: 0.34888059701492535
[2m[36m(func pid=4309)[0m top5: 0.8740671641791045
[2m[36m(func pid=4309)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=4309)[0m f1_macro: 0.33649748380915245
[2m[36m(func pid=4309)[0m f1_weighted: 0.37755510698755934
[2m[36m(func pid=4309)[0m f1_per_class: [0.311, 0.263, 0.786, 0.444, 0.099, 0.391, 0.428, 0.267, 0.209, 0.167]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=181342)[0m top1: 0.30550373134328357
[2m[36m(func pid=181342)[0m top5: 0.8073694029850746
[2m[36m(func pid=181342)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=181342)[0m f1_macro: 0.2534768063596196
[2m[36m(func pid=181342)[0m f1_weighted: 0.32566035067422866
[2m[36m(func pid=181342)[0m f1_per_class: [0.245, 0.213, 0.229, 0.415, 0.06, 0.312, 0.352, 0.323, 0.131, 0.255]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3451492537313433
[2m[36m(func pid=5547)[0m top5: 0.8386194029850746
[2m[36m(func pid=5547)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=5547)[0m f1_macro: 0.32720587317296557
[2m[36m(func pid=5547)[0m f1_weighted: 0.37126100213632174
[2m[36m(func pid=5547)[0m f1_per_class: [0.238, 0.222, 0.774, 0.391, 0.099, 0.343, 0.495, 0.307, 0.21, 0.193]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5900 | Steps: 2 | Val loss: 1.9250 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0906 | Steps: 2 | Val loss: 2.9702 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.6533 | Steps: 2 | Val loss: 1.9961 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0088 | Steps: 2 | Val loss: 9.1885 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=188206)[0m top1: 0.3451492537313433
[2m[36m(func pid=188206)[0m top5: 0.8941231343283582
[2m[36m(func pid=188206)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=188206)[0m f1_macro: 0.34331245691335377
[2m[36m(func pid=188206)[0m f1_weighted: 0.37185078884868905
[2m[36m(func pid=188206)[0m f1_per_class: [0.377, 0.305, 0.733, 0.405, 0.068, 0.397, 0.41, 0.283, 0.199, 0.255]
[2m[36m(func pid=188206)[0m 
== Status ==
Current time: 2024-01-07 02:08:05 (running for 00:34:26.14)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.705 |      0.253 |                   70 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.59  |      0.343 |                   42 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.072 |      0.336 |                   22 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.122 |      0.327 |                   18 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.3050373134328358
[2m[36m(func pid=181342)[0m top5: 0.8115671641791045
[2m[36m(func pid=181342)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=181342)[0m f1_macro: 0.25433196135723857
[2m[36m(func pid=181342)[0m f1_weighted: 0.3244678490048354
[2m[36m(func pid=181342)[0m f1_per_class: [0.245, 0.212, 0.244, 0.418, 0.06, 0.311, 0.345, 0.32, 0.135, 0.253]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=4309)[0m top1: 0.3498134328358209
[2m[36m(func pid=4309)[0m top5: 0.8736007462686567
[2m[36m(func pid=4309)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=4309)[0m f1_macro: 0.33711044779616434
[2m[36m(func pid=4309)[0m f1_weighted: 0.37774932001505074
[2m[36m(func pid=4309)[0m f1_per_class: [0.317, 0.265, 0.786, 0.445, 0.102, 0.394, 0.426, 0.262, 0.216, 0.159]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.34048507462686567
[2m[36m(func pid=5547)[0m top5: 0.832089552238806
[2m[36m(func pid=5547)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=5547)[0m f1_macro: 0.3279994739256294
[2m[36m(func pid=5547)[0m f1_weighted: 0.36785127279884317
[2m[36m(func pid=5547)[0m f1_per_class: [0.23, 0.203, 0.8, 0.412, 0.087, 0.329, 0.478, 0.313, 0.218, 0.211]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4811 | Steps: 2 | Val loss: 1.9389 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.6450 | Steps: 2 | Val loss: 1.9886 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2402 | Steps: 2 | Val loss: 3.0719 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0002 | Steps: 2 | Val loss: 9.6671 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=188206)[0m top1: 0.3423507462686567
[2m[36m(func pid=188206)[0m top5: 0.8917910447761194
[2m[36m(func pid=188206)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=188206)[0m f1_macro: 0.3404819671009228
[2m[36m(func pid=188206)[0m f1_weighted: 0.3687965675330814
[2m[36m(func pid=188206)[0m f1_per_class: [0.374, 0.302, 0.733, 0.404, 0.071, 0.39, 0.406, 0.288, 0.192, 0.244]
[2m[36m(func pid=188206)[0m 
== Status ==
Current time: 2024-01-07 02:08:11 (running for 00:34:32.32)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.645 |      0.257 |                   72 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.481 |      0.34  |                   43 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.091 |      0.337 |                   23 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.009 |      0.328 |                   19 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.3064365671641791
[2m[36m(func pid=181342)[0m top5: 0.8115671641791045
[2m[36m(func pid=181342)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=181342)[0m f1_macro: 0.2566796860931092
[2m[36m(func pid=181342)[0m f1_weighted: 0.3255728883085828
[2m[36m(func pid=181342)[0m f1_per_class: [0.251, 0.211, 0.256, 0.42, 0.059, 0.313, 0.346, 0.321, 0.137, 0.253]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=4309)[0m top1: 0.34794776119402987
[2m[36m(func pid=4309)[0m top5: 0.8754664179104478
[2m[36m(func pid=4309)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=4309)[0m f1_macro: 0.3357875638635643
[2m[36m(func pid=4309)[0m f1_weighted: 0.37657385881295663
[2m[36m(func pid=4309)[0m f1_per_class: [0.312, 0.254, 0.786, 0.451, 0.103, 0.393, 0.424, 0.259, 0.215, 0.162]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.333955223880597
[2m[36m(func pid=5547)[0m top5: 0.8334888059701493
[2m[36m(func pid=5547)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=5547)[0m f1_macro: 0.3254025257798111
[2m[36m(func pid=5547)[0m f1_weighted: 0.35868476230933966
[2m[36m(func pid=5547)[0m f1_per_class: [0.232, 0.186, 0.8, 0.429, 0.102, 0.315, 0.447, 0.314, 0.204, 0.225]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4825 | Steps: 2 | Val loss: 1.9469 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.7273 | Steps: 2 | Val loss: 1.9801 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0467 | Steps: 2 | Val loss: 3.1967 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=188206)[0m top1: 0.3414179104477612
[2m[36m(func pid=188206)[0m top5: 0.8941231343283582
[2m[36m(func pid=188206)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=188206)[0m f1_macro: 0.33967643724878654
[2m[36m(func pid=188206)[0m f1_weighted: 0.36708518057705414
[2m[36m(func pid=188206)[0m f1_per_class: [0.377, 0.3, 0.733, 0.402, 0.074, 0.376, 0.407, 0.295, 0.189, 0.244]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0220 | Steps: 2 | Val loss: 10.0957 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 02:08:17 (running for 00:34:37.96)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.727 |      0.257 |                   73 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.482 |      0.34  |                   44 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.24  |      0.336 |                   24 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.325 |                   20 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m top1: 0.30783582089552236
[2m[36m(func pid=181342)[0m top5: 0.8171641791044776
[2m[36m(func pid=181342)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=181342)[0m f1_macro: 0.2570050205524125
[2m[36m(func pid=181342)[0m f1_weighted: 0.32650260439547935
[2m[36m(func pid=181342)[0m f1_per_class: [0.246, 0.213, 0.262, 0.419, 0.062, 0.321, 0.348, 0.31, 0.137, 0.251]
[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=4309)[0m top1: 0.345615671641791
[2m[36m(func pid=4309)[0m top5: 0.8726679104477612
[2m[36m(func pid=4309)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=4309)[0m f1_macro: 0.3355574217351625
[2m[36m(func pid=4309)[0m f1_weighted: 0.374614514299348
[2m[36m(func pid=4309)[0m f1_per_class: [0.315, 0.259, 0.786, 0.447, 0.102, 0.392, 0.418, 0.261, 0.216, 0.16]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4912 | Steps: 2 | Val loss: 1.9629 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=5547)[0m top1: 0.32882462686567165
[2m[36m(func pid=5547)[0m top5: 0.832089552238806
[2m[36m(func pid=5547)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=5547)[0m f1_macro: 0.31378537941142237
[2m[36m(func pid=5547)[0m f1_weighted: 0.35221295593996094
[2m[36m(func pid=5547)[0m f1_per_class: [0.238, 0.177, 0.71, 0.44, 0.091, 0.294, 0.428, 0.315, 0.213, 0.233]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.6583 | Steps: 2 | Val loss: 1.9771 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0399 | Steps: 2 | Val loss: 3.3196 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=188206)[0m top1: 0.3414179104477612
[2m[36m(func pid=188206)[0m top5: 0.894589552238806
[2m[36m(func pid=188206)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=188206)[0m f1_macro: 0.3436914103251455
[2m[36m(func pid=188206)[0m f1_weighted: 0.36832463804698307
[2m[36m(func pid=188206)[0m f1_per_class: [0.377, 0.296, 0.759, 0.405, 0.072, 0.376, 0.406, 0.319, 0.188, 0.24]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0943 | Steps: 2 | Val loss: 10.6241 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=181342)[0m top1: 0.3069029850746269
[2m[36m(func pid=181342)[0m top5: 0.8199626865671642
[2m[36m(func pid=181342)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=181342)[0m f1_macro: 0.2572014869843101
[2m[36m(func pid=181342)[0m f1_weighted: 0.3262560128404406
[2m[36m(func pid=181342)[0m f1_per_class: [0.24, 0.218, 0.265, 0.418, 0.06, 0.327, 0.342, 0.323, 0.122, 0.257]
== Status ==
Current time: 2024-01-07 02:08:22 (running for 00:34:43.30)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.3165
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00016 | RUNNING    | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.658 |      0.257 |                   74 |
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.491 |      0.344 |                   45 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.047 |      0.336 |                   25 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.022 |      0.314 |                   21 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=181342)[0m 
[2m[36m(func pid=4309)[0m top1: 0.34794776119402987
[2m[36m(func pid=4309)[0m top5: 0.8703358208955224
[2m[36m(func pid=4309)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=4309)[0m f1_macro: 0.3374645411249607
[2m[36m(func pid=4309)[0m f1_weighted: 0.37781622766295303
[2m[36m(func pid=4309)[0m f1_per_class: [0.32, 0.256, 0.786, 0.447, 0.1, 0.402, 0.425, 0.267, 0.215, 0.157]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3755 | Steps: 2 | Val loss: 1.9837 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=5547)[0m top1: 0.3218283582089552
[2m[36m(func pid=5547)[0m top5: 0.8316231343283582
[2m[36m(func pid=5547)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=5547)[0m f1_macro: 0.3152209288271629
[2m[36m(func pid=5547)[0m f1_weighted: 0.3434986141799135
[2m[36m(func pid=5547)[0m f1_per_class: [0.232, 0.174, 0.75, 0.444, 0.082, 0.28, 0.401, 0.322, 0.198, 0.27]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=181342)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.6047 | Steps: 2 | Val loss: 1.9735 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=188206)[0m top1: 0.34095149253731344
[2m[36m(func pid=188206)[0m top5: 0.8922574626865671
[2m[36m(func pid=188206)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=188206)[0m f1_macro: 0.3404359173266658
[2m[36m(func pid=188206)[0m f1_weighted: 0.36789413053869113
[2m[36m(func pid=188206)[0m f1_per_class: [0.369, 0.289, 0.733, 0.409, 0.072, 0.376, 0.405, 0.325, 0.189, 0.237]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0573 | Steps: 2 | Val loss: 3.4402 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0006 | Steps: 2 | Val loss: 11.1432 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=181342)[0m top1: 0.3064365671641791
[2m[36m(func pid=181342)[0m top5: 0.8194962686567164
[2m[36m(func pid=181342)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=181342)[0m f1_macro: 0.25791902005887596
[2m[36m(func pid=181342)[0m f1_weighted: 0.3265717182016706
[2m[36m(func pid=181342)[0m f1_per_class: [0.242, 0.223, 0.275, 0.42, 0.05, 0.318, 0.341, 0.326, 0.12, 0.264]
== Status ==
Current time: 2024-01-07 02:08:28 (running for 00:34:48.83)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 3 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.376 |      0.34  |                   46 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.04  |      0.337 |                   26 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.094 |      0.315 |                   22 |
| train_66d79_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3713 | Steps: 2 | Val loss: 1.9993 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=4309)[0m top1: 0.35027985074626866
[2m[36m(func pid=4309)[0m top5: 0.867070895522388
[2m[36m(func pid=4309)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=4309)[0m f1_macro: 0.34058836725998665
[2m[36m(func pid=4309)[0m f1_weighted: 0.379730270547495
[2m[36m(func pid=4309)[0m f1_per_class: [0.333, 0.26, 0.786, 0.451, 0.104, 0.396, 0.426, 0.264, 0.226, 0.16]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3180970149253731
[2m[36m(func pid=5547)[0m top5: 0.8278917910447762
[2m[36m(func pid=5547)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=5547)[0m f1_macro: 0.31268569980345184
[2m[36m(func pid=5547)[0m f1_weighted: 0.3386961797035486
[2m[36m(func pid=5547)[0m f1_per_class: [0.228, 0.17, 0.727, 0.449, 0.075, 0.277, 0.384, 0.318, 0.192, 0.305]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m top1: 0.33861940298507465
[2m[36m(func pid=188206)[0m top5: 0.8908582089552238
[2m[36m(func pid=188206)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=188206)[0m f1_macro: 0.3408855881146303
[2m[36m(func pid=188206)[0m f1_weighted: 0.36637089796136235
[2m[36m(func pid=188206)[0m f1_per_class: [0.377, 0.286, 0.759, 0.408, 0.062, 0.371, 0.405, 0.323, 0.182, 0.237]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.1794 | Steps: 2 | Val loss: 3.5272 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0010 | Steps: 2 | Val loss: 11.6906 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4397 | Steps: 2 | Val loss: 2.0098 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=4309)[0m top1: 0.353544776119403
[2m[36m(func pid=4309)[0m top5: 0.8680037313432836
[2m[36m(func pid=4309)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=4309)[0m f1_macro: 0.3415224516897458
[2m[36m(func pid=4309)[0m f1_weighted: 0.38445666528017214
[2m[36m(func pid=4309)[0m f1_per_class: [0.32, 0.266, 0.786, 0.451, 0.103, 0.386, 0.44, 0.282, 0.227, 0.156]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3185634328358209
[2m[36m(func pid=5547)[0m top5: 0.8269589552238806
[2m[36m(func pid=5547)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=5547)[0m f1_macro: 0.3089386821586481
[2m[36m(func pid=5547)[0m f1_weighted: 0.3387217164768197
[2m[36m(func pid=5547)[0m f1_per_class: [0.226, 0.172, 0.686, 0.464, 0.07, 0.278, 0.371, 0.313, 0.193, 0.317]
[2m[36m(func pid=188206)[0m top1: 0.33861940298507465
[2m[36m(func pid=188206)[0m top5: 0.8871268656716418
[2m[36m(func pid=188206)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=188206)[0m f1_macro: 0.3426273685882417
[2m[36m(func pid=188206)[0m f1_weighted: 0.36738388678405925
[2m[36m(func pid=188206)[0m f1_per_class: [0.4, 0.281, 0.759, 0.415, 0.066, 0.367, 0.407, 0.307, 0.19, 0.235]
== Status ==
Current time: 2024-01-07 02:08:34 (running for 00:34:54.81)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.371 |      0.341 |                   47 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.179 |      0.342 |                   28 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.001 |      0.313 |                   23 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=11358)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=11358)[0m Configuration completed!
[2m[36m(func pid=11358)[0m New optimizer parameters:
[2m[36m(func pid=11358)[0m SGD (
[2m[36m(func pid=11358)[0m Parameter Group 0
[2m[36m(func pid=11358)[0m     dampening: 0
[2m[36m(func pid=11358)[0m     differentiable: False
[2m[36m(func pid=11358)[0m     foreach: None
[2m[36m(func pid=11358)[0m     lr: 0.0001
[2m[36m(func pid=11358)[0m     maximize: False
[2m[36m(func pid=11358)[0m     momentum: 0.9
[2m[36m(func pid=11358)[0m     nesterov: False
[2m[36m(func pid=11358)[0m     weight_decay: 1e-05
[2m[36m(func pid=11358)[0m )
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.1756 | Steps: 2 | Val loss: 3.6690 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4399 | Steps: 2 | Val loss: 2.0260 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0276 | Steps: 2 | Val loss: 12.1772 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=4309)[0m top1: 0.3498134328358209
[2m[36m(func pid=4309)[0m top5: 0.8642723880597015
[2m[36m(func pid=4309)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=4309)[0m f1_macro: 0.3416245758533735
[2m[36m(func pid=4309)[0m f1_weighted: 0.3804940622807278
[2m[36m(func pid=4309)[0m f1_per_class: [0.343, 0.278, 0.786, 0.44, 0.096, 0.378, 0.432, 0.28, 0.224, 0.16]
[2m[36m(func pid=4309)[0m 
== Status ==
Current time: 2024-01-07 02:08:39 (running for 00:35:00.38)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.44  |      0.343 |                   48 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.176 |      0.342 |                   29 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.001 |      0.309 |                   24 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9527 | Steps: 2 | Val loss: 2.3197 | Batch size: 32 | lr: 0.0001 | Duration: 4.56s
[2m[36m(func pid=188206)[0m top1: 0.3400186567164179
[2m[36m(func pid=188206)[0m top5: 0.8880597014925373
[2m[36m(func pid=188206)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=188206)[0m f1_macro: 0.34244419477716065
[2m[36m(func pid=188206)[0m f1_weighted: 0.3689070326928111
[2m[36m(func pid=188206)[0m f1_per_class: [0.384, 0.282, 0.786, 0.419, 0.07, 0.357, 0.414, 0.308, 0.177, 0.228]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m top1: 0.31576492537313433
[2m[36m(func pid=5547)[0m top5: 0.8260261194029851
[2m[36m(func pid=5547)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=5547)[0m f1_macro: 0.30530252271548386
[2m[36m(func pid=5547)[0m f1_weighted: 0.3335631505154364
[2m[36m(func pid=5547)[0m f1_per_class: [0.222, 0.161, 0.686, 0.472, 0.067, 0.259, 0.36, 0.313, 0.188, 0.325]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0282 | Steps: 2 | Val loss: 3.7930 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=11358)[0m top1: 0.17583955223880596
[2m[36m(func pid=11358)[0m top5: 0.5331156716417911
[2m[36m(func pid=11358)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=11358)[0m f1_macro: 0.1196467235045705
[2m[36m(func pid=11358)[0m f1_weighted: 0.1254390272110013
[2m[36m(func pid=11358)[0m f1_per_class: [0.308, 0.348, 0.0, 0.092, 0.0, 0.211, 0.021, 0.013, 0.0, 0.203]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4550 | Steps: 2 | Val loss: 2.0504 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0023 | Steps: 2 | Val loss: 12.7425 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 02:08:45 (running for 00:35:05.75)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.44  |      0.342 |                   49 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.028 |      0.34  |                   30 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.028 |      0.305 |                   25 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.953 |      0.12  |                    1 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=4309)[0m top1: 0.3460820895522388
[2m[36m(func pid=4309)[0m top5: 0.8680037313432836
[2m[36m(func pid=4309)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=4309)[0m f1_macro: 0.3398703745574049
[2m[36m(func pid=4309)[0m f1_weighted: 0.37783294004654383
[2m[36m(func pid=4309)[0m f1_per_class: [0.335, 0.281, 0.786, 0.424, 0.094, 0.378, 0.437, 0.279, 0.23, 0.156]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9759 | Steps: 2 | Val loss: 2.3230 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=188206)[0m top1: 0.3362873134328358
[2m[36m(func pid=188206)[0m top5: 0.8843283582089553
[2m[36m(func pid=188206)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=188206)[0m f1_macro: 0.33900784865288136
[2m[36m(func pid=188206)[0m f1_weighted: 0.3652797506604733
[2m[36m(func pid=188206)[0m f1_per_class: [0.365, 0.277, 0.786, 0.414, 0.077, 0.359, 0.409, 0.309, 0.178, 0.217]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m top1: 0.30830223880597013
[2m[36m(func pid=5547)[0m top5: 0.8236940298507462
[2m[36m(func pid=5547)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=5547)[0m f1_macro: 0.3020310928816797
[2m[36m(func pid=5547)[0m f1_weighted: 0.32390597771243035
[2m[36m(func pid=5547)[0m f1_per_class: [0.217, 0.139, 0.686, 0.47, 0.066, 0.259, 0.343, 0.297, 0.21, 0.333]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0882 | Steps: 2 | Val loss: 3.8922 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=11358)[0m top1: 0.17957089552238806
[2m[36m(func pid=11358)[0m top5: 0.5303171641791045
[2m[36m(func pid=11358)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=11358)[0m f1_macro: 0.11388387081133469
[2m[36m(func pid=11358)[0m f1_weighted: 0.12830380859726903
[2m[36m(func pid=11358)[0m f1_per_class: [0.263, 0.325, 0.0, 0.101, 0.01, 0.271, 0.015, 0.035, 0.0, 0.12]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3296 | Steps: 2 | Val loss: 2.0737 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0294 | Steps: 2 | Val loss: 13.3772 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 02:08:50 (running for 00:35:11.34)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.455 |      0.339 |                   50 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.088 |      0.336 |                   31 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.002 |      0.302 |                   26 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.976 |      0.114 |                    2 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=4309)[0m top1: 0.341884328358209
[2m[36m(func pid=4309)[0m top5: 0.8689365671641791
[2m[36m(func pid=4309)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=4309)[0m f1_macro: 0.3356456612247249
[2m[36m(func pid=4309)[0m f1_weighted: 0.3739959051985439
[2m[36m(func pid=4309)[0m f1_per_class: [0.333, 0.278, 0.786, 0.421, 0.094, 0.362, 0.438, 0.268, 0.22, 0.157]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9309 | Steps: 2 | Val loss: 2.3326 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=188206)[0m top1: 0.3344216417910448
[2m[36m(func pid=188206)[0m top5: 0.886660447761194
[2m[36m(func pid=188206)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=188206)[0m f1_macro: 0.33611979953087723
[2m[36m(func pid=188206)[0m f1_weighted: 0.3628756247116633
[2m[36m(func pid=188206)[0m f1_per_class: [0.352, 0.279, 0.786, 0.414, 0.072, 0.356, 0.402, 0.306, 0.182, 0.213]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3069029850746269
[2m[36m(func pid=5547)[0m top5: 0.8190298507462687
[2m[36m(func pid=5547)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=5547)[0m f1_macro: 0.2967442993804855
[2m[36m(func pid=5547)[0m f1_weighted: 0.32232379674754913
[2m[36m(func pid=5547)[0m f1_per_class: [0.225, 0.147, 0.632, 0.467, 0.064, 0.256, 0.337, 0.302, 0.197, 0.339]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0487 | Steps: 2 | Val loss: 4.0252 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=11358)[0m top1: 0.17490671641791045
[2m[36m(func pid=11358)[0m top5: 0.519589552238806
[2m[36m(func pid=11358)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=11358)[0m f1_macro: 0.09923139718148963
[2m[36m(func pid=11358)[0m f1_weighted: 0.12551176916648574
[2m[36m(func pid=11358)[0m f1_per_class: [0.154, 0.308, 0.0, 0.11, 0.009, 0.294, 0.009, 0.021, 0.0, 0.087]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4198 | Steps: 2 | Val loss: 2.0819 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.2162 | Steps: 2 | Val loss: 13.8192 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=4309)[0m top1: 0.34095149253731344
[2m[36m(func pid=4309)[0m top5: 0.8684701492537313
[2m[36m(func pid=4309)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=4309)[0m f1_macro: 0.33754253983395505
[2m[36m(func pid=4309)[0m f1_weighted: 0.3730565388486678
[2m[36m(func pid=4309)[0m f1_per_class: [0.337, 0.284, 0.786, 0.41, 0.089, 0.376, 0.434, 0.274, 0.227, 0.158]
== Status ==
Current time: 2024-01-07 02:08:56 (running for 00:35:17.02)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.33  |      0.336 |                   51 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.049 |      0.338 |                   32 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.029 |      0.297 |                   27 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.931 |      0.099 |                    3 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9893 | Steps: 2 | Val loss: 2.3377 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=188206)[0m top1: 0.33955223880597013
[2m[36m(func pid=188206)[0m top5: 0.8861940298507462
[2m[36m(func pid=188206)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=188206)[0m f1_macro: 0.34018600212225
[2m[36m(func pid=188206)[0m f1_weighted: 0.36687955305712827
[2m[36m(func pid=188206)[0m f1_per_class: [0.372, 0.287, 0.786, 0.419, 0.068, 0.353, 0.405, 0.311, 0.184, 0.216]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3087686567164179
[2m[36m(func pid=5547)[0m top5: 0.8185634328358209
[2m[36m(func pid=5547)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=5547)[0m f1_macro: 0.2998558983454306
[2m[36m(func pid=5547)[0m f1_weighted: 0.3217523761195038
[2m[36m(func pid=5547)[0m f1_per_class: [0.223, 0.134, 0.667, 0.473, 0.064, 0.258, 0.336, 0.309, 0.191, 0.345]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0185 | Steps: 2 | Val loss: 4.1601 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=11358)[0m top1: 0.17164179104477612
[2m[36m(func pid=11358)[0m top5: 0.5083955223880597
[2m[36m(func pid=11358)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=11358)[0m f1_macro: 0.09568051246258033
[2m[36m(func pid=11358)[0m f1_weighted: 0.12520598924483098
[2m[36m(func pid=11358)[0m f1_per_class: [0.13, 0.29, 0.0, 0.115, 0.009, 0.304, 0.012, 0.02, 0.0, 0.077]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2945 | Steps: 2 | Val loss: 2.1028 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0001 | Steps: 2 | Val loss: 14.0999 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 02:09:01 (running for 00:35:22.68)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.42  |      0.34  |                   52 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.019 |      0.337 |                   33 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.216 |      0.3   |                   28 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.989 |      0.096 |                    4 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=4309)[0m top1: 0.33908582089552236
[2m[36m(func pid=4309)[0m top5: 0.871268656716418
[2m[36m(func pid=4309)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=4309)[0m f1_macro: 0.33716357341609676
[2m[36m(func pid=4309)[0m f1_weighted: 0.3708551336236923
[2m[36m(func pid=4309)[0m f1_per_class: [0.346, 0.292, 0.786, 0.406, 0.088, 0.373, 0.427, 0.272, 0.224, 0.158]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m top1: 0.33908582089552236
[2m[36m(func pid=188206)[0m top5: 0.8889925373134329
[2m[36m(func pid=188206)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=188206)[0m f1_macro: 0.33775580432284524
[2m[36m(func pid=188206)[0m f1_weighted: 0.3673828542218808
[2m[36m(func pid=188206)[0m f1_per_class: [0.352, 0.28, 0.786, 0.42, 0.067, 0.354, 0.411, 0.311, 0.188, 0.209]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9892 | Steps: 2 | Val loss: 2.3410 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=5547)[0m top1: 0.31576492537313433
[2m[36m(func pid=5547)[0m top5: 0.8199626865671642
[2m[36m(func pid=5547)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=5547)[0m f1_macro: 0.3031578106797073
[2m[36m(func pid=5547)[0m f1_weighted: 0.3290125253176373
[2m[36m(func pid=5547)[0m f1_per_class: [0.221, 0.131, 0.667, 0.477, 0.066, 0.27, 0.353, 0.31, 0.19, 0.348]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m top1: 0.16138059701492538
[2m[36m(func pid=11358)[0m top5: 0.49953358208955223
[2m[36m(func pid=11358)[0m f1_micro: 0.16138059701492538
[2m[36m(func pid=11358)[0m f1_macro: 0.08696324461473182
[2m[36m(func pid=11358)[0m f1_weighted: 0.12116981017322968
[2m[36m(func pid=11358)[0m f1_per_class: [0.113, 0.273, 0.0, 0.109, 0.008, 0.297, 0.021, 0.018, 0.0, 0.031]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3175 | Steps: 2 | Val loss: 2.1200 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.1277 | Steps: 2 | Val loss: 4.3156 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0072 | Steps: 2 | Val loss: 14.3651 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 02:09:07 (running for 00:35:27.87)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.318 |      0.334 |                   54 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.019 |      0.337 |                   33 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.303 |                   29 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.989 |      0.087 |                    5 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.33908582089552236
[2m[36m(func pid=188206)[0m top5: 0.8927238805970149
[2m[36m(func pid=188206)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=188206)[0m f1_macro: 0.33436897727014114
[2m[36m(func pid=188206)[0m f1_weighted: 0.36669893363575917
[2m[36m(func pid=188206)[0m f1_per_class: [0.341, 0.289, 0.759, 0.41, 0.071, 0.353, 0.415, 0.31, 0.186, 0.211]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9748 | Steps: 2 | Val loss: 2.3432 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=4309)[0m top1: 0.332089552238806
[2m[36m(func pid=4309)[0m top5: 0.8708022388059702
[2m[36m(func pid=4309)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=4309)[0m f1_macro: 0.3318410825846081
[2m[36m(func pid=4309)[0m f1_weighted: 0.36376764622345353
[2m[36m(func pid=4309)[0m f1_per_class: [0.346, 0.293, 0.786, 0.393, 0.087, 0.346, 0.427, 0.267, 0.22, 0.154]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3185634328358209
[2m[36m(func pid=5547)[0m top5: 0.820429104477612
[2m[36m(func pid=5547)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=5547)[0m f1_macro: 0.30724292467581854
[2m[36m(func pid=5547)[0m f1_weighted: 0.33129788257601106
[2m[36m(func pid=5547)[0m f1_per_class: [0.22, 0.124, 0.686, 0.477, 0.068, 0.3, 0.355, 0.296, 0.2, 0.348]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m top1: 0.15904850746268656
[2m[36m(func pid=11358)[0m top5: 0.498134328358209
[2m[36m(func pid=11358)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=11358)[0m f1_macro: 0.08639909282428368
[2m[36m(func pid=11358)[0m f1_weighted: 0.12487043765194726
[2m[36m(func pid=11358)[0m f1_per_class: [0.093, 0.252, 0.0, 0.123, 0.008, 0.304, 0.029, 0.027, 0.0, 0.028]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4127 | Steps: 2 | Val loss: 2.1232 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0197 | Steps: 2 | Val loss: 4.4664 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=188206)[0m top1: 0.33955223880597013
[2m[36m(func pid=188206)[0m top5: 0.8922574626865671
[2m[36m(func pid=188206)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=188206)[0m f1_macro: 0.33411293488853533
[2m[36m(func pid=188206)[0m f1_weighted: 0.3667020217177036
[2m[36m(func pid=188206)[0m f1_per_class: [0.349, 0.286, 0.759, 0.41, 0.073, 0.349, 0.418, 0.305, 0.186, 0.205]
[2m[36m(func pid=188206)[0m 
== Status ==
Current time: 2024-01-07 02:09:12 (running for 00:35:33.06)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.413 |      0.334 |                   55 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.128 |      0.332 |                   34 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.007 |      0.307 |                   30 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.975 |      0.086 |                    6 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0479 | Steps: 2 | Val loss: 14.5639 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9496 | Steps: 2 | Val loss: 2.3444 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=4309)[0m top1: 0.32975746268656714
[2m[36m(func pid=4309)[0m top5: 0.8684701492537313
[2m[36m(func pid=4309)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=4309)[0m f1_macro: 0.33186406830173426
[2m[36m(func pid=4309)[0m f1_weighted: 0.36070737571050904
[2m[36m(func pid=4309)[0m f1_per_class: [0.344, 0.295, 0.786, 0.379, 0.085, 0.367, 0.419, 0.28, 0.207, 0.157]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2909 | Steps: 2 | Val loss: 2.1423 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
[2m[36m(func pid=5547)[0m top1: 0.32276119402985076
[2m[36m(func pid=5547)[0m top5: 0.8218283582089553
[2m[36m(func pid=5547)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=5547)[0m f1_macro: 0.3040039012086248
[2m[36m(func pid=5547)[0m f1_weighted: 0.33621900957012457
[2m[36m(func pid=5547)[0m f1_per_class: [0.213, 0.128, 0.667, 0.478, 0.072, 0.327, 0.367, 0.25, 0.199, 0.339]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m top1: 0.1501865671641791
[2m[36m(func pid=11358)[0m top5: 0.4976679104477612
[2m[36m(func pid=11358)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=11358)[0m f1_macro: 0.083618602493887
[2m[36m(func pid=11358)[0m f1_weighted: 0.12450610436294192
[2m[36m(func pid=11358)[0m f1_per_class: [0.085, 0.22, 0.0, 0.126, 0.015, 0.298, 0.043, 0.049, 0.0, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0649 | Steps: 2 | Val loss: 4.5756 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=188206)[0m top1: 0.33908582089552236
[2m[36m(func pid=188206)[0m top5: 0.8936567164179104
[2m[36m(func pid=188206)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=188206)[0m f1_macro: 0.3343744968418766
[2m[36m(func pid=188206)[0m f1_weighted: 0.3659593208866609
[2m[36m(func pid=188206)[0m f1_per_class: [0.36, 0.285, 0.759, 0.406, 0.074, 0.35, 0.42, 0.3, 0.19, 0.2]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9607 | Steps: 2 | Val loss: 2.3429 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2964 | Steps: 2 | Val loss: 14.8063 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 02:09:18 (running for 00:35:39.26)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.291 |      0.334 |                   56 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.065 |      0.334 |                   36 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.048 |      0.304 |                   31 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.95  |      0.084 |                    7 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=4309)[0m top1: 0.33348880597014924
[2m[36m(func pid=4309)[0m top5: 0.8703358208955224
[2m[36m(func pid=4309)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=4309)[0m f1_macro: 0.33424661896100494
[2m[36m(func pid=4309)[0m f1_weighted: 0.36386618087759764
[2m[36m(func pid=4309)[0m f1_per_class: [0.352, 0.308, 0.786, 0.372, 0.084, 0.372, 0.426, 0.277, 0.21, 0.154]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2427 | Steps: 2 | Val loss: 2.1603 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=11358)[0m top1: 0.14878731343283583
[2m[36m(func pid=11358)[0m top5: 0.5088619402985075
[2m[36m(func pid=11358)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=11358)[0m f1_macro: 0.08362178697050666
[2m[36m(func pid=11358)[0m f1_weighted: 0.12741471832084644
[2m[36m(func pid=11358)[0m f1_per_class: [0.058, 0.207, 0.0, 0.133, 0.023, 0.291, 0.054, 0.07, 0.0, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3255597014925373
[2m[36m(func pid=5547)[0m top5: 0.8208955223880597
[2m[36m(func pid=5547)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=5547)[0m f1_macro: 0.3033310710146475
[2m[36m(func pid=5547)[0m f1_weighted: 0.3413566619334218
[2m[36m(func pid=5547)[0m f1_per_class: [0.208, 0.126, 0.667, 0.484, 0.074, 0.335, 0.381, 0.238, 0.195, 0.325]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0354 | Steps: 2 | Val loss: 4.7053 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=188206)[0m top1: 0.33955223880597013
[2m[36m(func pid=188206)[0m top5: 0.894589552238806
[2m[36m(func pid=188206)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=188206)[0m f1_macro: 0.3345562755502694
[2m[36m(func pid=188206)[0m f1_weighted: 0.3657841103488598
[2m[36m(func pid=188206)[0m f1_per_class: [0.364, 0.286, 0.759, 0.409, 0.074, 0.355, 0.416, 0.285, 0.198, 0.201]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9588 | Steps: 2 | Val loss: 2.3399 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0065 | Steps: 2 | Val loss: 15.0190 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 02:09:24 (running for 00:35:44.75)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.243 |      0.335 |                   57 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.035 |      0.342 |                   37 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.296 |      0.303 |                   32 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.961 |      0.084 |                    8 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=4309)[0m top1: 0.3381529850746269
[2m[36m(func pid=4309)[0m top5: 0.8740671641791045
[2m[36m(func pid=4309)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=4309)[0m f1_macro: 0.34167689281728386
[2m[36m(func pid=4309)[0m f1_weighted: 0.3678196095861912
[2m[36m(func pid=4309)[0m f1_per_class: [0.365, 0.318, 0.815, 0.377, 0.084, 0.365, 0.428, 0.288, 0.217, 0.161]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2971 | Steps: 2 | Val loss: 2.1932 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=11358)[0m top1: 0.1501865671641791
[2m[36m(func pid=11358)[0m top5: 0.5051305970149254
[2m[36m(func pid=11358)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=11358)[0m f1_macro: 0.0855504400193974
[2m[36m(func pid=11358)[0m f1_weighted: 0.13229370971072008
[2m[36m(func pid=11358)[0m f1_per_class: [0.054, 0.192, 0.0, 0.142, 0.021, 0.307, 0.063, 0.076, 0.0, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m top1: 0.324160447761194
[2m[36m(func pid=5547)[0m top5: 0.8236940298507462
[2m[36m(func pid=5547)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=5547)[0m f1_macro: 0.3014535059325348
[2m[36m(func pid=5547)[0m f1_weighted: 0.34541308346620037
[2m[36m(func pid=5547)[0m f1_per_class: [0.192, 0.144, 0.649, 0.47, 0.075, 0.339, 0.399, 0.229, 0.195, 0.323]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0353 | Steps: 2 | Val loss: 4.8336 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=188206)[0m top1: 0.3381529850746269
[2m[36m(func pid=188206)[0m top5: 0.8927238805970149
[2m[36m(func pid=188206)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=188206)[0m f1_macro: 0.33288054744927625
[2m[36m(func pid=188206)[0m f1_weighted: 0.3651888175242696
[2m[36m(func pid=188206)[0m f1_per_class: [0.357, 0.288, 0.759, 0.403, 0.072, 0.358, 0.419, 0.286, 0.188, 0.199]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9661 | Steps: 2 | Val loss: 2.3366 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5593 | Steps: 2 | Val loss: 15.2280 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 02:09:29 (running for 00:35:50.16)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.297 |      0.333 |                   58 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.035 |      0.337 |                   38 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.006 |      0.301 |                   33 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.959 |      0.086 |                    9 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=4309)[0m top1: 0.3316231343283582
[2m[36m(func pid=4309)[0m top5: 0.8736007462686567
[2m[36m(func pid=4309)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=4309)[0m f1_macro: 0.33744553683200273
[2m[36m(func pid=4309)[0m f1_weighted: 0.3619874580420662
[2m[36m(func pid=4309)[0m f1_per_class: [0.362, 0.313, 0.815, 0.373, 0.073, 0.369, 0.415, 0.284, 0.218, 0.153]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2543 | Steps: 2 | Val loss: 2.2031 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=11358)[0m top1: 0.14832089552238806
[2m[36m(func pid=11358)[0m top5: 0.5041977611940298
[2m[36m(func pid=11358)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=11358)[0m f1_macro: 0.08519557928352738
[2m[36m(func pid=11358)[0m f1_weighted: 0.13579086697085674
[2m[36m(func pid=11358)[0m f1_per_class: [0.047, 0.182, 0.0, 0.157, 0.02, 0.297, 0.071, 0.077, 0.0, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3255597014925373
[2m[36m(func pid=5547)[0m top5: 0.8199626865671642
[2m[36m(func pid=5547)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=5547)[0m f1_macro: 0.3029166092103826
[2m[36m(func pid=5547)[0m f1_weighted: 0.3506943102980095
[2m[36m(func pid=5547)[0m f1_per_class: [0.179, 0.158, 0.649, 0.455, 0.08, 0.348, 0.42, 0.23, 0.19, 0.32]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m top1: 0.3358208955223881
[2m[36m(func pid=188206)[0m top5: 0.8922574626865671
[2m[36m(func pid=188206)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=188206)[0m f1_macro: 0.331316744947162
[2m[36m(func pid=188206)[0m f1_weighted: 0.3614777810067995
[2m[36m(func pid=188206)[0m f1_per_class: [0.361, 0.288, 0.759, 0.405, 0.073, 0.342, 0.411, 0.278, 0.195, 0.201]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0302 | Steps: 2 | Val loss: 4.8991 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9246 | Steps: 2 | Val loss: 2.3340 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0105 | Steps: 2 | Val loss: 15.5735 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 02:09:35 (running for 00:35:55.74)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.254 |      0.331 |                   59 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.03  |      0.336 |                   39 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.559 |      0.303 |                   34 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.966 |      0.085 |                   10 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=4309)[0m top1: 0.333955223880597
[2m[36m(func pid=4309)[0m top5: 0.8745335820895522
[2m[36m(func pid=4309)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=4309)[0m f1_macro: 0.33645226277660784
[2m[36m(func pid=4309)[0m f1_weighted: 0.3638079812438996
[2m[36m(func pid=4309)[0m f1_per_class: [0.351, 0.317, 0.815, 0.37, 0.074, 0.355, 0.427, 0.284, 0.224, 0.148]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.1889 | Steps: 2 | Val loss: 2.2113 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=11358)[0m top1: 0.14598880597014927
[2m[36m(func pid=11358)[0m top5: 0.5144589552238806
[2m[36m(func pid=11358)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=11358)[0m f1_macro: 0.09069610433038193
[2m[36m(func pid=11358)[0m f1_weighted: 0.1348415665235135
[2m[36m(func pid=11358)[0m f1_per_class: [0.046, 0.169, 0.057, 0.158, 0.026, 0.296, 0.073, 0.082, 0.0, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3208955223880597
[2m[36m(func pid=5547)[0m top5: 0.8176305970149254
[2m[36m(func pid=5547)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=5547)[0m f1_macro: 0.3036552533133102
[2m[36m(func pid=5547)[0m f1_weighted: 0.3494222264585451
[2m[36m(func pid=5547)[0m f1_per_class: [0.162, 0.17, 0.686, 0.426, 0.089, 0.347, 0.438, 0.227, 0.191, 0.301]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m top1: 0.33722014925373134
[2m[36m(func pid=188206)[0m top5: 0.894589552238806
[2m[36m(func pid=188206)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=188206)[0m f1_macro: 0.3313035329277615
[2m[36m(func pid=188206)[0m f1_weighted: 0.3630263231127036
[2m[36m(func pid=188206)[0m f1_per_class: [0.357, 0.288, 0.759, 0.403, 0.074, 0.349, 0.416, 0.277, 0.189, 0.2]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0969 | Steps: 2 | Val loss: 4.9923 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9557 | Steps: 2 | Val loss: 2.3295 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2352 | Steps: 2 | Val loss: 15.8698 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3227 | Steps: 2 | Val loss: 2.2166 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
== Status ==
Current time: 2024-01-07 02:09:40 (running for 00:36:01.23)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.189 |      0.331 |                   60 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.097 |      0.338 |                   40 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.011 |      0.304 |                   35 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.925 |      0.091 |                   11 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=4309)[0m top1: 0.3358208955223881
[2m[36m(func pid=4309)[0m top5: 0.8773320895522388
[2m[36m(func pid=4309)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=4309)[0m f1_macro: 0.33797979439347425
[2m[36m(func pid=4309)[0m f1_weighted: 0.36491560701298653
[2m[36m(func pid=4309)[0m f1_per_class: [0.361, 0.318, 0.815, 0.374, 0.075, 0.354, 0.426, 0.284, 0.217, 0.156]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=11358)[0m top1: 0.1478544776119403
[2m[36m(func pid=11358)[0m top5: 0.5223880597014925
[2m[36m(func pid=11358)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=11358)[0m f1_macro: 0.09220313217052026
[2m[36m(func pid=11358)[0m f1_weighted: 0.13798202927540684
[2m[36m(func pid=11358)[0m f1_per_class: [0.043, 0.165, 0.059, 0.161, 0.026, 0.297, 0.08, 0.091, 0.0, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m top1: 0.32276119402985076
[2m[36m(func pid=5547)[0m top5: 0.8171641791044776
[2m[36m(func pid=5547)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=5547)[0m f1_macro: 0.30520367414969857
[2m[36m(func pid=5547)[0m f1_weighted: 0.35316787739821537
[2m[36m(func pid=5547)[0m f1_per_class: [0.166, 0.189, 0.686, 0.4, 0.094, 0.344, 0.463, 0.24, 0.191, 0.279]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m top1: 0.3423507462686567
[2m[36m(func pid=188206)[0m top5: 0.8941231343283582
[2m[36m(func pid=188206)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=188206)[0m f1_macro: 0.3369576891192315
[2m[36m(func pid=188206)[0m f1_weighted: 0.36880177527813485
[2m[36m(func pid=188206)[0m f1_per_class: [0.361, 0.288, 0.786, 0.413, 0.077, 0.345, 0.424, 0.288, 0.188, 0.198]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0746 | Steps: 2 | Val loss: 5.0698 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.9484 | Steps: 2 | Val loss: 2.3251 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4249 | Steps: 2 | Val loss: 2.2323 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2118 | Steps: 2 | Val loss: 16.5817 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 02:09:46 (running for 00:36:06.73)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.323 |      0.337 |                   61 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.097 |      0.338 |                   40 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.235 |      0.305 |                   36 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.948 |      0.1   |                   13 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.1501865671641791
[2m[36m(func pid=11358)[0m top5: 0.5284514925373134
[2m[36m(func pid=11358)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=11358)[0m f1_macro: 0.09970778089549168
[2m[36m(func pid=11358)[0m f1_weighted: 0.14004218495891865
[2m[36m(func pid=11358)[0m f1_per_class: [0.043, 0.166, 0.108, 0.162, 0.025, 0.304, 0.08, 0.098, 0.012, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m top1: 0.333955223880597
[2m[36m(func pid=4309)[0m top5: 0.8773320895522388
[2m[36m(func pid=4309)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=4309)[0m f1_macro: 0.33757938513895136
[2m[36m(func pid=4309)[0m f1_weighted: 0.3636477814032124
[2m[36m(func pid=4309)[0m f1_per_class: [0.375, 0.31, 0.815, 0.374, 0.074, 0.355, 0.426, 0.284, 0.21, 0.153]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m top1: 0.34095149253731344
[2m[36m(func pid=188206)[0m top5: 0.8950559701492538
[2m[36m(func pid=188206)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=188206)[0m f1_macro: 0.3336549324024933
[2m[36m(func pid=188206)[0m f1_weighted: 0.36604287807159636
[2m[36m(func pid=188206)[0m f1_per_class: [0.355, 0.292, 0.759, 0.408, 0.077, 0.353, 0.417, 0.277, 0.194, 0.205]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m top1: 0.31716417910447764
[2m[36m(func pid=5547)[0m top5: 0.8134328358208955
[2m[36m(func pid=5547)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=5547)[0m f1_macro: 0.30385757835296634
[2m[36m(func pid=5547)[0m f1_weighted: 0.3470456269587258
[2m[36m(func pid=5547)[0m f1_per_class: [0.16, 0.199, 0.706, 0.357, 0.102, 0.348, 0.477, 0.237, 0.197, 0.257]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.9517 | Steps: 2 | Val loss: 2.3248 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.1105 | Steps: 2 | Val loss: 5.1164 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2024 | Steps: 2 | Val loss: 2.2514 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.1416 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 02:09:51 (running for 00:36:12.10)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.425 |      0.334 |                   62 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.075 |      0.338 |                   41 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.212 |      0.304 |                   37 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.952 |      0.101 |                   14 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.15065298507462688
[2m[36m(func pid=11358)[0m top5: 0.5289179104477612
[2m[36m(func pid=11358)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=11358)[0m f1_macro: 0.10092396524102487
[2m[36m(func pid=11358)[0m f1_weighted: 0.14149203221293627
[2m[36m(func pid=11358)[0m f1_per_class: [0.054, 0.167, 0.1, 0.159, 0.025, 0.303, 0.085, 0.104, 0.011, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m top1: 0.332089552238806
[2m[36m(func pid=4309)[0m top5: 0.878731343283582
[2m[36m(func pid=4309)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=4309)[0m f1_macro: 0.33564725293199127
[2m[36m(func pid=4309)[0m f1_weighted: 0.36133070510745374
[2m[36m(func pid=4309)[0m f1_per_class: [0.377, 0.308, 0.815, 0.377, 0.083, 0.341, 0.426, 0.267, 0.212, 0.152]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m top1: 0.34375
[2m[36m(func pid=188206)[0m top5: 0.8973880597014925
[2m[36m(func pid=188206)[0m f1_micro: 0.34375
[2m[36m(func pid=188206)[0m f1_macro: 0.33609656437045826
[2m[36m(func pid=188206)[0m f1_weighted: 0.3697281659853192
[2m[36m(func pid=188206)[0m f1_per_class: [0.355, 0.292, 0.759, 0.411, 0.078, 0.361, 0.422, 0.287, 0.197, 0.2]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3101679104477612
[2m[36m(func pid=5547)[0m top5: 0.8125
[2m[36m(func pid=5547)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=5547)[0m f1_macro: 0.30230507850958455
[2m[36m(func pid=5547)[0m f1_weighted: 0.3382125279837068
[2m[36m(func pid=5547)[0m f1_per_class: [0.157, 0.202, 0.727, 0.325, 0.112, 0.347, 0.475, 0.237, 0.197, 0.244]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8970 | Steps: 2 | Val loss: 2.3203 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0462 | Steps: 2 | Val loss: 5.2591 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.1934 | Steps: 2 | Val loss: 2.2676 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.7670 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=11358)[0m top1: 0.14925373134328357
[2m[36m(func pid=11358)[0m top5: 0.5307835820895522
[2m[36m(func pid=11358)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=11358)[0m f1_macro: 0.1031648284140076
[2m[36m(func pid=11358)[0m f1_weighted: 0.14083920390439067
[2m[36m(func pid=11358)[0m f1_per_class: [0.052, 0.158, 0.13, 0.16, 0.024, 0.303, 0.087, 0.106, 0.012, 0.0]
== Status ==
Current time: 2024-01-07 02:09:56 (running for 00:36:17.30)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.202 |      0.336 |                   63 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.11  |      0.336 |                   42 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.302 |                   38 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.897 |      0.103 |                   15 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m top1: 0.32742537313432835
[2m[36m(func pid=4309)[0m top5: 0.8698694029850746
[2m[36m(func pid=4309)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=4309)[0m f1_macro: 0.3279463650076891
[2m[36m(func pid=4309)[0m f1_weighted: 0.35695032055943793
[2m[36m(func pid=4309)[0m f1_per_class: [0.347, 0.304, 0.786, 0.37, 0.069, 0.345, 0.421, 0.267, 0.209, 0.163]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m top1: 0.3451492537313433
[2m[36m(func pid=188206)[0m top5: 0.8969216417910447
[2m[36m(func pid=188206)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=188206)[0m f1_macro: 0.33815233320897964
[2m[36m(func pid=188206)[0m f1_weighted: 0.3705527067059711
[2m[36m(func pid=188206)[0m f1_per_class: [0.358, 0.29, 0.786, 0.411, 0.08, 0.354, 0.43, 0.274, 0.198, 0.201]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m top1: 0.30130597014925375
[2m[36m(func pid=5547)[0m top5: 0.8125
[2m[36m(func pid=5547)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=5547)[0m f1_macro: 0.2974498344885137
[2m[36m(func pid=5547)[0m f1_weighted: 0.3262781784558887
[2m[36m(func pid=5547)[0m f1_per_class: [0.149, 0.207, 0.727, 0.278, 0.11, 0.347, 0.477, 0.235, 0.199, 0.245]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.9350 | Steps: 2 | Val loss: 2.3181 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.1899 | Steps: 2 | Val loss: 2.2946 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0028 | Steps: 2 | Val loss: 5.3508 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 02:10:01 (running for 00:36:22.57)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.193 |      0.338 |                   64 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.046 |      0.328 |                   43 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.297 |                   39 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.935 |      0.102 |                   16 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0011 | Steps: 2 | Val loss: 18.4991 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=11358)[0m top1: 0.14738805970149255
[2m[36m(func pid=11358)[0m top5: 0.5331156716417911
[2m[36m(func pid=11358)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=11358)[0m f1_macro: 0.10233710604446282
[2m[36m(func pid=11358)[0m f1_weighted: 0.1405133187807644
[2m[36m(func pid=11358)[0m f1_per_class: [0.05, 0.152, 0.13, 0.164, 0.023, 0.301, 0.087, 0.105, 0.012, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m top1: 0.34654850746268656
[2m[36m(func pid=188206)[0m top5: 0.8950559701492538
[2m[36m(func pid=188206)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=188206)[0m f1_macro: 0.3368544794931672
[2m[36m(func pid=188206)[0m f1_weighted: 0.372823029237961
[2m[36m(func pid=188206)[0m f1_per_class: [0.356, 0.292, 0.759, 0.41, 0.079, 0.362, 0.433, 0.281, 0.201, 0.195]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m top1: 0.32742537313432835
[2m[36m(func pid=4309)[0m top5: 0.8726679104477612
[2m[36m(func pid=4309)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=4309)[0m f1_macro: 0.32731851103235376
[2m[36m(func pid=4309)[0m f1_weighted: 0.35743186352161876
[2m[36m(func pid=4309)[0m f1_per_class: [0.345, 0.302, 0.786, 0.368, 0.067, 0.345, 0.426, 0.262, 0.217, 0.156]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.29757462686567165
[2m[36m(func pid=5547)[0m top5: 0.8083022388059702
[2m[36m(func pid=5547)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=5547)[0m f1_macro: 0.29626034113687244
[2m[36m(func pid=5547)[0m f1_weighted: 0.3197904609736415
[2m[36m(func pid=5547)[0m f1_per_class: [0.145, 0.209, 0.774, 0.245, 0.11, 0.354, 0.486, 0.219, 0.196, 0.225]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.9222 | Steps: 2 | Val loss: 2.3177 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2191 | Steps: 2 | Val loss: 2.3310 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0332 | Steps: 2 | Val loss: 5.4208 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 02:10:07 (running for 00:36:27.87)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.19  |      0.337 |                   65 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.003 |      0.327 |                   44 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.001 |      0.296 |                   40 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.922 |      0.103 |                   17 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.14972014925373134
[2m[36m(func pid=11358)[0m top5: 0.5363805970149254
[2m[36m(func pid=11358)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=11358)[0m f1_macro: 0.1030865660949655
[2m[36m(func pid=11358)[0m f1_weighted: 0.14179263716934928
[2m[36m(func pid=11358)[0m f1_per_class: [0.048, 0.157, 0.128, 0.164, 0.023, 0.31, 0.084, 0.104, 0.012, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0000 | Steps: 2 | Val loss: 19.2762 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=188206)[0m top1: 0.34281716417910446
[2m[36m(func pid=188206)[0m top5: 0.8917910447761194
[2m[36m(func pid=188206)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=188206)[0m f1_macro: 0.3334171310931056
[2m[36m(func pid=188206)[0m f1_weighted: 0.3703535920319451
[2m[36m(func pid=188206)[0m f1_per_class: [0.341, 0.283, 0.759, 0.412, 0.077, 0.363, 0.429, 0.28, 0.201, 0.189]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m top1: 0.3292910447761194
[2m[36m(func pid=4309)[0m top5: 0.8708022388059702
[2m[36m(func pid=4309)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=4309)[0m f1_macro: 0.3309068387563944
[2m[36m(func pid=4309)[0m f1_weighted: 0.3588955799613244
[2m[36m(func pid=4309)[0m f1_per_class: [0.344, 0.299, 0.786, 0.373, 0.078, 0.349, 0.424, 0.264, 0.234, 0.159]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.2947761194029851
[2m[36m(func pid=5547)[0m top5: 0.8078358208955224
[2m[36m(func pid=5547)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=5547)[0m f1_macro: 0.29353943934768484
[2m[36m(func pid=5547)[0m f1_weighted: 0.3153649355753072
[2m[36m(func pid=5547)[0m f1_per_class: [0.139, 0.212, 0.774, 0.223, 0.107, 0.361, 0.488, 0.22, 0.194, 0.217]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.9173 | Steps: 2 | Val loss: 2.3164 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.1749 | Steps: 2 | Val loss: 2.3377 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0242 | Steps: 2 | Val loss: 5.4623 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 02:10:12 (running for 00:36:33.18)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.219 |      0.333 |                   66 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.033 |      0.331 |                   45 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.294 |                   41 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.917 |      0.103 |                   18 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.15065298507462688
[2m[36m(func pid=11358)[0m top5: 0.5373134328358209
[2m[36m(func pid=11358)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=11358)[0m f1_macro: 0.10287738432828848
[2m[36m(func pid=11358)[0m f1_weighted: 0.14477364988154223
[2m[36m(func pid=11358)[0m f1_per_class: [0.047, 0.156, 0.125, 0.171, 0.024, 0.308, 0.092, 0.096, 0.011, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m top1: 0.3451492537313433
[2m[36m(func pid=188206)[0m top5: 0.8913246268656716
[2m[36m(func pid=188206)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=188206)[0m f1_macro: 0.33406983342120533
[2m[36m(func pid=188206)[0m f1_weighted: 0.372832660577551
[2m[36m(func pid=188206)[0m f1_per_class: [0.344, 0.284, 0.759, 0.416, 0.079, 0.36, 0.435, 0.278, 0.199, 0.186]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3170 | Steps: 2 | Val loss: 20.1115 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=4309)[0m top1: 0.32742537313432835
[2m[36m(func pid=4309)[0m top5: 0.8708022388059702
[2m[36m(func pid=4309)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=4309)[0m f1_macro: 0.3274983093038187
[2m[36m(func pid=4309)[0m f1_weighted: 0.3576027027192621
[2m[36m(func pid=4309)[0m f1_per_class: [0.332, 0.294, 0.786, 0.38, 0.078, 0.357, 0.418, 0.249, 0.225, 0.158]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.8800 | Steps: 2 | Val loss: 2.3124 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=5547)[0m top1: 0.28451492537313433
[2m[36m(func pid=5547)[0m top5: 0.7989738805970149
[2m[36m(func pid=5547)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=5547)[0m f1_macro: 0.2874103535283531
[2m[36m(func pid=5547)[0m f1_weighted: 0.30364831993068186
[2m[36m(func pid=5547)[0m f1_per_class: [0.136, 0.215, 0.774, 0.193, 0.101, 0.349, 0.48, 0.23, 0.179, 0.218]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.1427 | Steps: 2 | Val loss: 2.3603 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.1355 | Steps: 2 | Val loss: 5.5218 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 02:10:17 (running for 00:36:38.37)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.175 |      0.334 |                   67 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.024 |      0.327 |                   46 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.317 |      0.287 |                   42 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.88  |      0.105 |                   19 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.15298507462686567
[2m[36m(func pid=11358)[0m top5: 0.5419776119402985
[2m[36m(func pid=11358)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=11358)[0m f1_macro: 0.10495478892851892
[2m[36m(func pid=11358)[0m f1_weighted: 0.14815951223247212
[2m[36m(func pid=11358)[0m f1_per_class: [0.046, 0.147, 0.125, 0.182, 0.024, 0.312, 0.093, 0.108, 0.011, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m top1: 0.34421641791044777
[2m[36m(func pid=188206)[0m top5: 0.8917910447761194
[2m[36m(func pid=188206)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=188206)[0m f1_macro: 0.33344182845339104
[2m[36m(func pid=188206)[0m f1_weighted: 0.3722753179216716
[2m[36m(func pid=188206)[0m f1_per_class: [0.341, 0.283, 0.759, 0.416, 0.078, 0.368, 0.432, 0.274, 0.199, 0.185]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2518 | Steps: 2 | Val loss: 20.3013 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=4309)[0m top1: 0.32509328358208955
[2m[36m(func pid=4309)[0m top5: 0.8722014925373134
[2m[36m(func pid=4309)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=4309)[0m f1_macro: 0.32476108976895807
[2m[36m(func pid=4309)[0m f1_weighted: 0.3556636659983824
[2m[36m(func pid=4309)[0m f1_per_class: [0.305, 0.291, 0.786, 0.379, 0.073, 0.346, 0.416, 0.264, 0.23, 0.157]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.8726 | Steps: 2 | Val loss: 2.3111 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.1945 | Steps: 2 | Val loss: 2.3888 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=5547)[0m top1: 0.2943097014925373
[2m[36m(func pid=5547)[0m top5: 0.8069029850746269
[2m[36m(func pid=5547)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=5547)[0m f1_macro: 0.28942798695761335
[2m[36m(func pid=5547)[0m f1_weighted: 0.3055013939459569
[2m[36m(func pid=5547)[0m f1_per_class: [0.139, 0.236, 0.75, 0.163, 0.124, 0.378, 0.491, 0.237, 0.172, 0.205]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0494 | Steps: 2 | Val loss: 5.6627 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 02:10:23 (running for 00:36:43.74)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.143 |      0.333 |                   68 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.136 |      0.325 |                   47 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.252 |      0.289 |                   43 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.873 |      0.104 |                   20 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.15345149253731344
[2m[36m(func pid=11358)[0m top5: 0.5443097014925373
[2m[36m(func pid=11358)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=11358)[0m f1_macro: 0.10429842802387039
[2m[36m(func pid=11358)[0m f1_weighted: 0.14845739491048907
[2m[36m(func pid=11358)[0m f1_per_class: [0.046, 0.144, 0.118, 0.185, 0.025, 0.313, 0.094, 0.108, 0.011, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m top1: 0.34281716417910446
[2m[36m(func pid=188206)[0m top5: 0.8889925373134329
[2m[36m(func pid=188206)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=188206)[0m f1_macro: 0.332083615337074
[2m[36m(func pid=188206)[0m f1_weighted: 0.3708828681453073
[2m[36m(func pid=188206)[0m f1_per_class: [0.339, 0.283, 0.759, 0.416, 0.079, 0.36, 0.431, 0.271, 0.199, 0.184]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.0513 | Steps: 2 | Val loss: 20.6165 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=4309)[0m top1: 0.322294776119403
[2m[36m(func pid=4309)[0m top5: 0.8680037313432836
[2m[36m(func pid=4309)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=4309)[0m f1_macro: 0.32198514929878913
[2m[36m(func pid=4309)[0m f1_weighted: 0.3541111912362253
[2m[36m(func pid=4309)[0m f1_per_class: [0.296, 0.279, 0.786, 0.38, 0.07, 0.346, 0.419, 0.267, 0.222, 0.156]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.8506 | Steps: 2 | Val loss: 2.3064 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2821 | Steps: 2 | Val loss: 2.3934 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=5547)[0m top1: 0.29244402985074625
[2m[36m(func pid=5547)[0m top5: 0.8078358208955224
[2m[36m(func pid=5547)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=5547)[0m f1_macro: 0.28755804230812493
[2m[36m(func pid=5547)[0m f1_weighted: 0.30084328431739327
[2m[36m(func pid=5547)[0m f1_per_class: [0.142, 0.242, 0.75, 0.153, 0.125, 0.375, 0.482, 0.232, 0.178, 0.197]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0037 | Steps: 2 | Val loss: 5.7204 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 02:10:28 (running for 00:36:48.99)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.195 |      0.332 |                   69 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.049 |      0.322 |                   48 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  1.051 |      0.288 |                   44 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.851 |      0.105 |                   21 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.15485074626865672
[2m[36m(func pid=11358)[0m top5: 0.5550373134328358
[2m[36m(func pid=11358)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=11358)[0m f1_macro: 0.10542835864651545
[2m[36m(func pid=11358)[0m f1_weighted: 0.1504771832220648
[2m[36m(func pid=11358)[0m f1_per_class: [0.045, 0.145, 0.118, 0.189, 0.025, 0.312, 0.095, 0.114, 0.011, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m top1: 0.345615671641791
[2m[36m(func pid=188206)[0m top5: 0.8936567164179104
[2m[36m(func pid=188206)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=188206)[0m f1_macro: 0.33542507947452455
[2m[36m(func pid=188206)[0m f1_weighted: 0.3719494010074563
[2m[36m(func pid=188206)[0m f1_per_class: [0.346, 0.298, 0.759, 0.402, 0.084, 0.35, 0.44, 0.28, 0.203, 0.193]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0434 | Steps: 2 | Val loss: 20.5229 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=4309)[0m top1: 0.32322761194029853
[2m[36m(func pid=4309)[0m top5: 0.8675373134328358
[2m[36m(func pid=4309)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=4309)[0m f1_macro: 0.3195813167515409
[2m[36m(func pid=4309)[0m f1_weighted: 0.35589310394304163
[2m[36m(func pid=4309)[0m f1_per_class: [0.284, 0.266, 0.786, 0.392, 0.07, 0.344, 0.424, 0.262, 0.211, 0.156]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.1369 | Steps: 2 | Val loss: 2.4163 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.8983 | Steps: 2 | Val loss: 2.3020 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=5547)[0m top1: 0.3064365671641791
[2m[36m(func pid=5547)[0m top5: 0.8138992537313433
[2m[36m(func pid=5547)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=5547)[0m f1_macro: 0.2954383366097559
[2m[36m(func pid=5547)[0m f1_weighted: 0.31468363626889445
[2m[36m(func pid=5547)[0m f1_per_class: [0.154, 0.247, 0.774, 0.162, 0.119, 0.39, 0.51, 0.233, 0.176, 0.189]
[2m[36m(func pid=5547)[0m 
== Status ==
Current time: 2024-01-07 02:10:33 (running for 00:36:54.35)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.137 |      0.332 |                   71 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.004 |      0.32  |                   49 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.043 |      0.295 |                   45 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.851 |      0.105 |                   21 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.34328358208955223
[2m[36m(func pid=188206)[0m top5: 0.8931902985074627
[2m[36m(func pid=188206)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=188206)[0m f1_macro: 0.3317905517413019
[2m[36m(func pid=188206)[0m f1_weighted: 0.37086525481078864
[2m[36m(func pid=188206)[0m f1_per_class: [0.339, 0.292, 0.759, 0.402, 0.083, 0.341, 0.446, 0.279, 0.194, 0.185]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0032 | Steps: 2 | Val loss: 5.8002 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=11358)[0m top1: 0.15345149253731344
[2m[36m(func pid=11358)[0m top5: 0.5615671641791045
[2m[36m(func pid=11358)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=11358)[0m f1_macro: 0.10274465009713274
[2m[36m(func pid=11358)[0m f1_weighted: 0.14924849096136825
[2m[36m(func pid=11358)[0m f1_per_class: [0.045, 0.142, 0.105, 0.189, 0.019, 0.308, 0.095, 0.113, 0.012, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0036 | Steps: 2 | Val loss: 20.4887 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=4309)[0m top1: 0.322294776119403
[2m[36m(func pid=4309)[0m top5: 0.8675373134328358
[2m[36m(func pid=4309)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=4309)[0m f1_macro: 0.3179607725625868
[2m[36m(func pid=4309)[0m f1_weighted: 0.3562350855054316
[2m[36m(func pid=4309)[0m f1_per_class: [0.282, 0.247, 0.786, 0.394, 0.067, 0.341, 0.436, 0.258, 0.213, 0.155]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2124 | Steps: 2 | Val loss: 2.4487 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.8626 | Steps: 2 | Val loss: 2.2990 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=5547)[0m top1: 0.314365671641791
[2m[36m(func pid=5547)[0m top5: 0.8194962686567164
[2m[36m(func pid=5547)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=5547)[0m f1_macro: 0.2965351883682652
[2m[36m(func pid=5547)[0m f1_weighted: 0.3219129118702872
[2m[36m(func pid=5547)[0m f1_per_class: [0.174, 0.254, 0.733, 0.17, 0.127, 0.389, 0.523, 0.229, 0.185, 0.181]
[2m[36m(func pid=5547)[0m 
== Status ==
Current time: 2024-01-07 02:10:38 (running for 00:36:59.53)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.212 |      0.327 |                   72 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.003 |      0.318 |                   50 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.004 |      0.297 |                   46 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.898 |      0.103 |                   22 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.33722014925373134
[2m[36m(func pid=188206)[0m top5: 0.8852611940298507
[2m[36m(func pid=188206)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=188206)[0m f1_macro: 0.3266045303532047
[2m[36m(func pid=188206)[0m f1_weighted: 0.36589018416696933
[2m[36m(func pid=188206)[0m f1_per_class: [0.328, 0.277, 0.759, 0.402, 0.081, 0.349, 0.439, 0.261, 0.191, 0.179]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m top1: 0.15578358208955223
[2m[36m(func pid=11358)[0m top5: 0.5671641791044776
[2m[36m(func pid=11358)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=11358)[0m f1_macro: 0.10401804210469365
[2m[36m(func pid=11358)[0m f1_weighted: 0.1527878264108085
[2m[36m(func pid=11358)[0m f1_per_class: [0.045, 0.145, 0.105, 0.198, 0.019, 0.315, 0.095, 0.106, 0.011, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0044 | Steps: 2 | Val loss: 5.9028 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2181 | Steps: 2 | Val loss: 20.4373 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.1422 | Steps: 2 | Val loss: 2.4714 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=4309)[0m top1: 0.3218283582089552
[2m[36m(func pid=4309)[0m top5: 0.8684701492537313
[2m[36m(func pid=4309)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=4309)[0m f1_macro: 0.3142121152748823
[2m[36m(func pid=4309)[0m f1_weighted: 0.35682407468671684
[2m[36m(func pid=4309)[0m f1_per_class: [0.264, 0.243, 0.786, 0.399, 0.065, 0.344, 0.439, 0.259, 0.19, 0.154]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.8631 | Steps: 2 | Val loss: 2.2969 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=5547)[0m top1: 0.3180970149253731
[2m[36m(func pid=5547)[0m top5: 0.8306902985074627
[2m[36m(func pid=5547)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=5547)[0m f1_macro: 0.2985355552052645
[2m[36m(func pid=5547)[0m f1_weighted: 0.32721835875067273
[2m[36m(func pid=5547)[0m f1_per_class: [0.168, 0.256, 0.759, 0.184, 0.121, 0.376, 0.53, 0.244, 0.177, 0.172]
[2m[36m(func pid=5547)[0m 
== Status ==
Current time: 2024-01-07 02:10:44 (running for 00:37:04.76)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.142 |      0.326 |                   73 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.004 |      0.314 |                   51 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.218 |      0.299 |                   47 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.863 |      0.104 |                   23 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.3358208955223881
[2m[36m(func pid=188206)[0m top5: 0.8815298507462687
[2m[36m(func pid=188206)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=188206)[0m f1_macro: 0.32631485137817734
[2m[36m(func pid=188206)[0m f1_weighted: 0.36530547433663024
[2m[36m(func pid=188206)[0m f1_per_class: [0.328, 0.278, 0.759, 0.399, 0.079, 0.35, 0.437, 0.279, 0.179, 0.176]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m top1: 0.15578358208955223
[2m[36m(func pid=11358)[0m top5: 0.5755597014925373
[2m[36m(func pid=11358)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=11358)[0m f1_macro: 0.10474622207158366
[2m[36m(func pid=11358)[0m f1_weighted: 0.15484534523270893
[2m[36m(func pid=11358)[0m f1_per_class: [0.046, 0.131, 0.113, 0.217, 0.019, 0.306, 0.095, 0.109, 0.011, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0059 | Steps: 2 | Val loss: 5.9794 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0552 | Steps: 2 | Val loss: 20.4889 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.1486 | Steps: 2 | Val loss: 2.4966 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=4309)[0m top1: 0.31949626865671643
[2m[36m(func pid=4309)[0m top5: 0.8656716417910447
[2m[36m(func pid=4309)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=4309)[0m f1_macro: 0.3119443797743409
[2m[36m(func pid=4309)[0m f1_weighted: 0.35447384819375904
[2m[36m(func pid=4309)[0m f1_per_class: [0.255, 0.235, 0.786, 0.4, 0.064, 0.345, 0.434, 0.259, 0.187, 0.154]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.8428 | Steps: 2 | Val loss: 2.2965 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=5547)[0m top1: 0.32276119402985076
[2m[36m(func pid=5547)[0m top5: 0.8390858208955224
[2m[36m(func pid=5547)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=5547)[0m f1_macro: 0.29951989974368537
[2m[36m(func pid=5547)[0m f1_weighted: 0.3337091592939886
[2m[36m(func pid=5547)[0m f1_per_class: [0.184, 0.267, 0.733, 0.204, 0.121, 0.366, 0.53, 0.24, 0.181, 0.168]
[2m[36m(func pid=5547)[0m 
== Status ==
Current time: 2024-01-07 02:10:49 (running for 00:37:09.91)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.315
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.149 |      0.325 |                   74 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.006 |      0.312 |                   52 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.055 |      0.3   |                   48 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.863 |      0.105 |                   24 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.333955223880597
[2m[36m(func pid=188206)[0m top5: 0.8801305970149254
[2m[36m(func pid=188206)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=188206)[0m f1_macro: 0.32500882291381794
[2m[36m(func pid=188206)[0m f1_weighted: 0.3629871862640121
[2m[36m(func pid=188206)[0m f1_per_class: [0.332, 0.278, 0.759, 0.39, 0.08, 0.345, 0.44, 0.274, 0.177, 0.176]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m top1: 0.15485074626865672
[2m[36m(func pid=11358)[0m top5: 0.5755597014925373
[2m[36m(func pid=11358)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=11358)[0m f1_macro: 0.10505687429820867
[2m[36m(func pid=11358)[0m f1_weighted: 0.15361239920305858
[2m[36m(func pid=11358)[0m f1_per_class: [0.045, 0.141, 0.105, 0.208, 0.026, 0.309, 0.092, 0.114, 0.011, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0050 | Steps: 2 | Val loss: 6.0478 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.1369 | Steps: 2 | Val loss: 2.5135 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0046 | Steps: 2 | Val loss: 20.3349 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.8544 | Steps: 2 | Val loss: 2.2946 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=4309)[0m top1: 0.32136194029850745
[2m[36m(func pid=4309)[0m top5: 0.8652052238805971
[2m[36m(func pid=4309)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=4309)[0m f1_macro: 0.31318113991293134
[2m[36m(func pid=4309)[0m f1_weighted: 0.35698208533608383
[2m[36m(func pid=4309)[0m f1_per_class: [0.245, 0.238, 0.786, 0.401, 0.063, 0.338, 0.442, 0.259, 0.204, 0.156]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m top1: 0.3358208955223881
[2m[36m(func pid=188206)[0m top5: 0.882929104477612
[2m[36m(func pid=188206)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=188206)[0m f1_macro: 0.32662363315469267
[2m[36m(func pid=188206)[0m f1_weighted: 0.364857161036567
[2m[36m(func pid=188206)[0m f1_per_class: [0.333, 0.281, 0.759, 0.388, 0.081, 0.345, 0.445, 0.283, 0.18, 0.172]
[2m[36m(func pid=188206)[0m 
== Status ==
Current time: 2024-01-07 02:10:54 (running for 00:37:15.29)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.137 |      0.327 |                   75 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.005 |      0.313 |                   53 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.005 |      0.306 |                   49 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.843 |      0.105 |                   25 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5547)[0m top1: 0.3269589552238806
[2m[36m(func pid=5547)[0m top5: 0.8414179104477612
[2m[36m(func pid=5547)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=5547)[0m f1_macro: 0.3058172441904289
[2m[36m(func pid=5547)[0m f1_weighted: 0.3436695381484576
[2m[36m(func pid=5547)[0m f1_per_class: [0.195, 0.269, 0.75, 0.245, 0.125, 0.377, 0.521, 0.234, 0.174, 0.168]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m top1: 0.15811567164179105
[2m[36m(func pid=11358)[0m top5: 0.5806902985074627
[2m[36m(func pid=11358)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=11358)[0m f1_macro: 0.10779434934104648
[2m[36m(func pid=11358)[0m f1_weighted: 0.157282829992268
[2m[36m(func pid=11358)[0m f1_per_class: [0.054, 0.147, 0.102, 0.206, 0.026, 0.313, 0.099, 0.12, 0.011, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0403 | Steps: 2 | Val loss: 6.1033 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.1659 | Steps: 2 | Val loss: 2.5114 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.1724 | Steps: 2 | Val loss: 20.0090 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.8748 | Steps: 2 | Val loss: 2.2902 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=4309)[0m top1: 0.32322761194029853
[2m[36m(func pid=4309)[0m top5: 0.863339552238806
[2m[36m(func pid=4309)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=4309)[0m f1_macro: 0.3133592136751485
[2m[36m(func pid=4309)[0m f1_weighted: 0.3589519036355143
[2m[36m(func pid=4309)[0m f1_per_class: [0.239, 0.238, 0.786, 0.405, 0.062, 0.343, 0.443, 0.262, 0.193, 0.162]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m top1: 0.3414179104477612
[2m[36m(func pid=188206)[0m top5: 0.8824626865671642
[2m[36m(func pid=188206)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=188206)[0m f1_macro: 0.3274207243435393
[2m[36m(func pid=188206)[0m f1_weighted: 0.3713903544058187
[2m[36m(func pid=188206)[0m f1_per_class: [0.328, 0.28, 0.759, 0.395, 0.084, 0.345, 0.462, 0.273, 0.18, 0.168]
[2m[36m(func pid=188206)[0m 
== Status ==
Current time: 2024-01-07 02:11:00 (running for 00:37:20.77)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.166 |      0.327 |                   76 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.04  |      0.313 |                   54 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  1.172 |      0.311 |                   50 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.854 |      0.108 |                   26 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5547)[0m top1: 0.33255597014925375
[2m[36m(func pid=5547)[0m top5: 0.8456156716417911
[2m[36m(func pid=5547)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=5547)[0m f1_macro: 0.3112473969903299
[2m[36m(func pid=5547)[0m f1_weighted: 0.3530089726915486
[2m[36m(func pid=5547)[0m f1_per_class: [0.205, 0.264, 0.75, 0.269, 0.131, 0.382, 0.527, 0.248, 0.176, 0.161]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m top1: 0.15811567164179105
[2m[36m(func pid=11358)[0m top5: 0.585820895522388
[2m[36m(func pid=11358)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=11358)[0m f1_macro: 0.10768085963473703
[2m[36m(func pid=11358)[0m f1_weighted: 0.1576647455972134
[2m[36m(func pid=11358)[0m f1_per_class: [0.054, 0.144, 0.103, 0.212, 0.026, 0.317, 0.096, 0.113, 0.011, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0133 | Steps: 2 | Val loss: 6.2471 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0940 | Steps: 2 | Val loss: 2.5244 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.8465 | Steps: 2 | Val loss: 2.2899 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0233 | Steps: 2 | Val loss: 19.8320 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=4309)[0m top1: 0.31529850746268656
[2m[36m(func pid=4309)[0m top5: 0.8582089552238806
[2m[36m(func pid=4309)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=4309)[0m f1_macro: 0.30841700341163314
[2m[36m(func pid=4309)[0m f1_weighted: 0.350912691731829
[2m[36m(func pid=4309)[0m f1_per_class: [0.224, 0.226, 0.786, 0.397, 0.069, 0.336, 0.437, 0.247, 0.203, 0.159]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m top1: 0.34048507462686567
[2m[36m(func pid=188206)[0m top5: 0.8833955223880597
[2m[36m(func pid=188206)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=188206)[0m f1_macro: 0.32740065408757313
[2m[36m(func pid=188206)[0m f1_weighted: 0.37057218737475334
[2m[36m(func pid=188206)[0m f1_per_class: [0.323, 0.278, 0.759, 0.404, 0.083, 0.341, 0.453, 0.275, 0.19, 0.168]
[2m[36m(func pid=188206)[0m 
== Status ==
Current time: 2024-01-07 02:11:05 (running for 00:37:26.22)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.094 |      0.327 |                   77 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.013 |      0.308 |                   55 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  1.172 |      0.311 |                   50 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.847 |      0.108 |                   28 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.15951492537313433
[2m[36m(func pid=11358)[0m top5: 0.5867537313432836
[2m[36m(func pid=11358)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=11358)[0m f1_macro: 0.10849771899084723
[2m[36m(func pid=11358)[0m f1_weighted: 0.1608902598898827
[2m[36m(func pid=11358)[0m f1_per_class: [0.052, 0.146, 0.098, 0.216, 0.027, 0.322, 0.101, 0.114, 0.01, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m top1: 0.332089552238806
[2m[36m(func pid=5547)[0m top5: 0.8521455223880597
[2m[36m(func pid=5547)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=5547)[0m f1_macro: 0.312240012541531
[2m[36m(func pid=5547)[0m f1_weighted: 0.3567388244636944
[2m[36m(func pid=5547)[0m f1_per_class: [0.212, 0.262, 0.75, 0.304, 0.143, 0.36, 0.515, 0.252, 0.178, 0.145]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0062 | Steps: 2 | Val loss: 6.3884 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.1848 | Steps: 2 | Val loss: 2.5595 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.8448 | Steps: 2 | Val loss: 2.2877 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0000 | Steps: 2 | Val loss: 19.4491 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=188206)[0m top1: 0.3400186567164179
[2m[36m(func pid=188206)[0m top5: 0.8824626865671642
[2m[36m(func pid=188206)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=188206)[0m f1_macro: 0.3275947792437206
[2m[36m(func pid=188206)[0m f1_weighted: 0.3691482688086661
[2m[36m(func pid=188206)[0m f1_per_class: [0.333, 0.284, 0.759, 0.4, 0.082, 0.35, 0.448, 0.258, 0.191, 0.171]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m top1: 0.31203358208955223
[2m[36m(func pid=4309)[0m top5: 0.855410447761194
[2m[36m(func pid=4309)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=4309)[0m f1_macro: 0.3079732534804141
[2m[36m(func pid=4309)[0m f1_weighted: 0.3472508855639114
[2m[36m(func pid=4309)[0m f1_per_class: [0.227, 0.228, 0.786, 0.395, 0.068, 0.335, 0.425, 0.249, 0.211, 0.157]
[2m[36m(func pid=4309)[0m 
== Status ==
Current time: 2024-01-07 02:11:11 (running for 00:37:31.76)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.185 |      0.328 |                   78 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.006 |      0.308 |                   56 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.023 |      0.312 |                   51 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.845 |      0.111 |                   29 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.1623134328358209
[2m[36m(func pid=11358)[0m top5: 0.5895522388059702
[2m[36m(func pid=11358)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=11358)[0m f1_macro: 0.11092337970906833
[2m[36m(func pid=11358)[0m f1_weighted: 0.1628404093980358
[2m[36m(func pid=11358)[0m f1_per_class: [0.055, 0.16, 0.102, 0.216, 0.026, 0.321, 0.098, 0.121, 0.011, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m top1: 0.34375
[2m[36m(func pid=5547)[0m top5: 0.8642723880597015
[2m[36m(func pid=5547)[0m f1_micro: 0.34375
[2m[36m(func pid=5547)[0m f1_macro: 0.3168665307881946
[2m[36m(func pid=5547)[0m f1_weighted: 0.37038913740530777
[2m[36m(func pid=5547)[0m f1_per_class: [0.22, 0.265, 0.727, 0.361, 0.151, 0.358, 0.507, 0.246, 0.183, 0.152]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.1077 | Steps: 2 | Val loss: 2.5817 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0061 | Steps: 2 | Val loss: 6.5159 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.8370 | Steps: 2 | Val loss: 2.2867 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0128 | Steps: 2 | Val loss: 19.2949 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=188206)[0m top1: 0.34095149253731344
[2m[36m(func pid=188206)[0m top5: 0.8791977611940298
[2m[36m(func pid=188206)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=188206)[0m f1_macro: 0.32731909005709386
[2m[36m(func pid=188206)[0m f1_weighted: 0.37094183246983264
[2m[36m(func pid=188206)[0m f1_per_class: [0.333, 0.28, 0.759, 0.398, 0.083, 0.346, 0.459, 0.258, 0.19, 0.167]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m top1: 0.3101679104477612
[2m[36m(func pid=4309)[0m top5: 0.8512126865671642
[2m[36m(func pid=4309)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=4309)[0m f1_macro: 0.30744885460458793
[2m[36m(func pid=4309)[0m f1_weighted: 0.34598350426615626
[2m[36m(func pid=4309)[0m f1_per_class: [0.223, 0.227, 0.786, 0.393, 0.071, 0.331, 0.424, 0.248, 0.212, 0.16]
[2m[36m(func pid=4309)[0m 
== Status ==
Current time: 2024-01-07 02:11:16 (running for 00:37:37.15)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.108 |      0.327 |                   79 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.006 |      0.307 |                   57 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.317 |                   52 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.837 |      0.111 |                   30 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.16277985074626866
[2m[36m(func pid=11358)[0m top5: 0.5886194029850746
[2m[36m(func pid=11358)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=11358)[0m f1_macro: 0.11132904874173863
[2m[36m(func pid=11358)[0m f1_weighted: 0.1649887843625749
[2m[36m(func pid=11358)[0m f1_per_class: [0.055, 0.156, 0.098, 0.22, 0.025, 0.326, 0.102, 0.12, 0.011, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m top1: 0.34654850746268656
[2m[36m(func pid=5547)[0m top5: 0.8628731343283582
[2m[36m(func pid=5547)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=5547)[0m f1_macro: 0.3120146837171256
[2m[36m(func pid=5547)[0m f1_weighted: 0.3737301419457416
[2m[36m(func pid=5547)[0m f1_per_class: [0.187, 0.257, 0.706, 0.401, 0.154, 0.35, 0.492, 0.243, 0.184, 0.147]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2376 | Steps: 2 | Val loss: 2.6086 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.1418 | Steps: 2 | Val loss: 6.5659 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.8261 | Steps: 2 | Val loss: 2.2860 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0000 | Steps: 2 | Val loss: 19.1780 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=188206)[0m top1: 0.3376865671641791
[2m[36m(func pid=188206)[0m top5: 0.8782649253731343
[2m[36m(func pid=188206)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=188206)[0m f1_macro: 0.32722161662818494
[2m[36m(func pid=188206)[0m f1_weighted: 0.36674825033921715
[2m[36m(func pid=188206)[0m f1_per_class: [0.337, 0.281, 0.759, 0.384, 0.083, 0.352, 0.454, 0.26, 0.197, 0.166]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m top1: 0.3050373134328358
[2m[36m(func pid=4309)[0m top5: 0.8535447761194029
[2m[36m(func pid=4309)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=4309)[0m f1_macro: 0.30443364551636715
[2m[36m(func pid=4309)[0m f1_weighted: 0.34078922840805165
[2m[36m(func pid=4309)[0m f1_per_class: [0.212, 0.228, 0.786, 0.388, 0.063, 0.314, 0.417, 0.252, 0.217, 0.167]
[2m[36m(func pid=4309)[0m 
== Status ==
Current time: 2024-01-07 02:11:21 (running for 00:37:42.38)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.238 |      0.327 |                   80 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.142 |      0.304 |                   58 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.013 |      0.312 |                   53 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.826 |      0.112 |                   31 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.1646455223880597
[2m[36m(func pid=11358)[0m top5: 0.5918843283582089
[2m[36m(func pid=11358)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=11358)[0m f1_macro: 0.11225866972313717
[2m[36m(func pid=11358)[0m f1_weighted: 0.16790327473227404
[2m[36m(func pid=11358)[0m f1_per_class: [0.054, 0.154, 0.098, 0.229, 0.026, 0.326, 0.104, 0.12, 0.011, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3521455223880597
[2m[36m(func pid=5547)[0m top5: 0.8661380597014925
[2m[36m(func pid=5547)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=5547)[0m f1_macro: 0.31041201053807027
[2m[36m(func pid=5547)[0m f1_weighted: 0.3772131255021172
[2m[36m(func pid=5547)[0m f1_per_class: [0.18, 0.247, 0.686, 0.446, 0.152, 0.342, 0.468, 0.259, 0.183, 0.141]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.1089 | Steps: 2 | Val loss: 2.6272 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0959 | Steps: 2 | Val loss: 6.5856 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.8354 | Steps: 2 | Val loss: 2.2819 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=188206)[0m top1: 0.3344216417910448
[2m[36m(func pid=188206)[0m top5: 0.875
[2m[36m(func pid=188206)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=188206)[0m f1_macro: 0.32284427555313305
[2m[36m(func pid=188206)[0m f1_weighted: 0.36471619411182
[2m[36m(func pid=188206)[0m f1_per_class: [0.317, 0.276, 0.759, 0.39, 0.084, 0.346, 0.45, 0.258, 0.189, 0.16]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0000 | Steps: 2 | Val loss: 19.4772 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=4309)[0m top1: 0.30970149253731344
[2m[36m(func pid=4309)[0m top5: 0.8540111940298507
[2m[36m(func pid=4309)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=4309)[0m f1_macro: 0.30804615235786126
[2m[36m(func pid=4309)[0m f1_weighted: 0.3447837923630352
[2m[36m(func pid=4309)[0m f1_per_class: [0.215, 0.227, 0.786, 0.391, 0.066, 0.323, 0.422, 0.254, 0.226, 0.17]
[2m[36m(func pid=4309)[0m 
== Status ==
Current time: 2024-01-07 02:11:26 (running for 00:37:47.66)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.109 |      0.323 |                   81 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.096 |      0.308 |                   59 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.31  |                   54 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.835 |      0.114 |                   32 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.16744402985074627
[2m[36m(func pid=11358)[0m top5: 0.5984141791044776
[2m[36m(func pid=11358)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=11358)[0m f1_macro: 0.11394506095673702
[2m[36m(func pid=11358)[0m f1_weighted: 0.1712154838372825
[2m[36m(func pid=11358)[0m f1_per_class: [0.056, 0.158, 0.095, 0.232, 0.027, 0.325, 0.109, 0.127, 0.01, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0904 | Steps: 2 | Val loss: 2.6458 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=5547)[0m top1: 0.34328358208955223
[2m[36m(func pid=5547)[0m top5: 0.8624067164179104
[2m[36m(func pid=5547)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=5547)[0m f1_macro: 0.30451471506483363
[2m[36m(func pid=5547)[0m f1_weighted: 0.3645347135567323
[2m[36m(func pid=5547)[0m f1_per_class: [0.154, 0.233, 0.706, 0.454, 0.154, 0.323, 0.434, 0.266, 0.185, 0.137]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0201 | Steps: 2 | Val loss: 6.6103 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=188206)[0m top1: 0.3376865671641791
[2m[36m(func pid=188206)[0m top5: 0.8754664179104478
[2m[36m(func pid=188206)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=188206)[0m f1_macro: 0.3267424191653131
[2m[36m(func pid=188206)[0m f1_weighted: 0.3679179175391342
[2m[36m(func pid=188206)[0m f1_per_class: [0.319, 0.281, 0.759, 0.389, 0.082, 0.359, 0.451, 0.264, 0.194, 0.169]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.8305 | Steps: 2 | Val loss: 2.2842 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.1163 | Steps: 2 | Val loss: 19.8700 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=4309)[0m top1: 0.31296641791044777
[2m[36m(func pid=4309)[0m top5: 0.8465485074626866
[2m[36m(func pid=4309)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=4309)[0m f1_macro: 0.31064565897345287
[2m[36m(func pid=4309)[0m f1_weighted: 0.3490792940769036
[2m[36m(func pid=4309)[0m f1_per_class: [0.213, 0.224, 0.786, 0.397, 0.067, 0.33, 0.428, 0.27, 0.226, 0.166]
[2m[36m(func pid=4309)[0m 
== Status ==
Current time: 2024-01-07 02:11:32 (running for 00:37:52.95)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.09  |      0.327 |                   82 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.02  |      0.311 |                   60 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.305 |                   55 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.83  |      0.116 |                   33 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.16791044776119404
[2m[36m(func pid=11358)[0m top5: 0.6021455223880597
[2m[36m(func pid=11358)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=11358)[0m f1_macro: 0.11607111356473235
[2m[36m(func pid=11358)[0m f1_weighted: 0.17246629248172649
[2m[36m(func pid=11358)[0m f1_per_class: [0.063, 0.151, 0.095, 0.229, 0.026, 0.325, 0.116, 0.144, 0.01, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.1457 | Steps: 2 | Val loss: 2.6533 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=5547)[0m top1: 0.34281716417910446
[2m[36m(func pid=5547)[0m top5: 0.8586753731343284
[2m[36m(func pid=5547)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=5547)[0m f1_macro: 0.3006496904926136
[2m[36m(func pid=5547)[0m f1_weighted: 0.359806267292401
[2m[36m(func pid=5547)[0m f1_per_class: [0.182, 0.229, 0.667, 0.467, 0.139, 0.33, 0.405, 0.261, 0.197, 0.131]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0804 | Steps: 2 | Val loss: 6.6197 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=188206)[0m top1: 0.34048507462686567
[2m[36m(func pid=188206)[0m top5: 0.8717350746268657
[2m[36m(func pid=188206)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=188206)[0m f1_macro: 0.32862614726059025
[2m[36m(func pid=188206)[0m f1_weighted: 0.37197563304104825
[2m[36m(func pid=188206)[0m f1_per_class: [0.321, 0.28, 0.759, 0.394, 0.081, 0.36, 0.461, 0.267, 0.195, 0.171]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.8404 | Steps: 2 | Val loss: 2.2857 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0001 | Steps: 2 | Val loss: 20.4021 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 02:11:37 (running for 00:37:58.13)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.146 |      0.329 |                   83 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.08  |      0.307 |                   61 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.116 |      0.301 |                   56 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.83  |      0.116 |                   33 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=4309)[0m top1: 0.3069029850746269
[2m[36m(func pid=4309)[0m top5: 0.8470149253731343
[2m[36m(func pid=4309)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=4309)[0m f1_macro: 0.3067479271655143
[2m[36m(func pid=4309)[0m f1_weighted: 0.3414285630377106
[2m[36m(func pid=4309)[0m f1_per_class: [0.208, 0.224, 0.786, 0.39, 0.07, 0.332, 0.411, 0.256, 0.221, 0.17]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=11358)[0m top1: 0.16557835820895522
[2m[36m(func pid=11358)[0m top5: 0.6002798507462687
[2m[36m(func pid=11358)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=11358)[0m f1_macro: 0.11560783639478547
[2m[36m(func pid=11358)[0m f1_weighted: 0.16955385154715838
[2m[36m(func pid=11358)[0m f1_per_class: [0.07, 0.155, 0.103, 0.221, 0.026, 0.329, 0.114, 0.117, 0.02, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.1615 | Steps: 2 | Val loss: 2.6223 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=5547)[0m top1: 0.3423507462686567
[2m[36m(func pid=5547)[0m top5: 0.8549440298507462
[2m[36m(func pid=5547)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=5547)[0m f1_macro: 0.3014942642447032
[2m[36m(func pid=5547)[0m f1_weighted: 0.35421290801320177
[2m[36m(func pid=5547)[0m f1_per_class: [0.196, 0.209, 0.686, 0.476, 0.135, 0.332, 0.386, 0.26, 0.205, 0.13]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m top1: 0.34375
[2m[36m(func pid=188206)[0m top5: 0.8759328358208955
[2m[36m(func pid=188206)[0m f1_micro: 0.34375
[2m[36m(func pid=188206)[0m f1_macro: 0.3319331908419049
[2m[36m(func pid=188206)[0m f1_weighted: 0.375203195828402
[2m[36m(func pid=188206)[0m f1_per_class: [0.317, 0.278, 0.786, 0.4, 0.087, 0.356, 0.467, 0.269, 0.19, 0.167]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.8369 | Steps: 2 | Val loss: 2.2840 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0106 | Steps: 2 | Val loss: 6.6513 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0599 | Steps: 2 | Val loss: 20.9039 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 02:11:42 (running for 00:38:03.41)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.162 |      0.332 |                   84 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.08  |      0.307 |                   61 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.301 |                   57 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.837 |      0.118 |                   35 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=4309)[0m top1: 0.3101679104477612
[2m[36m(func pid=4309)[0m top5: 0.8451492537313433
[2m[36m(func pid=4309)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=4309)[0m f1_macro: 0.31001283819515957
[2m[36m(func pid=4309)[0m f1_weighted: 0.34361957109408764
[2m[36m(func pid=4309)[0m f1_per_class: [0.212, 0.224, 0.786, 0.402, 0.071, 0.333, 0.403, 0.267, 0.23, 0.172]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=11358)[0m top1: 0.16557835820895522
[2m[36m(func pid=11358)[0m top5: 0.6026119402985075
[2m[36m(func pid=11358)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=11358)[0m f1_macro: 0.11845647878134033
[2m[36m(func pid=11358)[0m f1_weighted: 0.1690996662320255
[2m[36m(func pid=11358)[0m f1_per_class: [0.07, 0.153, 0.133, 0.221, 0.026, 0.331, 0.111, 0.129, 0.01, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.1095 | Steps: 2 | Val loss: 2.6430 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=5547)[0m top1: 0.3414179104477612
[2m[36m(func pid=5547)[0m top5: 0.8465485074626866
[2m[36m(func pid=5547)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=5547)[0m f1_macro: 0.29954563471234075
[2m[36m(func pid=5547)[0m f1_weighted: 0.34635593539354165
[2m[36m(func pid=5547)[0m f1_per_class: [0.186, 0.193, 0.727, 0.488, 0.114, 0.332, 0.357, 0.269, 0.203, 0.127]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m top1: 0.345615671641791
[2m[36m(func pid=188206)[0m top5: 0.8736007462686567
[2m[36m(func pid=188206)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=188206)[0m f1_macro: 0.33095842650432367
[2m[36m(func pid=188206)[0m f1_weighted: 0.37648286091108385
[2m[36m(func pid=188206)[0m f1_per_class: [0.33, 0.28, 0.759, 0.406, 0.088, 0.35, 0.467, 0.267, 0.194, 0.169]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.7822 | Steps: 2 | Val loss: 2.2827 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0200 | Steps: 2 | Val loss: 6.7061 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.1012 | Steps: 2 | Val loss: 21.4013 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 02:11:48 (running for 00:38:08.81)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.109 |      0.331 |                   85 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.011 |      0.31  |                   62 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.06  |      0.3   |                   58 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.782 |      0.12  |                   36 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2183 | Steps: 2 | Val loss: 2.6448 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=11358)[0m top1: 0.16884328358208955
[2m[36m(func pid=11358)[0m top5: 0.6030783582089553
[2m[36m(func pid=11358)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=11358)[0m f1_macro: 0.1198749164661643
[2m[36m(func pid=11358)[0m f1_weighted: 0.17257626531865358
[2m[36m(func pid=11358)[0m f1_per_class: [0.07, 0.16, 0.129, 0.225, 0.033, 0.332, 0.115, 0.124, 0.01, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m top1: 0.3069029850746269
[2m[36m(func pid=4309)[0m top5: 0.8456156716417911
[2m[36m(func pid=4309)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=4309)[0m f1_macro: 0.3077847422464105
[2m[36m(func pid=4309)[0m f1_weighted: 0.3398768572217003
[2m[36m(func pid=4309)[0m f1_per_class: [0.202, 0.219, 0.786, 0.402, 0.071, 0.338, 0.392, 0.271, 0.226, 0.172]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.34701492537313433
[2m[36m(func pid=5547)[0m top5: 0.8484141791044776
[2m[36m(func pid=5547)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=5547)[0m f1_macro: 0.30314135738983694
[2m[36m(func pid=5547)[0m f1_weighted: 0.3472880167191755
[2m[36m(func pid=5547)[0m f1_per_class: [0.215, 0.174, 0.71, 0.502, 0.116, 0.334, 0.348, 0.302, 0.207, 0.123]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m top1: 0.3423507462686567
[2m[36m(func pid=188206)[0m top5: 0.875
[2m[36m(func pid=188206)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=188206)[0m f1_macro: 0.3292369630401137
[2m[36m(func pid=188206)[0m f1_weighted: 0.3720244113534023
[2m[36m(func pid=188206)[0m f1_per_class: [0.31, 0.278, 0.759, 0.393, 0.09, 0.34, 0.467, 0.279, 0.196, 0.18]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.8232 | Steps: 2 | Val loss: 2.2801 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0017 | Steps: 2 | Val loss: 6.6964 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0001 | Steps: 2 | Val loss: 22.2300 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 02:11:53 (running for 00:38:14.21)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.218 |      0.329 |                   86 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.02  |      0.308 |                   63 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.101 |      0.303 |                   59 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.823 |      0.119 |                   37 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.1309 | Steps: 2 | Val loss: 2.6693 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=11358)[0m top1: 0.16744402985074627
[2m[36m(func pid=11358)[0m top5: 0.6110074626865671
[2m[36m(func pid=11358)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=11358)[0m f1_macro: 0.11937047902112004
[2m[36m(func pid=11358)[0m f1_weighted: 0.1712621832534311
[2m[36m(func pid=11358)[0m f1_per_class: [0.07, 0.152, 0.119, 0.22, 0.032, 0.338, 0.115, 0.136, 0.01, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m top1: 0.30970149253731344
[2m[36m(func pid=4309)[0m top5: 0.8423507462686567
[2m[36m(func pid=4309)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=4309)[0m f1_macro: 0.31055790871623346
[2m[36m(func pid=4309)[0m f1_weighted: 0.3420475282569948
[2m[36m(func pid=4309)[0m f1_per_class: [0.201, 0.218, 0.786, 0.404, 0.075, 0.341, 0.395, 0.273, 0.237, 0.175]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.34468283582089554
[2m[36m(func pid=5547)[0m top5: 0.8428171641791045
[2m[36m(func pid=5547)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=5547)[0m f1_macro: 0.30334166890009273
[2m[36m(func pid=5547)[0m f1_weighted: 0.3368137307609774
[2m[36m(func pid=5547)[0m f1_per_class: [0.237, 0.137, 0.759, 0.511, 0.114, 0.309, 0.334, 0.295, 0.219, 0.12]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m top1: 0.34048507462686567
[2m[36m(func pid=188206)[0m top5: 0.8717350746268657
[2m[36m(func pid=188206)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=188206)[0m f1_macro: 0.326763696641989
[2m[36m(func pid=188206)[0m f1_weighted: 0.3712264507798196
[2m[36m(func pid=188206)[0m f1_per_class: [0.309, 0.276, 0.759, 0.4, 0.091, 0.341, 0.463, 0.265, 0.189, 0.175]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.7916 | Steps: 2 | Val loss: 2.2774 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0226 | Steps: 2 | Val loss: 6.7934 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0839 | Steps: 2 | Val loss: 22.7100 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2056 | Steps: 2 | Val loss: 2.6676 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 02:11:58 (running for 00:38:19.53)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.131 |      0.327 |                   87 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.002 |      0.311 |                   64 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.303 |                   60 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.792 |      0.121 |                   38 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.16977611940298507
[2m[36m(func pid=11358)[0m top5: 0.6128731343283582
[2m[36m(func pid=11358)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=11358)[0m f1_macro: 0.1213714078247603
[2m[36m(func pid=11358)[0m f1_weighted: 0.17393925820414985
[2m[36m(func pid=11358)[0m f1_per_class: [0.07, 0.155, 0.121, 0.223, 0.033, 0.342, 0.119, 0.131, 0.021, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m top1: 0.30830223880597013
[2m[36m(func pid=4309)[0m top5: 0.8409514925373134
[2m[36m(func pid=4309)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=4309)[0m f1_macro: 0.30989894960205217
[2m[36m(func pid=4309)[0m f1_weighted: 0.34048113307909955
[2m[36m(func pid=4309)[0m f1_per_class: [0.196, 0.208, 0.786, 0.408, 0.075, 0.357, 0.386, 0.274, 0.239, 0.17]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.34421641791044777
[2m[36m(func pid=5547)[0m top5: 0.8414179104477612
[2m[36m(func pid=5547)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=5547)[0m f1_macro: 0.3044124137652398
[2m[36m(func pid=5547)[0m f1_weighted: 0.33388867256953547
[2m[36m(func pid=5547)[0m f1_per_class: [0.244, 0.135, 0.774, 0.509, 0.111, 0.293, 0.33, 0.304, 0.222, 0.122]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m top1: 0.34328358208955223
[2m[36m(func pid=188206)[0m top5: 0.8736007462686567
[2m[36m(func pid=188206)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=188206)[0m f1_macro: 0.3326742671046176
[2m[36m(func pid=188206)[0m f1_weighted: 0.3730680738189249
[2m[36m(func pid=188206)[0m f1_per_class: [0.324, 0.286, 0.786, 0.399, 0.094, 0.344, 0.46, 0.27, 0.188, 0.175]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.7959 | Steps: 2 | Val loss: 2.2769 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.1763 | Steps: 2 | Val loss: 6.8848 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.1645 | Steps: 2 | Val loss: 23.3331 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0774 | Steps: 2 | Val loss: 2.6953 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 02:12:04 (running for 00:38:24.84)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.206 |      0.333 |                   88 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.023 |      0.31  |                   65 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.084 |      0.304 |                   61 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.796 |      0.122 |                   39 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.16977611940298507
[2m[36m(func pid=11358)[0m top5: 0.6142723880597015
[2m[36m(func pid=11358)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=11358)[0m f1_macro: 0.12189386729600367
[2m[36m(func pid=11358)[0m f1_weighted: 0.175281886016987
[2m[36m(func pid=11358)[0m f1_per_class: [0.07, 0.158, 0.119, 0.228, 0.032, 0.338, 0.117, 0.137, 0.02, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m top1: 0.3064365671641791
[2m[36m(func pid=4309)[0m top5: 0.840018656716418
[2m[36m(func pid=4309)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=4309)[0m f1_macro: 0.3104040794989665
[2m[36m(func pid=4309)[0m f1_weighted: 0.3358654643585723
[2m[36m(func pid=4309)[0m f1_per_class: [0.201, 0.203, 0.786, 0.409, 0.076, 0.359, 0.367, 0.295, 0.235, 0.173]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3451492537313433
[2m[36m(func pid=5547)[0m top5: 0.8386194029850746
[2m[36m(func pid=5547)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=5547)[0m f1_macro: 0.29885420963720405
[2m[36m(func pid=5547)[0m f1_weighted: 0.3327621744522667
[2m[36m(func pid=5547)[0m f1_per_class: [0.234, 0.134, 0.733, 0.514, 0.114, 0.29, 0.325, 0.309, 0.215, 0.121]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m top1: 0.34048507462686567
[2m[36m(func pid=188206)[0m top5: 0.8726679104477612
[2m[36m(func pid=188206)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=188206)[0m f1_macro: 0.32842343125050744
[2m[36m(func pid=188206)[0m f1_weighted: 0.37003670917722625
[2m[36m(func pid=188206)[0m f1_per_class: [0.323, 0.282, 0.759, 0.396, 0.094, 0.342, 0.457, 0.267, 0.19, 0.175]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.7935 | Steps: 2 | Val loss: 2.2793 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0045 | Steps: 2 | Val loss: 6.9227 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2422 | Steps: 2 | Val loss: 23.5135 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0743 | Steps: 2 | Val loss: 2.7211 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 02:12:09 (running for 00:38:30.25)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.077 |      0.328 |                   89 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.176 |      0.31  |                   66 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.164 |      0.299 |                   62 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.794 |      0.121 |                   40 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.16837686567164178
[2m[36m(func pid=11358)[0m top5: 0.6091417910447762
[2m[36m(func pid=11358)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=11358)[0m f1_macro: 0.12146789987332784
[2m[36m(func pid=11358)[0m f1_weighted: 0.1732851374826829
[2m[36m(func pid=11358)[0m f1_per_class: [0.068, 0.152, 0.119, 0.222, 0.032, 0.34, 0.118, 0.143, 0.02, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m top1: 0.3111007462686567
[2m[36m(func pid=4309)[0m top5: 0.8395522388059702
[2m[36m(func pid=4309)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=4309)[0m f1_macro: 0.3111349239737843
[2m[36m(func pid=4309)[0m f1_weighted: 0.33934130112437283
[2m[36m(func pid=4309)[0m f1_per_class: [0.203, 0.196, 0.786, 0.427, 0.078, 0.365, 0.366, 0.284, 0.235, 0.172]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.34701492537313433
[2m[36m(func pid=5547)[0m top5: 0.8386194029850746
[2m[36m(func pid=5547)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=5547)[0m f1_macro: 0.30282897560007915
[2m[36m(func pid=5547)[0m f1_weighted: 0.3369325204629438
[2m[36m(func pid=5547)[0m f1_per_class: [0.275, 0.129, 0.733, 0.514, 0.103, 0.276, 0.342, 0.313, 0.22, 0.123]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m top1: 0.3400186567164179
[2m[36m(func pid=188206)[0m top5: 0.8722014925373134
[2m[36m(func pid=188206)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=188206)[0m f1_macro: 0.3316481169693098
[2m[36m(func pid=188206)[0m f1_weighted: 0.3694716793700728
[2m[36m(func pid=188206)[0m f1_per_class: [0.33, 0.281, 0.786, 0.397, 0.093, 0.341, 0.454, 0.27, 0.189, 0.176]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.7798 | Steps: 2 | Val loss: 2.2803 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0017 | Steps: 2 | Val loss: 6.9286 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2386 | Steps: 2 | Val loss: 2.7719 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.2269 | Steps: 2 | Val loss: 23.5498 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 02:12:14 (running for 00:38:35.45)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.074 |      0.332 |                   90 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.005 |      0.311 |                   67 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.242 |      0.303 |                   63 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.78  |      0.122 |                   41 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.16930970149253732
[2m[36m(func pid=11358)[0m top5: 0.6035447761194029
[2m[36m(func pid=11358)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=11358)[0m f1_macro: 0.12211034589402263
[2m[36m(func pid=11358)[0m f1_weighted: 0.1752781191881994
[2m[36m(func pid=11358)[0m f1_per_class: [0.077, 0.154, 0.111, 0.229, 0.032, 0.331, 0.119, 0.149, 0.02, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=4309)[0m top1: 0.31296641791044777
[2m[36m(func pid=4309)[0m top5: 0.8381529850746269
[2m[36m(func pid=4309)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=4309)[0m f1_macro: 0.31257876555989716
[2m[36m(func pid=4309)[0m f1_weighted: 0.3416526219804503
[2m[36m(func pid=4309)[0m f1_per_class: [0.202, 0.195, 0.786, 0.433, 0.076, 0.363, 0.368, 0.286, 0.246, 0.171]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.34421641791044777
[2m[36m(func pid=5547)[0m top5: 0.8395522388059702
[2m[36m(func pid=5547)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=5547)[0m f1_macro: 0.3027132899421784
[2m[36m(func pid=5547)[0m f1_weighted: 0.33631386578264544
[2m[36m(func pid=5547)[0m f1_per_class: [0.286, 0.133, 0.71, 0.513, 0.123, 0.27, 0.343, 0.299, 0.231, 0.12]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=188206)[0m top1: 0.33302238805970147
[2m[36m(func pid=188206)[0m top5: 0.8698694029850746
[2m[36m(func pid=188206)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=188206)[0m f1_macro: 0.3222764233899294
[2m[36m(func pid=188206)[0m f1_weighted: 0.36357240371635446
[2m[36m(func pid=188206)[0m f1_per_class: [0.304, 0.279, 0.759, 0.386, 0.089, 0.335, 0.453, 0.264, 0.184, 0.172]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.7708 | Steps: 2 | Val loss: 2.2762 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0032 | Steps: 2 | Val loss: 6.9712 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0675 | Steps: 2 | Val loss: 2.7922 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0000 | Steps: 2 | Val loss: 23.4907 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 02:12:20 (running for 00:38:41.01)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.239 |      0.322 |                   91 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.002 |      0.313 |                   68 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  1.227 |      0.303 |                   64 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.771 |      0.125 |                   42 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.1730410447761194
[2m[36m(func pid=11358)[0m top5: 0.6147388059701493
[2m[36m(func pid=11358)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=11358)[0m f1_macro: 0.12469426579190905
[2m[36m(func pid=11358)[0m f1_weighted: 0.18056072815580068
[2m[36m(func pid=11358)[0m f1_per_class: [0.078, 0.155, 0.111, 0.232, 0.032, 0.338, 0.129, 0.152, 0.02, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m top1: 0.333955223880597
[2m[36m(func pid=188206)[0m top5: 0.8684701492537313
[2m[36m(func pid=188206)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=188206)[0m f1_macro: 0.3223630129334921
[2m[36m(func pid=188206)[0m f1_weighted: 0.3652922987969664
[2m[36m(func pid=188206)[0m f1_per_class: [0.308, 0.278, 0.759, 0.393, 0.087, 0.33, 0.453, 0.267, 0.179, 0.169]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m top1: 0.31902985074626866
[2m[36m(func pid=4309)[0m top5: 0.8344216417910447
[2m[36m(func pid=4309)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=4309)[0m f1_macro: 0.31711310514823976
[2m[36m(func pid=4309)[0m f1_weighted: 0.3470153141441513
[2m[36m(func pid=4309)[0m f1_per_class: [0.21, 0.197, 0.786, 0.441, 0.079, 0.369, 0.372, 0.295, 0.255, 0.167]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.34281716417910446
[2m[36m(func pid=5547)[0m top5: 0.8353544776119403
[2m[36m(func pid=5547)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=5547)[0m f1_macro: 0.3056681268738384
[2m[36m(func pid=5547)[0m f1_weighted: 0.33745323444474334
[2m[36m(func pid=5547)[0m f1_per_class: [0.276, 0.138, 0.71, 0.507, 0.144, 0.271, 0.347, 0.305, 0.235, 0.124]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.7998 | Steps: 2 | Val loss: 2.2729 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.1137 | Steps: 2 | Val loss: 2.7926 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0086 | Steps: 2 | Val loss: 7.0112 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.1131 | Steps: 2 | Val loss: 23.4250 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 02:12:25 (running for 00:38:46.28)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.067 |      0.322 |                   92 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.003 |      0.317 |                   69 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.306 |                   65 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.8   |      0.124 |                   43 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.17257462686567165
[2m[36m(func pid=11358)[0m top5: 0.6222014925373134
[2m[36m(func pid=11358)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=11358)[0m f1_macro: 0.12399329140597826
[2m[36m(func pid=11358)[0m f1_weighted: 0.18110786100645176
[2m[36m(func pid=11358)[0m f1_per_class: [0.078, 0.154, 0.111, 0.237, 0.031, 0.335, 0.13, 0.144, 0.019, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m top1: 0.3316231343283582
[2m[36m(func pid=188206)[0m top5: 0.8652052238805971
[2m[36m(func pid=188206)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=188206)[0m f1_macro: 0.32452508974109134
[2m[36m(func pid=188206)[0m f1_weighted: 0.36281936709106133
[2m[36m(func pid=188206)[0m f1_per_class: [0.303, 0.269, 0.786, 0.392, 0.09, 0.342, 0.446, 0.27, 0.179, 0.168]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m top1: 0.3180970149253731
[2m[36m(func pid=4309)[0m top5: 0.8362873134328358
[2m[36m(func pid=4309)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=4309)[0m f1_macro: 0.3144739150715006
[2m[36m(func pid=4309)[0m f1_weighted: 0.3446235078640144
[2m[36m(func pid=4309)[0m f1_per_class: [0.205, 0.199, 0.786, 0.444, 0.08, 0.37, 0.361, 0.299, 0.234, 0.166]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.34328358208955223
[2m[36m(func pid=5547)[0m top5: 0.8316231343283582
[2m[36m(func pid=5547)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=5547)[0m f1_macro: 0.30303183364784664
[2m[36m(func pid=5547)[0m f1_weighted: 0.34233213597435186
[2m[36m(func pid=5547)[0m f1_per_class: [0.27, 0.147, 0.688, 0.504, 0.125, 0.27, 0.361, 0.317, 0.224, 0.125]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.7579 | Steps: 2 | Val loss: 2.2731 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0861 | Steps: 2 | Val loss: 2.8162 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0013 | Steps: 2 | Val loss: 7.0636 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 02:12:30 (running for 00:38:51.48)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.114 |      0.325 |                   93 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.009 |      0.314 |                   70 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.113 |      0.303 |                   66 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.758 |      0.124 |                   44 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.17117537313432835
[2m[36m(func pid=11358)[0m top5: 0.6189365671641791
[2m[36m(func pid=11358)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=11358)[0m f1_macro: 0.12416898824996978
[2m[36m(func pid=11358)[0m f1_weighted: 0.1792797775345043
[2m[36m(func pid=11358)[0m f1_per_class: [0.087, 0.141, 0.108, 0.238, 0.031, 0.332, 0.128, 0.157, 0.019, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.2081 | Steps: 2 | Val loss: 23.2580 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=188206)[0m top1: 0.3278917910447761
[2m[36m(func pid=188206)[0m top5: 0.8647388059701493
[2m[36m(func pid=188206)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=188206)[0m f1_macro: 0.32165606579808564
[2m[36m(func pid=188206)[0m f1_weighted: 0.3601701583424867
[2m[36m(func pid=188206)[0m f1_per_class: [0.302, 0.263, 0.786, 0.39, 0.086, 0.343, 0.444, 0.261, 0.176, 0.165]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m top1: 0.31763059701492535
[2m[36m(func pid=4309)[0m top5: 0.8362873134328358
[2m[36m(func pid=4309)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=4309)[0m f1_macro: 0.31431699113053746
[2m[36m(func pid=4309)[0m f1_weighted: 0.3414003772697582
[2m[36m(func pid=4309)[0m f1_per_class: [0.208, 0.192, 0.786, 0.446, 0.084, 0.376, 0.349, 0.302, 0.234, 0.166]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3451492537313433
[2m[36m(func pid=5547)[0m top5: 0.8302238805970149
[2m[36m(func pid=5547)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=5547)[0m f1_macro: 0.310658162152765
[2m[36m(func pid=5547)[0m f1_weighted: 0.3501242071043957
[2m[36m(func pid=5547)[0m f1_per_class: [0.307, 0.171, 0.706, 0.497, 0.108, 0.274, 0.376, 0.317, 0.221, 0.13]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.7555 | Steps: 2 | Val loss: 2.2707 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0633 | Steps: 2 | Val loss: 2.8418 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0007 | Steps: 2 | Val loss: 7.0341 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 02:12:36 (running for 00:38:56.90)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.086 |      0.322 |                   94 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.001 |      0.314 |                   71 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  1.208 |      0.311 |                   67 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.755 |      0.127 |                   45 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0000 | Steps: 2 | Val loss: 23.0184 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=11358)[0m top1: 0.17630597014925373
[2m[36m(func pid=11358)[0m top5: 0.6198694029850746
[2m[36m(func pid=11358)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=11358)[0m f1_macro: 0.1274144486346745
[2m[36m(func pid=11358)[0m f1_weighted: 0.18331719362231272
[2m[36m(func pid=11358)[0m f1_per_class: [0.091, 0.143, 0.104, 0.239, 0.031, 0.344, 0.133, 0.158, 0.03, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m top1: 0.32882462686567165
[2m[36m(func pid=188206)[0m top5: 0.863339552238806
[2m[36m(func pid=188206)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=188206)[0m f1_macro: 0.3231568021613584
[2m[36m(func pid=188206)[0m f1_weighted: 0.3609611252478669
[2m[36m(func pid=188206)[0m f1_per_class: [0.3, 0.266, 0.786, 0.391, 0.086, 0.346, 0.441, 0.272, 0.18, 0.165]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m top1: 0.3208955223880597
[2m[36m(func pid=4309)[0m top5: 0.8381529850746269
[2m[36m(func pid=4309)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=4309)[0m f1_macro: 0.31677828447148976
[2m[36m(func pid=4309)[0m f1_weighted: 0.3437813209354216
[2m[36m(func pid=4309)[0m f1_per_class: [0.211, 0.183, 0.786, 0.45, 0.088, 0.374, 0.357, 0.308, 0.246, 0.165]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=5547)[0m top1: 0.34095149253731344
[2m[36m(func pid=5547)[0m top5: 0.8255597014925373
[2m[36m(func pid=5547)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=5547)[0m f1_macro: 0.3083056070209195
[2m[36m(func pid=5547)[0m f1_weighted: 0.35202653167695347
[2m[36m(func pid=5547)[0m f1_per_class: [0.289, 0.172, 0.706, 0.492, 0.101, 0.281, 0.391, 0.292, 0.222, 0.138]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.7959 | Steps: 2 | Val loss: 2.2706 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0831 | Steps: 2 | Val loss: 2.8531 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 02:12:41 (running for 00:39:02.22)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.063 |      0.323 |                   95 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.001 |      0.317 |                   72 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.308 |                   68 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.796 |      0.131 |                   46 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.17583955223880596
[2m[36m(func pid=11358)[0m top5: 0.6231343283582089
[2m[36m(func pid=11358)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=11358)[0m f1_macro: 0.13076128899834263
[2m[36m(func pid=11358)[0m f1_weighted: 0.18320363918333674
[2m[36m(func pid=11358)[0m f1_per_class: [0.091, 0.146, 0.135, 0.237, 0.031, 0.338, 0.133, 0.166, 0.03, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0046 | Steps: 2 | Val loss: 23.0408 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0023 | Steps: 2 | Val loss: 7.0495 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=188206)[0m top1: 0.3292910447761194
[2m[36m(func pid=188206)[0m top5: 0.863339552238806
[2m[36m(func pid=188206)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=188206)[0m f1_macro: 0.3229678939530841
[2m[36m(func pid=188206)[0m f1_weighted: 0.3619230177530588
[2m[36m(func pid=188206)[0m f1_per_class: [0.294, 0.263, 0.786, 0.398, 0.085, 0.344, 0.439, 0.275, 0.182, 0.163]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=4309)[0m top1: 0.324160447761194
[2m[36m(func pid=4309)[0m top5: 0.8334888059701493
[2m[36m(func pid=4309)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=4309)[0m f1_macro: 0.3169526270062643
[2m[36m(func pid=4309)[0m f1_weighted: 0.3477856835596816
[2m[36m(func pid=4309)[0m f1_per_class: [0.213, 0.187, 0.786, 0.458, 0.087, 0.372, 0.364, 0.303, 0.235, 0.165]
[2m[36m(func pid=5547)[0m top1: 0.3344216417910448
[2m[36m(func pid=5547)[0m top5: 0.8213619402985075
[2m[36m(func pid=5547)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=5547)[0m f1_macro: 0.30386945582580993
[2m[36m(func pid=5547)[0m f1_weighted: 0.3504449055353849
[2m[36m(func pid=5547)[0m f1_per_class: [0.286, 0.188, 0.686, 0.471, 0.088, 0.256, 0.406, 0.287, 0.224, 0.147]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.7732 | Steps: 2 | Val loss: 2.2703 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2396 | Steps: 2 | Val loss: 2.8660 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 02:12:46 (running for 00:39:07.43)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.083 |      0.323 |                   96 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.002 |      0.317 |                   73 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.005 |      0.304 |                   69 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.773 |      0.133 |                   47 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.1791044776119403
[2m[36m(func pid=11358)[0m top5: 0.6245335820895522
[2m[36m(func pid=11358)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=11358)[0m f1_macro: 0.13290099791941917
[2m[36m(func pid=11358)[0m f1_weighted: 0.1863265766454859
[2m[36m(func pid=11358)[0m f1_per_class: [0.094, 0.16, 0.133, 0.234, 0.031, 0.341, 0.137, 0.168, 0.03, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0000 | Steps: 2 | Val loss: 23.1866 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0024 | Steps: 2 | Val loss: 7.0641 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=188206)[0m top1: 0.3260261194029851
[2m[36m(func pid=188206)[0m top5: 0.8591417910447762
[2m[36m(func pid=188206)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=188206)[0m f1_macro: 0.3212312435928014
[2m[36m(func pid=188206)[0m f1_weighted: 0.3590539967279417
[2m[36m(func pid=188206)[0m f1_per_class: [0.3, 0.259, 0.786, 0.394, 0.083, 0.346, 0.437, 0.268, 0.181, 0.158]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.8116 | Steps: 2 | Val loss: 2.2697 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=5547)[0m top1: 0.33115671641791045
[2m[36m(func pid=5547)[0m top5: 0.816231343283582
[2m[36m(func pid=5547)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=5547)[0m f1_macro: 0.3023191144724273
[2m[36m(func pid=5547)[0m f1_weighted: 0.35092841766607386
[2m[36m(func pid=5547)[0m f1_per_class: [0.271, 0.189, 0.706, 0.46, 0.087, 0.239, 0.428, 0.275, 0.21, 0.158]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m top1: 0.3218283582089552
[2m[36m(func pid=4309)[0m top5: 0.8325559701492538
[2m[36m(func pid=4309)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=4309)[0m f1_macro: 0.3170358908304413
[2m[36m(func pid=4309)[0m f1_weighted: 0.3430073048109821
[2m[36m(func pid=4309)[0m f1_per_class: [0.213, 0.181, 0.786, 0.457, 0.089, 0.372, 0.349, 0.309, 0.248, 0.167]
[2m[36m(func pid=4309)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0427 | Steps: 2 | Val loss: 2.8876 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 02:12:51 (running for 00:39:12.62)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.24  |      0.321 |                   97 |
| train_66d79_00018 | RUNNING    | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.002 |      0.317 |                   74 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.302 |                   70 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.812 |      0.133 |                   48 |
| train_66d79_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.177705223880597
[2m[36m(func pid=11358)[0m top5: 0.6184701492537313
[2m[36m(func pid=11358)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=11358)[0m f1_macro: 0.13290068996940874
[2m[36m(func pid=11358)[0m f1_weighted: 0.18345955095001049
[2m[36m(func pid=11358)[0m f1_per_class: [0.096, 0.172, 0.146, 0.225, 0.032, 0.342, 0.131, 0.156, 0.03, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0000 | Steps: 2 | Val loss: 23.3009 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=4309)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.8438 | Steps: 2 | Val loss: 7.0470 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=188206)[0m top1: 0.3255597014925373
[2m[36m(func pid=188206)[0m top5: 0.8549440298507462
[2m[36m(func pid=188206)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=188206)[0m f1_macro: 0.32140961251893674
[2m[36m(func pid=188206)[0m f1_weighted: 0.35873832173733794
[2m[36m(func pid=188206)[0m f1_per_class: [0.291, 0.256, 0.786, 0.392, 0.09, 0.346, 0.44, 0.267, 0.181, 0.165]
[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.7225 | Steps: 2 | Val loss: 2.2658 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=5547)[0m top1: 0.33255597014925375
[2m[36m(func pid=5547)[0m top5: 0.8143656716417911
[2m[36m(func pid=5547)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=5547)[0m f1_macro: 0.304794291403992
[2m[36m(func pid=5547)[0m f1_weighted: 0.3562177563046577
[2m[36m(func pid=5547)[0m f1_per_class: [0.274, 0.196, 0.706, 0.441, 0.08, 0.239, 0.458, 0.284, 0.212, 0.158]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=4309)[0m top1: 0.32649253731343286
[2m[36m(func pid=4309)[0m top5: 0.8353544776119403
[2m[36m(func pid=4309)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=4309)[0m f1_macro: 0.31947342332116835
[2m[36m(func pid=4309)[0m f1_weighted: 0.34846592666969356
[2m[36m(func pid=4309)[0m f1_per_class: [0.22, 0.181, 0.786, 0.459, 0.091, 0.374, 0.364, 0.315, 0.241, 0.163]
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2160 | Steps: 2 | Val loss: 2.8974 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=11358)[0m top1: 0.18236940298507462
[2m[36m(func pid=11358)[0m top5: 0.6268656716417911
[2m[36m(func pid=11358)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=11358)[0m f1_macro: 0.13580607007172435
[2m[36m(func pid=11358)[0m f1_weighted: 0.18924202826150777
[2m[36m(func pid=11358)[0m f1_per_class: [0.105, 0.166, 0.135, 0.237, 0.033, 0.35, 0.137, 0.167, 0.03, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m top1: 0.3278917910447761
[2m[36m(func pid=188206)[0m top5: 0.8540111940298507
[2m[36m(func pid=188206)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=188206)[0m f1_macro: 0.32149832702220393
[2m[36m(func pid=188206)[0m f1_weighted: 0.3610624674585687
[2m[36m(func pid=188206)[0m f1_per_class: [0.286, 0.255, 0.786, 0.399, 0.094, 0.343, 0.444, 0.265, 0.181, 0.163]
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0002 | Steps: 2 | Val loss: 23.3787 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.7697 | Steps: 2 | Val loss: 2.2609 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=5547)[0m top1: 0.3306902985074627
[2m[36m(func pid=5547)[0m top5: 0.8143656716417911
[2m[36m(func pid=5547)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=5547)[0m f1_macro: 0.30592038701817165
[2m[36m(func pid=5547)[0m f1_weighted: 0.3558890677110656
[2m[36m(func pid=5547)[0m f1_per_class: [0.271, 0.211, 0.686, 0.422, 0.078, 0.236, 0.464, 0.285, 0.239, 0.168]
[2m[36m(func pid=11358)[0m top1: 0.18236940298507462
[2m[36m(func pid=11358)[0m top5: 0.625
[2m[36m(func pid=11358)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=11358)[0m f1_macro: 0.13908831744147981
[2m[36m(func pid=11358)[0m f1_weighted: 0.1889131617144983
[2m[36m(func pid=11358)[0m f1_per_class: [0.124, 0.155, 0.159, 0.251, 0.033, 0.343, 0.13, 0.169, 0.028, 0.0]
== Status ==
Current time: 2024-01-07 02:12:57 (running for 00:39:18.33)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.043 |      0.321 |                   98 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.305 |                   71 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.723 |      0.136 |                   49 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m 
[2m[36m(func pid=22158)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=22158)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=22158)[0m Configuration completed!
[2m[36m(func pid=22158)[0m New optimizer parameters:
[2m[36m(func pid=22158)[0m SGD (
[2m[36m(func pid=22158)[0m Parameter Group 0
[2m[36m(func pid=22158)[0m     dampening: 0
[2m[36m(func pid=22158)[0m     differentiable: False
[2m[36m(func pid=22158)[0m     foreach: None
[2m[36m(func pid=22158)[0m     lr: 0.001
[2m[36m(func pid=22158)[0m     maximize: False
[2m[36m(func pid=22158)[0m     momentum: 0.9
[2m[36m(func pid=22158)[0m     nesterov: False
[2m[36m(func pid=22158)[0m     weight_decay: 1e-05
[2m[36m(func pid=22158)[0m )
[2m[36m(func pid=22158)[0m 
== Status ==
Current time: 2024-01-07 02:13:04 (running for 00:39:25.43)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.216 |      0.321 |                   99 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.305 |                   71 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.723 |      0.136 |                   49 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=188206)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0905 | Steps: 2 | Val loss: 2.9301 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.7762 | Steps: 2 | Val loss: 2.2619 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2199 | Steps: 2 | Val loss: 24.2544 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9660 | Steps: 2 | Val loss: 2.3191 | Batch size: 32 | lr: 0.001 | Duration: 4.47s
== Status ==
Current time: 2024-01-07 02:13:09 (running for 00:39:30.44)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00017 | RUNNING    | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.216 |      0.321 |                   99 |
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.306 |                   72 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.77  |      0.139 |                   50 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=188206)[0m top1: 0.3255597014925373
[2m[36m(func pid=188206)[0m top5: 0.8530783582089553
[2m[36m(func pid=188206)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=188206)[0m f1_macro: 0.3201232068187098
[2m[36m(func pid=188206)[0m f1_weighted: 0.3586988010141135
[2m[36m(func pid=188206)[0m f1_per_class: [0.284, 0.256, 0.786, 0.392, 0.09, 0.333, 0.445, 0.269, 0.185, 0.162]
[2m[36m(func pid=11358)[0m top1: 0.18190298507462688
[2m[36m(func pid=11358)[0m top5: 0.6273320895522388
[2m[36m(func pid=11358)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=11358)[0m f1_macro: 0.14106527511228512
[2m[36m(func pid=11358)[0m f1_weighted: 0.18805043759287932
[2m[36m(func pid=11358)[0m f1_per_class: [0.134, 0.152, 0.176, 0.25, 0.033, 0.342, 0.129, 0.168, 0.027, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m top1: 0.3180970149253731
[2m[36m(func pid=5547)[0m top5: 0.8097014925373134
[2m[36m(func pid=5547)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=5547)[0m f1_macro: 0.30067841242898713
[2m[36m(func pid=5547)[0m f1_weighted: 0.3451904112641267
[2m[36m(func pid=5547)[0m f1_per_class: [0.264, 0.209, 0.686, 0.379, 0.073, 0.253, 0.465, 0.278, 0.225, 0.174]
[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=22158)[0m top1: 0.17257462686567165
[2m[36m(func pid=22158)[0m top5: 0.5340485074626866
[2m[36m(func pid=22158)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=22158)[0m f1_macro: 0.11812033993562125
[2m[36m(func pid=22158)[0m f1_weighted: 0.12310607152569139
[2m[36m(func pid=22158)[0m f1_per_class: [0.325, 0.346, 0.0, 0.09, 0.0, 0.202, 0.021, 0.0, 0.0, 0.197]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.7492 | Steps: 2 | Val loss: 2.2621 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0001 | Steps: 2 | Val loss: 25.0241 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9704 | Steps: 2 | Val loss: 2.3185 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=11358)[0m top1: 0.18097014925373134
[2m[36m(func pid=11358)[0m top5: 0.6226679104477612
[2m[36m(func pid=11358)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=11358)[0m f1_macro: 0.13984718857011677
[2m[36m(func pid=11358)[0m f1_weighted: 0.18856458086985545
[2m[36m(func pid=11358)[0m f1_per_class: [0.129, 0.148, 0.17, 0.255, 0.027, 0.34, 0.128, 0.173, 0.027, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=5547)[0m top1: 0.30830223880597013
[2m[36m(func pid=5547)[0m top5: 0.7938432835820896
[2m[36m(func pid=5547)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=5547)[0m f1_macro: 0.29683216862727885
[2m[36m(func pid=5547)[0m f1_weighted: 0.3359469917288977
[2m[36m(func pid=5547)[0m f1_per_class: [0.253, 0.206, 0.667, 0.355, 0.073, 0.249, 0.462, 0.259, 0.25, 0.194]
[2m[36m(func pid=22158)[0m top1: 0.18423507462686567
[2m[36m(func pid=22158)[0m top5: 0.5340485074626866
[2m[36m(func pid=22158)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=22158)[0m f1_macro: 0.1186099760627394
[2m[36m(func pid=22158)[0m f1_weighted: 0.1319302641431296
[2m[36m(func pid=22158)[0m f1_per_class: [0.304, 0.333, 0.0, 0.1, 0.01, 0.278, 0.018, 0.036, 0.0, 0.107]
== Status ==
Current time: 2024-01-07 02:13:15 (running for 00:39:36.12)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0.22  |      0.301 |                   73 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.749 |      0.14  |                   52 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.966 |      0.118 |                    1 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5547)[0m 
[2m[36m(func pid=22837)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22837)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=22837)[0m Configuration completed!
[2m[36m(func pid=22837)[0m New optimizer parameters:
[2m[36m(func pid=22837)[0m SGD (
[2m[36m(func pid=22837)[0m Parameter Group 0
[2m[36m(func pid=22837)[0m     dampening: 0
[2m[36m(func pid=22837)[0m     differentiable: False
[2m[36m(func pid=22837)[0m     foreach: None
[2m[36m(func pid=22837)[0m     lr: 0.01
[2m[36m(func pid=22837)[0m     maximize: False
[2m[36m(func pid=22837)[0m     momentum: 0.9
[2m[36m(func pid=22837)[0m     nesterov: False
[2m[36m(func pid=22837)[0m     weight_decay: 1e-05
[2m[36m(func pid=22837)[0m )
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.7436 | Steps: 2 | Val loss: 2.2600 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 02:13:20 (running for 00:39:41.49)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00019 | RUNNING    | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.297 |                   74 |
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.744 |      0.141 |                   53 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.97  |      0.119 |                    2 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m top1: 0.18330223880597016
[2m[36m(func pid=11358)[0m top5: 0.6333955223880597
[2m[36m(func pid=11358)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=11358)[0m f1_macro: 0.14138488986519598
[2m[36m(func pid=11358)[0m f1_weighted: 0.19129124867751893
[2m[36m(func pid=11358)[0m f1_per_class: [0.128, 0.148, 0.176, 0.256, 0.026, 0.347, 0.134, 0.17, 0.028, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9062 | Steps: 2 | Val loss: 2.3211 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=5547)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0001 | Steps: 2 | Val loss: 25.7624 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9970 | Steps: 2 | Val loss: 2.3132 | Batch size: 32 | lr: 0.01 | Duration: 4.40s
[2m[36m(func pid=22158)[0m top1: 0.18703358208955223
[2m[36m(func pid=22158)[0m top5: 0.523320895522388
[2m[36m(func pid=22158)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=22158)[0m f1_macro: 0.1189564047463745
[2m[36m(func pid=22158)[0m f1_weighted: 0.13671082209194227
[2m[36m(func pid=22158)[0m f1_per_class: [0.299, 0.312, 0.0, 0.114, 0.011, 0.331, 0.015, 0.033, 0.0, 0.075]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=5547)[0m top1: 0.2994402985074627
[2m[36m(func pid=5547)[0m top5: 0.7859141791044776
[2m[36m(func pid=5547)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=5547)[0m f1_macro: 0.29328192055349245
[2m[36m(func pid=5547)[0m f1_weighted: 0.3261551675725008
[2m[36m(func pid=5547)[0m f1_per_class: [0.251, 0.204, 0.667, 0.32, 0.074, 0.259, 0.461, 0.252, 0.245, 0.2]
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.7175 | Steps: 2 | Val loss: 2.2574 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=22837)[0m top1: 0.18563432835820895
[2m[36m(func pid=22837)[0m top5: 0.5359141791044776
[2m[36m(func pid=22837)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=22837)[0m f1_macro: 0.13322836550345357
[2m[36m(func pid=22837)[0m f1_weighted: 0.13505999000890878
[2m[36m(func pid=22837)[0m f1_per_class: [0.393, 0.352, 0.0, 0.11, 0.0, 0.2, 0.027, 0.04, 0.0, 0.211]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9100 | Steps: 2 | Val loss: 2.3222 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=11358)[0m top1: 0.18470149253731344
[2m[36m(func pid=11358)[0m top5: 0.6380597014925373
[2m[36m(func pid=11358)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=11358)[0m f1_macro: 0.1403864385534676
[2m[36m(func pid=11358)[0m f1_weighted: 0.19414490874463844
[2m[36m(func pid=11358)[0m f1_per_class: [0.125, 0.147, 0.158, 0.264, 0.027, 0.348, 0.138, 0.169, 0.028, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8758 | Steps: 2 | Val loss: 2.2869 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=22158)[0m top1: 0.18050373134328357
[2m[36m(func pid=22158)[0m top5: 0.5247201492537313
[2m[36m(func pid=22158)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=22158)[0m f1_macro: 0.11873000859728902
[2m[36m(func pid=22158)[0m f1_weighted: 0.1380433768534004
[2m[36m(func pid=22158)[0m f1_per_class: [0.315, 0.304, 0.0, 0.12, 0.01, 0.321, 0.024, 0.019, 0.0, 0.075]
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.7218 | Steps: 2 | Val loss: 2.2603 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=22837)[0m top1: 0.20755597014925373
[2m[36m(func pid=22837)[0m top5: 0.5517723880597015
[2m[36m(func pid=22837)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=22837)[0m f1_macro: 0.1681572724869507
[2m[36m(func pid=22837)[0m f1_weighted: 0.16841571186645657
[2m[36m(func pid=22837)[0m f1_per_class: [0.37, 0.338, 0.08, 0.147, 0.021, 0.301, 0.044, 0.197, 0.0, 0.183]
[2m[36m(func pid=11358)[0m top1: 0.18516791044776118
[2m[36m(func pid=11358)[0m top5: 0.6324626865671642
[2m[36m(func pid=11358)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=11358)[0m f1_macro: 0.14026334602660176
[2m[36m(func pid=11358)[0m f1_weighted: 0.19491594564980164
[2m[36m(func pid=11358)[0m f1_per_class: [0.137, 0.148, 0.151, 0.259, 0.027, 0.356, 0.143, 0.163, 0.018, 0.0]
== Status ==
Current time: 2024-01-07 02:13:26 (running for 00:39:46.88)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.717 |      0.14  |                   54 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.906 |      0.119 |                    3 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  2.997 |      0.133 |                    1 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 02:13:31 (running for 00:39:52.66)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.722 |      0.14  |                   55 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.906 |      0.119 |                    3 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  2.997 |      0.133 |                    1 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=23659)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=23659)[0m Configuration completed!
[2m[36m(func pid=23659)[0m New optimizer parameters:
[2m[36m(func pid=23659)[0m SGD (
[2m[36m(func pid=23659)[0m Parameter Group 0
[2m[36m(func pid=23659)[0m     dampening: 0
[2m[36m(func pid=23659)[0m     differentiable: False
[2m[36m(func pid=23659)[0m     foreach: None
[2m[36m(func pid=23659)[0m     lr: 0.1
[2m[36m(func pid=23659)[0m     maximize: False
[2m[36m(func pid=23659)[0m     momentum: 0.9
[2m[36m(func pid=23659)[0m     nesterov: False
[2m[36m(func pid=23659)[0m     weight_decay: 1e-05
[2m[36m(func pid=23659)[0m )
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.7229 | Steps: 2 | Val loss: 2.2566 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9017 | Steps: 2 | Val loss: 2.3190 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.7414 | Steps: 2 | Val loss: 2.2478 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9346 | Steps: 2 | Val loss: 2.3045 | Batch size: 32 | lr: 0.1 | Duration: 4.38s
== Status ==
Current time: 2024-01-07 02:13:36 (running for 00:39:57.69)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.722 |      0.14  |                   55 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.91  |      0.119 |                    4 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  2.876 |      0.168 |                    2 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.228544776119403
[2m[36m(func pid=22837)[0m top5: 0.6100746268656716
[2m[36m(func pid=22837)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=22837)[0m f1_macro: 0.20544317557612493
[2m[36m(func pid=22837)[0m f1_weighted: 0.1939208439292565
[2m[36m(func pid=22837)[0m f1_per_class: [0.397, 0.34, 0.198, 0.203, 0.029, 0.288, 0.055, 0.272, 0.046, 0.226]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.177705223880597
[2m[36m(func pid=22158)[0m top5: 0.5205223880597015
[2m[36m(func pid=22158)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=22158)[0m f1_macro: 0.12578744356404156
[2m[36m(func pid=22158)[0m f1_weighted: 0.14494638664521198
[2m[36m(func pid=22158)[0m f1_per_class: [0.288, 0.289, 0.056, 0.128, 0.01, 0.319, 0.043, 0.043, 0.018, 0.065]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=11358)[0m top1: 0.18563432835820895
[2m[36m(func pid=11358)[0m top5: 0.6403917910447762
[2m[36m(func pid=11358)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=11358)[0m f1_macro: 0.139585678312517
[2m[36m(func pid=11358)[0m f1_weighted: 0.1965305372173201
[2m[36m(func pid=11358)[0m f1_per_class: [0.111, 0.143, 0.139, 0.257, 0.033, 0.361, 0.151, 0.165, 0.037, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=23659)[0m top1: 0.15391791044776118
[2m[36m(func pid=23659)[0m top5: 0.542910447761194
[2m[36m(func pid=23659)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=23659)[0m f1_macro: 0.1342872051956929
[2m[36m(func pid=23659)[0m f1_weighted: 0.12866504894110373
[2m[36m(func pid=23659)[0m f1_per_class: [0.12, 0.32, 0.146, 0.132, 0.0, 0.053, 0.021, 0.314, 0.0, 0.236]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.5617 | Steps: 2 | Val loss: 2.2243 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8302 | Steps: 2 | Val loss: 2.3116 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.7235 | Steps: 2 | Val loss: 2.2533 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.0209 | Steps: 2 | Val loss: 2.2265 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 02:13:42 (running for 00:40:03.22)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.723 |      0.14  |                   56 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.902 |      0.126 |                    5 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  2.562 |      0.191 |                    4 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  2.935 |      0.134 |                    1 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.2150186567164179
[2m[36m(func pid=22837)[0m top5: 0.6436567164179104
[2m[36m(func pid=22837)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=22837)[0m f1_macro: 0.19144962902064638
[2m[36m(func pid=22837)[0m f1_weighted: 0.1891606468234048
[2m[36m(func pid=22837)[0m f1_per_class: [0.28, 0.307, 0.138, 0.217, 0.053, 0.299, 0.044, 0.287, 0.079, 0.211]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.17863805970149255
[2m[36m(func pid=22158)[0m top5: 0.5401119402985075
[2m[36m(func pid=22158)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=22158)[0m f1_macro: 0.12703891856959268
[2m[36m(func pid=22158)[0m f1_weighted: 0.15513734289591302
[2m[36m(func pid=22158)[0m f1_per_class: [0.24, 0.28, 0.043, 0.136, 0.019, 0.335, 0.066, 0.078, 0.014, 0.059]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=11358)[0m top1: 0.1884328358208955
[2m[36m(func pid=11358)[0m top5: 0.6427238805970149
[2m[36m(func pid=11358)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=11358)[0m f1_macro: 0.14310792003537548
[2m[36m(func pid=11358)[0m f1_weighted: 0.1989769465746033
[2m[36m(func pid=11358)[0m f1_per_class: [0.114, 0.15, 0.16, 0.262, 0.04, 0.359, 0.15, 0.168, 0.028, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=23659)[0m top1: 0.18423507462686567
[2m[36m(func pid=23659)[0m top5: 0.7220149253731343
[2m[36m(func pid=23659)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=23659)[0m f1_macro: 0.203692643606704
[2m[36m(func pid=23659)[0m f1_weighted: 0.17804086485282156
[2m[36m(func pid=23659)[0m f1_per_class: [0.103, 0.284, 0.282, 0.187, 0.048, 0.298, 0.028, 0.423, 0.078, 0.305]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.2612 | Steps: 2 | Val loss: 2.1869 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.6985 | Steps: 2 | Val loss: 2.2509 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8259 | Steps: 2 | Val loss: 2.3004 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.2003 | Steps: 2 | Val loss: 2.1635 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 02:13:47 (running for 00:40:08.45)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.723 |      0.143 |                   57 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.83  |      0.127 |                    6 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  2.261 |      0.209 |                    5 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  2.021 |      0.204 |                    2 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.22667910447761194
[2m[36m(func pid=22837)[0m top5: 0.6730410447761194
[2m[36m(func pid=22837)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=22837)[0m f1_macro: 0.2087713658689474
[2m[36m(func pid=22837)[0m f1_weighted: 0.20383685194270962
[2m[36m(func pid=22837)[0m f1_per_class: [0.211, 0.303, 0.147, 0.236, 0.073, 0.371, 0.041, 0.324, 0.098, 0.283]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=11358)[0m top1: 0.1875
[2m[36m(func pid=11358)[0m top5: 0.6436567164179104
[2m[36m(func pid=11358)[0m f1_micro: 0.1875
[2m[36m(func pid=11358)[0m f1_macro: 0.14339703190626724
[2m[36m(func pid=11358)[0m f1_weighted: 0.1983031396027851
[2m[36m(func pid=11358)[0m f1_per_class: [0.113, 0.147, 0.158, 0.264, 0.04, 0.356, 0.148, 0.171, 0.037, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=22158)[0m top1: 0.18190298507462688
[2m[36m(func pid=22158)[0m top5: 0.5592350746268657
[2m[36m(func pid=22158)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=22158)[0m f1_macro: 0.1411903183314694
[2m[36m(func pid=22158)[0m f1_weighted: 0.16764442409762845
[2m[36m(func pid=22158)[0m f1_per_class: [0.208, 0.257, 0.115, 0.163, 0.026, 0.357, 0.081, 0.104, 0.036, 0.065]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.21828358208955223
[2m[36m(func pid=23659)[0m top5: 0.8521455223880597
[2m[36m(func pid=23659)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=23659)[0m f1_macro: 0.2552065032720346
[2m[36m(func pid=23659)[0m f1_weighted: 0.22588113690100942
[2m[36m(func pid=23659)[0m f1_per_class: [0.095, 0.143, 0.545, 0.275, 0.111, 0.389, 0.157, 0.327, 0.109, 0.4]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.0094 | Steps: 2 | Val loss: 2.1473 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.7007 | Steps: 2 | Val loss: 2.2504 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8055 | Steps: 2 | Val loss: 2.2843 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.7006 | Steps: 2 | Val loss: 2.4637 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 02:13:52 (running for 00:40:13.51)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.699 |      0.143 |                   58 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.826 |      0.141 |                    7 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  2.009 |      0.222 |                    6 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  1.2   |      0.255 |                    3 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.23274253731343283
[2m[36m(func pid=22837)[0m top5: 0.7103544776119403
[2m[36m(func pid=22837)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=22837)[0m f1_macro: 0.22210625044659493
[2m[36m(func pid=22837)[0m f1_weighted: 0.2140991837207879
[2m[36m(func pid=22837)[0m f1_per_class: [0.194, 0.279, 0.216, 0.258, 0.074, 0.406, 0.053, 0.337, 0.099, 0.306]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=11358)[0m top1: 0.1875
[2m[36m(func pid=11358)[0m top5: 0.644589552238806
[2m[36m(func pid=11358)[0m f1_micro: 0.1875
[2m[36m(func pid=11358)[0m f1_macro: 0.14528878101192957
[2m[36m(func pid=11358)[0m f1_weighted: 0.19763304875196924
[2m[36m(func pid=11358)[0m f1_per_class: [0.133, 0.158, 0.158, 0.255, 0.04, 0.35, 0.148, 0.173, 0.037, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=22158)[0m top1: 0.18983208955223882
[2m[36m(func pid=22158)[0m top5: 0.5783582089552238
[2m[36m(func pid=22158)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=22158)[0m f1_macro: 0.15036076408802218
[2m[36m(func pid=22158)[0m f1_weighted: 0.1840176495554114
[2m[36m(func pid=22158)[0m f1_per_class: [0.209, 0.242, 0.133, 0.184, 0.025, 0.365, 0.115, 0.146, 0.023, 0.062]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.23647388059701493
[2m[36m(func pid=23659)[0m top5: 0.8003731343283582
[2m[36m(func pid=23659)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=23659)[0m f1_macro: 0.2688594012974107
[2m[36m(func pid=23659)[0m f1_weighted: 0.23922914151507826
[2m[36m(func pid=23659)[0m f1_per_class: [0.106, 0.137, 0.579, 0.322, 0.131, 0.407, 0.155, 0.304, 0.117, 0.431]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.8144 | Steps: 2 | Val loss: 2.0844 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7770 | Steps: 2 | Val loss: 2.2756 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.7113 | Steps: 2 | Val loss: 2.2489 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5992 | Steps: 2 | Val loss: 2.5505 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=22837)[0m top1: 0.23600746268656717
[2m[36m(func pid=22837)[0m top5: 0.7513992537313433
[2m[36m(func pid=22837)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=22837)[0m f1_macro: 0.2337633414361005
[2m[36m(func pid=22837)[0m f1_weighted: 0.2229422747627193
[2m[36m(func pid=22837)[0m f1_per_class: [0.168, 0.228, 0.329, 0.299, 0.079, 0.408, 0.069, 0.351, 0.102, 0.306]
[2m[36m(func pid=22837)[0m 
== Status ==
Current time: 2024-01-07 02:13:58 (running for 00:40:18.79)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.701 |      0.145 |                   59 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.806 |      0.15  |                    8 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  1.814 |      0.234 |                    7 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.701 |      0.269 |                    4 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22158)[0m top1: 0.1875
[2m[36m(func pid=22158)[0m top5: 0.5960820895522388
[2m[36m(func pid=22158)[0m f1_micro: 0.1875
[2m[36m(func pid=22158)[0m f1_macro: 0.15182673258484108
[2m[36m(func pid=22158)[0m f1_weighted: 0.1852236735042968
[2m[36m(func pid=22158)[0m f1_per_class: [0.208, 0.231, 0.139, 0.183, 0.018, 0.359, 0.123, 0.174, 0.021, 0.062]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=11358)[0m top1: 0.19029850746268656
[2m[36m(func pid=11358)[0m top5: 0.6455223880597015
[2m[36m(func pid=11358)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=11358)[0m f1_macro: 0.14656477901471435
[2m[36m(func pid=11358)[0m f1_weighted: 0.19965250445407343
[2m[36m(func pid=11358)[0m f1_per_class: [0.133, 0.16, 0.157, 0.257, 0.039, 0.362, 0.146, 0.182, 0.029, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=23659)[0m top1: 0.2583955223880597
[2m[36m(func pid=23659)[0m top5: 0.8078358208955224
[2m[36m(func pid=23659)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=23659)[0m f1_macro: 0.2841814024377326
[2m[36m(func pid=23659)[0m f1_weighted: 0.2735060514888276
[2m[36m(func pid=23659)[0m f1_per_class: [0.139, 0.153, 0.667, 0.344, 0.121, 0.415, 0.243, 0.271, 0.099, 0.39]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.5882 | Steps: 2 | Val loss: 2.0115 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7261 | Steps: 2 | Val loss: 2.2629 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.7380 | Steps: 2 | Val loss: 2.2476 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.2268 | Steps: 2 | Val loss: 2.6125 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 02:14:03 (running for 00:40:24.01)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.711 |      0.147 |                   60 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.777 |      0.152 |                    9 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  1.588 |      0.254 |                    8 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.599 |      0.284 |                    5 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.2579291044776119
[2m[36m(func pid=22837)[0m top5: 0.7910447761194029
[2m[36m(func pid=22837)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=22837)[0m f1_macro: 0.254283378506807
[2m[36m(func pid=22837)[0m f1_weighted: 0.25191903257874126
[2m[36m(func pid=22837)[0m f1_per_class: [0.164, 0.224, 0.421, 0.343, 0.089, 0.414, 0.124, 0.344, 0.097, 0.322]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.18983208955223882
[2m[36m(func pid=22158)[0m top5: 0.613339552238806
[2m[36m(func pid=22158)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=22158)[0m f1_macro: 0.15647081613802882
[2m[36m(func pid=22158)[0m f1_weighted: 0.19305081143722122
[2m[36m(func pid=22158)[0m f1_per_class: [0.208, 0.206, 0.156, 0.215, 0.025, 0.356, 0.133, 0.178, 0.02, 0.068]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=11358)[0m top1: 0.18889925373134328
[2m[36m(func pid=11358)[0m top5: 0.6478544776119403
[2m[36m(func pid=11358)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=11358)[0m f1_macro: 0.14352061191848664
[2m[36m(func pid=11358)[0m f1_weighted: 0.19911479637122986
[2m[36m(func pid=11358)[0m f1_per_class: [0.127, 0.159, 0.144, 0.254, 0.034, 0.366, 0.149, 0.173, 0.028, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=23659)[0m top1: 0.2943097014925373
[2m[36m(func pid=23659)[0m top5: 0.8227611940298507
[2m[36m(func pid=23659)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=23659)[0m f1_macro: 0.2995249507970952
[2m[36m(func pid=23659)[0m f1_weighted: 0.3316565678388705
[2m[36m(func pid=23659)[0m f1_per_class: [0.178, 0.17, 0.75, 0.321, 0.105, 0.367, 0.482, 0.203, 0.082, 0.337]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.4404 | Steps: 2 | Val loss: 1.9360 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.6375 | Steps: 2 | Val loss: 2.2482 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.6473 | Steps: 2 | Val loss: 2.2484 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.2725 | Steps: 2 | Val loss: 2.8252 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 02:14:08 (running for 00:40:29.19)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.738 |      0.144 |                   61 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.726 |      0.156 |                   10 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  1.44  |      0.28  |                    9 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.227 |      0.3   |                    6 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.2873134328358209
[2m[36m(func pid=22837)[0m top5: 0.8180970149253731
[2m[36m(func pid=22837)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=22837)[0m f1_macro: 0.27987110909297785
[2m[36m(func pid=22837)[0m f1_weighted: 0.29350548287354866
[2m[36m(func pid=22837)[0m f1_per_class: [0.184, 0.224, 0.522, 0.381, 0.081, 0.415, 0.223, 0.348, 0.115, 0.307]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=11358)[0m top1: 0.18889925373134328
[2m[36m(func pid=11358)[0m top5: 0.6511194029850746
[2m[36m(func pid=11358)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=11358)[0m f1_macro: 0.14439966274169475
[2m[36m(func pid=11358)[0m f1_weighted: 0.19993178259283698
[2m[36m(func pid=11358)[0m f1_per_class: [0.129, 0.161, 0.147, 0.252, 0.04, 0.358, 0.155, 0.173, 0.028, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=22158)[0m top1: 0.19309701492537312
[2m[36m(func pid=22158)[0m top5: 0.6371268656716418
[2m[36m(func pid=22158)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=22158)[0m f1_macro: 0.1634746572212198
[2m[36m(func pid=22158)[0m f1_weighted: 0.2010064794323195
[2m[36m(func pid=22158)[0m f1_per_class: [0.19, 0.191, 0.171, 0.234, 0.031, 0.357, 0.148, 0.183, 0.03, 0.098]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.2826492537313433
[2m[36m(func pid=23659)[0m top5: 0.8218283582089553
[2m[36m(func pid=23659)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=23659)[0m f1_macro: 0.2984711241597149
[2m[36m(func pid=23659)[0m f1_weighted: 0.3138812508910902
[2m[36m(func pid=23659)[0m f1_per_class: [0.199, 0.227, 0.727, 0.315, 0.111, 0.349, 0.382, 0.292, 0.117, 0.265]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.2166 | Steps: 2 | Val loss: 1.8833 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.7157 | Steps: 2 | Val loss: 2.2461 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5655 | Steps: 2 | Val loss: 2.2326 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.2265 | Steps: 2 | Val loss: 3.1737 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 02:14:13 (running for 00:40:34.43)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.637 |      0.144 |                   62 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.647 |      0.163 |                   11 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  1.217 |      0.294 |                   10 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.272 |      0.298 |                    7 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3138992537313433
[2m[36m(func pid=22837)[0m top5: 0.8278917910447762
[2m[36m(func pid=22837)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=22837)[0m f1_macro: 0.2938635262466942
[2m[36m(func pid=22837)[0m f1_weighted: 0.3308579686742995
[2m[36m(func pid=22837)[0m f1_per_class: [0.208, 0.224, 0.545, 0.41, 0.075, 0.402, 0.326, 0.34, 0.106, 0.302]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=11358)[0m top1: 0.1884328358208955
[2m[36m(func pid=11358)[0m top5: 0.6557835820895522
[2m[36m(func pid=11358)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=11358)[0m f1_macro: 0.1458551776183176
[2m[36m(func pid=11358)[0m f1_weighted: 0.20013376996910415
[2m[36m(func pid=11358)[0m f1_per_class: [0.149, 0.165, 0.147, 0.254, 0.034, 0.348, 0.153, 0.182, 0.027, 0.0]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=22158)[0m top1: 0.20662313432835822
[2m[36m(func pid=22158)[0m top5: 0.6665111940298507
[2m[36m(func pid=22158)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=22158)[0m f1_macro: 0.1739683078683322
[2m[36m(func pid=22158)[0m f1_weighted: 0.2191715098687376
[2m[36m(func pid=22158)[0m f1_per_class: [0.214, 0.192, 0.171, 0.261, 0.031, 0.363, 0.181, 0.171, 0.031, 0.125]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.25652985074626866
[2m[36m(func pid=23659)[0m top5: 0.8055037313432836
[2m[36m(func pid=23659)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=23659)[0m f1_macro: 0.2824094466621531
[2m[36m(func pid=23659)[0m f1_weighted: 0.2603268818522488
[2m[36m(func pid=23659)[0m f1_per_class: [0.229, 0.232, 0.75, 0.342, 0.098, 0.314, 0.181, 0.31, 0.135, 0.232]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.0342 | Steps: 2 | Val loss: 1.8410 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.6958 | Steps: 2 | Val loss: 2.2452 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.5650 | Steps: 2 | Val loss: 2.2215 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.1601 | Steps: 2 | Val loss: 3.2978 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 02:14:18 (running for 00:40:39.64)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.716 |      0.146 |                   63 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.566 |      0.174 |                   12 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  1.034 |      0.31  |                   11 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.226 |      0.282 |                    8 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.33302238805970147
[2m[36m(func pid=22837)[0m top5: 0.8512126865671642
[2m[36m(func pid=22837)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=22837)[0m f1_macro: 0.30972188956880564
[2m[36m(func pid=22837)[0m f1_weighted: 0.3538046796445038
[2m[36m(func pid=22837)[0m f1_per_class: [0.258, 0.236, 0.595, 0.43, 0.069, 0.401, 0.374, 0.333, 0.121, 0.28]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=11358)[0m top1: 0.18936567164179105
[2m[36m(func pid=11358)[0m top5: 0.6571828358208955
[2m[36m(func pid=11358)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=11358)[0m f1_macro: 0.14711858206823542
[2m[36m(func pid=11358)[0m f1_weighted: 0.20197045330516647
[2m[36m(func pid=11358)[0m f1_per_class: [0.145, 0.156, 0.138, 0.259, 0.035, 0.347, 0.16, 0.176, 0.028, 0.028]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=22158)[0m top1: 0.2126865671641791
[2m[36m(func pid=22158)[0m top5: 0.6800373134328358
[2m[36m(func pid=22158)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=22158)[0m f1_macro: 0.17960626647794325
[2m[36m(func pid=22158)[0m f1_weighted: 0.22872903049023152
[2m[36m(func pid=22158)[0m f1_per_class: [0.21, 0.191, 0.164, 0.267, 0.037, 0.354, 0.207, 0.187, 0.032, 0.147]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.2667910447761194
[2m[36m(func pid=23659)[0m top5: 0.8143656716417911
[2m[36m(func pid=23659)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=23659)[0m f1_macro: 0.2850935441982944
[2m[36m(func pid=23659)[0m f1_weighted: 0.2747074722616637
[2m[36m(func pid=23659)[0m f1_per_class: [0.233, 0.246, 0.75, 0.355, 0.096, 0.318, 0.209, 0.3, 0.147, 0.196]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.0986 | Steps: 2 | Val loss: 1.8146 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5204 | Steps: 2 | Val loss: 2.2100 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6933 | Steps: 2 | Val loss: 2.2472 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.1413 | Steps: 2 | Val loss: 3.1999 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 02:14:24 (running for 00:40:44.75)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.696 |      0.147 |                   64 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.565 |      0.18  |                   13 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  1.099 |      0.318 |                   12 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.16  |      0.285 |                    9 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.34281716417910446
[2m[36m(func pid=22837)[0m top5: 0.8675373134328358
[2m[36m(func pid=22837)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=22837)[0m f1_macro: 0.3181625809604076
[2m[36m(func pid=22837)[0m f1_weighted: 0.36598255686778886
[2m[36m(func pid=22837)[0m f1_per_class: [0.294, 0.25, 0.629, 0.419, 0.068, 0.388, 0.425, 0.276, 0.168, 0.264]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.21921641791044777
[2m[36m(func pid=22158)[0m top5: 0.6944962686567164
[2m[36m(func pid=22158)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=22158)[0m f1_macro: 0.19022677833792703
[2m[36m(func pid=22158)[0m f1_weighted: 0.23484970337066158
[2m[36m(func pid=22158)[0m f1_per_class: [0.219, 0.172, 0.164, 0.283, 0.037, 0.36, 0.213, 0.211, 0.034, 0.208]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=11358)[0m top1: 0.18796641791044777
[2m[36m(func pid=11358)[0m top5: 0.6511194029850746
[2m[36m(func pid=11358)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=11358)[0m f1_macro: 0.1470551354973873
[2m[36m(func pid=11358)[0m f1_weighted: 0.19939205433174165
[2m[36m(func pid=11358)[0m f1_per_class: [0.147, 0.157, 0.137, 0.256, 0.034, 0.352, 0.152, 0.178, 0.028, 0.031]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=23659)[0m top1: 0.3208955223880597
[2m[36m(func pid=23659)[0m top5: 0.8418843283582089
[2m[36m(func pid=23659)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=23659)[0m f1_macro: 0.30402251984633744
[2m[36m(func pid=23659)[0m f1_weighted: 0.35124409367297205
[2m[36m(func pid=23659)[0m f1_per_class: [0.231, 0.24, 0.75, 0.398, 0.095, 0.336, 0.438, 0.225, 0.153, 0.174]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.9251 | Steps: 2 | Val loss: 1.8237 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.4686 | Steps: 2 | Val loss: 2.1993 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.6820 | Steps: 2 | Val loss: 2.2491 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=22837)[0m top1: 0.3376865671641791
[2m[36m(func pid=22837)[0m top5: 0.8759328358208955
[2m[36m(func pid=22837)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=22837)[0m f1_macro: 0.3190291344635237
[2m[36m(func pid=22837)[0m f1_weighted: 0.3627201960396158
[2m[36m(func pid=22837)[0m f1_per_class: [0.301, 0.264, 0.667, 0.412, 0.057, 0.391, 0.413, 0.266, 0.174, 0.246]
[2m[36m(func pid=22837)[0m 
== Status ==
Current time: 2024-01-07 02:14:29 (running for 00:40:49.85)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.693 |      0.147 |                   65 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.52  |      0.19  |                   14 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.925 |      0.319 |                   13 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.141 |      0.304 |                   10 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.1020 | Steps: 2 | Val loss: 3.2982 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=22158)[0m top1: 0.22434701492537312
[2m[36m(func pid=22158)[0m top5: 0.7117537313432836
[2m[36m(func pid=22158)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=22158)[0m f1_macro: 0.19804240532364775
[2m[36m(func pid=22158)[0m f1_weighted: 0.2399860448977289
[2m[36m(func pid=22158)[0m f1_per_class: [0.229, 0.169, 0.15, 0.292, 0.037, 0.367, 0.217, 0.217, 0.038, 0.265]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=11358)[0m top1: 0.1875
[2m[36m(func pid=11358)[0m top5: 0.6478544776119403
[2m[36m(func pid=11358)[0m f1_micro: 0.1875
[2m[36m(func pid=11358)[0m f1_macro: 0.1490610248816474
[2m[36m(func pid=11358)[0m f1_weighted: 0.1977298321868211
[2m[36m(func pid=11358)[0m f1_per_class: [0.166, 0.15, 0.139, 0.255, 0.04, 0.353, 0.148, 0.181, 0.027, 0.032]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=23659)[0m top1: 0.33255597014925375
[2m[36m(func pid=23659)[0m top5: 0.8460820895522388
[2m[36m(func pid=23659)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=23659)[0m f1_macro: 0.29785864187155114
[2m[36m(func pid=23659)[0m f1_weighted: 0.3573027697490229
[2m[36m(func pid=23659)[0m f1_per_class: [0.211, 0.228, 0.774, 0.41, 0.1, 0.305, 0.484, 0.126, 0.157, 0.183]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8704 | Steps: 2 | Val loss: 1.8234 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.6618 | Steps: 2 | Val loss: 2.2480 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.4312 | Steps: 2 | Val loss: 2.1890 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 02:14:34 (running for 00:40:55.25)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.682 |      0.149 |                   66 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.469 |      0.198 |                   15 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.87  |      0.324 |                   14 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.102 |      0.298 |                   11 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.34328358208955223
[2m[36m(func pid=22837)[0m top5: 0.8843283582089553
[2m[36m(func pid=22837)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=22837)[0m f1_macro: 0.32428853287914117
[2m[36m(func pid=22837)[0m f1_weighted: 0.3682522686996383
[2m[36m(func pid=22837)[0m f1_per_class: [0.3, 0.273, 0.688, 0.41, 0.057, 0.397, 0.426, 0.253, 0.193, 0.247]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.1556 | Steps: 2 | Val loss: 3.4462 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=22158)[0m top1: 0.22621268656716417
[2m[36m(func pid=22158)[0m top5: 0.7206156716417911
[2m[36m(func pid=22158)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=22158)[0m f1_macro: 0.1983920652101723
[2m[36m(func pid=22158)[0m f1_weighted: 0.2432361000118941
[2m[36m(func pid=22158)[0m f1_per_class: [0.217, 0.176, 0.14, 0.295, 0.038, 0.363, 0.224, 0.213, 0.039, 0.279]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=11358)[0m top1: 0.1884328358208955
[2m[36m(func pid=11358)[0m top5: 0.6469216417910447
[2m[36m(func pid=11358)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=11358)[0m f1_macro: 0.15711521295315883
[2m[36m(func pid=11358)[0m f1_weighted: 0.19659671638717363
[2m[36m(func pid=11358)[0m f1_per_class: [0.164, 0.141, 0.154, 0.255, 0.041, 0.36, 0.142, 0.191, 0.036, 0.088]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=23659)[0m top1: 0.33302238805970147
[2m[36m(func pid=23659)[0m top5: 0.8306902985074627
[2m[36m(func pid=23659)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=23659)[0m f1_macro: 0.29920359326298634
[2m[36m(func pid=23659)[0m f1_weighted: 0.35981577862406816
[2m[36m(func pid=23659)[0m f1_per_class: [0.21, 0.212, 0.774, 0.435, 0.093, 0.313, 0.473, 0.133, 0.172, 0.175]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8237 | Steps: 2 | Val loss: 1.8340 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.4059 | Steps: 2 | Val loss: 2.1766 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.6865 | Steps: 2 | Val loss: 2.2451 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 02:14:39 (running for 00:41:00.57)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.662 |      0.157 |                   67 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.431 |      0.198 |                   16 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.824 |      0.329 |                   15 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.156 |      0.299 |                   12 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3460820895522388
[2m[36m(func pid=22837)[0m top5: 0.8871268656716418
[2m[36m(func pid=22837)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=22837)[0m f1_macro: 0.3292702519352904
[2m[36m(func pid=22837)[0m f1_weighted: 0.37121625445227774
[2m[36m(func pid=22837)[0m f1_per_class: [0.311, 0.275, 0.688, 0.412, 0.067, 0.392, 0.43, 0.265, 0.218, 0.237]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0409 | Steps: 2 | Val loss: 3.6102 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=22158)[0m top1: 0.23694029850746268
[2m[36m(func pid=22158)[0m top5: 0.7290111940298507
[2m[36m(func pid=22158)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=22158)[0m f1_macro: 0.2037966257465452
[2m[36m(func pid=22158)[0m f1_weighted: 0.2545595522578659
[2m[36m(func pid=22158)[0m f1_per_class: [0.224, 0.178, 0.132, 0.317, 0.046, 0.368, 0.237, 0.218, 0.043, 0.277]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=11358)[0m top1: 0.18889925373134328
[2m[36m(func pid=11358)[0m top5: 0.6520522388059702
[2m[36m(func pid=11358)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=11358)[0m f1_macro: 0.15225301507441052
[2m[36m(func pid=11358)[0m f1_weighted: 0.19817104104070268
[2m[36m(func pid=11358)[0m f1_per_class: [0.169, 0.151, 0.159, 0.254, 0.034, 0.351, 0.148, 0.19, 0.036, 0.031]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=23659)[0m top1: 0.31716417910447764
[2m[36m(func pid=23659)[0m top5: 0.8148320895522388
[2m[36m(func pid=23659)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=23659)[0m f1_macro: 0.3041916146817372
[2m[36m(func pid=23659)[0m f1_weighted: 0.34482049600945675
[2m[36m(func pid=23659)[0m f1_per_class: [0.209, 0.207, 0.774, 0.442, 0.091, 0.331, 0.393, 0.221, 0.191, 0.182]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7131 | Steps: 2 | Val loss: 1.8548 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.3423 | Steps: 2 | Val loss: 2.1645 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.6860 | Steps: 2 | Val loss: 2.2404 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 02:14:45 (running for 00:41:05.86)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.686 |      0.152 |                   68 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.406 |      0.204 |                   17 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.713 |      0.328 |                   16 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.041 |      0.304 |                   13 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3414179104477612
[2m[36m(func pid=22837)[0m top5: 0.8810634328358209
[2m[36m(func pid=22837)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=22837)[0m f1_macro: 0.328273631368314
[2m[36m(func pid=22837)[0m f1_weighted: 0.3675784998990147
[2m[36m(func pid=22837)[0m f1_per_class: [0.315, 0.273, 0.688, 0.408, 0.063, 0.392, 0.419, 0.286, 0.203, 0.237]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0558 | Steps: 2 | Val loss: 3.7295 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=22158)[0m top1: 0.24486940298507462
[2m[36m(func pid=22158)[0m top5: 0.7336753731343284
[2m[36m(func pid=22158)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=22158)[0m f1_macro: 0.2072398430196596
[2m[36m(func pid=22158)[0m f1_weighted: 0.2633141769573972
[2m[36m(func pid=22158)[0m f1_per_class: [0.226, 0.19, 0.133, 0.325, 0.048, 0.378, 0.247, 0.221, 0.045, 0.26]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=11358)[0m top1: 0.18936567164179105
[2m[36m(func pid=11358)[0m top5: 0.6595149253731343
[2m[36m(func pid=11358)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=11358)[0m f1_macro: 0.1518918153934002
[2m[36m(func pid=11358)[0m f1_weighted: 0.19889547052395992
[2m[36m(func pid=11358)[0m f1_per_class: [0.17, 0.142, 0.164, 0.258, 0.035, 0.352, 0.152, 0.189, 0.027, 0.03]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=23659)[0m top1: 0.2947761194029851
[2m[36m(func pid=23659)[0m top5: 0.808768656716418
[2m[36m(func pid=23659)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=23659)[0m f1_macro: 0.2991792925611532
[2m[36m(func pid=23659)[0m f1_weighted: 0.31311611628791947
[2m[36m(func pid=23659)[0m f1_per_class: [0.236, 0.207, 0.774, 0.435, 0.083, 0.328, 0.287, 0.238, 0.208, 0.195]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6704 | Steps: 2 | Val loss: 1.8745 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.3188 | Steps: 2 | Val loss: 2.1556 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.6733 | Steps: 2 | Val loss: 2.2371 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=22837)[0m top1: 0.33722014925373134
[2m[36m(func pid=22837)[0m top5: 0.8754664179104478
[2m[36m(func pid=22837)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=22837)[0m f1_macro: 0.32520750919580166
[2m[36m(func pid=22837)[0m f1_weighted: 0.364943519663741
[2m[36m(func pid=22837)[0m f1_per_class: [0.302, 0.257, 0.688, 0.416, 0.061, 0.38, 0.415, 0.3, 0.201, 0.232]
[2m[36m(func pid=22837)[0m 
== Status ==
Current time: 2024-01-07 02:14:50 (running for 00:41:11.14)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.686 |      0.152 |                   69 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.342 |      0.207 |                   18 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.67  |      0.325 |                   17 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.056 |      0.299 |                   14 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0287 | Steps: 2 | Val loss: 3.8154 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=11358)[0m top1: 0.19076492537313433
[2m[36m(func pid=11358)[0m top5: 0.667910447761194
[2m[36m(func pid=11358)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=11358)[0m f1_macro: 0.15464789557143285
[2m[36m(func pid=11358)[0m f1_weighted: 0.2011711307710519
[2m[36m(func pid=11358)[0m f1_per_class: [0.156, 0.145, 0.164, 0.256, 0.035, 0.358, 0.157, 0.189, 0.027, 0.059]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=22158)[0m top1: 0.24673507462686567
[2m[36m(func pid=22158)[0m top5: 0.738339552238806
[2m[36m(func pid=22158)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=22158)[0m f1_macro: 0.20732822211097246
[2m[36m(func pid=22158)[0m f1_weighted: 0.2651780548215878
[2m[36m(func pid=22158)[0m f1_per_class: [0.212, 0.18, 0.134, 0.33, 0.05, 0.372, 0.256, 0.226, 0.046, 0.267]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.28451492537313433
[2m[36m(func pid=23659)[0m top5: 0.8022388059701493
[2m[36m(func pid=23659)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=23659)[0m f1_macro: 0.29825661493778194
[2m[36m(func pid=23659)[0m f1_weighted: 0.2927386370165498
[2m[36m(func pid=23659)[0m f1_per_class: [0.257, 0.214, 0.774, 0.427, 0.086, 0.329, 0.213, 0.273, 0.215, 0.194]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6341 | Steps: 2 | Val loss: 1.8735 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.6372 | Steps: 2 | Val loss: 2.2392 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.2423 | Steps: 2 | Val loss: 2.1469 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=22837)[0m top1: 0.3414179104477612
[2m[36m(func pid=22837)[0m top5: 0.8708022388059702
[2m[36m(func pid=22837)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=22837)[0m f1_macro: 0.3267985218373692
[2m[36m(func pid=22837)[0m f1_weighted: 0.36797969632271216
[2m[36m(func pid=22837)[0m f1_per_class: [0.314, 0.259, 0.688, 0.431, 0.064, 0.376, 0.413, 0.295, 0.2, 0.23]
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0293 | Steps: 2 | Val loss: 3.8761 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 02:14:55 (running for 00:41:16.37)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.673 |      0.155 |                   70 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.319 |      0.207 |                   19 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.634 |      0.327 |                   18 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.029 |      0.298 |                   15 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=11358)[0m top1: 0.19029850746268656
[2m[36m(func pid=11358)[0m top5: 0.6618470149253731
[2m[36m(func pid=11358)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=11358)[0m f1_macro: 0.1547044034572087
[2m[36m(func pid=11358)[0m f1_weighted: 0.20145534880426594
[2m[36m(func pid=11358)[0m f1_per_class: [0.155, 0.148, 0.158, 0.251, 0.035, 0.356, 0.161, 0.195, 0.027, 0.062]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=22158)[0m top1: 0.25279850746268656
[2m[36m(func pid=22158)[0m top5: 0.7453358208955224
[2m[36m(func pid=22158)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=22158)[0m f1_macro: 0.21307923140348084
[2m[36m(func pid=22158)[0m f1_weighted: 0.27155999923384827
[2m[36m(func pid=22158)[0m f1_per_class: [0.222, 0.18, 0.136, 0.338, 0.048, 0.356, 0.269, 0.258, 0.049, 0.275]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.28777985074626866
[2m[36m(func pid=23659)[0m top5: 0.8003731343283582
[2m[36m(func pid=23659)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=23659)[0m f1_macro: 0.30298571625759846
[2m[36m(func pid=23659)[0m f1_weighted: 0.2918949592047589
[2m[36m(func pid=23659)[0m f1_per_class: [0.266, 0.221, 0.774, 0.43, 0.09, 0.354, 0.19, 0.285, 0.223, 0.197]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6883 | Steps: 2 | Val loss: 1.8929 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.6597 | Steps: 2 | Val loss: 2.2402 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.2110 | Steps: 2 | Val loss: 2.1392 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 02:15:00 (running for 00:41:21.64)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.637 |      0.155 |                   71 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.242 |      0.213 |                   20 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.688 |      0.326 |                   19 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.029 |      0.303 |                   16 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.34048507462686567
[2m[36m(func pid=22837)[0m top5: 0.8689365671641791
[2m[36m(func pid=22837)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=22837)[0m f1_macro: 0.32626793696030343
[2m[36m(func pid=22837)[0m f1_weighted: 0.3673789035820233
[2m[36m(func pid=22837)[0m f1_per_class: [0.298, 0.257, 0.688, 0.426, 0.065, 0.378, 0.412, 0.325, 0.179, 0.233]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.1240 | Steps: 2 | Val loss: 3.7758 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=22158)[0m top1: 0.25093283582089554
[2m[36m(func pid=22158)[0m top5: 0.7476679104477612
[2m[36m(func pid=22158)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=22158)[0m f1_macro: 0.21297414086023844
[2m[36m(func pid=22158)[0m f1_weighted: 0.2687954427694841
[2m[36m(func pid=22158)[0m f1_per_class: [0.213, 0.17, 0.138, 0.34, 0.048, 0.35, 0.264, 0.269, 0.049, 0.288]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=11358)[0m top1: 0.19309701492537312
[2m[36m(func pid=11358)[0m top5: 0.6609141791044776
[2m[36m(func pid=11358)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=11358)[0m f1_macro: 0.16323201177798974
[2m[36m(func pid=11358)[0m f1_weighted: 0.20336437916940842
[2m[36m(func pid=11358)[0m f1_per_class: [0.157, 0.156, 0.158, 0.25, 0.042, 0.355, 0.161, 0.196, 0.036, 0.123]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=23659)[0m top1: 0.29524253731343286
[2m[36m(func pid=23659)[0m top5: 0.8213619402985075
[2m[36m(func pid=23659)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=23659)[0m f1_macro: 0.30623848431446066
[2m[36m(func pid=23659)[0m f1_weighted: 0.30723136490664643
[2m[36m(func pid=23659)[0m f1_per_class: [0.275, 0.232, 0.774, 0.421, 0.092, 0.364, 0.245, 0.262, 0.208, 0.19]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5071 | Steps: 2 | Val loss: 1.9026 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.1889 | Steps: 2 | Val loss: 2.1297 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.6243 | Steps: 2 | Val loss: 2.2389 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 02:15:06 (running for 00:41:26.74)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.66  |      0.163 |                   72 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.211 |      0.213 |                   21 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.507 |      0.331 |                   20 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.124 |      0.306 |                   17 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.34328358208955223
[2m[36m(func pid=22837)[0m top5: 0.8708022388059702
[2m[36m(func pid=22837)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=22837)[0m f1_macro: 0.3309580759879224
[2m[36m(func pid=22837)[0m f1_weighted: 0.36954242307425317
[2m[36m(func pid=22837)[0m f1_per_class: [0.294, 0.258, 0.71, 0.415, 0.075, 0.391, 0.423, 0.317, 0.203, 0.223]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.2537313432835821
[2m[36m(func pid=22158)[0m top5: 0.7532649253731343
[2m[36m(func pid=22158)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=22158)[0m f1_macro: 0.21584424756516843
[2m[36m(func pid=22158)[0m f1_weighted: 0.2718093505192277
[2m[36m(func pid=22158)[0m f1_per_class: [0.212, 0.177, 0.144, 0.336, 0.05, 0.351, 0.273, 0.264, 0.049, 0.303]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0182 | Steps: 2 | Val loss: 3.6226 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=11358)[0m top1: 0.19263059701492538
[2m[36m(func pid=11358)[0m top5: 0.664179104477612
[2m[36m(func pid=11358)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=11358)[0m f1_macro: 0.15925706497006725
[2m[36m(func pid=11358)[0m f1_weighted: 0.20486884429810073
[2m[36m(func pid=11358)[0m f1_per_class: [0.155, 0.155, 0.155, 0.246, 0.035, 0.362, 0.17, 0.192, 0.027, 0.095]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5962 | Steps: 2 | Val loss: 1.9202 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=23659)[0m top1: 0.31576492537313433
[2m[36m(func pid=23659)[0m top5: 0.8507462686567164
[2m[36m(func pid=23659)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=23659)[0m f1_macro: 0.31216024804872433
[2m[36m(func pid=23659)[0m f1_weighted: 0.33747982951654404
[2m[36m(func pid=23659)[0m f1_per_class: [0.274, 0.243, 0.774, 0.411, 0.089, 0.383, 0.353, 0.213, 0.2, 0.182]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.1597 | Steps: 2 | Val loss: 2.1187 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.6427 | Steps: 2 | Val loss: 2.2371 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 02:15:11 (running for 00:41:31.91)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.624 |      0.159 |                   73 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.189 |      0.216 |                   22 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.596 |      0.333 |                   21 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.018 |      0.312 |                   18 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3460820895522388
[2m[36m(func pid=22837)[0m top5: 0.8642723880597015
[2m[36m(func pid=22837)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=22837)[0m f1_macro: 0.3331140623899201
[2m[36m(func pid=22837)[0m f1_weighted: 0.3721999705092589
[2m[36m(func pid=22837)[0m f1_per_class: [0.291, 0.276, 0.71, 0.403, 0.079, 0.393, 0.432, 0.31, 0.212, 0.225]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.1214 | Steps: 2 | Val loss: 3.5771 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=22158)[0m top1: 0.261660447761194
[2m[36m(func pid=22158)[0m top5: 0.761660447761194
[2m[36m(func pid=22158)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=22158)[0m f1_macro: 0.22090032393234357
[2m[36m(func pid=22158)[0m f1_weighted: 0.27976753937746324
[2m[36m(func pid=22158)[0m f1_per_class: [0.217, 0.174, 0.155, 0.349, 0.057, 0.348, 0.289, 0.271, 0.051, 0.298]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=11358)[0m top1: 0.19263059701492538
[2m[36m(func pid=11358)[0m top5: 0.6721082089552238
[2m[36m(func pid=11358)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=11358)[0m f1_macro: 0.15823872941373424
[2m[36m(func pid=11358)[0m f1_weighted: 0.20563185441005738
[2m[36m(func pid=11358)[0m f1_per_class: [0.156, 0.159, 0.146, 0.244, 0.041, 0.356, 0.176, 0.186, 0.028, 0.091]
[2m[36m(func pid=11358)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4560 | Steps: 2 | Val loss: 1.9242 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=23659)[0m top1: 0.33675373134328357
[2m[36m(func pid=23659)[0m top5: 0.8694029850746269
[2m[36m(func pid=23659)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=23659)[0m f1_macro: 0.3177292570112848
[2m[36m(func pid=23659)[0m f1_weighted: 0.36323970619649576
[2m[36m(func pid=23659)[0m f1_per_class: [0.263, 0.268, 0.774, 0.398, 0.091, 0.385, 0.444, 0.186, 0.19, 0.179]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.1792 | Steps: 2 | Val loss: 2.1040 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=11358)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.6355 | Steps: 2 | Val loss: 2.2335 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 02:15:16 (running for 00:41:37.04)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.3195
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00020 | RUNNING    | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.643 |      0.158 |                   74 |
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.16  |      0.221 |                   23 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.456 |      0.333 |                   22 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.121 |      0.318 |                   19 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3493470149253731
[2m[36m(func pid=22837)[0m top5: 0.8694029850746269
[2m[36m(func pid=22837)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=22837)[0m f1_macro: 0.33267438678244227
[2m[36m(func pid=22837)[0m f1_weighted: 0.3757927022579641
[2m[36m(func pid=22837)[0m f1_per_class: [0.315, 0.278, 0.688, 0.413, 0.084, 0.397, 0.434, 0.306, 0.206, 0.207]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.27052238805970147
[2m[36m(func pid=22158)[0m top5: 0.7681902985074627
[2m[36m(func pid=22158)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=22158)[0m f1_macro: 0.2292146554484072
[2m[36m(func pid=22158)[0m f1_weighted: 0.28836577274603215
[2m[36m(func pid=22158)[0m f1_per_class: [0.218, 0.182, 0.165, 0.359, 0.053, 0.348, 0.298, 0.276, 0.088, 0.305]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0341 | Steps: 2 | Val loss: 3.6223 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=11358)[0m top1: 0.19776119402985073
[2m[36m(func pid=11358)[0m top5: 0.6744402985074627
[2m[36m(func pid=11358)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=11358)[0m f1_macro: 0.16347882050690513
[2m[36m(func pid=11358)[0m f1_weighted: 0.21048697298544686
[2m[36m(func pid=11358)[0m f1_per_class: [0.162, 0.173, 0.155, 0.251, 0.041, 0.353, 0.175, 0.199, 0.028, 0.097]
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4927 | Steps: 2 | Val loss: 1.9210 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=23659)[0m top1: 0.3451492537313433
[2m[36m(func pid=23659)[0m top5: 0.8740671641791045
[2m[36m(func pid=23659)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=23659)[0m f1_macro: 0.31782066752411764
[2m[36m(func pid=23659)[0m f1_weighted: 0.3714304393555181
[2m[36m(func pid=23659)[0m f1_per_class: [0.276, 0.283, 0.774, 0.39, 0.084, 0.345, 0.489, 0.162, 0.194, 0.183]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.0853 | Steps: 2 | Val loss: 2.0978 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 02:15:21 (running for 00:41:42.47)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.179 |      0.229 |                   24 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.493 |      0.328 |                   23 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.034 |      0.318 |                   20 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3493470149253731
[2m[36m(func pid=22837)[0m top5: 0.8722014925373134
[2m[36m(func pid=22837)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=22837)[0m f1_macro: 0.327855458004019
[2m[36m(func pid=22837)[0m f1_weighted: 0.37683077174814966
[2m[36m(func pid=22837)[0m f1_per_class: [0.309, 0.282, 0.688, 0.416, 0.078, 0.386, 0.443, 0.285, 0.19, 0.203]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.27052238805970147
[2m[36m(func pid=22158)[0m top5: 0.7705223880597015
[2m[36m(func pid=22158)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=22158)[0m f1_macro: 0.22936871155008132
[2m[36m(func pid=22158)[0m f1_weighted: 0.28867971802707243
[2m[36m(func pid=22158)[0m f1_per_class: [0.216, 0.18, 0.186, 0.357, 0.051, 0.351, 0.302, 0.278, 0.073, 0.299]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0119 | Steps: 2 | Val loss: 3.7205 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3416 | Steps: 2 | Val loss: 1.9353 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=23659)[0m top1: 0.34328358208955223
[2m[36m(func pid=23659)[0m top5: 0.8652052238805971
[2m[36m(func pid=23659)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=23659)[0m f1_macro: 0.3123106796193743
[2m[36m(func pid=23659)[0m f1_weighted: 0.36858924034371593
[2m[36m(func pid=23659)[0m f1_per_class: [0.276, 0.279, 0.774, 0.381, 0.076, 0.311, 0.506, 0.153, 0.181, 0.186]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.0581 | Steps: 2 | Val loss: 2.0873 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 02:15:27 (running for 00:41:47.81)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.085 |      0.229 |                   25 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.342 |      0.33  |                   24 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.012 |      0.312 |                   21 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.34888059701492535
[2m[36m(func pid=22837)[0m top5: 0.8745335820895522
[2m[36m(func pid=22837)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=22837)[0m f1_macro: 0.3296758285948399
[2m[36m(func pid=22837)[0m f1_weighted: 0.376011313146238
[2m[36m(func pid=22837)[0m f1_per_class: [0.319, 0.28, 0.71, 0.413, 0.092, 0.391, 0.444, 0.259, 0.197, 0.19]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.2756529850746269
[2m[36m(func pid=22158)[0m top5: 0.777518656716418
[2m[36m(func pid=22158)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=22158)[0m f1_macro: 0.23286742691102552
[2m[36m(func pid=22158)[0m f1_weighted: 0.29400827617041464
[2m[36m(func pid=22158)[0m f1_per_class: [0.211, 0.183, 0.202, 0.366, 0.052, 0.355, 0.309, 0.275, 0.075, 0.301]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0305 | Steps: 2 | Val loss: 3.8119 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5010 | Steps: 2 | Val loss: 1.9488 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=23659)[0m top1: 0.34048507462686567
[2m[36m(func pid=23659)[0m top5: 0.8614738805970149
[2m[36m(func pid=23659)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=23659)[0m f1_macro: 0.304494466369387
[2m[36m(func pid=23659)[0m f1_weighted: 0.366070644314777
[2m[36m(func pid=23659)[0m f1_per_class: [0.264, 0.28, 0.75, 0.382, 0.073, 0.286, 0.51, 0.15, 0.167, 0.184]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.0123 | Steps: 2 | Val loss: 2.0763 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 02:15:32 (running for 00:41:53.15)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.058 |      0.233 |                   26 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.501 |      0.324 |                   25 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.03  |      0.304 |                   22 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.34421641791044777
[2m[36m(func pid=22837)[0m top5: 0.8754664179104478
[2m[36m(func pid=22837)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=22837)[0m f1_macro: 0.323934587694508
[2m[36m(func pid=22837)[0m f1_weighted: 0.3710528560570487
[2m[36m(func pid=22837)[0m f1_per_class: [0.326, 0.281, 0.71, 0.409, 0.092, 0.387, 0.441, 0.225, 0.184, 0.184]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.279384328358209
[2m[36m(func pid=22158)[0m top5: 0.7817164179104478
[2m[36m(func pid=22158)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=22158)[0m f1_macro: 0.24046490231475412
[2m[36m(func pid=22158)[0m f1_weighted: 0.29724552116984837
[2m[36m(func pid=22158)[0m f1_per_class: [0.227, 0.199, 0.232, 0.363, 0.05, 0.351, 0.314, 0.263, 0.071, 0.333]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.1244 | Steps: 2 | Val loss: 3.9560 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3329 | Steps: 2 | Val loss: 1.9571 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=23659)[0m top1: 0.32369402985074625
[2m[36m(func pid=23659)[0m top5: 0.8563432835820896
[2m[36m(func pid=23659)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=23659)[0m f1_macro: 0.30003670026288576
[2m[36m(func pid=23659)[0m f1_weighted: 0.35148722256033077
[2m[36m(func pid=23659)[0m f1_per_class: [0.265, 0.288, 0.75, 0.364, 0.065, 0.234, 0.484, 0.187, 0.179, 0.183]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.9914 | Steps: 2 | Val loss: 2.0680 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 02:15:37 (running for 00:41:58.55)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.012 |      0.24  |                   27 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.333 |      0.325 |                   26 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.124 |      0.3   |                   23 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.345615671641791
[2m[36m(func pid=22837)[0m top5: 0.8768656716417911
[2m[36m(func pid=22837)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=22837)[0m f1_macro: 0.3252417097314161
[2m[36m(func pid=22837)[0m f1_weighted: 0.371636156780528
[2m[36m(func pid=22837)[0m f1_per_class: [0.341, 0.283, 0.71, 0.418, 0.094, 0.382, 0.435, 0.213, 0.196, 0.181]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.2891791044776119
[2m[36m(func pid=22158)[0m top5: 0.7868470149253731
[2m[36m(func pid=22158)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=22158)[0m f1_macro: 0.247789696200237
[2m[36m(func pid=22158)[0m f1_weighted: 0.30747223363805404
[2m[36m(func pid=22158)[0m f1_per_class: [0.242, 0.223, 0.224, 0.378, 0.049, 0.37, 0.31, 0.265, 0.092, 0.324]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0073 | Steps: 2 | Val loss: 4.1213 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3143 | Steps: 2 | Val loss: 1.9642 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=23659)[0m top1: 0.30130597014925375
[2m[36m(func pid=23659)[0m top5: 0.8390858208955224
[2m[36m(func pid=23659)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=23659)[0m f1_macro: 0.29240628253304174
[2m[36m(func pid=23659)[0m f1_weighted: 0.32895328536885304
[2m[36m(func pid=23659)[0m f1_per_class: [0.259, 0.286, 0.75, 0.348, 0.069, 0.199, 0.43, 0.238, 0.171, 0.175]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.9720 | Steps: 2 | Val loss: 2.0593 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 02:15:43 (running for 00:42:04.31)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.991 |      0.248 |                   28 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.314 |      0.323 |                   27 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.007 |      0.292 |                   24 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3460820895522388
[2m[36m(func pid=22837)[0m top5: 0.878731343283582
[2m[36m(func pid=22837)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=22837)[0m f1_macro: 0.32284882178007857
[2m[36m(func pid=22837)[0m f1_weighted: 0.37282121083742004
[2m[36m(func pid=22837)[0m f1_per_class: [0.323, 0.282, 0.71, 0.422, 0.091, 0.382, 0.438, 0.213, 0.188, 0.179]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.29197761194029853
[2m[36m(func pid=22158)[0m top5: 0.7901119402985075
[2m[36m(func pid=22158)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=22158)[0m f1_macro: 0.2519813942734493
[2m[36m(func pid=22158)[0m f1_weighted: 0.3109907352578334
[2m[36m(func pid=22158)[0m f1_per_class: [0.248, 0.225, 0.232, 0.375, 0.048, 0.378, 0.318, 0.277, 0.091, 0.329]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0161 | Steps: 2 | Val loss: 4.3548 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=23659)[0m top1: 0.28218283582089554
[2m[36m(func pid=23659)[0m top5: 0.8227611940298507
[2m[36m(func pid=23659)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=23659)[0m f1_macro: 0.2888697812043471
[2m[36m(func pid=23659)[0m f1_weighted: 0.30465618939624234
[2m[36m(func pid=23659)[0m f1_per_class: [0.27, 0.286, 0.75, 0.332, 0.068, 0.196, 0.356, 0.278, 0.174, 0.179]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3436 | Steps: 2 | Val loss: 1.9627 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.0185 | Steps: 2 | Val loss: 2.0530 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 02:15:49 (running for 00:42:09.79)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.972 |      0.252 |                   29 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.344 |      0.318 |                   28 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.016 |      0.289 |                   25 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.34701492537313433
[2m[36m(func pid=22837)[0m top5: 0.8815298507462687
[2m[36m(func pid=22837)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=22837)[0m f1_macro: 0.31758764751266066
[2m[36m(func pid=22837)[0m f1_weighted: 0.3728171911953174
[2m[36m(func pid=22837)[0m f1_per_class: [0.335, 0.283, 0.667, 0.428, 0.09, 0.361, 0.443, 0.199, 0.19, 0.179]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0618 | Steps: 2 | Val loss: 4.5464 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=22158)[0m top1: 0.28824626865671643
[2m[36m(func pid=22158)[0m top5: 0.7915111940298507
[2m[36m(func pid=22158)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=22158)[0m f1_macro: 0.25104073470045807
[2m[36m(func pid=22158)[0m f1_weighted: 0.3074021237669349
[2m[36m(func pid=22158)[0m f1_per_class: [0.248, 0.221, 0.234, 0.37, 0.046, 0.376, 0.313, 0.272, 0.109, 0.322]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.2667910447761194
[2m[36m(func pid=23659)[0m top5: 0.8050373134328358
[2m[36m(func pid=23659)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=23659)[0m f1_macro: 0.2782018708609365
[2m[36m(func pid=23659)[0m f1_weighted: 0.2849574361371772
[2m[36m(func pid=23659)[0m f1_per_class: [0.267, 0.283, 0.727, 0.319, 0.069, 0.186, 0.311, 0.258, 0.171, 0.19]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.9688 | Steps: 2 | Val loss: 2.0495 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3204 | Steps: 2 | Val loss: 1.9702 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=22837)[0m top1: 0.34654850746268656
[2m[36m(func pid=22837)[0m top5: 0.882929104477612
[2m[36m(func pid=22837)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=22837)[0m f1_macro: 0.32021493394098016
[2m[36m(func pid=22837)[0m f1_weighted: 0.370954921642692
[2m[36m(func pid=22837)[0m f1_per_class: [0.344, 0.283, 0.667, 0.422, 0.088, 0.363, 0.437, 0.206, 0.211, 0.182]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0361 | Steps: 2 | Val loss: 4.6234 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 02:15:54 (running for 00:42:15.23)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  2.019 |      0.251 |                   30 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.32  |      0.32  |                   29 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.062 |      0.278 |                   26 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22158)[0m top1: 0.28638059701492535
[2m[36m(func pid=22158)[0m top5: 0.7915111940298507
[2m[36m(func pid=22158)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=22158)[0m f1_macro: 0.25116561806329896
[2m[36m(func pid=22158)[0m f1_weighted: 0.3060311836132885
[2m[36m(func pid=22158)[0m f1_per_class: [0.253, 0.226, 0.242, 0.355, 0.051, 0.378, 0.32, 0.266, 0.11, 0.311]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.2635261194029851
[2m[36m(func pid=23659)[0m top5: 0.7947761194029851
[2m[36m(func pid=23659)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=23659)[0m f1_macro: 0.27833357772495393
[2m[36m(func pid=23659)[0m f1_weighted: 0.2796902955551001
[2m[36m(func pid=23659)[0m f1_per_class: [0.269, 0.284, 0.727, 0.31, 0.071, 0.193, 0.297, 0.263, 0.182, 0.188]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2875 | Steps: 2 | Val loss: 1.9810 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.8203 | Steps: 2 | Val loss: 2.0415 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0814 | Steps: 2 | Val loss: 4.4732 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 02:15:59 (running for 00:42:20.52)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.969 |      0.251 |                   31 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.287 |      0.324 |                   30 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.036 |      0.278 |                   27 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.35027985074626866
[2m[36m(func pid=22837)[0m top5: 0.8810634328358209
[2m[36m(func pid=22837)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=22837)[0m f1_macro: 0.32379494001195586
[2m[36m(func pid=22837)[0m f1_weighted: 0.3763576468735698
[2m[36m(func pid=22837)[0m f1_per_class: [0.344, 0.287, 0.667, 0.437, 0.084, 0.357, 0.435, 0.24, 0.216, 0.172]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.29011194029850745
[2m[36m(func pid=22158)[0m top5: 0.7947761194029851
[2m[36m(func pid=22158)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=22158)[0m f1_macro: 0.2589684083781121
[2m[36m(func pid=22158)[0m f1_weighted: 0.30973223941471595
[2m[36m(func pid=22158)[0m f1_per_class: [0.26, 0.23, 0.286, 0.36, 0.051, 0.383, 0.32, 0.268, 0.124, 0.309]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.27005597014925375
[2m[36m(func pid=23659)[0m top5: 0.8050373134328358
[2m[36m(func pid=23659)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=23659)[0m f1_macro: 0.28225502770716404
[2m[36m(func pid=23659)[0m f1_weighted: 0.2912676091471152
[2m[36m(func pid=23659)[0m f1_per_class: [0.255, 0.272, 0.727, 0.315, 0.06, 0.231, 0.323, 0.27, 0.19, 0.18]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2747 | Steps: 2 | Val loss: 1.9991 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.8389 | Steps: 2 | Val loss: 2.0347 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0293 | Steps: 2 | Val loss: 4.2966 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 02:16:05 (running for 00:42:25.90)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.82  |      0.259 |                   32 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.275 |      0.326 |                   31 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.081 |      0.282 |                   28 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.35027985074626866
[2m[36m(func pid=22837)[0m top5: 0.8777985074626866
[2m[36m(func pid=22837)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=22837)[0m f1_macro: 0.32561258354545775
[2m[36m(func pid=22837)[0m f1_weighted: 0.3766658844613521
[2m[36m(func pid=22837)[0m f1_per_class: [0.348, 0.286, 0.667, 0.443, 0.08, 0.35, 0.428, 0.258, 0.224, 0.172]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.2896455223880597
[2m[36m(func pid=22158)[0m top5: 0.7975746268656716
[2m[36m(func pid=22158)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=22158)[0m f1_macro: 0.2611998581159375
[2m[36m(func pid=22158)[0m f1_weighted: 0.30828331865885905
[2m[36m(func pid=22158)[0m f1_per_class: [0.27, 0.233, 0.297, 0.361, 0.056, 0.387, 0.311, 0.262, 0.123, 0.312]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.2849813432835821
[2m[36m(func pid=23659)[0m top5: 0.8152985074626866
[2m[36m(func pid=23659)[0m f1_micro: 0.2849813432835821
[2m[36m(func pid=23659)[0m f1_macro: 0.29081650147935434
[2m[36m(func pid=23659)[0m f1_weighted: 0.3138219072540589
[2m[36m(func pid=23659)[0m f1_per_class: [0.223, 0.252, 0.75, 0.349, 0.071, 0.241, 0.372, 0.295, 0.186, 0.17]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3300 | Steps: 2 | Val loss: 2.0211 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.8331 | Steps: 2 | Val loss: 2.0217 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 02:16:10 (running for 00:42:31.24)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.839 |      0.261 |                   33 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.33  |      0.328 |                   32 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.029 |      0.291 |                   29 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3493470149253731
[2m[36m(func pid=22837)[0m top5: 0.8768656716417911
[2m[36m(func pid=22837)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=22837)[0m f1_macro: 0.32831166735296163
[2m[36m(func pid=22837)[0m f1_weighted: 0.3769472614547969
[2m[36m(func pid=22837)[0m f1_per_class: [0.332, 0.285, 0.688, 0.439, 0.076, 0.355, 0.428, 0.275, 0.235, 0.17]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0175 | Steps: 2 | Val loss: 4.1875 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=22158)[0m top1: 0.2980410447761194
[2m[36m(func pid=22158)[0m top5: 0.8041044776119403
[2m[36m(func pid=22158)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=22158)[0m f1_macro: 0.26879830715469505
[2m[36m(func pid=22158)[0m f1_weighted: 0.3173840632813423
[2m[36m(func pid=22158)[0m f1_per_class: [0.275, 0.236, 0.328, 0.373, 0.058, 0.374, 0.33, 0.277, 0.122, 0.316]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.29244402985074625
[2m[36m(func pid=23659)[0m top5: 0.8264925373134329
[2m[36m(func pid=23659)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=23659)[0m f1_macro: 0.28897658635131873
[2m[36m(func pid=23659)[0m f1_weighted: 0.32560306623635754
[2m[36m(func pid=23659)[0m f1_per_class: [0.216, 0.237, 0.71, 0.357, 0.075, 0.273, 0.408, 0.257, 0.195, 0.162]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2504 | Steps: 2 | Val loss: 2.0392 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.7789 | Steps: 2 | Val loss: 2.0169 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=22837)[0m top1: 0.34701492537313433
[2m[36m(func pid=22837)[0m top5: 0.871268656716418
[2m[36m(func pid=22837)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=22837)[0m f1_macro: 0.32756971672196394
[2m[36m(func pid=22837)[0m f1_weighted: 0.3755053640547078
[2m[36m(func pid=22837)[0m f1_per_class: [0.325, 0.283, 0.688, 0.438, 0.074, 0.354, 0.426, 0.278, 0.24, 0.171]
[2m[36m(func pid=22837)[0m 
== Status ==
Current time: 2024-01-07 02:16:15 (running for 00:42:36.42)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.833 |      0.269 |                   34 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.25  |      0.328 |                   33 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.017 |      0.289 |                   30 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0110 | Steps: 2 | Val loss: 4.1041 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=22158)[0m top1: 0.2933768656716418
[2m[36m(func pid=22158)[0m top5: 0.8064365671641791
[2m[36m(func pid=22158)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=22158)[0m f1_macro: 0.26705376134879527
[2m[36m(func pid=22158)[0m f1_weighted: 0.31219967529395554
[2m[36m(func pid=22158)[0m f1_per_class: [0.27, 0.238, 0.328, 0.362, 0.057, 0.381, 0.32, 0.271, 0.124, 0.32]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.30597014925373134
[2m[36m(func pid=23659)[0m top5: 0.8348880597014925
[2m[36m(func pid=23659)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=23659)[0m f1_macro: 0.2918076580136699
[2m[36m(func pid=23659)[0m f1_weighted: 0.34188512695598
[2m[36m(func pid=23659)[0m f1_per_class: [0.214, 0.235, 0.71, 0.391, 0.076, 0.286, 0.432, 0.239, 0.184, 0.152]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3183 | Steps: 2 | Val loss: 2.0635 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.7454 | Steps: 2 | Val loss: 2.0092 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 02:16:20 (running for 00:42:41.66)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.779 |      0.267 |                   35 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.318 |      0.328 |                   34 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.011 |      0.292 |                   31 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.34281716417910446
[2m[36m(func pid=22837)[0m top5: 0.8684701492537313
[2m[36m(func pid=22837)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=22837)[0m f1_macro: 0.32812045411588264
[2m[36m(func pid=22837)[0m f1_weighted: 0.372926373959041
[2m[36m(func pid=22837)[0m f1_per_class: [0.316, 0.28, 0.688, 0.418, 0.074, 0.362, 0.429, 0.306, 0.238, 0.169]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0131 | Steps: 2 | Val loss: 4.0144 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=22158)[0m top1: 0.292910447761194
[2m[36m(func pid=22158)[0m top5: 0.8083022388059702
[2m[36m(func pid=22158)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=22158)[0m f1_macro: 0.265797028480481
[2m[36m(func pid=22158)[0m f1_weighted: 0.3117696540185764
[2m[36m(func pid=22158)[0m f1_per_class: [0.269, 0.236, 0.333, 0.361, 0.058, 0.374, 0.324, 0.274, 0.123, 0.308]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.31763059701492535
[2m[36m(func pid=23659)[0m top5: 0.8353544776119403
[2m[36m(func pid=23659)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=23659)[0m f1_macro: 0.2944186614714805
[2m[36m(func pid=23659)[0m f1_weighted: 0.35356617103241117
[2m[36m(func pid=23659)[0m f1_per_class: [0.201, 0.234, 0.71, 0.399, 0.082, 0.308, 0.461, 0.218, 0.181, 0.15]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3395 | Steps: 2 | Val loss: 2.1000 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.6746 | Steps: 2 | Val loss: 2.0037 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=22837)[0m top1: 0.33722014925373134
[2m[36m(func pid=22837)[0m top5: 0.8624067164179104
[2m[36m(func pid=22837)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=22837)[0m f1_macro: 0.3231008808637971
[2m[36m(func pid=22837)[0m f1_weighted: 0.36791385601328763
[2m[36m(func pid=22837)[0m f1_per_class: [0.301, 0.283, 0.688, 0.406, 0.075, 0.364, 0.425, 0.3, 0.221, 0.167]
[2m[36m(func pid=22837)[0m 
== Status ==
Current time: 2024-01-07 02:16:26 (running for 00:42:46.86)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.745 |      0.266 |                   36 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.339 |      0.323 |                   35 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.013 |      0.294 |                   32 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0144 | Steps: 2 | Val loss: 3.9711 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=22158)[0m top1: 0.29151119402985076
[2m[36m(func pid=22158)[0m top5: 0.8115671641791045
[2m[36m(func pid=22158)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=22158)[0m f1_macro: 0.2686354688003933
[2m[36m(func pid=22158)[0m f1_weighted: 0.31059423783489987
[2m[36m(func pid=22158)[0m f1_per_class: [0.261, 0.238, 0.367, 0.355, 0.058, 0.375, 0.32, 0.288, 0.122, 0.303]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.32509328358208955
[2m[36m(func pid=23659)[0m top5: 0.8348880597014925
[2m[36m(func pid=23659)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=23659)[0m f1_macro: 0.296874988792333
[2m[36m(func pid=23659)[0m f1_weighted: 0.36017024064851816
[2m[36m(func pid=23659)[0m f1_per_class: [0.2, 0.233, 0.71, 0.406, 0.083, 0.328, 0.472, 0.205, 0.181, 0.151]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2698 | Steps: 2 | Val loss: 2.1273 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.7052 | Steps: 2 | Val loss: 1.9985 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 02:16:31 (running for 00:42:52.26)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.675 |      0.269 |                   37 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.27  |      0.318 |                   36 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.014 |      0.297 |                   33 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3353544776119403
[2m[36m(func pid=22837)[0m top5: 0.8507462686567164
[2m[36m(func pid=22837)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=22837)[0m f1_macro: 0.3177275786269468
[2m[36m(func pid=22837)[0m f1_weighted: 0.36705948421244833
[2m[36m(func pid=22837)[0m f1_per_class: [0.298, 0.276, 0.688, 0.405, 0.08, 0.355, 0.441, 0.264, 0.203, 0.168]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0060 | Steps: 2 | Val loss: 3.9423 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=22158)[0m top1: 0.291044776119403
[2m[36m(func pid=22158)[0m top5: 0.8101679104477612
[2m[36m(func pid=22158)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=22158)[0m f1_macro: 0.267569057773669
[2m[36m(func pid=22158)[0m f1_weighted: 0.310406052395058
[2m[36m(func pid=22158)[0m f1_per_class: [0.255, 0.242, 0.367, 0.354, 0.057, 0.38, 0.319, 0.283, 0.121, 0.299]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.32975746268656714
[2m[36m(func pid=23659)[0m top5: 0.8344216417910447
[2m[36m(func pid=23659)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=23659)[0m f1_macro: 0.29772070997180194
[2m[36m(func pid=23659)[0m f1_weighted: 0.36505452476472455
[2m[36m(func pid=23659)[0m f1_per_class: [0.202, 0.237, 0.71, 0.411, 0.081, 0.333, 0.483, 0.191, 0.178, 0.153]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3203 | Steps: 2 | Val loss: 2.1358 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.7766 | Steps: 2 | Val loss: 1.9879 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 02:16:37 (running for 00:42:57.77)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.705 |      0.268 |                   38 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.32  |      0.316 |                   37 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.006 |      0.298 |                   34 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.332089552238806
[2m[36m(func pid=22837)[0m top5: 0.8512126865671642
[2m[36m(func pid=22837)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=22837)[0m f1_macro: 0.3161048647178203
[2m[36m(func pid=22837)[0m f1_weighted: 0.36358954336713867
[2m[36m(func pid=22837)[0m f1_per_class: [0.279, 0.274, 0.71, 0.391, 0.08, 0.349, 0.447, 0.256, 0.203, 0.172]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0598 | Steps: 2 | Val loss: 3.8603 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=22158)[0m top1: 0.29757462686567165
[2m[36m(func pid=22158)[0m top5: 0.8134328358208955
[2m[36m(func pid=22158)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=22158)[0m f1_macro: 0.2734716331806903
[2m[36m(func pid=22158)[0m f1_weighted: 0.31695885466409357
[2m[36m(func pid=22158)[0m f1_per_class: [0.271, 0.244, 0.393, 0.366, 0.052, 0.377, 0.325, 0.293, 0.124, 0.289]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.332089552238806
[2m[36m(func pid=23659)[0m top5: 0.84375
[2m[36m(func pid=23659)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=23659)[0m f1_macro: 0.29455424966564736
[2m[36m(func pid=23659)[0m f1_weighted: 0.3636060942028609
[2m[36m(func pid=23659)[0m f1_per_class: [0.217, 0.234, 0.71, 0.418, 0.084, 0.304, 0.489, 0.157, 0.181, 0.152]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3046 | Steps: 2 | Val loss: 2.1358 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.6908 | Steps: 2 | Val loss: 1.9788 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 02:16:42 (running for 00:43:03.05)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.777 |      0.273 |                   39 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.305 |      0.317 |                   38 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.06  |      0.295 |                   35 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3358208955223881
[2m[36m(func pid=22837)[0m top5: 0.8470149253731343
[2m[36m(func pid=22837)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=22837)[0m f1_macro: 0.31715423104927454
[2m[36m(func pid=22837)[0m f1_weighted: 0.36753509034466514
[2m[36m(func pid=22837)[0m f1_per_class: [0.278, 0.275, 0.71, 0.391, 0.08, 0.356, 0.459, 0.251, 0.2, 0.172]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0157 | Steps: 2 | Val loss: 3.8427 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=22158)[0m top1: 0.29850746268656714
[2m[36m(func pid=22158)[0m top5: 0.8176305970149254
[2m[36m(func pid=22158)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=22158)[0m f1_macro: 0.2758389155702606
[2m[36m(func pid=22158)[0m f1_weighted: 0.316230795371804
[2m[36m(func pid=22158)[0m f1_per_class: [0.276, 0.246, 0.4, 0.371, 0.053, 0.381, 0.313, 0.301, 0.126, 0.291]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.3400186567164179
[2m[36m(func pid=23659)[0m top5: 0.8479477611940298
[2m[36m(func pid=23659)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=23659)[0m f1_macro: 0.29695644132536037
[2m[36m(func pid=23659)[0m f1_weighted: 0.36875782035740823
[2m[36m(func pid=23659)[0m f1_per_class: [0.233, 0.227, 0.71, 0.427, 0.081, 0.283, 0.509, 0.141, 0.203, 0.154]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2193 | Steps: 2 | Val loss: 2.1460 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.5892 | Steps: 2 | Val loss: 1.9746 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 02:16:47 (running for 00:43:08.55)
Memory usage on this node: 21.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.691 |      0.276 |                   40 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.219 |      0.314 |                   39 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.016 |      0.297 |                   36 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.33302238805970147
[2m[36m(func pid=22837)[0m top5: 0.8498134328358209
[2m[36m(func pid=22837)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=22837)[0m f1_macro: 0.3140934675242525
[2m[36m(func pid=22837)[0m f1_weighted: 0.36401062384027677
[2m[36m(func pid=22837)[0m f1_per_class: [0.281, 0.277, 0.71, 0.383, 0.077, 0.355, 0.457, 0.238, 0.193, 0.172]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0190 | Steps: 2 | Val loss: 3.8393 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=22158)[0m top1: 0.30130597014925375
[2m[36m(func pid=22158)[0m top5: 0.8185634328358209
[2m[36m(func pid=22158)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=22158)[0m f1_macro: 0.27893673478826375
[2m[36m(func pid=22158)[0m f1_weighted: 0.3198563218751619
[2m[36m(func pid=22158)[0m f1_per_class: [0.281, 0.247, 0.423, 0.381, 0.059, 0.371, 0.32, 0.293, 0.125, 0.289]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.34328358208955223
[2m[36m(func pid=23659)[0m top5: 0.8530783582089553
[2m[36m(func pid=23659)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=23659)[0m f1_macro: 0.295817758709526
[2m[36m(func pid=23659)[0m f1_weighted: 0.3705205890667853
[2m[36m(func pid=23659)[0m f1_per_class: [0.239, 0.236, 0.71, 0.433, 0.079, 0.258, 0.515, 0.142, 0.197, 0.15]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2525 | Steps: 2 | Val loss: 2.1483 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.7208 | Steps: 2 | Val loss: 1.9776 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 02:16:53 (running for 00:43:13.78)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.589 |      0.279 |                   41 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.252 |      0.314 |                   40 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.019 |      0.296 |                   37 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.332089552238806
[2m[36m(func pid=22837)[0m top5: 0.8521455223880597
[2m[36m(func pid=22837)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=22837)[0m f1_macro: 0.3140907009917789
[2m[36m(func pid=22837)[0m f1_weighted: 0.3611863902011002
[2m[36m(func pid=22837)[0m f1_per_class: [0.287, 0.273, 0.71, 0.367, 0.075, 0.36, 0.462, 0.231, 0.194, 0.18]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.292910447761194
[2m[36m(func pid=22158)[0m top5: 0.816231343283582
[2m[36m(func pid=22158)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=22158)[0m f1_macro: 0.27156726187269936
[2m[36m(func pid=22158)[0m f1_weighted: 0.3115039361617845
[2m[36m(func pid=22158)[0m f1_per_class: [0.269, 0.245, 0.423, 0.373, 0.057, 0.373, 0.308, 0.258, 0.123, 0.288]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0221 | Steps: 2 | Val loss: 3.8342 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=23659)[0m top1: 0.345615671641791
[2m[36m(func pid=23659)[0m top5: 0.8610074626865671
[2m[36m(func pid=23659)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=23659)[0m f1_macro: 0.2979083261975844
[2m[36m(func pid=23659)[0m f1_weighted: 0.3721119052594947
[2m[36m(func pid=23659)[0m f1_per_class: [0.257, 0.246, 0.71, 0.434, 0.077, 0.247, 0.516, 0.149, 0.192, 0.153]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3455 | Steps: 2 | Val loss: 2.1355 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.6286 | Steps: 2 | Val loss: 1.9801 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 02:16:58 (running for 00:43:19.09)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.721 |      0.272 |                   42 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.345 |      0.314 |                   41 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.022 |      0.298 |                   38 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.33675373134328357
[2m[36m(func pid=22837)[0m top5: 0.8530783582089553
[2m[36m(func pid=22837)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=22837)[0m f1_macro: 0.31360856991675085
[2m[36m(func pid=22837)[0m f1_weighted: 0.36373750251072673
[2m[36m(func pid=22837)[0m f1_per_class: [0.277, 0.273, 0.71, 0.371, 0.08, 0.351, 0.475, 0.205, 0.2, 0.194]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.29244402985074625
[2m[36m(func pid=22158)[0m top5: 0.8138992537313433
[2m[36m(func pid=22158)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=22158)[0m f1_macro: 0.27160452762495535
[2m[36m(func pid=22158)[0m f1_weighted: 0.3121522944245581
[2m[36m(func pid=22158)[0m f1_per_class: [0.281, 0.246, 0.4, 0.365, 0.055, 0.376, 0.313, 0.262, 0.143, 0.275]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0076 | Steps: 2 | Val loss: 3.8253 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=23659)[0m top1: 0.34468283582089554
[2m[36m(func pid=23659)[0m top5: 0.8610074626865671
[2m[36m(func pid=23659)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=23659)[0m f1_macro: 0.30006624395865134
[2m[36m(func pid=23659)[0m f1_weighted: 0.372005089105979
[2m[36m(func pid=23659)[0m f1_per_class: [0.261, 0.244, 0.71, 0.436, 0.074, 0.232, 0.513, 0.191, 0.187, 0.153]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.1875 | Steps: 2 | Val loss: 2.1229 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.6315 | Steps: 2 | Val loss: 1.9767 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 02:17:03 (running for 00:43:24.38)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.629 |      0.272 |                   43 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.187 |      0.32  |                   42 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.008 |      0.3   |                   39 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.345615671641791
[2m[36m(func pid=22837)[0m top5: 0.8563432835820896
[2m[36m(func pid=22837)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=22837)[0m f1_macro: 0.3196835875692644
[2m[36m(func pid=22837)[0m f1_weighted: 0.3725688763268823
[2m[36m(func pid=22837)[0m f1_per_class: [0.297, 0.278, 0.71, 0.385, 0.079, 0.354, 0.485, 0.211, 0.206, 0.191]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.2966417910447761
[2m[36m(func pid=22158)[0m top5: 0.8148320895522388
[2m[36m(func pid=22158)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=22158)[0m f1_macro: 0.27317176237786633
[2m[36m(func pid=22158)[0m f1_weighted: 0.3181227253627334
[2m[36m(func pid=22158)[0m f1_per_class: [0.279, 0.248, 0.393, 0.375, 0.054, 0.369, 0.322, 0.286, 0.139, 0.267]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0132 | Steps: 2 | Val loss: 3.8669 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=23659)[0m top1: 0.3381529850746269
[2m[36m(func pid=23659)[0m top5: 0.8610074626865671
[2m[36m(func pid=23659)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=23659)[0m f1_macro: 0.2964727221959483
[2m[36m(func pid=23659)[0m f1_weighted: 0.36648205847765153
[2m[36m(func pid=23659)[0m f1_per_class: [0.259, 0.246, 0.71, 0.431, 0.069, 0.213, 0.505, 0.184, 0.193, 0.154]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2447 | Steps: 2 | Val loss: 2.1170 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.5975 | Steps: 2 | Val loss: 1.9693 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 02:17:09 (running for 00:43:29.87)
Memory usage on this node: 21.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.632 |      0.273 |                   44 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.245 |      0.322 |                   43 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.013 |      0.296 |                   40 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22158)[0m top1: 0.29990671641791045
[2m[36m(func pid=22158)[0m top5: 0.816231343283582
[2m[36m(func pid=22158)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=22158)[0m f1_macro: 0.27796487793830726
[2m[36m(func pid=22158)[0m f1_weighted: 0.32174812502595274
[2m[36m(func pid=22158)[0m f1_per_class: [0.282, 0.246, 0.431, 0.376, 0.054, 0.365, 0.336, 0.287, 0.127, 0.275]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=22837)[0m top1: 0.34888059701492535
[2m[36m(func pid=22837)[0m top5: 0.8572761194029851
[2m[36m(func pid=22837)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=22837)[0m f1_macro: 0.32229834739726426
[2m[36m(func pid=22837)[0m f1_weighted: 0.3780632941787647
[2m[36m(func pid=22837)[0m f1_per_class: [0.299, 0.279, 0.71, 0.401, 0.075, 0.355, 0.485, 0.224, 0.209, 0.186]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0060 | Steps: 2 | Val loss: 3.8959 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=23659)[0m top1: 0.332089552238806
[2m[36m(func pid=23659)[0m top5: 0.8586753731343284
[2m[36m(func pid=23659)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=23659)[0m f1_macro: 0.2964695756344616
[2m[36m(func pid=23659)[0m f1_weighted: 0.36139780870968247
[2m[36m(func pid=23659)[0m f1_per_class: [0.261, 0.251, 0.71, 0.425, 0.067, 0.206, 0.489, 0.205, 0.198, 0.153]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.4860 | Steps: 2 | Val loss: 1.9699 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2394 | Steps: 2 | Val loss: 2.1144 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 02:17:14 (running for 00:43:35.24)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.597 |      0.278 |                   45 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.239 |      0.324 |                   44 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.006 |      0.296 |                   41 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3498134328358209
[2m[36m(func pid=22837)[0m top5: 0.8624067164179104
[2m[36m(func pid=22837)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=22837)[0m f1_macro: 0.3239184539128689
[2m[36m(func pid=22837)[0m f1_weighted: 0.37828374136164405
[2m[36m(func pid=22837)[0m f1_per_class: [0.315, 0.279, 0.688, 0.408, 0.075, 0.351, 0.476, 0.242, 0.217, 0.188]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0390 | Steps: 2 | Val loss: 3.9133 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=22158)[0m top1: 0.29524253731343286
[2m[36m(func pid=22158)[0m top5: 0.8176305970149254
[2m[36m(func pid=22158)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=22158)[0m f1_macro: 0.2761667343678004
[2m[36m(func pid=22158)[0m f1_weighted: 0.3165963941366462
[2m[36m(func pid=22158)[0m f1_per_class: [0.285, 0.244, 0.431, 0.365, 0.053, 0.369, 0.329, 0.281, 0.124, 0.28]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.3302238805970149
[2m[36m(func pid=23659)[0m top5: 0.8596082089552238
[2m[36m(func pid=23659)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=23659)[0m f1_macro: 0.299623363074699
[2m[36m(func pid=23659)[0m f1_weighted: 0.36095985715376444
[2m[36m(func pid=23659)[0m f1_per_class: [0.287, 0.258, 0.71, 0.43, 0.065, 0.207, 0.475, 0.216, 0.198, 0.151]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2253 | Steps: 2 | Val loss: 2.1273 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.5372 | Steps: 2 | Val loss: 1.9653 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0298 | Steps: 2 | Val loss: 3.9362 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 02:17:20 (running for 00:43:40.76)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.486 |      0.276 |                   46 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.225 |      0.322 |                   45 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.039 |      0.3   |                   42 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.34328358208955223
[2m[36m(func pid=22837)[0m top5: 0.8642723880597015
[2m[36m(func pid=22837)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=22837)[0m f1_macro: 0.32215387411495855
[2m[36m(func pid=22837)[0m f1_weighted: 0.37215186339920936
[2m[36m(func pid=22837)[0m f1_per_class: [0.32, 0.279, 0.688, 0.4, 0.074, 0.351, 0.46, 0.253, 0.214, 0.182]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.2943097014925373
[2m[36m(func pid=22158)[0m top5: 0.8185634328358209
[2m[36m(func pid=22158)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=22158)[0m f1_macro: 0.27784073863457875
[2m[36m(func pid=22158)[0m f1_weighted: 0.31594102366167287
[2m[36m(func pid=22158)[0m f1_per_class: [0.286, 0.249, 0.44, 0.355, 0.051, 0.371, 0.331, 0.283, 0.127, 0.284]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.324160447761194
[2m[36m(func pid=23659)[0m top5: 0.8600746268656716
[2m[36m(func pid=23659)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=23659)[0m f1_macro: 0.2970782636364286
[2m[36m(func pid=23659)[0m f1_weighted: 0.35511687941592557
[2m[36m(func pid=23659)[0m f1_per_class: [0.299, 0.27, 0.71, 0.41, 0.066, 0.201, 0.471, 0.219, 0.17, 0.155]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2727 | Steps: 2 | Val loss: 2.1282 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.5701 | Steps: 2 | Val loss: 1.9632 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0218 | Steps: 2 | Val loss: 3.9968 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 02:17:25 (running for 00:43:46.11)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.537 |      0.278 |                   47 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.273 |      0.323 |                   46 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.03  |      0.297 |                   43 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.34095149253731344
[2m[36m(func pid=22837)[0m top5: 0.8694029850746269
[2m[36m(func pid=22837)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=22837)[0m f1_macro: 0.3233319820631967
[2m[36m(func pid=22837)[0m f1_weighted: 0.36937293725468423
[2m[36m(func pid=22837)[0m f1_per_class: [0.315, 0.287, 0.688, 0.398, 0.072, 0.354, 0.445, 0.26, 0.225, 0.19]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.292910447761194
[2m[36m(func pid=22158)[0m top5: 0.8185634328358209
[2m[36m(func pid=22158)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=22158)[0m f1_macro: 0.2759888657457972
[2m[36m(func pid=22158)[0m f1_weighted: 0.3155653762200872
[2m[36m(func pid=22158)[0m f1_per_class: [0.276, 0.246, 0.431, 0.347, 0.051, 0.371, 0.339, 0.292, 0.128, 0.279]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.31949626865671643
[2m[36m(func pid=23659)[0m top5: 0.8535447761194029
[2m[36m(func pid=23659)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=23659)[0m f1_macro: 0.300238153452526
[2m[36m(func pid=23659)[0m f1_weighted: 0.34994996789463556
[2m[36m(func pid=23659)[0m f1_per_class: [0.327, 0.273, 0.71, 0.387, 0.073, 0.209, 0.468, 0.217, 0.174, 0.165]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.1898 | Steps: 2 | Val loss: 2.1505 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.4663 | Steps: 2 | Val loss: 1.9592 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0452 | Steps: 2 | Val loss: 4.0184 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=22837)[0m top1: 0.33675373134328357
[2m[36m(func pid=22837)[0m top5: 0.8689365671641791
[2m[36m(func pid=22837)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=22837)[0m f1_macro: 0.32216924179733125
[2m[36m(func pid=22837)[0m f1_weighted: 0.36691908171724447
[2m[36m(func pid=22837)[0m f1_per_class: [0.322, 0.288, 0.688, 0.4, 0.071, 0.343, 0.436, 0.275, 0.224, 0.176]
[2m[36m(func pid=22837)[0m 
== Status ==
Current time: 2024-01-07 02:17:30 (running for 00:43:51.58)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.57  |      0.276 |                   48 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.19  |      0.322 |                   47 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.022 |      0.3   |                   44 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22158)[0m top1: 0.2947761194029851
[2m[36m(func pid=22158)[0m top5: 0.8208955223880597
[2m[36m(func pid=22158)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=22158)[0m f1_macro: 0.27949451391623176
[2m[36m(func pid=22158)[0m f1_weighted: 0.31661982518493775
[2m[36m(func pid=22158)[0m f1_per_class: [0.289, 0.251, 0.449, 0.357, 0.051, 0.371, 0.329, 0.29, 0.126, 0.28]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.314365671641791
[2m[36m(func pid=23659)[0m top5: 0.8470149253731343
[2m[36m(func pid=23659)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=23659)[0m f1_macro: 0.30099445532245017
[2m[36m(func pid=23659)[0m f1_weighted: 0.34595780948998905
[2m[36m(func pid=23659)[0m f1_per_class: [0.311, 0.27, 0.75, 0.38, 0.071, 0.204, 0.464, 0.224, 0.174, 0.162]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2570 | Steps: 2 | Val loss: 2.1828 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.4741 | Steps: 2 | Val loss: 1.9565 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0102 | Steps: 2 | Val loss: 4.1047 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 02:17:36 (running for 00:43:56.77)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.466 |      0.279 |                   49 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.257 |      0.322 |                   48 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.045 |      0.301 |                   45 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3316231343283582
[2m[36m(func pid=22837)[0m top5: 0.8638059701492538
[2m[36m(func pid=22837)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=22837)[0m f1_macro: 0.3220644902368525
[2m[36m(func pid=22837)[0m f1_weighted: 0.36136987206758747
[2m[36m(func pid=22837)[0m f1_per_class: [0.31, 0.286, 0.71, 0.389, 0.072, 0.338, 0.428, 0.29, 0.225, 0.173]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.2957089552238806
[2m[36m(func pid=22158)[0m top5: 0.8208955223880597
[2m[36m(func pid=22158)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=22158)[0m f1_macro: 0.27934328378588025
[2m[36m(func pid=22158)[0m f1_weighted: 0.31712775207625676
[2m[36m(func pid=22158)[0m f1_per_class: [0.293, 0.256, 0.431, 0.359, 0.051, 0.371, 0.324, 0.301, 0.128, 0.279]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.30597014925373134
[2m[36m(func pid=23659)[0m top5: 0.8442164179104478
[2m[36m(func pid=23659)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=23659)[0m f1_macro: 0.29231959927929896
[2m[36m(func pid=23659)[0m f1_weighted: 0.33756088418844704
[2m[36m(func pid=23659)[0m f1_per_class: [0.294, 0.263, 0.727, 0.367, 0.073, 0.191, 0.46, 0.22, 0.166, 0.163]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2247 | Steps: 2 | Val loss: 2.1921 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.4354 | Steps: 2 | Val loss: 1.9510 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0269 | Steps: 2 | Val loss: 4.1944 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 02:17:41 (running for 00:44:02.23)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.474 |      0.279 |                   50 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.225 |      0.321 |                   49 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.01  |      0.292 |                   46 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.32882462686567165
[2m[36m(func pid=22837)[0m top5: 0.8614738805970149
[2m[36m(func pid=22837)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=22837)[0m f1_macro: 0.32068404745526957
[2m[36m(func pid=22837)[0m f1_weighted: 0.35868850565702415
[2m[36m(func pid=22837)[0m f1_per_class: [0.32, 0.279, 0.71, 0.394, 0.069, 0.326, 0.422, 0.295, 0.216, 0.176]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.2980410447761194
[2m[36m(func pid=22158)[0m top5: 0.8250932835820896
[2m[36m(func pid=22158)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=22158)[0m f1_macro: 0.281434290970474
[2m[36m(func pid=22158)[0m f1_weighted: 0.31848785244957806
[2m[36m(func pid=22158)[0m f1_per_class: [0.281, 0.259, 0.44, 0.362, 0.054, 0.382, 0.319, 0.303, 0.127, 0.286]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.300839552238806
[2m[36m(func pid=23659)[0m top5: 0.832089552238806
[2m[36m(func pid=23659)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=23659)[0m f1_macro: 0.2901028295243838
[2m[36m(func pid=23659)[0m f1_weighted: 0.3327538285172758
[2m[36m(func pid=23659)[0m f1_per_class: [0.292, 0.268, 0.706, 0.351, 0.067, 0.198, 0.452, 0.223, 0.174, 0.169]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2131 | Steps: 2 | Val loss: 2.1954 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.3716 | Steps: 2 | Val loss: 1.9445 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0721 | Steps: 2 | Val loss: 4.2335 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 02:17:46 (running for 00:44:07.63)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.435 |      0.281 |                   51 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.213 |      0.319 |                   50 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.027 |      0.29  |                   47 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.32649253731343286
[2m[36m(func pid=22837)[0m top5: 0.8572761194029851
[2m[36m(func pid=22837)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=22837)[0m f1_macro: 0.31880004180239047
[2m[36m(func pid=22837)[0m f1_weighted: 0.35650587908394155
[2m[36m(func pid=22837)[0m f1_per_class: [0.312, 0.27, 0.71, 0.399, 0.067, 0.322, 0.417, 0.298, 0.216, 0.177]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.300839552238806
[2m[36m(func pid=22158)[0m top5: 0.8269589552238806
[2m[36m(func pid=22158)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=22158)[0m f1_macro: 0.2813404552193436
[2m[36m(func pid=22158)[0m f1_weighted: 0.32106119375024295
[2m[36m(func pid=22158)[0m f1_per_class: [0.282, 0.261, 0.44, 0.371, 0.056, 0.374, 0.323, 0.298, 0.127, 0.28]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.29617537313432835
[2m[36m(func pid=23659)[0m top5: 0.8180970149253731
[2m[36m(func pid=23659)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=23659)[0m f1_macro: 0.28839267121511747
[2m[36m(func pid=23659)[0m f1_weighted: 0.33004507144366657
[2m[36m(func pid=23659)[0m f1_per_class: [0.275, 0.252, 0.706, 0.349, 0.068, 0.233, 0.443, 0.218, 0.171, 0.169]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.1694 | Steps: 2 | Val loss: 2.2117 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.4011 | Steps: 2 | Val loss: 1.9364 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0074 | Steps: 2 | Val loss: 4.1775 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 02:17:52 (running for 00:44:13.07)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.372 |      0.281 |                   52 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.169 |      0.319 |                   51 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.072 |      0.288 |                   48 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.32649253731343286
[2m[36m(func pid=22837)[0m top5: 0.8572761194029851
[2m[36m(func pid=22837)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=22837)[0m f1_macro: 0.3185632350780012
[2m[36m(func pid=22837)[0m f1_weighted: 0.3577677229136622
[2m[36m(func pid=22837)[0m f1_per_class: [0.308, 0.262, 0.71, 0.401, 0.067, 0.326, 0.423, 0.292, 0.228, 0.169]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.30130597014925375
[2m[36m(func pid=22158)[0m top5: 0.8283582089552238
[2m[36m(func pid=22158)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=22158)[0m f1_macro: 0.2869133096675194
[2m[36m(func pid=22158)[0m f1_weighted: 0.32089618075842263
[2m[36m(func pid=22158)[0m f1_per_class: [0.288, 0.256, 0.468, 0.374, 0.055, 0.372, 0.317, 0.323, 0.13, 0.286]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.29990671641791045
[2m[36m(func pid=23659)[0m top5: 0.8157649253731343
[2m[36m(func pid=23659)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=23659)[0m f1_macro: 0.2908990326512896
[2m[36m(func pid=23659)[0m f1_weighted: 0.3342527787220615
[2m[36m(func pid=23659)[0m f1_per_class: [0.255, 0.246, 0.706, 0.348, 0.072, 0.282, 0.445, 0.208, 0.175, 0.171]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.5179 | Steps: 2 | Val loss: 1.9357 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2151 | Steps: 2 | Val loss: 2.2253 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0077 | Steps: 2 | Val loss: 4.1553 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 02:17:57 (running for 00:44:18.37)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.401 |      0.287 |                   53 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.215 |      0.32  |                   52 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.007 |      0.291 |                   49 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.33255597014925375
[2m[36m(func pid=22837)[0m top5: 0.8549440298507462
[2m[36m(func pid=22837)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=22837)[0m f1_macro: 0.3202237092132646
[2m[36m(func pid=22837)[0m f1_weighted: 0.3650208482730779
[2m[36m(func pid=22837)[0m f1_per_class: [0.313, 0.258, 0.688, 0.402, 0.068, 0.334, 0.446, 0.295, 0.225, 0.175]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.3003731343283582
[2m[36m(func pid=22158)[0m top5: 0.8292910447761194
[2m[36m(func pid=22158)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=22158)[0m f1_macro: 0.28501395435204346
[2m[36m(func pid=22158)[0m f1_weighted: 0.31979576564999745
[2m[36m(func pid=22158)[0m f1_per_class: [0.298, 0.267, 0.468, 0.367, 0.056, 0.36, 0.322, 0.306, 0.126, 0.28]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.3003731343283582
[2m[36m(func pid=23659)[0m top5: 0.8083022388059702
[2m[36m(func pid=23659)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=23659)[0m f1_macro: 0.2926670164210343
[2m[36m(func pid=23659)[0m f1_weighted: 0.33383156074667375
[2m[36m(func pid=23659)[0m f1_per_class: [0.241, 0.238, 0.706, 0.342, 0.076, 0.325, 0.438, 0.208, 0.178, 0.176]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.1472 | Steps: 2 | Val loss: 2.2496 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.4614 | Steps: 2 | Val loss: 1.9296 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0133 | Steps: 2 | Val loss: 4.1084 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 02:18:03 (running for 00:44:23.88)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.518 |      0.285 |                   54 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.147 |      0.315 |                   53 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.008 |      0.293 |                   50 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3269589552238806
[2m[36m(func pid=22837)[0m top5: 0.8493470149253731
[2m[36m(func pid=22837)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=22837)[0m f1_macro: 0.31542823393930175
[2m[36m(func pid=22837)[0m f1_weighted: 0.3588897154015379
[2m[36m(func pid=22837)[0m f1_per_class: [0.32, 0.256, 0.688, 0.397, 0.069, 0.327, 0.439, 0.262, 0.224, 0.173]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.302705223880597
[2m[36m(func pid=22158)[0m top5: 0.8306902985074627
[2m[36m(func pid=22158)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=22158)[0m f1_macro: 0.28821952832139236
[2m[36m(func pid=22158)[0m f1_weighted: 0.3221295586994578
[2m[36m(func pid=22158)[0m f1_per_class: [0.292, 0.265, 0.468, 0.376, 0.057, 0.355, 0.319, 0.324, 0.143, 0.284]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.30363805970149255
[2m[36m(func pid=23659)[0m top5: 0.8073694029850746
[2m[36m(func pid=23659)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=23659)[0m f1_macro: 0.29368169638548325
[2m[36m(func pid=23659)[0m f1_weighted: 0.3348017042156489
[2m[36m(func pid=23659)[0m f1_per_class: [0.226, 0.239, 0.706, 0.332, 0.081, 0.335, 0.446, 0.216, 0.171, 0.186]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.3918 | Steps: 2 | Val loss: 1.9210 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.1621 | Steps: 2 | Val loss: 2.2647 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0112 | Steps: 2 | Val loss: 4.0797 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 02:18:08 (running for 00:44:29.34)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.461 |      0.288 |                   55 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.162 |      0.312 |                   54 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.013 |      0.294 |                   51 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.322294776119403
[2m[36m(func pid=22837)[0m top5: 0.8498134328358209
[2m[36m(func pid=22837)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=22837)[0m f1_macro: 0.31200690277547605
[2m[36m(func pid=22837)[0m f1_weighted: 0.3543387657285452
[2m[36m(func pid=22837)[0m f1_per_class: [0.313, 0.251, 0.688, 0.389, 0.069, 0.329, 0.435, 0.258, 0.22, 0.168]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.30783582089552236
[2m[36m(func pid=22158)[0m top5: 0.835820895522388
[2m[36m(func pid=22158)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=22158)[0m f1_macro: 0.28839283395726734
[2m[36m(func pid=22158)[0m f1_weighted: 0.32774809091753143
[2m[36m(func pid=22158)[0m f1_per_class: [0.299, 0.273, 0.468, 0.38, 0.051, 0.357, 0.332, 0.318, 0.126, 0.28]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.3101679104477612
[2m[36m(func pid=23659)[0m top5: 0.8115671641791045
[2m[36m(func pid=23659)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=23659)[0m f1_macro: 0.3004491374662201
[2m[36m(func pid=23659)[0m f1_weighted: 0.3401268890026396
[2m[36m(func pid=23659)[0m f1_per_class: [0.219, 0.235, 0.727, 0.34, 0.088, 0.353, 0.45, 0.212, 0.19, 0.19]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.3891 | Steps: 2 | Val loss: 1.9169 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.1725 | Steps: 2 | Val loss: 2.2743 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0072 | Steps: 2 | Val loss: 4.0889 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 02:18:14 (running for 00:44:34.89)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.389 |      0.292 |                   57 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.162 |      0.312 |                   54 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.011 |      0.3   |                   52 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22158)[0m top1: 0.31156716417910446
[2m[36m(func pid=22158)[0m top5: 0.8353544776119403
[2m[36m(func pid=22158)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=22158)[0m f1_macro: 0.2919869664609459
[2m[36m(func pid=22158)[0m f1_weighted: 0.331396793227576
[2m[36m(func pid=22158)[0m f1_per_class: [0.297, 0.274, 0.478, 0.386, 0.051, 0.378, 0.33, 0.316, 0.127, 0.282]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=22837)[0m top1: 0.324160447761194
[2m[36m(func pid=22837)[0m top5: 0.8488805970149254
[2m[36m(func pid=22837)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=22837)[0m f1_macro: 0.3142542023284323
[2m[36m(func pid=22837)[0m f1_weighted: 0.3560603211777172
[2m[36m(func pid=22837)[0m f1_per_class: [0.32, 0.263, 0.688, 0.386, 0.068, 0.335, 0.433, 0.263, 0.214, 0.173]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m top1: 0.31343283582089554
[2m[36m(func pid=23659)[0m top5: 0.8120335820895522
[2m[36m(func pid=23659)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=23659)[0m f1_macro: 0.3030341593921514
[2m[36m(func pid=23659)[0m f1_weighted: 0.34190183277866915
[2m[36m(func pid=23659)[0m f1_per_class: [0.203, 0.233, 0.727, 0.325, 0.092, 0.369, 0.463, 0.228, 0.198, 0.194]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.4576 | Steps: 2 | Val loss: 1.9135 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.1867 | Steps: 2 | Val loss: 2.2655 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0058 | Steps: 2 | Val loss: 4.0811 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=22837)[0m top1: 0.32882462686567165
[2m[36m(func pid=22837)[0m top5: 0.8535447761194029
[2m[36m(func pid=22837)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=22837)[0m f1_macro: 0.3163427995438428
[2m[36m(func pid=22837)[0m f1_weighted: 0.3610461741190189
[2m[36m(func pid=22837)[0m f1_per_class: [0.326, 0.271, 0.688, 0.393, 0.069, 0.333, 0.441, 0.259, 0.216, 0.169]
== Status ==
Current time: 2024-01-07 02:18:19 (running for 00:44:40.25)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.389 |      0.292 |                   57 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.187 |      0.316 |                   56 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.007 |      0.303 |                   53 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.3101679104477612
[2m[36m(func pid=22158)[0m top5: 0.835820895522388
[2m[36m(func pid=22158)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=22158)[0m f1_macro: 0.29044501381063376
[2m[36m(func pid=22158)[0m f1_weighted: 0.3304179469733456
[2m[36m(func pid=22158)[0m f1_per_class: [0.293, 0.272, 0.468, 0.38, 0.052, 0.371, 0.335, 0.313, 0.142, 0.277]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.3148320895522388
[2m[36m(func pid=23659)[0m top5: 0.8092350746268657
[2m[36m(func pid=23659)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=23659)[0m f1_macro: 0.3039139154525321
[2m[36m(func pid=23659)[0m f1_weighted: 0.3414616465683669
[2m[36m(func pid=23659)[0m f1_per_class: [0.199, 0.227, 0.727, 0.317, 0.097, 0.375, 0.471, 0.216, 0.202, 0.209]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.1598 | Steps: 2 | Val loss: 2.2719 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.4635 | Steps: 2 | Val loss: 1.9218 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0090 | Steps: 2 | Val loss: 4.0886 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 02:18:24 (running for 00:44:45.68)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.458 |      0.29  |                   58 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.16  |      0.315 |                   57 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.006 |      0.304 |                   54 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22158)[0m top1: 0.3041044776119403
[2m[36m(func pid=22158)[0m top5: 0.832089552238806
[2m[36m(func pid=22158)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=22158)[0m f1_macro: 0.2892596765193805
[2m[36m(func pid=22158)[0m f1_weighted: 0.3246960430001801
[2m[36m(func pid=22158)[0m f1_per_class: [0.296, 0.266, 0.478, 0.374, 0.05, 0.365, 0.326, 0.322, 0.142, 0.275]
[2m[36m(func pid=22837)[0m top1: 0.3255597014925373
[2m[36m(func pid=22837)[0m top5: 0.8484141791044776
[2m[36m(func pid=22837)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=22837)[0m f1_macro: 0.31460279493963567
[2m[36m(func pid=22837)[0m f1_weighted: 0.3574839629846581
[2m[36m(func pid=22837)[0m f1_per_class: [0.311, 0.272, 0.71, 0.385, 0.068, 0.337, 0.438, 0.242, 0.206, 0.177]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.3180970149253731
[2m[36m(func pid=23659)[0m top5: 0.8078358208955224
[2m[36m(func pid=23659)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=23659)[0m f1_macro: 0.3045205459742121
[2m[36m(func pid=23659)[0m f1_weighted: 0.3455664912111403
[2m[36m(func pid=23659)[0m f1_per_class: [0.193, 0.222, 0.727, 0.336, 0.102, 0.37, 0.471, 0.216, 0.2, 0.207]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.1862 | Steps: 2 | Val loss: 2.2807 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.2482 | Steps: 2 | Val loss: 1.9203 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0130 | Steps: 2 | Val loss: 4.0894 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 02:18:30 (running for 00:44:50.95)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.463 |      0.289 |                   59 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.186 |      0.315 |                   58 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.009 |      0.305 |                   55 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.32882462686567165
[2m[36m(func pid=22837)[0m top5: 0.8479477611940298
[2m[36m(func pid=22837)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=22837)[0m f1_macro: 0.3149623681092023
[2m[36m(func pid=22837)[0m f1_weighted: 0.3615611833864177
[2m[36m(func pid=22837)[0m f1_per_class: [0.307, 0.276, 0.71, 0.391, 0.069, 0.338, 0.446, 0.243, 0.195, 0.177]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.3050373134328358
[2m[36m(func pid=22158)[0m top5: 0.8306902985074627
[2m[36m(func pid=22158)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=22158)[0m f1_macro: 0.2893087328731593
[2m[36m(func pid=22158)[0m f1_weighted: 0.3267786444528153
[2m[36m(func pid=22158)[0m f1_per_class: [0.293, 0.262, 0.478, 0.373, 0.05, 0.371, 0.333, 0.327, 0.142, 0.264]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.3138992537313433
[2m[36m(func pid=23659)[0m top5: 0.8055037313432836
[2m[36m(func pid=23659)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=23659)[0m f1_macro: 0.30187296131208396
[2m[36m(func pid=23659)[0m f1_weighted: 0.34027454763293263
[2m[36m(func pid=23659)[0m f1_per_class: [0.191, 0.222, 0.727, 0.329, 0.101, 0.367, 0.462, 0.216, 0.195, 0.208]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.1325 | Steps: 2 | Val loss: 2.2854 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.3550 | Steps: 2 | Val loss: 1.9198 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0051 | Steps: 2 | Val loss: 4.0642 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 02:18:35 (running for 00:44:56.16)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.248 |      0.289 |                   60 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.132 |      0.314 |                   59 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.013 |      0.302 |                   56 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3292910447761194
[2m[36m(func pid=22837)[0m top5: 0.8456156716417911
[2m[36m(func pid=22837)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=22837)[0m f1_macro: 0.3144224822990803
[2m[36m(func pid=22837)[0m f1_weighted: 0.3623083740829847
[2m[36m(func pid=22837)[0m f1_per_class: [0.306, 0.274, 0.71, 0.391, 0.068, 0.334, 0.452, 0.235, 0.198, 0.177]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.3069029850746269
[2m[36m(func pid=22158)[0m top5: 0.8292910447761194
[2m[36m(func pid=22158)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=22158)[0m f1_macro: 0.29176811933186997
[2m[36m(func pid=22158)[0m f1_weighted: 0.32945439581498615
[2m[36m(func pid=22158)[0m f1_per_class: [0.291, 0.262, 0.478, 0.376, 0.05, 0.365, 0.339, 0.324, 0.171, 0.261]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.31763059701492535
[2m[36m(func pid=23659)[0m top5: 0.8078358208955224
[2m[36m(func pid=23659)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=23659)[0m f1_macro: 0.3039190489326325
[2m[36m(func pid=23659)[0m f1_weighted: 0.34391298969088197
[2m[36m(func pid=23659)[0m f1_per_class: [0.196, 0.227, 0.727, 0.333, 0.102, 0.369, 0.468, 0.214, 0.195, 0.21]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.1193 | Steps: 2 | Val loss: 2.2653 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.2515 | Steps: 2 | Val loss: 1.9171 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0536 | Steps: 2 | Val loss: 4.0356 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 02:18:40 (running for 00:45:01.33)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.355 |      0.292 |                   61 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.119 |      0.314 |                   60 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.005 |      0.304 |                   57 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.33115671641791045
[2m[36m(func pid=22837)[0m top5: 0.8493470149253731
[2m[36m(func pid=22837)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=22837)[0m f1_macro: 0.3136923527328929
[2m[36m(func pid=22837)[0m f1_weighted: 0.3630403511142827
[2m[36m(func pid=22837)[0m f1_per_class: [0.312, 0.274, 0.71, 0.39, 0.07, 0.333, 0.46, 0.214, 0.196, 0.179]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.30736940298507465
[2m[36m(func pid=22158)[0m top5: 0.8292910447761194
[2m[36m(func pid=22158)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=22158)[0m f1_macro: 0.2957392498895884
[2m[36m(func pid=22158)[0m f1_weighted: 0.32917471059212516
[2m[36m(func pid=22158)[0m f1_per_class: [0.292, 0.259, 0.512, 0.383, 0.057, 0.363, 0.333, 0.328, 0.167, 0.264]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.31669776119402987
[2m[36m(func pid=23659)[0m top5: 0.8134328358208955
[2m[36m(func pid=23659)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=23659)[0m f1_macro: 0.3045867416277187
[2m[36m(func pid=23659)[0m f1_weighted: 0.34524827533332475
[2m[36m(func pid=23659)[0m f1_per_class: [0.202, 0.239, 0.727, 0.332, 0.094, 0.36, 0.468, 0.218, 0.197, 0.209]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.1794 | Steps: 2 | Val loss: 2.2616 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.2707 | Steps: 2 | Val loss: 1.9148 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0060 | Steps: 2 | Val loss: 4.0137 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=22837)[0m top1: 0.3362873134328358
[2m[36m(func pid=22837)[0m top5: 0.8521455223880597
[2m[36m(func pid=22837)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=22837)[0m f1_macro: 0.31654379489862106
[2m[36m(func pid=22837)[0m f1_weighted: 0.3668700844462239
[2m[36m(func pid=22837)[0m f1_per_class: [0.311, 0.28, 0.71, 0.383, 0.077, 0.338, 0.471, 0.222, 0.2, 0.174]
[2m[36m(func pid=22837)[0m 
== Status ==
Current time: 2024-01-07 02:18:45 (running for 00:45:06.66)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.252 |      0.296 |                   62 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.179 |      0.317 |                   61 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.054 |      0.305 |                   58 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22158)[0m top1: 0.30970149253731344
[2m[36m(func pid=22158)[0m top5: 0.8288246268656716
[2m[36m(func pid=22158)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=22158)[0m f1_macro: 0.3012109008453888
[2m[36m(func pid=22158)[0m f1_weighted: 0.331899992616078
[2m[36m(func pid=22158)[0m f1_per_class: [0.292, 0.257, 0.564, 0.38, 0.065, 0.379, 0.341, 0.324, 0.151, 0.258]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.3138992537313433
[2m[36m(func pid=23659)[0m top5: 0.8194962686567164
[2m[36m(func pid=23659)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=23659)[0m f1_macro: 0.30373296881533374
[2m[36m(func pid=23659)[0m f1_weighted: 0.34288890543062917
[2m[36m(func pid=23659)[0m f1_per_class: [0.222, 0.238, 0.727, 0.33, 0.086, 0.348, 0.466, 0.22, 0.199, 0.202]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.1231 | Steps: 2 | Val loss: 2.2591 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.3935 | Steps: 2 | Val loss: 1.9150 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0058 | Steps: 2 | Val loss: 4.0615 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 02:18:51 (running for 00:45:11.81)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.271 |      0.301 |                   63 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.123 |      0.319 |                   62 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.006 |      0.304 |                   59 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.33908582089552236
[2m[36m(func pid=22837)[0m top5: 0.8540111940298507
[2m[36m(func pid=22837)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=22837)[0m f1_macro: 0.31941344201119803
[2m[36m(func pid=22837)[0m f1_weighted: 0.3698118120888211
[2m[36m(func pid=22837)[0m f1_per_class: [0.315, 0.28, 0.71, 0.394, 0.077, 0.345, 0.466, 0.225, 0.212, 0.171]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.3101679104477612
[2m[36m(func pid=22158)[0m top5: 0.8269589552238806
[2m[36m(func pid=22158)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=22158)[0m f1_macro: 0.29962352906558815
[2m[36m(func pid=22158)[0m f1_weighted: 0.3333368187055479
[2m[36m(func pid=22158)[0m f1_per_class: [0.286, 0.257, 0.564, 0.385, 0.065, 0.372, 0.346, 0.316, 0.151, 0.254]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.30830223880597013
[2m[36m(func pid=23659)[0m top5: 0.8283582089552238
[2m[36m(func pid=23659)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=23659)[0m f1_macro: 0.29775400850653744
[2m[36m(func pid=23659)[0m f1_weighted: 0.33907181128431463
[2m[36m(func pid=23659)[0m f1_per_class: [0.229, 0.244, 0.727, 0.33, 0.078, 0.323, 0.463, 0.214, 0.167, 0.202]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.1701 | Steps: 2 | Val loss: 2.2622 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.2451 | Steps: 2 | Val loss: 1.9161 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0072 | Steps: 2 | Val loss: 4.0670 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 02:18:56 (running for 00:45:17.03)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.394 |      0.3   |                   64 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.17  |      0.318 |                   63 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.006 |      0.298 |                   60 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3381529850746269
[2m[36m(func pid=22837)[0m top5: 0.8540111940298507
[2m[36m(func pid=22837)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=22837)[0m f1_macro: 0.31757505196920854
[2m[36m(func pid=22837)[0m f1_weighted: 0.3699901973014642
[2m[36m(func pid=22837)[0m f1_per_class: [0.314, 0.276, 0.71, 0.403, 0.075, 0.342, 0.464, 0.221, 0.202, 0.169]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.30970149253731344
[2m[36m(func pid=22158)[0m top5: 0.8306902985074627
[2m[36m(func pid=22158)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=22158)[0m f1_macro: 0.30108448883177463
[2m[36m(func pid=22158)[0m f1_weighted: 0.3333331604715472
[2m[36m(func pid=22158)[0m f1_per_class: [0.28, 0.251, 0.595, 0.386, 0.065, 0.37, 0.349, 0.32, 0.148, 0.247]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.3101679104477612
[2m[36m(func pid=23659)[0m top5: 0.8353544776119403
[2m[36m(func pid=23659)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=23659)[0m f1_macro: 0.30054120062365064
[2m[36m(func pid=23659)[0m f1_weighted: 0.34306897392705904
[2m[36m(func pid=23659)[0m f1_per_class: [0.235, 0.25, 0.75, 0.338, 0.072, 0.324, 0.467, 0.206, 0.164, 0.2]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2710 | Steps: 2 | Val loss: 2.2523 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.1899 | Steps: 2 | Val loss: 1.9114 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0041 | Steps: 2 | Val loss: 4.0561 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 02:19:01 (running for 00:45:22.16)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.245 |      0.301 |                   65 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.271 |      0.316 |                   64 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.007 |      0.301 |                   61 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3362873134328358
[2m[36m(func pid=22837)[0m top5: 0.8544776119402985
[2m[36m(func pid=22837)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=22837)[0m f1_macro: 0.31556775906688234
[2m[36m(func pid=22837)[0m f1_weighted: 0.3686946715849886
[2m[36m(func pid=22837)[0m f1_per_class: [0.303, 0.266, 0.71, 0.411, 0.074, 0.344, 0.457, 0.225, 0.196, 0.169]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.3111007462686567
[2m[36m(func pid=22158)[0m top5: 0.8297574626865671
[2m[36m(func pid=22158)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=22158)[0m f1_macro: 0.3019635112466378
[2m[36m(func pid=22158)[0m f1_weighted: 0.3349431202648002
[2m[36m(func pid=22158)[0m f1_per_class: [0.286, 0.251, 0.595, 0.385, 0.065, 0.362, 0.357, 0.322, 0.149, 0.249]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.310634328358209
[2m[36m(func pid=23659)[0m top5: 0.8386194029850746
[2m[36m(func pid=23659)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=23659)[0m f1_macro: 0.30048259627751417
[2m[36m(func pid=23659)[0m f1_weighted: 0.34480341423326816
[2m[36m(func pid=23659)[0m f1_per_class: [0.241, 0.262, 0.75, 0.351, 0.072, 0.303, 0.459, 0.219, 0.168, 0.181]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.1211 | Steps: 2 | Val loss: 2.2518 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.2087 | Steps: 2 | Val loss: 1.8998 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0210 | Steps: 2 | Val loss: 4.0724 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 02:19:06 (running for 00:45:27.36)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.19  |      0.302 |                   66 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.121 |      0.313 |                   65 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.004 |      0.3   |                   62 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3344216417910448
[2m[36m(func pid=22837)[0m top5: 0.8568097014925373
[2m[36m(func pid=22837)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=22837)[0m f1_macro: 0.313412328424153
[2m[36m(func pid=22837)[0m f1_weighted: 0.3652728565306295
[2m[36m(func pid=22837)[0m f1_per_class: [0.303, 0.272, 0.688, 0.401, 0.077, 0.344, 0.451, 0.224, 0.203, 0.171]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=22158)[0m top1: 0.3138992537313433
[2m[36m(func pid=22158)[0m top5: 0.8348880597014925
[2m[36m(func pid=22158)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=22158)[0m f1_macro: 0.30307975332283116
[2m[36m(func pid=22158)[0m f1_weighted: 0.33736126078090006
[2m[36m(func pid=22158)[0m f1_per_class: [0.295, 0.254, 0.595, 0.384, 0.067, 0.368, 0.364, 0.314, 0.143, 0.246]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.30970149253731344
[2m[36m(func pid=23659)[0m top5: 0.8395522388059702
[2m[36m(func pid=23659)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=23659)[0m f1_macro: 0.2999305908537251
[2m[36m(func pid=23659)[0m f1_weighted: 0.34438669505175384
[2m[36m(func pid=23659)[0m f1_per_class: [0.248, 0.264, 0.75, 0.355, 0.071, 0.302, 0.455, 0.216, 0.163, 0.177]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0895 | Steps: 2 | Val loss: 2.2531 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.2079 | Steps: 2 | Val loss: 1.9006 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 02:19:11 (running for 00:45:32.63)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.209 |      0.303 |                   67 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.089 |      0.317 |                   66 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.021 |      0.3   |                   63 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.33675373134328357
[2m[36m(func pid=22837)[0m top5: 0.8572761194029851
[2m[36m(func pid=22837)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=22837)[0m f1_macro: 0.31747774292938813
[2m[36m(func pid=22837)[0m f1_weighted: 0.36739793336053805
[2m[36m(func pid=22837)[0m f1_per_class: [0.305, 0.281, 0.71, 0.404, 0.075, 0.342, 0.45, 0.235, 0.2, 0.173]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0326 | Steps: 2 | Val loss: 4.0906 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=22158)[0m top1: 0.31669776119402987
[2m[36m(func pid=22158)[0m top5: 0.8306902985074627
[2m[36m(func pid=22158)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=22158)[0m f1_macro: 0.3040288463589715
[2m[36m(func pid=22158)[0m f1_weighted: 0.34085755598448814
[2m[36m(func pid=22158)[0m f1_per_class: [0.3, 0.254, 0.579, 0.384, 0.066, 0.368, 0.375, 0.317, 0.142, 0.256]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.3087686567164179
[2m[36m(func pid=23659)[0m top5: 0.840018656716418
[2m[36m(func pid=23659)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=23659)[0m f1_macro: 0.30137174062329125
[2m[36m(func pid=23659)[0m f1_weighted: 0.3433591373362878
[2m[36m(func pid=23659)[0m f1_per_class: [0.259, 0.263, 0.75, 0.347, 0.065, 0.299, 0.456, 0.229, 0.168, 0.178]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2270 | Steps: 2 | Val loss: 2.2634 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.1806 | Steps: 2 | Val loss: 1.9006 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=22837)[0m top1: 0.3344216417910448
[2m[36m(func pid=22837)[0m top5: 0.8586753731343284
[2m[36m(func pid=22837)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=22837)[0m f1_macro: 0.31923626135921307
[2m[36m(func pid=22837)[0m f1_weighted: 0.3656794307369618
[2m[36m(func pid=22837)[0m f1_per_class: [0.305, 0.274, 0.733, 0.402, 0.073, 0.337, 0.449, 0.243, 0.2, 0.176]
[2m[36m(func pid=22837)[0m 
== Status ==
Current time: 2024-01-07 02:19:17 (running for 00:45:38.00)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.208 |      0.304 |                   68 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.227 |      0.319 |                   67 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.033 |      0.301 |                   64 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0257 | Steps: 2 | Val loss: 4.1072 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=22158)[0m top1: 0.31529850746268656
[2m[36m(func pid=22158)[0m top5: 0.8311567164179104
[2m[36m(func pid=22158)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=22158)[0m f1_macro: 0.303552532755032
[2m[36m(func pid=22158)[0m f1_weighted: 0.3400368379953851
[2m[36m(func pid=22158)[0m f1_per_class: [0.308, 0.254, 0.579, 0.38, 0.064, 0.368, 0.376, 0.317, 0.141, 0.25]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.1437 | Steps: 2 | Val loss: 2.2732 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=23659)[0m top1: 0.3050373134328358
[2m[36m(func pid=23659)[0m top5: 0.8348880597014925
[2m[36m(func pid=23659)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=23659)[0m f1_macro: 0.29863558710631816
[2m[36m(func pid=23659)[0m f1_weighted: 0.33989352827777286
[2m[36m(func pid=23659)[0m f1_per_class: [0.235, 0.255, 0.75, 0.35, 0.067, 0.295, 0.449, 0.224, 0.18, 0.181]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.2064 | Steps: 2 | Val loss: 1.8978 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 02:19:22 (running for 00:45:43.34)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.181 |      0.304 |                   69 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.144 |      0.312 |                   68 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.026 |      0.299 |                   65 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3260261194029851
[2m[36m(func pid=22837)[0m top5: 0.8596082089552238
[2m[36m(func pid=22837)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=22837)[0m f1_macro: 0.31217156392567225
[2m[36m(func pid=22837)[0m f1_weighted: 0.3570589314687057
[2m[36m(func pid=22837)[0m f1_per_class: [0.299, 0.268, 0.71, 0.396, 0.069, 0.347, 0.428, 0.241, 0.196, 0.169]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0032 | Steps: 2 | Val loss: 4.0925 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=22158)[0m top1: 0.31902985074626866
[2m[36m(func pid=22158)[0m top5: 0.8334888059701493
[2m[36m(func pid=22158)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=22158)[0m f1_macro: 0.3074084187386458
[2m[36m(func pid=22158)[0m f1_weighted: 0.3441147355984452
[2m[36m(func pid=22158)[0m f1_per_class: [0.323, 0.259, 0.579, 0.386, 0.064, 0.361, 0.381, 0.318, 0.153, 0.251]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=23659)[0m top1: 0.30223880597014924
[2m[36m(func pid=23659)[0m top5: 0.8246268656716418
[2m[36m(func pid=23659)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=23659)[0m f1_macro: 0.29518845136348487
[2m[36m(func pid=23659)[0m f1_weighted: 0.33730059328165757
[2m[36m(func pid=23659)[0m f1_per_class: [0.219, 0.244, 0.75, 0.353, 0.061, 0.281, 0.448, 0.229, 0.19, 0.178]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.1224 | Steps: 2 | Val loss: 2.2710 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.1994 | Steps: 2 | Val loss: 1.8961 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 02:19:27 (running for 00:45:48.53)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.206 |      0.307 |                   70 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.122 |      0.313 |                   69 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.003 |      0.295 |                   66 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.32882462686567165
[2m[36m(func pid=22837)[0m top5: 0.8624067164179104
[2m[36m(func pid=22837)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=22837)[0m f1_macro: 0.3125304269418228
[2m[36m(func pid=22837)[0m f1_weighted: 0.35958226361297024
[2m[36m(func pid=22837)[0m f1_per_class: [0.305, 0.265, 0.688, 0.4, 0.068, 0.338, 0.436, 0.247, 0.206, 0.173]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0042 | Steps: 2 | Val loss: 4.0793 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=22158)[0m top1: 0.31949626865671643
[2m[36m(func pid=22158)[0m top5: 0.8339552238805971
[2m[36m(func pid=22158)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=22158)[0m f1_macro: 0.30875174871202365
[2m[36m(func pid=22158)[0m f1_weighted: 0.34334685357633016
[2m[36m(func pid=22158)[0m f1_per_class: [0.322, 0.264, 0.579, 0.381, 0.065, 0.371, 0.375, 0.324, 0.153, 0.253]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.1639 | Steps: 2 | Val loss: 2.2613 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=23659)[0m top1: 0.3101679104477612
[2m[36m(func pid=23659)[0m top5: 0.8213619402985075
[2m[36m(func pid=23659)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=23659)[0m f1_macro: 0.2989830392018414
[2m[36m(func pid=23659)[0m f1_weighted: 0.34576059153960453
[2m[36m(func pid=23659)[0m f1_per_class: [0.23, 0.244, 0.75, 0.382, 0.068, 0.272, 0.452, 0.232, 0.194, 0.167]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.1932 | Steps: 2 | Val loss: 1.8926 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 02:19:32 (running for 00:45:53.67)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.199 |      0.309 |                   71 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.164 |      0.318 |                   70 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.004 |      0.299 |                   67 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3316231343283582
[2m[36m(func pid=22837)[0m top5: 0.8647388059701493
[2m[36m(func pid=22837)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=22837)[0m f1_macro: 0.31765389884650536
[2m[36m(func pid=22837)[0m f1_weighted: 0.36173927217068813
[2m[36m(func pid=22837)[0m f1_per_class: [0.322, 0.263, 0.71, 0.401, 0.07, 0.347, 0.437, 0.246, 0.209, 0.172]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0035 | Steps: 2 | Val loss: 4.0594 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=22158)[0m top1: 0.32136194029850745
[2m[36m(func pid=22158)[0m top5: 0.8353544776119403
[2m[36m(func pid=22158)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=22158)[0m f1_macro: 0.30725076477020796
[2m[36m(func pid=22158)[0m f1_weighted: 0.3446844808866888
[2m[36m(func pid=22158)[0m f1_per_class: [0.324, 0.26, 0.564, 0.394, 0.067, 0.371, 0.371, 0.32, 0.149, 0.251]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.1649 | Steps: 2 | Val loss: 2.2629 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=23659)[0m top1: 0.3125
[2m[36m(func pid=23659)[0m top5: 0.8199626865671642
[2m[36m(func pid=23659)[0m f1_micro: 0.3125
[2m[36m(func pid=23659)[0m f1_macro: 0.2984324648235527
[2m[36m(func pid=23659)[0m f1_weighted: 0.34855700555242985
[2m[36m(func pid=23659)[0m f1_per_class: [0.221, 0.246, 0.75, 0.394, 0.068, 0.286, 0.448, 0.207, 0.198, 0.165]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.1950 | Steps: 2 | Val loss: 1.8915 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 02:19:38 (running for 00:45:59.10)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.193 |      0.307 |                   72 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.165 |      0.322 |                   71 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.004 |      0.298 |                   68 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3344216417910448
[2m[36m(func pid=22837)[0m top5: 0.8605410447761194
[2m[36m(func pid=22837)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=22837)[0m f1_macro: 0.3218323402122299
[2m[36m(func pid=22837)[0m f1_weighted: 0.3647361732997209
[2m[36m(func pid=22837)[0m f1_per_class: [0.319, 0.263, 0.733, 0.412, 0.068, 0.351, 0.435, 0.248, 0.208, 0.181]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0047 | Steps: 2 | Val loss: 4.0678 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=22158)[0m top1: 0.31949626865671643
[2m[36m(func pid=22158)[0m top5: 0.8376865671641791
[2m[36m(func pid=22158)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=22158)[0m f1_macro: 0.30725039183951175
[2m[36m(func pid=22158)[0m f1_weighted: 0.3424372498531587
[2m[36m(func pid=22158)[0m f1_per_class: [0.319, 0.262, 0.564, 0.394, 0.067, 0.37, 0.363, 0.32, 0.158, 0.256]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.1218 | Steps: 2 | Val loss: 2.2638 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=23659)[0m top1: 0.30923507462686567
[2m[36m(func pid=23659)[0m top5: 0.8194962686567164
[2m[36m(func pid=23659)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=23659)[0m f1_macro: 0.29530367334905927
[2m[36m(func pid=23659)[0m f1_weighted: 0.34462188115658
[2m[36m(func pid=23659)[0m f1_per_class: [0.212, 0.242, 0.75, 0.398, 0.068, 0.272, 0.441, 0.198, 0.204, 0.167]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.1359 | Steps: 2 | Val loss: 1.8863 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 02:19:43 (running for 00:46:04.24)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.195 |      0.307 |                   73 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.122 |      0.319 |                   72 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.005 |      0.295 |                   69 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3344216417910448
[2m[36m(func pid=22837)[0m top5: 0.8600746268656716
[2m[36m(func pid=22837)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=22837)[0m f1_macro: 0.31898408703365233
[2m[36m(func pid=22837)[0m f1_weighted: 0.36567827059123104
[2m[36m(func pid=22837)[0m f1_per_class: [0.32, 0.261, 0.71, 0.421, 0.066, 0.339, 0.436, 0.248, 0.212, 0.178]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0113 | Steps: 2 | Val loss: 4.0816 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=22158)[0m top1: 0.3199626865671642
[2m[36m(func pid=22158)[0m top5: 0.8386194029850746
[2m[36m(func pid=22158)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=22158)[0m f1_macro: 0.3086628816945391
[2m[36m(func pid=22158)[0m f1_weighted: 0.34278000543583687
[2m[36m(func pid=22158)[0m f1_per_class: [0.324, 0.261, 0.564, 0.392, 0.06, 0.369, 0.364, 0.324, 0.169, 0.26]
[2m[36m(func pid=22158)[0m 
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0910 | Steps: 2 | Val loss: 2.2750 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=23659)[0m top1: 0.3064365671641791
[2m[36m(func pid=23659)[0m top5: 0.8171641791044776
[2m[36m(func pid=23659)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=23659)[0m f1_macro: 0.2927136829852671
[2m[36m(func pid=23659)[0m f1_weighted: 0.3419720882871999
[2m[36m(func pid=23659)[0m f1_per_class: [0.207, 0.239, 0.75, 0.396, 0.063, 0.27, 0.437, 0.196, 0.207, 0.162]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=22158)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.1237 | Steps: 2 | Val loss: 1.8839 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 02:19:48 (running for 00:46:09.45)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.319
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00021 | RUNNING    | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.136 |      0.309 |                   74 |
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.091 |      0.319 |                   73 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.011 |      0.293 |                   70 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.33302238805970147
[2m[36m(func pid=22837)[0m top5: 0.8582089552238806
[2m[36m(func pid=22837)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=22837)[0m f1_macro: 0.31907788030398543
[2m[36m(func pid=22837)[0m f1_weighted: 0.3644521297102383
[2m[36m(func pid=22837)[0m f1_per_class: [0.313, 0.263, 0.71, 0.419, 0.073, 0.335, 0.433, 0.249, 0.219, 0.177]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0040 | Steps: 2 | Val loss: 4.0911 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=22158)[0m top1: 0.3208955223880597
[2m[36m(func pid=22158)[0m top5: 0.840018656716418
[2m[36m(func pid=22158)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=22158)[0m f1_macro: 0.30895057296578654
[2m[36m(func pid=22158)[0m f1_weighted: 0.34447734490510395
[2m[36m(func pid=22158)[0m f1_per_class: [0.33, 0.257, 0.564, 0.394, 0.059, 0.367, 0.371, 0.323, 0.168, 0.257]
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0973 | Steps: 2 | Val loss: 2.3022 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=23659)[0m top1: 0.30830223880597013
[2m[36m(func pid=23659)[0m top5: 0.816231343283582
[2m[36m(func pid=23659)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=23659)[0m f1_macro: 0.2955451657208676
[2m[36m(func pid=23659)[0m f1_weighted: 0.3430548523369592
[2m[36m(func pid=23659)[0m f1_per_class: [0.208, 0.239, 0.75, 0.402, 0.075, 0.28, 0.429, 0.207, 0.202, 0.164]
[2m[36m(func pid=23659)[0m 
== Status ==
Current time: 2024-01-07 02:19:54 (running for 00:46:14.84)
Memory usage on this node: 19.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.318
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00022 | RUNNING    | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.097 |      0.316 |                   74 |
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.004 |      0.296 |                   71 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
| train_66d79_00017 | TERMINATED | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.091 |      0.32  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.32882462686567165
[2m[36m(func pid=22837)[0m top5: 0.8544776119402985
[2m[36m(func pid=22837)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=22837)[0m f1_macro: 0.3160852553366989
[2m[36m(func pid=22837)[0m f1_weighted: 0.3612316974134767
[2m[36m(func pid=22837)[0m f1_per_class: [0.311, 0.255, 0.71, 0.413, 0.071, 0.334, 0.434, 0.242, 0.219, 0.17]
[2m[36m(func pid=22837)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0079 | Steps: 2 | Val loss: 4.0771 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=22837)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.1015 | Steps: 2 | Val loss: 2.3196 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=23659)[0m top1: 0.30970149253731344
[2m[36m(func pid=23659)[0m top5: 0.816231343283582
[2m[36m(func pid=23659)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=23659)[0m f1_macro: 0.29600058926204426
[2m[36m(func pid=23659)[0m f1_weighted: 0.34465776733448766
[2m[36m(func pid=23659)[0m f1_per_class: [0.207, 0.239, 0.75, 0.407, 0.068, 0.282, 0.428, 0.211, 0.204, 0.164]
[2m[36m(func pid=23659)[0m 
== Status ==
Current time: 2024-01-07 02:19:59 (running for 00:46:20.24)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.317
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.008 |      0.296 |                   72 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
| train_66d79_00017 | TERMINATED | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.091 |      0.32  |                  100 |
| train_66d79_00018 | TERMINATED | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.844 |      0.319 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=22837)[0m top1: 0.3260261194029851
[2m[36m(func pid=22837)[0m top5: 0.8521455223880597
[2m[36m(func pid=22837)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=22837)[0m f1_macro: 0.3148412561694386
[2m[36m(func pid=22837)[0m f1_weighted: 0.35881761103593274
[2m[36m(func pid=22837)[0m f1_per_class: [0.317, 0.25, 0.71, 0.411, 0.063, 0.329, 0.431, 0.253, 0.217, 0.167]
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0043 | Steps: 2 | Val loss: 4.0980 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=23659)[0m top1: 0.3069029850746269
[2m[36m(func pid=23659)[0m top5: 0.8143656716417911
[2m[36m(func pid=23659)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=23659)[0m f1_macro: 0.2953612935277597
[2m[36m(func pid=23659)[0m f1_weighted: 0.3419340348487902
[2m[36m(func pid=23659)[0m f1_per_class: [0.204, 0.233, 0.75, 0.408, 0.067, 0.284, 0.42, 0.219, 0.205, 0.164]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0029 | Steps: 2 | Val loss: 4.0961 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 02:20:07 (running for 00:46:28.51)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.317
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.004 |      0.295 |                   73 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
| train_66d79_00017 | TERMINATED | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.091 |      0.32  |                  100 |
| train_66d79_00018 | TERMINATED | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.844 |      0.319 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23659)[0m top1: 0.3050373134328358
[2m[36m(func pid=23659)[0m top5: 0.8148320895522388
[2m[36m(func pid=23659)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=23659)[0m f1_macro: 0.2948459883154694
[2m[36m(func pid=23659)[0m f1_weighted: 0.3392227220386302
[2m[36m(func pid=23659)[0m f1_per_class: [0.202, 0.229, 0.75, 0.406, 0.068, 0.284, 0.414, 0.215, 0.211, 0.169]
[2m[36m(func pid=23659)[0m 
[2m[36m(func pid=23659)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0039 | Steps: 2 | Val loss: 4.0997 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 02:20:13 (running for 00:46:34.21)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.317
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00023 | RUNNING    | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.003 |      0.295 |                   74 |
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
| train_66d79_00017 | TERMINATED | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.091 |      0.32  |                  100 |
| train_66d79_00018 | TERMINATED | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.844 |      0.319 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 02:20:14 (running for 00:46:34.86)
Memory usage on this node: 16.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: 0.316
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/120.22 GiB heap, 0.0/55.52 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_66d79_00000 | TERMINATED | 192.168.7.53:111467 | 0.0001 |       0.99 |         0      |  1.612 |      0.263 |                   75 |
| train_66d79_00001 | TERMINATED | 192.168.7.53:111842 | 0.001  |       0.99 |         0      |  0.143 |      0.333 |                  100 |
| train_66d79_00002 | TERMINATED | 192.168.7.53:112258 | 0.01   |       0.99 |         0      |  0.011 |      0.315 |                   75 |
| train_66d79_00003 | TERMINATED | 192.168.7.53:112678 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   75 |
| train_66d79_00004 | TERMINATED | 192.168.7.53:128882 | 0.0001 |       0.9  |         0      |  2.623 |      0.16  |                   75 |
| train_66d79_00005 | TERMINATED | 192.168.7.53:129725 | 0.001  |       0.9  |         0      |  0.867 |      0.319 |                  100 |
| train_66d79_00006 | TERMINATED | 192.168.7.53:129730 | 0.01   |       0.9  |         0      |  0.083 |      0.319 |                  100 |
| train_66d79_00007 | TERMINATED | 192.168.7.53:134827 | 0.1    |       0.9  |         0      |  0.004 |      0.298 |                   75 |
| train_66d79_00008 | TERMINATED | 192.168.7.53:146329 | 0.0001 |       0.99 |         0.0001 |  1.641 |      0.248 |                   75 |
| train_66d79_00009 | TERMINATED | 192.168.7.53:151876 | 0.001  |       0.99 |         0.0001 |  0.058 |      0.334 |                  100 |
| train_66d79_00010 | TERMINATED | 192.168.7.53:151878 | 0.01   |       0.99 |         0.0001 |  0.002 |      0.315 |                   75 |
| train_66d79_00011 | TERMINATED | 192.168.7.53:154058 | 0.1    |       0.99 |         0.0001 |  0.097 |      0.296 |                   75 |
| train_66d79_00012 | TERMINATED | 192.168.7.53:163416 | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.16  |                   75 |
| train_66d79_00013 | TERMINATED | 192.168.7.53:169716 | 0.001  |       0.9  |         0.0001 |  0.885 |      0.326 |                  100 |
| train_66d79_00014 | TERMINATED | 192.168.7.53:171628 | 0.01   |       0.9  |         0.0001 |  0.121 |      0.315 |                   75 |
| train_66d79_00015 | TERMINATED | 192.168.7.53:174617 | 0.1    |       0.9  |         0.0001 |  0.005 |      0.296 |                   75 |
| train_66d79_00016 | TERMINATED | 192.168.7.53:181342 | 0.0001 |       0.99 |         1e-05  |  1.605 |      0.258 |                   75 |
| train_66d79_00017 | TERMINATED | 192.168.7.53:188206 | 0.001  |       0.99 |         1e-05  |  0.091 |      0.32  |                  100 |
| train_66d79_00018 | TERMINATED | 192.168.7.53:4309   | 0.01   |       0.99 |         1e-05  |  0.844 |      0.319 |                   75 |
| train_66d79_00019 | TERMINATED | 192.168.7.53:5547   | 0.1    |       0.99 |         1e-05  |  0     |      0.293 |                   75 |
| train_66d79_00020 | TERMINATED | 192.168.7.53:11358  | 0.0001 |       0.9  |         1e-05  |  2.635 |      0.163 |                   75 |
| train_66d79_00021 | TERMINATED | 192.168.7.53:22158  | 0.001  |       0.9  |         1e-05  |  1.124 |      0.309 |                   75 |
| train_66d79_00022 | TERMINATED | 192.168.7.53:22837  | 0.01   |       0.9  |         1e-05  |  0.101 |      0.315 |                   75 |
| train_66d79_00023 | TERMINATED | 192.168.7.53:23659  | 0.1    |       0.9  |         1e-05  |  0.004 |      0.296 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+


2024-01-07 02:20:14,158	INFO tune.py:798 -- Total run time: 2795.93 seconds (2794.84 seconds for the tuning loop).
[2m[36m(func pid=23659)[0m top1: 0.30597014925373134
[2m[36m(func pid=23659)[0m top5: 0.8166977611940298
[2m[36m(func pid=23659)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=23659)[0m f1_macro: 0.29635698888424195
[2m[36m(func pid=23659)[0m f1_weighted: 0.3409474216212474
[2m[36m(func pid=23659)[0m f1_per_class: [0.197, 0.227, 0.75, 0.409, 0.068, 0.291, 0.414, 0.221, 0.218, 0.168]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341335.1 ON aap04 CANCELLED AT 2024-01-07T02:20:21 ***
srun: error: aap04: task 0: Exited with exit code 1

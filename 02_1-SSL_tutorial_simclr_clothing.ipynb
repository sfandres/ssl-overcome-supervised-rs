{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "427af0ac-1f60-4a3d-bb81-804ca0ff6baa",
   "metadata": {},
   "source": [
    "**Tutorial 3: Train SimCLR on Clothing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a71e9-bd64-4ca4-866b-3f56ab7af491",
   "metadata": {},
   "source": [
    "https://docs.lightly.ai/tutorials/package/tutorial_simclr_clothing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb68760-ebbe-491e-9ca5-124c94a8119c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223376c1-368e-44e4-a31f-d56b8b27f4e5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5084c125-723e-4294-be96-580663ddffea",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb95c1c-4115-46d2-936b-220e772c09a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfandres/Documents/Git/lulc/lulc-venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import lightly\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1583bb24-41cf-4119-ba15-17017f6b94f4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72945ce-a4d4-4d76-955a-6342e30e0013",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b2b1d-9e65-4f36-9c88-343ec83f0e83",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7427c21-b47c-44d1-b5f4-5aea39548226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "# Training setup.\n",
    "num_workers = 8\n",
    "batch_size = 256\n",
    "seed = 1\n",
    "max_epochs = 20\n",
    "input_size = 128\n",
    "num_ftrs = 32\n",
    "\n",
    "# Replicability.\n",
    "pl.seed_everything(seed)\n",
    "\n",
    "# Path to dataset.\n",
    "path_to_data = 'datasets/clothing-dataset/images'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36dad12-ae1e-48c3-abb8-f178a1cb63e4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08860210-5cb7-42bf-8712-281c3c6f8e33",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff2ef2-971e-4937-ac58-8313e1530844",
   "metadata": {},
   "source": [
    "# Setup data augmentations and loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579376d-75c4-4bb8-82d1-65c7c6238aeb",
   "metadata": {},
   "source": [
    "The images from the dataset have been taken from above when the clothing was on a table, bed or floor. Therefore, we can make use of additional augmentations such as vertical flip or random rotation (90 degrees). By adding these augmentations we learn our model invariance regarding the orientation of the clothing piece. E.g. we donâ€™t care if a shirt is upside down but more about the strcture which make it a shirt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf7eeb3-2c8c-489a-a2e9-8bc43ea44a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = lightly.data.SimCLRCollateFunction(\n",
    "    input_size=input_size,\n",
    "    vf_prob=0.5,\n",
    "    rr_prob=0.5\n",
    ")\n",
    "\n",
    "# We create a torchvision transformation for\n",
    "# embedding the dataset after training.\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((input_size, input_size)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=lightly.data.collate.imagenet_normalize['mean'],\n",
    "        std=lightly.data.collate.imagenet_normalize['std'],\n",
    "    )\n",
    "])\n",
    "\n",
    "dataset_train_simclr = lightly.data.LightlyDataset(\n",
    "    input_dir=path_to_data\n",
    ")\n",
    "\n",
    "dataset_test = lightly.data.LightlyDataset(\n",
    "    input_dir=path_to_data,\n",
    "    transform=test_transforms\n",
    ")\n",
    "\n",
    "dataloader_train_simclr = torch.utils.data.DataLoader(\n",
    "    dataset_train_simclr,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd7b6c1-3b76-4717-97c9-8b884941b0c6",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b8e62-deb8-404f-b8d6-1af245518a20",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdc37d5-01c8-4ea0-bc2a-06e37f6b0db2",
   "metadata": {},
   "source": [
    "# Create the SimCLR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c87c71-78a8-4a52-abd1-1313d9e60142",
   "metadata": {},
   "source": [
    "Now we create the SimCLR model. We implement it as a PyTorch Lightning Module and use a ResNet-18 backbone from Torchvision. Lightly provides implementations of the SimCLR projection head and loss function in the SimCLRProjectionHead and NTXentLoss classes. We can simply import them and combine the building blocks in the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e79da858-5ce4-4a24-be10-8719aaec78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.models.modules.heads import SimCLRProjectionHead\n",
    "from lightly.loss import NTXentLoss\n",
    "\n",
    "\n",
    "class SimCLRModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        hidden_dim = resnet.fc.in_features\n",
    "        self.projection_head = SimCLRProjectionHead(hidden_dim, hidden_dim, 128)\n",
    "\n",
    "        self.criterion = NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(h)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optim, max_epochs\n",
    "        )\n",
    "        return [optim], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67748c58-8267-4fdf-b2d3-b28b22ca94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfandres/Documents/Git/lulc/lulc-venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M\n",
      "1 | projection_head | SimCLRProjectionHead | 328 K \n",
      "2 | criterion       | NTXentLoss           | 0     \n",
      "---------------------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "46.019    Total estimated model params size (MB)\n",
      "/home/sfandres/Documents/Git/lulc/lulc-venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:   0%|                                              | 0/22 [00:00<?, ?it/s, loss=5.26, v_num=2]"
     ]
    }
   ],
   "source": [
    "gpus = 1 if torch.cuda.is_available() else 0\n",
    "\n",
    "model = SimCLRModel()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs, gpus=gpus\n",
    ")\n",
    "trainer.fit(model, dataloader_train_simclr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72e45b-d5d6-4493-b5f3-57c376fe95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(model, dataloader):\n",
    "    \"\"\"Generates representations for all images in the dataloader with\n",
    "    the given model\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = []\n",
    "    filenames = []\n",
    "    with torch.no_grad():\n",
    "        for img, label, fnames in dataloader:\n",
    "            img = img.to(model.device)\n",
    "            emb = model.backbone(img).flatten(start_dim=1)\n",
    "            embeddings.append(emb)\n",
    "            filenames.extend(fnames)\n",
    "\n",
    "    embeddings = torch.cat(embeddings, 0)\n",
    "    embeddings = normalize(embeddings)\n",
    "    return embeddings, filenames\n",
    "\n",
    "model.eval()\n",
    "embeddings, filenames = generate_embeddings(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9e812-1777-4386-9a59-6009a3dbc0fa",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15183c7d-2f26-4607-9891-610c23a2ece3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eaa667-e01c-4aa9-a9e8-1a3783c828fa",
   "metadata": {},
   "source": [
    "# Visualize Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54af4a5-430f-423f-8186-75e5974fbbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_as_np_array(filename: str):\n",
    "    \"\"\"Returns an image as an numpy array\n",
    "    \"\"\"\n",
    "    img = Image.open(filename)\n",
    "    return np.asarray(img)\n",
    "\n",
    "\n",
    "def plot_knn_examples(embeddings, filenames, n_neighbors=4, num_examples=6):\n",
    "    \"\"\"Plots multiple rows of random images with their nearest neighbors\n",
    "    \"\"\"\n",
    "    # lets look at the nearest neighbors for some samples\n",
    "    # we use the sklearn library\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(embeddings)\n",
    "    distances, indices = nbrs.kneighbors(embeddings)\n",
    "\n",
    "    # get 5 random samples\n",
    "    samples_idx = np.random.choice(len(indices), size=num_examples, replace=False)\n",
    "\n",
    "    # loop through our randomly picked samples\n",
    "    for idx in samples_idx:\n",
    "        fig = plt.figure()\n",
    "        # loop through their nearest neighbors\n",
    "        for plot_x_offset, neighbor_idx in enumerate(indices[idx]):\n",
    "            # add the subplot\n",
    "            ax = fig.add_subplot(1, len(indices[idx]), plot_x_offset + 1)\n",
    "            # get the correponding filename for the current index\n",
    "            fname = os.path.join(path_to_data, filenames[neighbor_idx])\n",
    "            # plot the image\n",
    "            plt.imshow(get_image_as_np_array(fname))\n",
    "            # set the title to the distance of the neighbor\n",
    "            ax.set_title(f'd={distances[idx][plot_x_offset]:.3f}')\n",
    "            # let's disable the axis\n",
    "            plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5434084-365a-447d-a3cc-b95dac803035",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knn_examples(embeddings, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed318a4-b0c2-4708-ab45-430d01dc69f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You could use the pre-trained model and train a classifier on top.\n",
    "# pretrained_resnet_backbone = model.backbone\n",
    "\n",
    "# # you can also store the backbone and use it in another code\n",
    "# state_dict = {\n",
    "#     'resnet18_parameters': pretrained_resnet_backbone.state_dict()\n",
    "# }\n",
    "# torch.save(state_dict, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff97fd-408b-4761-ac61-cb65ba838ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the model in a new file for inference\n",
    "# resnet18_new = torchvision.models.resnet18()\n",
    "\n",
    "# # note that we need to create exactly the same backbone in order to load the weights\n",
    "# backbone_new = nn.Sequential(*list(resnet18_new.children())[:-1])\n",
    "\n",
    "# model2 = torch.load('model.pth')\n",
    "# backbone_new.load_state_dict(model2['resnet18_parameters'])\n",
    "\n",
    "# model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39525f87-997b-4269-a57b-20cc5a56509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d6375-0f69-4c43-a77a-6e8cff23e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.load('model.pth')\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c8238-e83a-4285-a1f1-e9a8c941fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings2, filenames2 = generate_embeddings(model2, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457092d1-01b4-4e4f-a76f-9bfc4de395d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knn_examples(embeddings2, filenames2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d4d82-9716-44ba-9a90-e4128faf73d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lulc-venv",
   "language": "python",
   "name": "lulc-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

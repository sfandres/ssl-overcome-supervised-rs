{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "293858ae-5611-498e-afd6-acddd993850f",
   "metadata": {},
   "source": [
    "**FINETUNING USING THE ANDALUCIA DATASET: RANDOM, IMAGENET, AND SSL-PRETRAINED WEIGHTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9469c-2666-4529-9eaf-1a1925b6eef0",
   "metadata": {},
   "source": [
    "Reference 1: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html <br>\n",
    "Reference 2: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24803b53-5c6f-4603-9e46-74b468dc6a4b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3123cba7-2168-4887-bf98-a65fa199db70",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d3b8f7-6c4c-47ec-a517-76e02872c1a6",
   "metadata": {},
   "source": [
    "# Initial configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c4791a-9d48-4508-8338-79dabc7c1754",
   "metadata": {},
   "source": [
    "## Libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c251f-635b-4c62-a485-7a7783aed523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom modules.\n",
    "from utils.other import is_notebook, build_paths\n",
    "from utils.reproducibility import set_seed, seed_worker\n",
    "from utils.dataset import (\n",
    "    AndaluciaDataset,\n",
    "    get_mean_std_dataloader,\n",
    "    load_mean_std_values\n",
    ")\n",
    "from utils.graphs import simple_bar_plot\n",
    "from utils.dataset import (\n",
    "    inv_norm_tensor,\n",
    "    show_one_batch\n",
    ")\n",
    "\n",
    "# Arguments and paths.\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "# PyTorch.\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import csv\n",
    "\n",
    "# Data management.\n",
    "import numpy as np\n",
    "\n",
    "# Performance metrics.\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Training checks.\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# For plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Showing images in the notebook.\n",
    "# from IPython.display import Image\n",
    "# from IPython.core.display import HTML\n",
    "# import matplotlib.font_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9b9da-9ac7-4864-a8fc-308776e01001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load notebook extensions.\n",
    "if is_notebook():\n",
    "    %load_ext tensorboard\n",
    "    # %load_ext pycodestyle_magic\n",
    "    # %pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a0080-01f5-42bd-8605-62f50b90b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AVAIL_SSL_MODELS = ['SimSiam', 'SimCLR', 'SimCLRv2', 'BarlowTwins']\n",
    "MODEL_CHOICES = ['Random', 'Imagenet'] + AVAIL_SSL_MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bd4ddc-304a-421e-a92b-9c38c40234c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Enable reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134ad27-2d2f-432d-86e3-c814f6858f54",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/docs/stable/notes/randomness.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884376b9-6fde-4e90-b2d5-0322f22481fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'torch initial seed:'.ljust(20)} {torch.initial_seed()}\")\n",
    "g = set_seed(SEED)\n",
    "print(f\"{'torch current seed:'.ljust(20)} {torch.initial_seed()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43662738-d0f4-4278-b066-8cf7a51a6d14",
   "metadata": {},
   "source": [
    "## Check torch CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4683d-d664-4d16-99ce-088085b452fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'torch.cuda.is_available():'.ljust(32)} {torch.cuda.is_available()}\")\n",
    "print(f\"{'torch.cuda.device_count():'.ljust(32)} {torch.cuda.device_count()}\")\n",
    "print(f\"{'torch.cuda.current_device():'.ljust(32)} {torch.cuda.current_device()}\")\n",
    "print(f\"{'torch.cuda.device(0):'.ljust(32)} {torch.cuda.device(0)}\")\n",
    "print(f\"{'torch.cuda.get_device_name(0):'.ljust(32)} {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"{'torch.backends.cudnn.benchmark:'.ljust(32)} {torch.backends.cudnn.benchmark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df2582-cdc9-45bd-9713-9dfd76d1736f",
   "metadata": {},
   "source": [
    "## Command line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06ff98-5690-4bcc-b267-7d963ee462ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser (get arguments).\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=(\"Script for evaluating the self-supervised learning\"\n",
    "                 \" models and compare them to standard approaches.\")\n",
    ")\n",
    "\n",
    "parser.add_argument('model_name', type=str, choices=MODEL_CHOICES,\n",
    "                    help=\"target model.\")\n",
    "\n",
    "parser.add_argument('task', type=str, choices=['multiclass', 'multilabel'],\n",
    "                    help=\"downstream task.\")\n",
    "\n",
    "parser.add_argument('--dataset_name', '-dn', type=str,\n",
    "                    default='Sentinel2AndaluciaLULC',\n",
    "                    help='dataset name for evaluation (default=Sentinel2AndaluciaLULC).')\n",
    "\n",
    "parser.add_argument('--dataset_level', '-dl', type=str, default='Level_N2',\n",
    "                    choices=['Level_N1', 'Level_N2'],\n",
    "                    help=\"dataset level (default=Level_N2).\")\n",
    "\n",
    "parser.add_argument('--dataset_train_pc', '-dtp', type=float, default=1.,\n",
    "                    help='dataset ratio for train subset (default=1.).')\n",
    "\n",
    "parser.add_argument('--epochs', '-e', type=int, default=25,\n",
    "                    help='number of epochs for training (default=25).')\n",
    "\n",
    "parser.add_argument('--batch_size', '-bs', type=int, default=64,\n",
    "                    help='number of images in a batch during training (default=64).')\n",
    "\n",
    "parser.add_argument('--show', '-s', action='store_true',\n",
    "                    help='the images should appear.');\n",
    "\n",
    "parser.add_argument('--cluster', '-c', action='store_true',\n",
    "                    help='the script runs on a cluster (large mem. space).');\n",
    "\n",
    "parser.add_argument('--torch_compile', '-tc', action='store_true',\n",
    "                    help='PyTorch 2.0 compile enabled.');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b55b9-6f6f-4af3-bb07-388df7ea37cf",
   "metadata": {},
   "source": [
    "## Simulate and get input arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59745726-57a0-4054-9b30-641b3bb5626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input arguments.\n",
    "if is_notebook():\n",
    "    args = parser.parse_args(\n",
    "        args=[\n",
    "            'SimSiam',\n",
    "            'multiclass',\n",
    "            '--show',\n",
    "            '--dataset_name=Sentinel2AndaluciaLULC',\n",
    "            '--dataset_train_pc=.05',\n",
    "            '--batch_size=64',\n",
    "            '--epochs=1'\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    args = parser.parse_args(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b1b6b-2f77-49c7-a929-5190d90dca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the parsed arguments into a dictionary and declare\n",
    "# variables with the same name as the arguments.\n",
    "args_dict = vars(args)\n",
    "for arg_name in args_dict:\n",
    "    globals()[arg_name] = args_dict[arg_name]\n",
    "\n",
    "# Iterate over the keys of the dictionary and check whether\n",
    "# the corresponding variables have been declared.\n",
    "print()\n",
    "for arg_name in args_dict:\n",
    "    if arg_name in globals():\n",
    "        arg_name_col = f'{arg_name}:'\n",
    "        print(f'{arg_name_col.ljust(20)} {globals()[arg_name]}')\n",
    "    else:\n",
    "        print(f'{arg_name} has not been declared')\n",
    "\n",
    "# Avoiding the runtimeError: \"Too many open files.\n",
    "# Communication with the workers is no longer possible.\"\n",
    "if is_notebook() or cluster:\n",
    "    print(' - Torch sharing strategy set to file_descriptor (default)')\n",
    "    torch.multiprocessing.set_sharing_strategy('file_descriptor')\n",
    "else:\n",
    "    print(' - Torch sharing strategy set to file_system (less memory)')\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "# Setting the device.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"{'Device:'.ljust(20)} {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c982140d-db8f-4b7a-80be-85eb3c4c7c92",
   "metadata": {},
   "source": [
    "## Build paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dfd417-6272-4990-a273-20aa6febb386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current directory.\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Build paths.\n",
    "paths = build_paths(cwd, os.path.join('finetuning', model_name))\n",
    "paths['runs'] = os.path.join(paths['runs'], f'finetuning_{dataset_train_pc:.3f}pc_train')\n",
    "\n",
    "# Show built paths.\n",
    "print()\n",
    "for path in paths:\n",
    "    path_name_col = f'{path}:'\n",
    "    print(f'{path_name_col.ljust(20)} {paths[path]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f81c54-5768-433e-930a-8658971f550f",
   "metadata": {},
   "source": [
    "## Settings and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f529ac0-3833-43b2-a29f-745d1c1de318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the images.\n",
    "input_size = 224\n",
    "\n",
    "# Format of the saved images.\n",
    "fig_format = '.png'\n",
    "\n",
    "# Number of digits to the right of the decimal\n",
    "decimal_places = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d940bd-cf06-40e7-aedd-65ff1f9980d7",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6ae757-5f26-46aa-8a7d-476f8a47c2b8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce58376-03c3-4e2d-be6d-8e5d50072143",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b0eab-f39d-41d1-9b6b-3fdad264b00b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compute normalization values (just once)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4e478-5a97-4a66-81a1-56232d0862e1",
   "metadata": {},
   "source": [
    "This block has to be run once before the rest since the splits are not created using the split-folder package, but according to a csv file instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef0cf3-2408-468e-9348-78f283d8fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = ['train', 'validation', 'test']\n",
    "\n",
    "# # Load Andalucia dataset.\n",
    "# andalucia_dataset = {x: AndaluciaDataset(\n",
    "#     root_dir=os.path.join(paths['datasets'], dataset_name),\n",
    "#     level=dataset_level,\n",
    "#     split=x,\n",
    "#     transform=transforms.ToTensor(),\n",
    "#     target_transform=None\n",
    "# ) for x in splits}\n",
    "\n",
    "# # Creating the dataloaders.\n",
    "# dataloaders = {x: torch.utils.data.DataLoader(\n",
    "#     andalucia_dataset[x],\n",
    "#     batch_size=256,\n",
    "#     worker_init_fn=seed_worker,\n",
    "#     generator=g\n",
    "# ) for x in splits}\n",
    "\n",
    "# # Loop over the train, val, and test datasets.\n",
    "# print('dataset_mean_std.txt')\n",
    "# for x in splits:\n",
    "\n",
    "#     # Computation.\n",
    "#     # print(f'Samples to be processed: '\n",
    "#     #       f'{len(dataloaders[x].dataset)}')\n",
    "#     print(f'{x}')\n",
    "#     mean, std = get_mean_std_dataloader(dataloaders[x])\n",
    "\n",
    "#     # Show mean and std.\n",
    "#     print(f'{mean}')\n",
    "#     print(f'{std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1005e-f112-49dd-8efb-c0b48df66f6e",
   "metadata": {},
   "source": [
    "## Load normalization values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc95e042-f78e-4074-bc23-b529cf485532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the path, mean and std values of each split from\n",
    "# a .txt file previously generated using a custom script.\n",
    "mean, std = load_mean_std_values(\n",
    "    os.path.join(\n",
    "        paths['datasets'],\n",
    "        os.path.join(dataset_name, dataset_level)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea756932-db0d-475b-ba0b-70dea5a3f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the key 'val' to 'validation'.\n",
    "mean['validation'] = mean.pop('val')\n",
    "std['validation'] = std.pop('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c45a9a-51d5-4eb9-9948-bf2b86ad4eac",
   "metadata": {},
   "source": [
    "## Custom transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d157ff-1fec-4727-a37c-b32d6cbf3637",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'validation', 'test']\n",
    "\n",
    "# Normalization transform (val and test).\n",
    "transform = {x: transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean[x],\n",
    "                         std=std[x])\n",
    "]) for x in splits[1:]}\n",
    "\n",
    "# Normalization transform (train).\n",
    "transform['train'] = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean['train'],\n",
    "                         std['train'])\n",
    "])\n",
    "\n",
    "for t in transform:\n",
    "    print(f'\\n{t}: {transform[t]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939dd570-eae9-43ad-9712-975afe63ef72",
   "metadata": {},
   "source": [
    "## Load custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac27b43-65d5-497b-8cae-37ed0bb252e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_abundances(abundances: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Transforms the abundances from tensor to max value.\n",
    "\n",
    "    Args:\n",
    "        abundances (torch.Tensor): list of abundances per batch.\n",
    "    \"\"\"\n",
    "\n",
    "    max_val, max_idx = torch.max(abundances, dim=0)\n",
    "\n",
    "    return max_idx.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d73852-bd1d-43d1-9bd8-33d3454e5293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Andalucia dataset with normalization.\n",
    "andalucia_dataset = {x: AndaluciaDataset(\n",
    "    root_dir=os.path.join(paths['datasets'], dataset_name),\n",
    "    level='Level_N2',\n",
    "    split=x,\n",
    "    train_ratio=dataset_train_pc,\n",
    "    transform=transform[x],\n",
    "    target_transform=transform_abundances if task == 'multiclass' else None,\n",
    "    verbose=True\n",
    ") for x in splits}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04129fc4-f4d3-4b41-be79-9005eb95ed5c",
   "metadata": {},
   "source": [
    "## PyTorch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a159bc-f8b4-4f6e-887f-500ad2209468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloaders.\n",
    "dataloader = {x: torch.utils.data.DataLoader(\n",
    "    andalucia_dataset[x],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g\n",
    ") for x in splits}\n",
    "\n",
    "# Check if shuffle is enabled.\n",
    "if isinstance(dataloader['train'].sampler, torch.utils.data.RandomSampler):\n",
    "    print('\\nShuffle enabled in training!')\n",
    "else:\n",
    "    print('\\nShuffle disabled in training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6316eda9-87b8-4c51-bef2-34054a64da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classes and number.\n",
    "class_names = andalucia_dataset['train'].classes\n",
    "# print(class_names)\n",
    "\n",
    "# Get dictionary of classes.\n",
    "idx_to_class = andalucia_dataset['train'].idx_to_class\n",
    "# print(idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9da9f-0b3f-4935-8fb3-f2dc60c3577f",
   "metadata": {},
   "source": [
    "## Check the balance and size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5efd3-73d9-4242-9ea5-6e0ffb4d622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some stats from the train dataset.\n",
    "for split in splits:\n",
    "    print(f\"\\n#Samples in {split} (from len(dataset)):\"\n",
    "          f\"       {len(andalucia_dataset[split])}\")\n",
    "    print(f\"#Samples in {split} (from dataloader.sampler):\"\n",
    "          f\" {len(dataloader[split].sampler)}\")\n",
    "    print(f\"#Samples in {split} (from dataloader.dataset):\"\n",
    "          f\" {len(dataloader[split].dataset)}\")\n",
    "    print(f\"#Batches in {split} (from dataloader):\"\n",
    "          f\"         {len(dataloader[split])}\")\n",
    "    if split == 'train':\n",
    "        print(f'Train subset ratio (%): {dataset_train_pc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87faa5b-5315-477b-9b6b-1a0f73384e32",
   "metadata": {},
   "source": [
    "## Check the distribution of samples in the train dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d890f8-2975-4338-ba21-46e5b955e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to save the labels.\n",
    "labels_list = []\n",
    "\n",
    "# Accessing Data and Targets in a PyTorch DataLoader.\n",
    "t0 = time.time()\n",
    "for i, (images, labels) in enumerate(dataloader['train']):\n",
    "    labels_list.append(labels)\n",
    "\n",
    "# Concatenate list of lists (batches).\n",
    "labels_list = torch.cat(labels_list, dim=0).numpy()\n",
    "print(f'\\nSample distribution computation in train dataset (s): '\n",
    "      f'{(time.time()-t0):.2f}')\n",
    "\n",
    "# Count number of unique values.\n",
    "data_x, data_y = np.unique(labels_list, return_counts=True)\n",
    "\n",
    "# New function to plot (suitable for execution in shell).\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "simple_bar_plot(ax,\n",
    "                data_x,\n",
    "                'Class',\n",
    "                data_y,\n",
    "                'N samples (dataloader)')\n",
    "\n",
    "plt.gcf().subplots_adjust(bottom=0.15)\n",
    "plt.gcf().subplots_adjust(left=0.15)\n",
    "fig_name_save = (f'sample_distribution')\n",
    "fig.savefig(os.path.join(paths['images'], fig_name_save+fig_format),\n",
    "            bbox_inches='tight')\n",
    "\n",
    "plt.show() if show else plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a4c9c-4d62-4292-b834-1e26bcb81151",
   "metadata": {},
   "source": [
    "## Look at some training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ccb1a-e035-4de0-9127-4cec1d095c39",
   "metadata": {},
   "source": [
    "### Only one sample from the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e34469-dddd-42fa-ba43-6f5da105f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label (w/ or w/o normalization).\n",
    "train_features, train_labels = next(iter(dataloader['train']))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "img = inv_norm_tensor(\n",
    "    img,\n",
    "    mean=mean['train'],\n",
    "    std=std['train']\n",
    ")\n",
    "label = train_labels[0]\n",
    "print(label)\n",
    "print(idx_to_class)\n",
    "if task == 'multiclass':\n",
    "    class_name = idx_to_class[int(label)]\n",
    "elif task == 'multilabel':\n",
    "    class_name = idx_to_class[int(torch.argmax(label))]\n",
    "plt.title(f'{class_name}')\n",
    "plt.imshow(torch.permute(img, (1, 2, 0)), cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show() if show else plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f07c9-f108-457a-a4af-16e04ea5f723",
   "metadata": {},
   "source": [
    "### One batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c0d95-30d8-4627-8a5e-e73d97ac1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with subplots.\n",
    "num_rows = int(dataloader['train'].batch_size ** 0.5)\n",
    "num_cols = (int(dataloader['train'].batch_size / num_rows)\n",
    "            + (dataloader['train'].batch_size % num_rows > 0))\n",
    "fig, axes = plt.subplots(nrows=num_rows,\n",
    "                         ncols=num_cols,\n",
    "                         figsize=(4*num_cols, 4*num_rows))\n",
    "\n",
    "# Take only one batch (inverse transform applied).\n",
    "inv_norm = True\n",
    "show_one_batch(axes, num_cols, dataloader['train'], task,\n",
    "               andalucia_dataset['train'].idx_to_class,\n",
    "               batch_id=0, inv_norm=inv_norm, mean=mean['train'],\n",
    "               std=std['train'])\n",
    "\n",
    "# Adjust and show image.\n",
    "plt.tight_layout()\n",
    "plt.show() if show else plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa5655-1d59-4912-991d-ccc1a360150a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11965017-e45d-4475-94df-3f7ab14a386b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c8e405-4183-4e6b-97f6-34edd7fbf17b",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad6c72-db21-4d2a-bd49-dd595cdbf197",
   "metadata": {},
   "source": [
    "Reference 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e8653-8807-455d-ac7e-2b9dbff00abd",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"><b>-----------------------------------------------------------------</b></p>\n",
    "<p style=\"color:red\"><b>----------> REVISED UP TO THIS POINT -----------</b></p>\n",
    "<p style=\"color:red\"><b>-----------------------------------------------------------------</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e877dbaf-9506-45ea-9b0b-4aea8e7104de",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a5fee-3e87-4bfa-ae29-31114a5ad960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer, batch_span_train):\n",
    "\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # ======================\n",
    "    # TRAINING COMPUTATION.\n",
    "    # Iterating through the dataloader\n",
    "    # We can track the batch index and do some intra-epoch reporting.\n",
    "    for i, data in enumerate(dataloader['train']):\n",
    "\n",
    "        # Every data instance is an input + label pair.\n",
    "        inputs = data[0].to(device)\n",
    "        labels = data[1].to(device)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch.\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients.\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report.\n",
    "        running_loss += loss.item()\n",
    "        if i % batch_span_train == batch_span_train - 1:\n",
    "            last_loss = running_loss / batch_span_train  # loss per batch\n",
    "            print(f'  batch {i+1:03d} loss: {last_loss:.4f}')\n",
    "            tb_x = epoch_index * len(dataloader['train']) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7c2564-9d4f-42fe-b9a4-8c34f30ce180",
   "metadata": {},
   "source": [
    "## Models and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979fb45-5118-4bba-96d0-007f0eedec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: resnet with random weights.\n",
    "if model_name == 'Random':\n",
    "    print('\\nModel without pretrained weights')\n",
    "    model = torchvision.models.resnet18(weights=None)\n",
    "\n",
    "    # Get the number of input features to the layer.\n",
    "    # Adjust the final layer to the current number of classes.\n",
    "    print(f'\\nOld final fully-connected layer: {model.fc}')\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(num_ftrs, len(class_names))\n",
    "    print(f'New final fully-connected layer: {model.fc}')\n",
    "\n",
    "    # Parameters of newly constructed modules\n",
    "    # have requires_grad=True by default.\n",
    "    # Freezing all the network except the final layer.\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Model: resnet with pretrained weights (Imagenet-1k).\n",
    "elif model_name == 'Imagenet':\n",
    "    print('\\nModel with pretrained weights on imagenet-1k')\n",
    "    model = torchvision.models.resnet18(\n",
    "        weights=torchvision.models.ResNet18_Weights.DEFAULT\n",
    "    )\n",
    "\n",
    "    # Get the number of input features to the layer.\n",
    "    # Adjust the final layer to the current number of classes.\n",
    "    print(f'\\nOld final fully-connected layer: {model.fc}')\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(num_ftrs, len(class_names))\n",
    "    print(f'New final fully-connected layer: {model.fc}')\n",
    "\n",
    "    # Parameters of newly constructed modules\n",
    "    # have requires_grad=True by default.\n",
    "    # Freezing all the network except the final layer.\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Model: resnet with pretrained weights (SSL).\n",
    "elif model_name in AVAIL_SSL_MODELS:\n",
    "    print('\\nModel with pretrained weights using SSL')\n",
    "    resnet18 = torchvision.models.resnet18(weights=None)\n",
    "\n",
    "    # Only backbone.\n",
    "    pt_backbone = torch.nn.Sequential(*list(resnet18.children())[:-1])\n",
    "\n",
    "    # List of trained models.\n",
    "    model_list = []\n",
    "    print()\n",
    "    for root, dirs, files in os.walk(paths['log_checkpoints']):\n",
    "        for filename in files:\n",
    "            if model_name in filename and 'ckpt' not in filename:\n",
    "                model_list.append(os.path.join(root, filename))\n",
    "\n",
    "    # Sort model list and show the items.\n",
    "    model_list = sorted(model_list, reverse=True)\n",
    "    for i, model in enumerate(model_list):\n",
    "        print(f'{i:02} --> {model}')\n",
    "\n",
    "    # Loading model.\n",
    "    idx = 0\n",
    "    pt_backbone.load_state_dict(torch.load(model_list[idx]))\n",
    "    print(f'\\nLoaded: {model_list[idx]}')\n",
    "\n",
    "    # # Get SSL model name and overwrite it.\n",
    "    # model_name = model_list[idx].split('/')[10].split('-')[0]\n",
    "    # print(f'Model name: {model_name}')\n",
    "\n",
    "    # Adding a linear layer on top of the model (linear classifier).\n",
    "    # Here, the output of the model is directly passed to the CrossEntropyLoss function\n",
    "    # without any activation function. The CrossEntropyLoss function applies the softmax\n",
    "    # activation internally, so you don't need to include a softmax layer in your model.\n",
    "    model = torch.nn.Sequential(\n",
    "        pt_backbone,\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(in_features=512,\n",
    "                        out_features=len(class_names),\n",
    "                        bias=True),\n",
    "    )\n",
    "\n",
    "    # Check if the model is loaded on GPU.\n",
    "    print(f'Saved in GPU: {next(pt_backbone.parameters()).is_cuda}')\n",
    "\n",
    "    # Get the number of input features to the layer.\n",
    "    # Adjust the final layer to the current number of classes.\n",
    "    # print(f'\\nOld final fully-connected layer: {model[-1]}')\n",
    "    # num_ftrs = model[-1].in_features\n",
    "    # model[-1] = torch.nn.Linear(num_ftrs, len(class_names))\n",
    "    print(f'New final fully-connected layer: {model[-1]}\\n')\n",
    "\n",
    "    # Parameters of newly constructed modules\n",
    "    # have requires_grad=True by default.\n",
    "    # Freezing all the network except the final layer.\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model[-1].parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869076ea-b152-4f3f-8aa1-d55fe560a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model structure.\n",
    "if show:\n",
    "    print(summary(\n",
    "        model,\n",
    "        input_size=(batch_size, 3, input_size, input_size),\n",
    "        device=device)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a88e773-ea52-4f9a-8b27-c3fc56ef7d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set loss.\n",
    "if task == 'multiclass':\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "elif task == 'multilabel':\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "print(f'\\nLoss: {loss_fn}')\n",
    "\n",
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "print(f'Optimizer:\\n{optimizer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f97db-8905-4055-8235-e89b11f11451",
   "metadata": {},
   "source": [
    "## Per-epoch activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733e6e5-1938-4fa9-92cc-b5d449bce3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# SET UP WRITERS AND VARIABLES.\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run.\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_path = os.path.join(paths['runs'],\n",
    "                        f'{task}_{model_name}_{timestamp}')\n",
    "print(f'\\nRun path: {run_path}')\n",
    "writer = SummaryWriter(run_path)\n",
    "\n",
    "# Open the file in the write mode.\n",
    "header = ['epoch', 'avg_loss', 'avg_vloss']\n",
    "csv_file = os.path.join(run_path,\n",
    "                        f'training_info_{task}_{model_name}_{dataset_train_pc:.3f}pc_train.csv')\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    csv_writer = csv.writer(file)  # Write the header.\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "# Initial variables.\n",
    "epoch_number = 0\n",
    "batch_span_train = max(len(dataloader['train']) // 5,\n",
    "                       len(dataloader['train']) // 2)\n",
    "batch_span_val = max(len(dataloader['validation']) // 5,\n",
    "                     len(dataloader['validation']) // 2)\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "# Compile model (only for PT2.0).\n",
    "if torch_compile:\n",
    "    model = torch.compile(model)\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Device used for training.\n",
    "print(f'Using {device} device')\n",
    "model.to(device)\n",
    "\n",
    "# ======================\n",
    "# TRAINING LOOP.\n",
    "# Iterating over the epochs.\n",
    "t0 = time.time()\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nEPOCH {epoch_number + 1}:')\n",
    "\n",
    "    # ======================\n",
    "    # TRAINING LOSS.\n",
    "    # Make sure gradient tracking is on, and do a pass over the data.\n",
    "    print('Running training...')\n",
    "    model.train()\n",
    "    avg_loss = train_one_epoch(epoch_number, writer, batch_span_train)\n",
    "    print('Training completed!')\n",
    "\n",
    "    # ======================\n",
    "    # EVALUATION COMPUTATION.\n",
    "    # We don't need gradients on to do reporting.\n",
    "    print('Running evaluation...')\n",
    "    model.eval()\n",
    "    running_vloss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(dataloader['validation']):\n",
    "            vinputs = vdata[0].to(device)\n",
    "            vlabels = vdata[1].to(device)            \n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss.item()\n",
    "            if i % batch_span_val == batch_span_val - 1:\n",
    "                print(f'  batch {i+1:03d} vloss: {vloss:.4f}')\n",
    "\n",
    "    print('Evaluation completed!')\n",
    "\n",
    "    # ======================\n",
    "    # VALIDATION LOSS.\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(f'LOSS train {avg_loss} val {avg_vloss}')\n",
    "\n",
    "    # ======================\n",
    "    # SAVING DATA.\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation.\n",
    "    # writer.add_scalars('Training vs. Validation Loss',\n",
    "    #                    {'Training': avg_loss, 'Validation': avg_vloss},\n",
    "    #                    epoch_number + 1)\n",
    "    writer.add_scalars('Validation Loss',\n",
    "                       {'Validation': avg_vloss},\n",
    "                       epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Open the file in the append mode.\n",
    "    data = [epoch, avg_loss, avg_vloss]\n",
    "    with open(csv_file, 'a', newline='') as file:\n",
    "        csv_writer = csv.writer(file)  # Write the data.\n",
    "        csv_writer.writerow(data)\n",
    "\n",
    "    # ======================\n",
    "    # SAVING CHECKPOINT.\n",
    "    # Track best performance, and save the model's state.\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = os.path.join(run_path,\n",
    "                                  f'{model_name}_vloss={avg_vloss:.4f}_e={epoch_number:03d}_t={timestamp}')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n",
    "\n",
    "print(f'\\nTotal duration: {(time.time()-t0):.2f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8b45de-22bc-4145-b35b-90af50ebbb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_notebook():\n",
    "    %tensorboard --logdir=output/runs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f960e4-a577-4f64-b16c-24d3bfcd605b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba5d7e-df44-4e65-8bec-9dcadf00dfda",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c618a0ac-b818-43fa-b4a7-48e48b43da72",
   "metadata": {},
   "source": [
    "# Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5275a-3ce3-428f-88e4-1fca2b7c9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoiding \"CUDA out of memory\" in PyTorch.\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize the probabilities, predictions and labels lists.\n",
    "y_prob = []\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Initialize the counters for top1 and top5 accuracy.\n",
    "top1_correct = 0\n",
    "top5_correct = 0\n",
    "\n",
    "# Since we're not training, we don't need to calculate\n",
    "# the gradients for our outputs with torch.no_grad():\n",
    "batches_test = len(dataloader['test'])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dataloader['test']):\n",
    "\n",
    "        # Dataset.\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Forward pass.\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get model predictions.\n",
    "        if task == 'multiclass':\n",
    "            # Convert logits to probabilities using a softmax function.\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            # Take the argmax of the probabilities to obtain the predicted class labels.\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "        elif task == 'multilabel':\n",
    "            # Convert logits to probabilities using a sigmoid function.\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            # Scale predicted abundances to sum to 1 across all the classes for each sample.\n",
    "            preds_sum = probs.sum(dim=1, keepdim=True)\n",
    "            preds = probs / preds_sum\n",
    "\n",
    "        # Append true and predicted labels to the lists (option 2).\n",
    "        y_prob.append(probs)\n",
    "        y_pred.append(preds)\n",
    "        y_true.append(labels)\n",
    "\n",
    "        # Compute top1 and top5 accuracy (option 1).\n",
    "        # if task == 'multiclass':\n",
    "        #     top1_correct += torch.sum(preds == labels).item()\n",
    "        #     top5_correct += torch.sum(torch.topk(probs, k=5, dim=1)[1] == labels.view(-1, 1)).item()\n",
    "\n",
    "        # Progress bar.\n",
    "        if i % (batches_test//8) == (batches_test//8 - 1) or i == batches_test - 1:\n",
    "            progress = 100 * (i + 1) // batches_test\n",
    "            if progress == 100:\n",
    "                print(\"Progress: 100%\", end='\\n', flush=True)\n",
    "            else:\n",
    "                print(f\"Progress: {progress}%\", end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eed801-c27c-4ec9-8710-d9d1aded6f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if task == 'multiclass':\n",
    "    # Compute top1 and top5 accuracy (option 1).\n",
    "    # top1_accuracy = top1_correct / len(dataloader['test'].dataset)\n",
    "    # top5_accuracy = top5_correct / len(dataloader['test'].dataset)\n",
    "    # print(f\"\\nTop-1 Accuracy: {top1_accuracy:.4f}\")\n",
    "    # print(f\"Top-5 Accuracy: {top5_accuracy:.4f}\")\n",
    "\n",
    "    # Concatenate the lists into tensors (option 2).\n",
    "    y_prob_cpu = torch.cat(y_prob).to('cpu')\n",
    "    y_pred_cpu = torch.cat(y_pred).to('cpu')\n",
    "    y_true_cpu = torch.cat(y_true).to('cpu')\n",
    "\n",
    "    # Compute top1 and top5 accuracy (option 2).\n",
    "    top1_accuracy = torch.sum(torch.eq(y_pred_cpu, y_true_cpu)).item() / len(y_true_cpu)\n",
    "    top5_accuracy = torch.sum(torch.topk(y_prob_cpu, k=5, dim=1)[1] == y_true_cpu.view(-1, 1)).item() / len(y_true_cpu)\n",
    "    print(f\"\\nTop-1 accuracy: {top1_accuracy:.4f}\")\n",
    "    print(f\"Top-5 accuracy: {top5_accuracy:.4f}\")\n",
    "\n",
    "    # F1 metrics.\n",
    "    f1_per_class = f1_score(y_true_cpu, y_pred_cpu, average=None)\n",
    "    f1_micro = f1_score(y_true_cpu, y_pred_cpu, average='micro')\n",
    "    f1_macro = f1_score(y_true_cpu, y_pred_cpu, average='macro')\n",
    "    f1_weighted = f1_score(y_true_cpu, y_pred_cpu, average='weighted')\n",
    "    print(f\"\\nPer class F-1 score:\\n{f1_per_class}\")\n",
    "    print(f\"\\nF1-Score micro: {f1_micro:.4f}\")\n",
    "    print(f\"F1-Score macro: {f1_macro:.4f}\")\n",
    "    print(f\"F1-Score weighted: {f1_weighted:.4f}\")\n",
    "\n",
    "    # Data to be written.\n",
    "    data2 = []\n",
    "    header = ['model', 'vloss', 'f1_micro', 'f1_macro', 'f1_weighted', 'f1_per_class', 'top1_accuracy', 'top5_accuracy']\n",
    "    data = [model_name, avg_vloss, f1_micro, f1_macro, f1_weighted, np.round(f1_per_class, decimal_places), top1_accuracy, top5_accuracy]\n",
    "    data_rounded = [round(elem, decimal_places) if isinstance(elem, float) else elem for elem in data]\n",
    "    print(f'\\nData to csv: {data_rounded}')\n",
    "\n",
    "elif task == 'multilabel':\n",
    "\n",
    "    # Concatenate the lists into tensors.\n",
    "    y_prob_cpu = torch.cat(y_prob).to('cpu')\n",
    "    y_pred_cpu = torch.cat(y_pred).to('cpu')\n",
    "    y_true_cpu = torch.cat(y_true).to('cpu')\n",
    "\n",
    "    # Compute global RMSE and MAE.\n",
    "    rmse = mean_squared_error(y_true_cpu, y_pred_cpu, squared=False)\n",
    "    mae = mean_absolute_error(y_true_cpu, y_pred_cpu)\n",
    "    print(f\"\\nRMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "    # Compute RMSE and MAE per class.\n",
    "    rmse_per_class = mean_squared_error(y_true_cpu, y_pred_cpu, multioutput='raw_values', squared=False)\n",
    "    mae_per_class = mean_absolute_error(y_true_cpu, y_pred_cpu, multioutput='raw_values')\n",
    "    print(f\"\\nRMSE per class:\\n{rmse_per_class}\")\n",
    "    print(f\"MAE per class:\\n{mae_per_class}\")\n",
    "\n",
    "    # Data to be written.\n",
    "    header = ['model', 'vloss', 'rmse', 'mae', 'rmse_per_class', 'mae_per_class']\n",
    "    data = [model_name, avg_vloss, rmse, mae, np.round(rmse_per_class, decimal_places), np.round(mae_per_class, decimal_places)]\n",
    "    data_rounded = [round(elem, decimal_places) if isinstance(elem, float) else elem for elem in data]\n",
    "    print(f'\\nData to csv: {data_rounded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02924c7-963a-45bc-b11e-df2d49a12aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in the write mode.\n",
    "csv_file = os.path.join(paths['runs'], f'results_on_test_set_{task}_{dataset_train_pc:.3f}pc_train.csv')\n",
    "\n",
    "# Check if file exists (header).\n",
    "if not os.path.isfile(csv_file):\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "\n",
    "# Write data.\n",
    "with open(csv_file, 'a', newline='') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    csv_writer.writerow(data_rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a1daa-ce5d-47ee-b179-7abfb2066b8d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249490a6-7fc7-45bd-a99f-4d8f85eee838",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f2baf-4a21-4fae-8935-e0ee7656a31d",
   "metadata": {},
   "source": [
    "UP TO HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06196e35-2417-40f2-9bad-f197103911de",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2512f-dd8c-4bcf-b5b6-fbb648845fd5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437632b1-b33e-46a7-9c0c-5c0046318faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training function.\n",
    "# def train_model(model, criterion, optimizer, device,\n",
    "#                 epochs=10, save_best_model=False):\n",
    "#     \"\"\"\n",
    "#     Main training function.\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(f\"Using {exp.device} device\")\n",
    "\n",
    "#     # Avoiding \"CUDA out of memory\" in PyTorch.\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     # Loss history.\n",
    "#     loss_values = {}\n",
    "#     loss_values['train'] = []\n",
    "#     loss_values['val'] = []\n",
    "#     total_time = 0\n",
    "\n",
    "#     # Saving best model's weights.\n",
    "#     best_model_val_wts = copy.deepcopy(model.state_dict())\n",
    "#     lowest_val_loss = 10000\n",
    "\n",
    "#     # Model to GPU if available.\n",
    "#     model.to(exp.device)\n",
    "\n",
    "#     # Iterating over the epochs.\n",
    "#     for epoch in range(epochs):\n",
    "\n",
    "#         # Initialize training loss.\n",
    "#         running_train_loss = 0.0\n",
    "\n",
    "#         # Start timer.\n",
    "#         since = time.time()\n",
    "\n",
    "#         # Enable training.\n",
    "#         model.train()\n",
    "\n",
    "#         for i, data in enumerate(dataloader_train):\n",
    "\n",
    "#             # Get the inputs; data is a list of [inputs, labels].\n",
    "#             inputs, labels = data[0].to(exp.device), data[1].to(exp.device)\n",
    "\n",
    "#             # Zero the parameter gradients.\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # Forward: make predictions.\n",
    "#             outputs = model(inputs)\n",
    "\n",
    "#             # Compute the loss and its gradients.\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Averaged loss across all training examples * batch_size.\n",
    "#             running_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "#             if i % 200 == 199:\n",
    "#                 print(f'T[{epoch + 1}, {i + 1:5d}] | '\n",
    "#                       f'Running loss: '\n",
    "#                       f'{running_train_loss/(i*inputs.size(0)):.4f}')\n",
    "\n",
    "#             # Adjust learning weights.\n",
    "#             optimizer.step()\n",
    "\n",
    "#         # Loss averaged across all training examples for the current epoch.\n",
    "#         epoch_train_loss = running_train_loss / len(dataloader_train.sampler)\n",
    "\n",
    "#         # Change model to evaluation mode.\n",
    "#         model.eval()\n",
    "\n",
    "#         # Initialize validating loss.\n",
    "#         running_val_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for j, vdata in enumerate(dataloader_val):\n",
    "\n",
    "#                 # Get the inputs; data is a list of [inputs, labels].\n",
    "#                 vinputs, vlabels = vdata[0].to(exp.device), vdata[1].to(exp.device)\n",
    "\n",
    "#                 # Forward: make predictions.\n",
    "#                 voutputs = model(vinputs)\n",
    "\n",
    "#                 # Compute the loss (w/o gradients).\n",
    "#                 vloss = criterion(voutputs, vlabels)\n",
    "\n",
    "#                 # Averaged loss across all validating examples * batch_size.\n",
    "#                 running_val_loss += vloss.item() * vinputs.size(0)\n",
    "\n",
    "#                 if j % 50 == 49:\n",
    "#                     print(f'V[{epoch + 1}, {j + 1:5d}] | '\n",
    "#                           f'Running loss: '\n",
    "#                           f'{running_val_loss/(j*inputs.size(0)):.4f}')\n",
    "\n",
    "#         # Loss averaged across all validating examples for the current epoch.\n",
    "#         epoch_val_loss = running_val_loss / len(dataloader_val.sampler)\n",
    "\n",
    "#         # Append loss values.\n",
    "#         loss_values['train'].append(epoch_train_loss)\n",
    "#         loss_values['val'].append(epoch_val_loss)\n",
    "\n",
    "#         # Deep copy the weights of the model.\n",
    "#         save_weights = epoch_val_loss < lowest_val_loss\n",
    "#         if save_weights:\n",
    "#             lowest_val_loss = epoch_val_loss\n",
    "#             best_model_train_loss = epoch_train_loss\n",
    "#             best_model_val_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "#         # End timer.\n",
    "#         time_elapsed = time.time() - since\n",
    "#         total_time += time_elapsed\n",
    "\n",
    "#         # Show stats.\n",
    "#         print(f'Epoch: {epoch} | '\n",
    "#               f'Train loss: {epoch_train_loss:.4f} | '\n",
    "#               f'Val loss: {epoch_val_loss:.4f} | '\n",
    "#               f'Elapsed: {time_elapsed // 60:.0f}m '\n",
    "#               f'{time_elapsed % 60:.0f}s | '\n",
    "#               f'Save weights: {save_weights}')\n",
    "\n",
    "#     print(f'\\nTraining completed in {total_time // 60:.0f}m '\n",
    "#           f'{total_time % 60:.0f}s')\n",
    "\n",
    "#     # Load best model weights.\n",
    "#     model.load_state_dict(best_model_val_wts)\n",
    "\n",
    "#     if save_best_model:\n",
    "\n",
    "#         # Move to CPU before saving it.\n",
    "#         model.to('cpu')\n",
    "\n",
    "#         # Filename with stats.\n",
    "#         save_path = f'pytorch_models/resnet18' \\\n",
    "#                     f'-losses={best_model_train_loss:.2f}' \\\n",
    "#                     f'_{lowest_val_loss:.2f}' \\\n",
    "#                     f'-time={datetime.now():%Y_%m_%d-%H_%M_%S}'\n",
    "\n",
    "#         # Save this pretrained model (recommended approach).\n",
    "#         torch.save(model.state_dict(), save_path)\n",
    "\n",
    "#         print('Model successfuly saved')\n",
    "\n",
    "#         # Move back to the GPU.\n",
    "#         model.to(exp.device)\n",
    "\n",
    "#     return model, loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a7f82-4423-4d81-85a8-0ecbaba41c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_losses(loss_history, title='', save_fig=False):\n",
    "#     \"\"\"\n",
    "#     Function for plotting the training and validation losses\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     fig = plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(loss_history['train'], label='Train')\n",
    "#     plt.plot(loss_history['val'], label='Validation')\n",
    "#     plt.xlabel('Epoch', labelpad=15)\n",
    "#     plt.ylabel('Loss', labelpad=15)\n",
    "#     plt.title(title)\n",
    "#     plt.gcf().subplots_adjust(bottom=0.15)\n",
    "#     plt.gcf().subplots_adjust(left=0.15)\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     if save_fig:\n",
    "#         fig.savefig('plt_loss_values.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff272e-4786-4a49-b7fc-73ce774a88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluation_on_test(model, device):\n",
    "#     \"\"\"\n",
    "#     Function to evaluate the performance\n",
    "#     of the model on the test dataset.\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Avoiding \"CUDA out of memory\" in PyTorch.\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     # Since we're not training, we don't need to calculate\n",
    "#     # the gradients for our outputs with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for i, data in enumerate(dataloader_test):\n",
    "\n",
    "#             # Dataset.\n",
    "#             inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "#             # Calculate outputs by running images through the network.\n",
    "#             outputs = model(inputs)\n",
    "\n",
    "#             # The class with the highest energy is what we\n",
    "#             # choose as prediction.\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#             # Progress bar.\n",
    "#             if i % 50 == 49:\n",
    "#                 print(f'Progress: {100 * i // len(dataloader_test)}%',\n",
    "#                       end='\\r',\n",
    "#                       flush=True)\n",
    "\n",
    "#     print(f'Accuracy of the network on the {total} '\n",
    "#           f'test images: {100 * correct // total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c48da-0289-4350-8641-62c207d44143",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18762958-4bb5-4a9e-a642-e5cecbc0f8f1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce24b86-e4e5-442a-876a-782db427e0c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ResNet18 from scrath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5b72a3-8fd8-45fe-8353-e77cdc5f99d6",
   "metadata": {},
   "source": [
    "## Definition and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b373932-506b-45a0-9301-d182e038d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model: resnet with random weights.\n",
    "# model = torchvision.models.resnet18(weights=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8627587-6558-4637-b5b2-f8e0cdc7b3ea",
   "metadata": {},
   "source": [
    "## Adjust final layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d714e83-002f-4f1a-b240-cf7e80ae41ca",
   "metadata": {},
   "source": [
    "Type: linear not softmax for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ad1ef-8697-41ab-a1af-1b6f615715c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check old final layer.\n",
    "# print(model.fc)\n",
    "\n",
    "# # Get the number of input features to the layer.\n",
    "# num_ftrs = model.fc.in_features\n",
    "# print(num_ftrs)\n",
    "\n",
    "# # Adjust the final layer to the current number of classes.\n",
    "# model.fc = torch.nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "# # Check new final layer.\n",
    "# print(model.fc)\n",
    "\n",
    "# # Freezing all the network except the final layer.\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.fc.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # Model structure.\n",
    "# summary(\n",
    "#     model,\n",
    "#     input_size=(exp.batch_size, 3, exp.input_size, exp.input_size),\n",
    "#     device=exp.device\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fdf194-24c8-45c3-bf12-d6ac176df30e",
   "metadata": {},
   "source": [
    "## Loss fcn and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea9cdf-c59e-4e10-94b5-f6017bda668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss function: cross-entropy loss.\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# # Optimizers: specified in the torch.optim package\n",
    "# optimizer = torch.optim.Adam(model.parameters(),\n",
    "#                              lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee9848-8c21-489d-8adc-2fc0e0bff9b0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba464d96-1913-43ef-96c4-59b5fdf9b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, loss_history = train_model(\n",
    "#     model,\n",
    "#     loss_fn,\n",
    "#     optimizer,\n",
    "#     exp.device,\n",
    "#     epochs=exp.epochs,\n",
    "#     save_best_model=True\n",
    "# )\n",
    "\n",
    "# plot_losses(loss_history, 'Model w/o pretrained weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0504b3-ae1c-4836-87e9-3c45e93b92d7",
   "metadata": {},
   "source": [
    "## Check performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515fcfa-ed16-42f9-8b70-992fde11c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_on_test(model, exp.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978fc629-e5a5-4608-bab6-ceee0b554bed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d0dd0-1d74-45aa-b3ca-0b12d79ca712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confusion matrix\n",
    "# conf_mat, class_accuracy = utils.create_confusion_matrix(\n",
    "#     model,\n",
    "#     dataloader_test,\n",
    "#     exp.device,\n",
    "#     class_names\n",
    "# )\n",
    "\n",
    "# # Bar plot for accuracy values.\n",
    "# utils.simple_bar_plot(range(len(class_names)),\n",
    "#                       class_accuracy,\n",
    "#                       'Classes',\n",
    "#                       'Accuracy (%)',\n",
    "#                       'class_accuracy',\n",
    "#                       fig_size=(15, 5),\n",
    "#                       save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d101e8-426c-408d-a5fc-345d08a4434d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d76c1c-8bf2-4b5e-ae24-9fd71ab6b7b0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b22ad-185d-4251-8545-744f9e403c59",
   "metadata": {},
   "source": [
    "# ResNet18 with pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e46b01-7e1d-4215-b2ae-c0afa84c82ac",
   "metadata": {},
   "source": [
    "## Definition and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472cdf9a-2854-4bd0-8ef5-4e5d179f0019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model: resnet with pretrained weights.\n",
    "# del model\n",
    "# model = torchvision.models.resnet18(\n",
    "#     weights=torchvision.models.ResNet18_Weights.DEFAULT\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9f8c6-dc14-4557-9ed2-b9d6084093b5",
   "metadata": {},
   "source": [
    "## Adjust final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63f366-3feb-40fa-bbcf-117fbca74817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check old final layer.\n",
    "# print(model.fc)\n",
    "\n",
    "# # Get the number of input features to the layer.\n",
    "# num_ftrs = model.fc.in_features\n",
    "# print(num_ftrs)\n",
    "\n",
    "# # Adjust the final layer to the current number of classes.\n",
    "# model.fc = torch.nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "# # Parameters of newly constructed modules\n",
    "# # have requires_grad=True by default.\n",
    "# # Check new final layer.\n",
    "# print(model.fc)\n",
    "\n",
    "# # Freezing all the network except the final layer.\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.fc.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # Model structure.\n",
    "# summary(\n",
    "#     model,\n",
    "#     input_size=(exp.batch_size, 3, exp.input_size, exp.input_size),\n",
    "#     device=exp.device\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e916e25-8b38-4e14-beb4-f311126e01f1",
   "metadata": {},
   "source": [
    "## Loss fcn and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaccb2dc-7957-4857-9bf7-b9eab949bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss function: cross-entropy loss.\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# # Optimizers: specified in the torch.optim package\n",
    "# optimizer = torch.optim.Adam(model.parameters(),\n",
    "#                              lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ec5c5-ddc4-4938-9720-8e07068997d0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea3d48-0270-41dc-a48d-15953ffb23e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, loss_history = train_model(\n",
    "#     model,\n",
    "#     loss_fn,\n",
    "#     optimizer,\n",
    "#     exp.device,\n",
    "#     epochs=exp.epochs,\n",
    "#     save_best_model=True\n",
    "# )\n",
    "\n",
    "# plot_losses(loss_history, 'Model w/ pretrained weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a40815-9743-42c6-8643-a963ce8533e6",
   "metadata": {},
   "source": [
    "## Check performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9e847-5b2e-49fc-ad8e-92529b5410e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_on_test(model, exp.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ab91b-c609-44a1-bfd7-61a456c6a22b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790a3a2-2350-49a7-9461-dbcca50335e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confusion matrix\n",
    "# conf_mat, class_accuracy = utils.create_confusion_matrix(\n",
    "#     model,\n",
    "#     dataloader_test,\n",
    "#     exp.device,\n",
    "#     class_names\n",
    "# )\n",
    "\n",
    "# # Bar plot for accuracy values.\n",
    "# utils.simple_bar_plot(range(len(class_names)),\n",
    "#                       class_accuracy,\n",
    "#                       'Classes',\n",
    "#                       'Accuracy (%)',\n",
    "#                       'class_accuracy',\n",
    "#                       fig_size=(15, 5),\n",
    "#                       save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bba927-e499-45b7-9d32-bd5e83b8d257",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a486bc-8b38-451a-9424-477585524559",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f806d186-1c7c-4423-8337-5d7f9433c746",
   "metadata": {},
   "source": [
    "# SSL model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839bbef-1ec9-4899-972b-9d6db8f2613c",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42234674-8005-4fcd-a76f-5d54f586fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # State model class.\n",
    "# resnet18 = torchvision.models.resnet18(weights=None)\n",
    "\n",
    "# # Only backbone (w/o final fc layer).\n",
    "# pt_backbone = torch.nn.Sequential(*list(resnet18.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147bd03f-ee69-4664-a0f3-57ea6aa78331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of trained models.\n",
    "# model_list = []\n",
    "# for root, dirs, files in os.walk('pytorch_models/history_log/'):\n",
    "#     for i, filename in enumerate(sorted(files, reverse=True)):\n",
    "#         model_list.append(root + filename)\n",
    "#         print(f'{i:02}: {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad21f18d-da7d-4007-a8be-edd49331ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading model.\n",
    "# idx = 0\n",
    "# print(model_list[idx])\n",
    "# pt_backbone.load_state_dict(torch.load(model_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b953213-478a-41fc-8f77-c418c5cf9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if the model is loaded on GPU.\n",
    "# next(pt_backbone.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebf71e-7453-442a-898a-1a38825e025b",
   "metadata": {},
   "source": [
    "## Checking the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d413d46-02cf-49a2-9a86-b39fbfaac08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First convolutional layer weights.\n",
    "# # print(backbone)\n",
    "# print(pt_backbone[0])\n",
    "# print(pt_backbone[0].weight[63])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd91df-abe1-42d3-ace9-8a8b45fbb0f8",
   "metadata": {},
   "source": [
    "## Adding a final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768a0a1-85c5-40a4-b843-4d50d44fd161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adding a linear layer on top of the model (linear classifier).\n",
    "# model_ssl = torch.nn.Sequential(\n",
    "#     pt_backbone,\n",
    "#     torch.nn.Flatten(),\n",
    "#     torch.nn.Linear(in_features=512, out_features=len(class_names), bias=True),\n",
    "#     # torch.nn.Softmax(dim=1)\n",
    "# )\n",
    "\n",
    "# # Freezing all the network except the final layer.\n",
    "# for param in model_ssl.parameters():\n",
    "#     param.requires_grad = False\n",
    "# # for param in model_ssl[0][7].parameters():\n",
    "# #     param.requires_grad = True\n",
    "# for param in model_ssl[2].parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# # Model structure.\n",
    "# summary(\n",
    "#     model_ssl,\n",
    "#     input_size=(exp.batch_size, 3, exp.input_size, exp.input_size),\n",
    "#     device=exp.device\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c0096-fb2f-4183-bdab-dfc345be59c7",
   "metadata": {},
   "source": [
    "## Loss fcn and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f3ed4b-1948-4843-96ca-b4cced958a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss function: cross-entropy loss.\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# # Optimizers: specified in the torch.optim package\n",
    "# optimizer = torch.optim.Adam(model_ssl.parameters(),\n",
    "#                              lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106ebc3-d159-45f8-b956-abd33add0552",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9d4f1-bc8a-45fe-88f5-dbcb71b0c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ssl, loss_history = train_model(\n",
    "#     model_ssl,\n",
    "#     loss_fn,\n",
    "#     optimizer,\n",
    "#     exp.device,\n",
    "#     epochs=exp.epochs,\n",
    "#     save_best_model=True\n",
    "# )\n",
    "\n",
    "# plot_losses(loss_history, 'Model w/ ssl pretrained weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815545a-4949-41fa-b208-774516170576",
   "metadata": {},
   "source": [
    "## Checking the weights after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea1ae5-4c44-4c2a-8efe-ac210a30ed79",
   "metadata": {},
   "source": [
    "They should have remained the same (frozen) except for the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f242c-38db-4d11-b84b-4bbab655297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First convolutional layer weights.\n",
    "# # print(backbone)\n",
    "# print(pt_backbone[0])\n",
    "# print(pt_backbone[0].weight[63])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9dff9a-1277-4301-b08e-8152110abe26",
   "metadata": {},
   "source": [
    "## Check performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636a4a8-8ee4-4861-9aa8-920f0df40b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_on_test(model_ssl, exp.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc6156-7506-449e-bc56-873c23aaf091",
   "metadata": {
    "tags": []
   },
   "source": [
    "## F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5024fea-751f-40a7-93ac-3a897285f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confusion matrix\n",
    "# conf_mat, class_accuracy = utils.create_confusion_matrix(\n",
    "#     model_ssl,\n",
    "#     dataloader_test,\n",
    "#     exp.device,\n",
    "#     class_names\n",
    "# )\n",
    "\n",
    "# # Bar plot for accuracy values.\n",
    "# utils.simple_bar_plot(range(len(class_names)),\n",
    "#                       class_accuracy,\n",
    "#                       'Classes',\n",
    "#                       'Accuracy (%)',\n",
    "#                       'class_accuracy',\n",
    "#                       fig_size=(15, 5),\n",
    "#                       save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ad0aa-d143-42dd-96f9-63c9cf6ad81f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238c6eb-d2e8-419d-b24c-1c828f995abe",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lulc2-conda",
   "language": "python",
   "name": "lulc2-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "19b3d86a20f2229bb5b60544c2c1729b82d0d9e897d3203c1cf087eb7e47686c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

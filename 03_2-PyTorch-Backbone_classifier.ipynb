{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "293858ae-5611-498e-afd6-acddd993850f",
   "metadata": {},
   "source": [
    "**USING RESNET18 WITH AND WITHOUT PRETRAINED WEIGHTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90375b5c-0b66-445d-9260-f68b8bf991df",
   "metadata": {},
   "source": [
    "**LOADING THE SSL MODEL AND TRAINING A CLASSIFIER ON TOP OF IT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9469c-2666-4529-9eaf-1a1925b6eef0",
   "metadata": {},
   "source": [
    "Reference 1: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26195bb6-2965-4abd-b858-161c062294b6",
   "metadata": {},
   "source": [
    "Reference 2: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e0369-a80d-44d1-b6cf-339cefc3c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b7dd78-3b06-441e-9cb8-4ca54a7b955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if is_notebook():\n",
    "#     %load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d06a82-af2e-4dc0-a873-1a2a225e284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if is_notebook():\n",
    "#     %pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9b9da-9ac7-4864-a8fc-308776e01001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "if is_notebook():\n",
    "    %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24803b53-5c6f-4603-9e46-74b468dc6a4b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3123cba7-2168-4887-bf98-a65fa199db70",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d3b8f7-6c4c-47ec-a517-76e02872c1a6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c4791a-9d48-4508-8338-79dabc7c1754",
   "metadata": {},
   "source": [
    "## Libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c251f-635b-4c62-a485-7a7783aed523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom modules.\n",
    "from utils.computation import Experiment, pca_computation, tsne_computation\n",
    "from utils.dataset import (\n",
    "    AndaluciaDataset,\n",
    "    get_mean_std_dataloader,\n",
    "    show_one_batch,\n",
    "    inv_norm_tensor\n",
    ")\n",
    "\n",
    "# OS module.\n",
    "import os\n",
    "\n",
    "# PyTorch.\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "# Data management.\n",
    "import numpy as np\n",
    "\n",
    "# import lightly\n",
    "\n",
    "# Training checks.\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "\n",
    "# import random\n",
    "\n",
    "# For plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Showing images in the notebook.\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "# from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c253e418-09b4-4dbe-a981-eff60e2f981a",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddfb34f-1fbe-4add-9202-4cc35d9aee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround to make bool options work as expected.\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05856067-f415-40be-9864-a4bbf307c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser (get arguments).\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    import sys\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=(\"Script for evaluating the self-supervised learning\"\n",
    "                     \" models and compare them to standard approaches.\")\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        'model',\n",
    "        type=str,\n",
    "        choices=['scratch', 'imagenet', 'ssl'],\n",
    "        help=(\"Model for finetuning. \"\n",
    "              \"Use 'scratch', 'imagenet' or 'ssl'.\")\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--dataset',\n",
    "        type=str,\n",
    "        default='Sentinel2AndaluciaLULC',\n",
    "        choices=['Sentinel2GlobalLULC', 'Sentinel2AndaluciaLULC'],\n",
    "        help='Dataset name for evaluation.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--ratio',\n",
    "        type=str,\n",
    "        default='(0.700,0.0900,0.2100)',\n",
    "        help='Dataset ratio for evaluation.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--epochs',\n",
    "        type=int,\n",
    "        default=15,\n",
    "        help='Number of epochs for training.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        type=int,\n",
    "        default=64,\n",
    "        help='Number of images in a batch during training.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--show_fig',\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help='Whether the images should appear.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--cluster',\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=('Whether the script runs on a cluster '\n",
    "              '(large memory space available).')\n",
    "    )\n",
    "\n",
    "if is_notebook():\n",
    "    args = parser.parse_args(\n",
    "        args=['scratch',\n",
    "              '--dataset',\n",
    "              'Sentinel2AndaluciaLULC',\n",
    "              '--epochs',\n",
    "              '25',\n",
    "              '--batch_size',\n",
    "              '64',\n",
    "              '--show_fig',\n",
    "              'True'])\n",
    "else:\n",
    "    args = parser.parse_args(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80aa854-ce45-46ef-806a-18a1fac093f7",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb597468-acb3-4683-8b29-20be68742de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Target model.\n",
    "model_name = args.model\n",
    "print(f'\\nTarget model for finetuning: {model_name}')\n",
    "\n",
    "# Target dataset name.\n",
    "dataset_name = args.dataset\n",
    "print(f'Target dataset name: {dataset_name}')\n",
    "\n",
    "# # Target dataset ratio.\n",
    "# dataset_ratio = args.ratio\n",
    "# print(f'Target dataset ratio: {dataset_ratio}')\n",
    "\n",
    "# Setting number of epochs.\n",
    "epochs = args.epochs\n",
    "print(f'Number of epochs: {epochs}')\n",
    "\n",
    "# Setting batch size.\n",
    "batch_size = args.batch_size\n",
    "print(f'Batch size: {batch_size}')\n",
    "\n",
    "# Show figures.\n",
    "show = args.show_fig\n",
    "print(f'Showing figures: {show}')\n",
    "\n",
    "# Supercomputer?.\n",
    "cluster = args.cluster\n",
    "print(f'\\nExecution on cluster: {cluster}')\n",
    "\n",
    "# Avoiding the runtimeError: Too many open files.\n",
    "# Communication with the workers is no longer possible.\n",
    "if is_notebook() or cluster:\n",
    "    print(f'Execution on jupyter or cluster: '\n",
    "          f'Torch sharing strategy set to file_system (default)')\n",
    "    torch.multiprocessing.set_sharing_strategy('file_descriptor')\n",
    "else:\n",
    "    print(f'Execution on shell with few resources: '\n",
    "          'Torch sharing strategy set to file_system')\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1964900e-5f35-4ca5-af59-14bfedf11387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamenters.\n",
    "exp = Experiment(\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    input_size=224,\n",
    ")\n",
    "print(f'\\nDevice: {exp.device}\\n')\n",
    "\n",
    "# Get current directory.\n",
    "cwd = os.getcwd()\n",
    "print(f'Working directory: {cwd}')\n",
    "\n",
    "# Input directory where the datasets are stored.\n",
    "input_dir_datasets = os.path.join(cwd, 'datasets')\n",
    "print(f'\\nInput directory for datasets: {input_dir_datasets}')\n",
    "\n",
    "# Output directory.\n",
    "output_dir = os.path.join(cwd, 'output')\n",
    "\n",
    "# Output directory to save the model checkpoint.\n",
    "output_dir_models = os.path.join(os.path.join(output_dir, 'pytorch_models'),\n",
    "                                 'finetuning')\n",
    "print(f'Output directory for model checkpoints: {output_dir_models}')\n",
    "\n",
    "# Folder to save the figures.\n",
    "output_dir_figs = os.path.join(os.path.join(output_dir, 'figures'),\n",
    "                               'finetuning')\n",
    "fig_format = '.png'  # .pdf\n",
    "print(f'Output directory for figures ({fig_format} format): {output_dir_figs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f3f16-a19f-48f2-beab-cb5c97159106",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a5121-5498-4692-a9cc-b2ce6f2687ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.reproducibility()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d940bd-cf06-40e7-aedd-65ff1f9980d7",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6ae757-5f26-46aa-8a7d-476f8a47c2b8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce58376-03c3-4e2d-be6d-8e5d50072143",
   "metadata": {},
   "source": [
    "# Custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b0eab-f39d-41d1-9b6b-3fdad264b00b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Compute normalization values (just once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef0cf3-2408-468e-9348-78f283d8fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = ['train', 'validation', 'test']\n",
    "\n",
    "# # Load Andalucia dataset.\n",
    "# andalucia_dataset = {x: AndaluciaDataset(\n",
    "#     root_dir=os.path.join(input_dir_datasets, dataset_name),\n",
    "#     level='Level_N2',\n",
    "#     split=x,\n",
    "#     transform=transforms.ToTensor(),\n",
    "#     target_transform=None\n",
    "# ) for x in splits}\n",
    "\n",
    "# # Creating the dataloaders.\n",
    "# dataloaders = {x: DataLoader(\n",
    "#     andalucia_dataset[x],\n",
    "#     batch_size=128,\n",
    "#     worker_init_fn=exp.seed_worker,\n",
    "#     generator=exp.g\n",
    "# ) for x in splits}\n",
    "\n",
    "# # Loop over the train, val, and test datasets.\n",
    "# for x in splits:\n",
    "\n",
    "#     # Computation.\n",
    "#     print(f'- {x}:')\n",
    "#     print(f'Samples to be processed: '\n",
    "#           f'{len(dataloaders[x].dataset)}')\n",
    "#     mean, std = get_mean_std_dataloader(dataloaders[x])\n",
    "\n",
    "#     # Show mean and std.\n",
    "#     print(f'mean: {mean}')\n",
    "#     print(f'std: {std}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01991198-3d9a-44f9-81ef-7373fc4aa604",
   "metadata": {},
   "source": [
    "## Load dataset w/ mean and std values (add transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a974e7-5520-48bb-82bb-8c868f0504f6",
   "metadata": {},
   "source": [
    "- train:\n",
    "Samples to be processed: 15038\n",
    "tensor([0.3036, 0.3045, 0.3224])\n",
    "tensor([0.1351, 0.0921, 0.0712])\n",
    "\n",
    "- validation:\n",
    "Samples to be processed: 2153\n",
    "tensor([0.3042, 0.3047, 0.3224])\n",
    "tensor([0.1338, 0.0910, 0.0701])\n",
    "\n",
    "- test:\n",
    "Samples to be processed: 4298\n",
    "tensor([0.3015, 0.3031, 0.3213])\n",
    "tensor([0.1314, 0.0894, 0.0689])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bce73-7a8a-46c1-88de-99b35a4371b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'validation', 'test']\n",
    "\n",
    "# Set mean and std values.\n",
    "mean_std_dict = {\n",
    "    'train': (torch.tensor([0.3036, 0.3045, 0.3224]),\n",
    "              torch.tensor([0.1351, 0.0921, 0.0712])),\n",
    "    'validation': (torch.tensor([0.3042, 0.3047, 0.3224]),\n",
    "                   torch.tensor([0.1338, 0.0910, 0.0701])),\n",
    "    'test': (torch.tensor([0.3015, 0.3031, 0.3213]),\n",
    "             torch.tensor([0.1314, 0.0894, 0.0689]))\n",
    "}\n",
    "\n",
    "# Normalization transform (val and test).\n",
    "transform_normal = {x: transforms.Compose([\n",
    "    transforms.Resize((exp.input_size, exp.input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean_std_dict[x][0],\n",
    "                         std=mean_std_dict[x][1])\n",
    "]) for x in splits[1:]}\n",
    "\n",
    "# Normalization transform (train).\n",
    "transform_normal['train'] = transforms.Compose([\n",
    "    transforms.Resize((exp.input_size, exp.input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean_std_dict['train'][0],\n",
    "                         std=mean_std_dict['train'][1])\n",
    "])\n",
    "print(f'\\n{transform_normal}')\n",
    "\n",
    "# Load the Andalucia dataset with normalization.\n",
    "andalucia_dataset_norm = {x: AndaluciaDataset(\n",
    "    root_dir=os.path.join(input_dir_datasets, dataset_name),\n",
    "    level='Level_N2',\n",
    "    split=x,\n",
    "    transform=transform_normal[x],\n",
    "    target_transform=None,\n",
    "    verbose=False\n",
    ") for x in splits}\n",
    "\n",
    "# Define dataloaders.\n",
    "dataloader = {x: torch.utils.data.DataLoader(\n",
    "    andalucia_dataset_norm[x],\n",
    "    batch_size=exp.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=exp.num_workers,\n",
    "    worker_init_fn=exp.seed_worker,\n",
    "    generator=exp.g\n",
    ") for x in splits}\n",
    "\n",
    "# Get classes and number.\n",
    "class_names = andalucia_dataset_norm['train'].classes\n",
    "print(class_names)\n",
    "\n",
    "# Get dictionary of classes.\n",
    "idx_to_class = andalucia_dataset_norm['train'].idx_to_class\n",
    "print(idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a4c9c-4d62-4292-b834-1e26bcb81151",
   "metadata": {},
   "source": [
    "## Look at some training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c0d95-30d8-4627-8a5e-e73d97ac1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with subplots.\n",
    "num_rows = int(dataloader['train'].batch_size ** 0.5)\n",
    "num_cols = (int(dataloader['train'].batch_size / num_rows)\n",
    "            + (dataloader['train'].batch_size % num_rows > 0))\n",
    "fig, axes = plt.subplots(nrows=num_rows,\n",
    "                         ncols=num_cols,\n",
    "                         figsize=(4*num_cols, 4*num_rows))\n",
    "\n",
    "# Take only one batch (inverse transform applied).\n",
    "inv_norm=True\n",
    "show_one_batch(axes, num_cols, dataloader['train'],\n",
    "               andalucia_dataset_norm['train'].idx_to_class,\n",
    "               batch_id=0, inv_norm=inv_norm, mean=mean_std_dict['train'][0],\n",
    "               std=mean_std_dict['train'][1])\n",
    "\n",
    "# Adjust and show image.\n",
    "plt.tight_layout()\n",
    "plt.show() if show else plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e34469-dddd-42fa-ba43-6f5da105f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label (w/ or w/o normalization).\n",
    "train_features, train_labels = next(iter(dataloader['train']))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "img = inv_norm_tensor(\n",
    "    img,\n",
    "    mean=mean_std_dict['train'][0],\n",
    "    std=mean_std_dict['train'][1]\n",
    ")\n",
    "label = train_labels[0]\n",
    "class_name = idx_to_class[int(torch.argmax(label))]\n",
    "print(label)\n",
    "plt.title(f'{class_name}')\n",
    "plt.imshow(torch.permute(img, (1, 2, 0)), cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show() if show else plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeaaff0-6d50-4888-85cd-63de0902668d",
   "metadata": {},
   "source": [
    "## Check balance and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5efd3-73d9-4242-9ea5-6e0ffb4d622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some stats from the train dataset.\n",
    "print(f\"\\n#Samples in train (from len(dataset)): {len(andalucia_dataset_norm['train'])}\")\n",
    "print(f\"#Samples in train (from dataloader.sampler): {len(dataloader['train'].sampler)}\")\n",
    "print(f\"#Samples in train (from dataloader.dataset): {len(dataloader['train'].dataset)}\")\n",
    "print(f\"#Batches in train (from dataloader): {len(dataloader['train'])}\")\n",
    "\n",
    "# Print some stats from the val dataset.\n",
    "print(f\"\\n#Samples in val (from dataset):    {len(andalucia_dataset_norm['validation'])}\")\n",
    "print(f\"#Samples in val (from dataloader): {len(dataloader['validation'].dataset)}\")\n",
    "print(f\"#Batches in val (from dataloader): {len(dataloader['validation'])}\")\n",
    "\n",
    "# Print some stats from the test dataset.\n",
    "print(f\"\\n#Samples in test (from dataset):    {len(andalucia_dataset_norm['test'])}\")\n",
    "print(f\"#Samples in test (from dataloader)  {len(dataloader['test'].dataset)}\")\n",
    "print(f\"#Batches in test (from dataloader): {len(dataloader['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa5655-1d59-4912-991d-ccc1a360150a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11965017-e45d-4475-94df-3f7ab14a386b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c8e405-4183-4e6b-97f6-34edd7fbf17b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad6c72-db21-4d2a-bd49-dd595cdbf197",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e877dbaf-9506-45ea-9b0b-4aea8e7104de",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a5fee-3e87-4bfa-ae29-31114a5ad960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer, batch_span_train):\n",
    "\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # ======================\n",
    "    # TRAINING COMPUTATION.\n",
    "    # Iterating through the dataloader\n",
    "    # We can track the batch index and do some intra-epoch reporting.\n",
    "    for i, data in enumerate(dataloader['train']):\n",
    "\n",
    "        # Every data instance is an input + label pair.\n",
    "        inputs = data[0].to(exp.device)\n",
    "        labels = data[1].to(exp.device)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch.\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients.\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report.\n",
    "        running_loss += loss.item()\n",
    "        if i % batch_span_train == batch_span_train - 1:\n",
    "            last_loss = running_loss / batch_span_train # loss per batch\n",
    "            print(f'  batch {i+1:03d} loss: {last_loss:.4f}')\n",
    "            tb_x = epoch_index * len(dataloader['train']) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7c2564-9d4f-42fe-b9a4-8c34f30ce180",
   "metadata": {},
   "source": [
    "## Models and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979fb45-5118-4bba-96d0-007f0eedec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: resnet with random weights.\n",
    "if model_name == 'scratch':\n",
    "    print('Model without pretrained weights')\n",
    "    model = torchvision.models.resnet18(weights=None)\n",
    "elif model_name == 'imagenet':\n",
    "    print('Model with pretrained weights on imagenet-1k')\n",
    "    model = torchvision.models.resnet18(\n",
    "        weights=torchvision.models.ResNet18_Weights.DEFAULT\n",
    "    )\n",
    "\n",
    "print(f'Old final fully-connected layer: {model.fc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d66c4-8606-4355-b233-8094d3d4b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of newly constructed modules\n",
    "# have requires_grad=True by default.\n",
    "# Freezing all the network except the final layer.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Model structure.\n",
    "summary(\n",
    "    model,\n",
    "    input_size=(exp.batch_size, 3, exp.input_size, exp.input_size),\n",
    "    device=exp.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010c89c-94ed-4ee2-90c3-97799b5ce5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of input features to the layer.\n",
    "# Adjust the final layer to the current number of classes.\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, len(class_names))\n",
    "print(f'New final fully-connected layer: {model.fc}')\n",
    "\n",
    "# Set loss.\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "print(f'Loss: {loss_fn}')\n",
    "\n",
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "print(f'Optimizer:\\n{optimizer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f97db-8905-4055-8235-e89b11f11451",
   "metadata": {},
   "source": [
    "## Per-epoch activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733e6e5-1938-4fa9-92cc-b5d449bce3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# SET UP WRITERS AND VARIABLES.\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run.\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_path = os.path.join('runs', f'lulc_finetuning_{model_name}_{timestamp}')\n",
    "print(f'Run path: {run_path}')\n",
    "writer = SummaryWriter(run_path)\n",
    "\n",
    "# Open the file in the write mode.\n",
    "header = ['epoch', 'avg_loss', 'avg_vloss']\n",
    "csv_file = os.path.join(run_path, 'training_info.csv')\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    csv_writer = csv.writer(file)  # Write the header.\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "# Initial variables.\n",
    "epoch_number = 0\n",
    "batch_span_train = len(dataloader['train']) // 5\n",
    "batch_span_val = len(dataloader['validation']) // 5\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "# Device used for training.\n",
    "print(f'Using {exp.device} device')\n",
    "model.to(exp.device)\n",
    "\n",
    "# ======================\n",
    "# TRAINING LOOP.\n",
    "# Iterating over the epochs.\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nEPOCH {epoch_number + 1}:')\n",
    "\n",
    "    # ======================\n",
    "    # TRAINING LOSS.\n",
    "    # Make sure gradient tracking is on, and do a pass over the data.\n",
    "    print('Running training...')\n",
    "    model.train()\n",
    "    avg_loss = train_one_epoch(epoch_number, writer, batch_span_train)\n",
    "    print('Training completed!')\n",
    "\n",
    "    # ======================\n",
    "    # EVALUATION COMPUTATION.\n",
    "    # We don't need gradients on to do reporting.\n",
    "    print('Running evaluation...')\n",
    "    model.eval()\n",
    "    running_vloss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(dataloader['validation']):\n",
    "            vinputs = vdata[0].to(exp.device)\n",
    "            vlabels = vdata[1].to(exp.device)            \n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss.item()\n",
    "            if i % batch_span_val == batch_span_val - 1:\n",
    "                print(f'  batch {i+1:03d} vloss: {vloss:.4f}')\n",
    "\n",
    "    print('Evaluation completed!')\n",
    "\n",
    "    # ======================\n",
    "    # VALIDATION LOSS.\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(f'LOSS train {avg_loss} val {avg_vloss}')\n",
    "\n",
    "    # ======================\n",
    "    # SAVING DATA.\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation.\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                       {'Training' : avg_loss, 'Validation' : avg_vloss},\n",
    "                       epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Open the file in the append mode.\n",
    "    data = [epoch, avg_loss, avg_vloss]\n",
    "    with open(csv_file, 'a', newline='') as file:\n",
    "        csv_writer = csv.writer(file)  # Write the data.\n",
    "        csv_writer.writerow(data)\n",
    "\n",
    "    # ======================\n",
    "    # SAVING CHECKPOINT.\n",
    "    # Track best performance, and save the model's state.\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = os.path.join(run_path, f'model_{model_name}_{timestamp}_{epoch_number}')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8b45de-22bc-4145-b35b-90af50ebbb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_notebook():\n",
    "    %tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a1daa-ce5d-47ee-b179-7abfb2066b8d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249490a6-7fc7-45bd-a99f-4d8f85eee838",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f2baf-4a21-4fae-8935-e0ee7656a31d",
   "metadata": {},
   "source": [
    "UP TO HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06196e35-2417-40f2-9bad-f197103911de",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2512f-dd8c-4bcf-b5b6-fbb648845fd5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437632b1-b33e-46a7-9c0c-5c0046318faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training function.\n",
    "# def train_model(model, criterion, optimizer, device,\n",
    "#                 epochs=10, save_best_model=False):\n",
    "#     \"\"\"\n",
    "#     Main training function.\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(f\"Using {exp.device} device\")\n",
    "\n",
    "#     # Avoiding \"CUDA out of memory\" in PyTorch.\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     # Loss history.\n",
    "#     loss_values = {}\n",
    "#     loss_values['train'] = []\n",
    "#     loss_values['val'] = []\n",
    "#     total_time = 0\n",
    "\n",
    "#     # Saving best model's weights.\n",
    "#     best_model_val_wts = copy.deepcopy(model.state_dict())\n",
    "#     lowest_val_loss = 10000\n",
    "\n",
    "#     # Model to GPU if available.\n",
    "#     model.to(exp.device)\n",
    "\n",
    "#     # Iterating over the epochs.\n",
    "#     for epoch in range(epochs):\n",
    "\n",
    "#         # Initialize training loss.\n",
    "#         running_train_loss = 0.0\n",
    "\n",
    "#         # Start timer.\n",
    "#         since = time.time()\n",
    "\n",
    "#         # Enable training.\n",
    "#         model.train()\n",
    "\n",
    "#         for i, data in enumerate(dataloader_train):\n",
    "\n",
    "#             # Get the inputs; data is a list of [inputs, labels].\n",
    "#             inputs, labels = data[0].to(exp.device), data[1].to(exp.device)\n",
    "\n",
    "#             # Zero the parameter gradients.\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # Forward: make predictions.\n",
    "#             outputs = model(inputs)\n",
    "\n",
    "#             # Compute the loss and its gradients.\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Averaged loss across all training examples * batch_size.\n",
    "#             running_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "#             if i % 200 == 199:\n",
    "#                 print(f'T[{epoch + 1}, {i + 1:5d}] | '\n",
    "#                       f'Running loss: '\n",
    "#                       f'{running_train_loss/(i*inputs.size(0)):.4f}')\n",
    "\n",
    "#             # Adjust learning weights.\n",
    "#             optimizer.step()\n",
    "\n",
    "#         # Loss averaged across all training examples for the current epoch.\n",
    "#         epoch_train_loss = running_train_loss / len(dataloader_train.sampler)\n",
    "\n",
    "#         # Change model to evaluation mode.\n",
    "#         model.eval()\n",
    "\n",
    "#         # Initialize validating loss.\n",
    "#         running_val_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for j, vdata in enumerate(dataloader_val):\n",
    "\n",
    "#                 # Get the inputs; data is a list of [inputs, labels].\n",
    "#                 vinputs, vlabels = vdata[0].to(exp.device), vdata[1].to(exp.device)\n",
    "\n",
    "#                 # Forward: make predictions.\n",
    "#                 voutputs = model(vinputs)\n",
    "\n",
    "#                 # Compute the loss (w/o gradients).\n",
    "#                 vloss = criterion(voutputs, vlabels)\n",
    "\n",
    "#                 # Averaged loss across all validating examples * batch_size.\n",
    "#                 running_val_loss += vloss.item() * vinputs.size(0)\n",
    "\n",
    "#                 if j % 50 == 49:\n",
    "#                     print(f'V[{epoch + 1}, {j + 1:5d}] | '\n",
    "#                           f'Running loss: '\n",
    "#                           f'{running_val_loss/(j*inputs.size(0)):.4f}')\n",
    "\n",
    "#         # Loss averaged across all validating examples for the current epoch.\n",
    "#         epoch_val_loss = running_val_loss / len(dataloader_val.sampler)\n",
    "\n",
    "#         # Append loss values.\n",
    "#         loss_values['train'].append(epoch_train_loss)\n",
    "#         loss_values['val'].append(epoch_val_loss)\n",
    "\n",
    "#         # Deep copy the weights of the model.\n",
    "#         save_weights = epoch_val_loss < lowest_val_loss\n",
    "#         if save_weights:\n",
    "#             lowest_val_loss = epoch_val_loss\n",
    "#             best_model_train_loss = epoch_train_loss\n",
    "#             best_model_val_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "#         # End timer.\n",
    "#         time_elapsed = time.time() - since\n",
    "#         total_time += time_elapsed\n",
    "\n",
    "#         # Show stats.\n",
    "#         print(f'Epoch: {epoch} | '\n",
    "#               f'Train loss: {epoch_train_loss:.4f} | '\n",
    "#               f'Val loss: {epoch_val_loss:.4f} | '\n",
    "#               f'Elapsed: {time_elapsed // 60:.0f}m '\n",
    "#               f'{time_elapsed % 60:.0f}s | '\n",
    "#               f'Save weights: {save_weights}')\n",
    "\n",
    "#     print(f'\\nTraining completed in {total_time // 60:.0f}m '\n",
    "#           f'{total_time % 60:.0f}s')\n",
    "\n",
    "#     # Load best model weights.\n",
    "#     model.load_state_dict(best_model_val_wts)\n",
    "\n",
    "#     if save_best_model:\n",
    "\n",
    "#         # Move to CPU before saving it.\n",
    "#         model.to('cpu')\n",
    "\n",
    "#         # Filename with stats.\n",
    "#         save_path = f'pytorch_models/resnet18' \\\n",
    "#                     f'-losses={best_model_train_loss:.2f}' \\\n",
    "#                     f'_{lowest_val_loss:.2f}' \\\n",
    "#                     f'-time={datetime.now():%Y_%m_%d-%H_%M_%S}'\n",
    "\n",
    "#         # Save this pretrained model (recommended approach).\n",
    "#         torch.save(model.state_dict(), save_path)\n",
    "\n",
    "#         print('Model successfuly saved')\n",
    "\n",
    "#         # Move back to the GPU.\n",
    "#         model.to(exp.device)\n",
    "\n",
    "#     return model, loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a7f82-4423-4d81-85a8-0ecbaba41c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_losses(loss_history, title='', save_fig=False):\n",
    "#     \"\"\"\n",
    "#     Function for plotting the training and validation losses\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     fig = plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(loss_history['train'], label='Train')\n",
    "#     plt.plot(loss_history['val'], label='Validation')\n",
    "#     plt.xlabel('Epoch', labelpad=15)\n",
    "#     plt.ylabel('Loss', labelpad=15)\n",
    "#     plt.title(title)\n",
    "#     plt.gcf().subplots_adjust(bottom=0.15)\n",
    "#     plt.gcf().subplots_adjust(left=0.15)\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     if save_fig:\n",
    "#         fig.savefig('plt_loss_values.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff272e-4786-4a49-b7fc-73ce774a88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluation_on_test(model, device):\n",
    "#     \"\"\"\n",
    "#     Function to evaluate the performance\n",
    "#     of the model on the test dataset.\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Avoiding \"CUDA out of memory\" in PyTorch.\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     # Since we're not training, we don't need to calculate\n",
    "#     # the gradients for our outputs with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for i, data in enumerate(dataloader_test):\n",
    "\n",
    "#             # Dataset.\n",
    "#             inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "#             # Calculate outputs by running images through the network.\n",
    "#             outputs = model(inputs)\n",
    "\n",
    "#             # The class with the highest energy is what we\n",
    "#             # choose as prediction.\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#             # Progress bar.\n",
    "#             if i % 50 == 49:\n",
    "#                 print(f'Progress: {100 * i // len(dataloader_test)}%',\n",
    "#                       end='\\r',\n",
    "#                       flush=True)\n",
    "\n",
    "#     print(f'Accuracy of the network on the {total} '\n",
    "#           f'test images: {100 * correct // total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c48da-0289-4350-8641-62c207d44143",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18762958-4bb5-4a9e-a642-e5cecbc0f8f1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce24b86-e4e5-442a-876a-782db427e0c3",
   "metadata": {},
   "source": [
    "# ResNet18 from scrath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5b72a3-8fd8-45fe-8353-e77cdc5f99d6",
   "metadata": {},
   "source": [
    "## Definition and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b373932-506b-45a0-9301-d182e038d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: resnet with random weights.\n",
    "model = torchvision.models.resnet18(weights=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8627587-6558-4637-b5b2-f8e0cdc7b3ea",
   "metadata": {},
   "source": [
    "## Adjust final layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d714e83-002f-4f1a-b240-cf7e80ae41ca",
   "metadata": {},
   "source": [
    "Type: linear not softmax for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ad1ef-8697-41ab-a1af-1b6f615715c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check old final layer.\n",
    "print(model.fc)\n",
    "\n",
    "# Get the number of input features to the layer.\n",
    "num_ftrs = model.fc.in_features\n",
    "print(num_ftrs)\n",
    "\n",
    "# Adjust the final layer to the current number of classes.\n",
    "model.fc = torch.nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "# Check new final layer.\n",
    "print(model.fc)\n",
    "\n",
    "# Freezing all the network except the final layer.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Model structure.\n",
    "summary(\n",
    "    model,\n",
    "    input_size=(exp.batch_size, 3, exp.input_size, exp.input_size),\n",
    "    device=exp.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fdf194-24c8-45c3-bf12-d6ac176df30e",
   "metadata": {},
   "source": [
    "## Loss fcn and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea9cdf-c59e-4e10-94b5-f6017bda668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: cross-entropy loss.\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizers: specified in the torch.optim package\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee9848-8c21-489d-8adc-2fc0e0bff9b0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba464d96-1913-43ef-96c4-59b5fdf9b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss_history = train_model(\n",
    "    model,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    exp.device,\n",
    "    epochs=exp.epochs,\n",
    "    save_best_model=True\n",
    ")\n",
    "\n",
    "plot_losses(loss_history, 'Model w/o pretrained weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0504b3-ae1c-4836-87e9-3c45e93b92d7",
   "metadata": {},
   "source": [
    "## Check performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515fcfa-ed16-42f9-8b70-992fde11c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_on_test(model, exp.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978fc629-e5a5-4608-bab6-ceee0b554bed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d0dd0-1d74-45aa-b3ca-0b12d79ca712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "conf_mat, class_accuracy = utils.create_confusion_matrix(\n",
    "    model,\n",
    "    dataloader_test,\n",
    "    exp.device,\n",
    "    class_names\n",
    ")\n",
    "\n",
    "# Bar plot for accuracy values.\n",
    "utils.simple_bar_plot(range(len(class_names)),\n",
    "                      class_accuracy,\n",
    "                      'Classes',\n",
    "                      'Accuracy (%)',\n",
    "                      'class_accuracy',\n",
    "                      fig_size=(15, 5),\n",
    "                      save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d101e8-426c-408d-a5fc-345d08a4434d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d76c1c-8bf2-4b5e-ae24-9fd71ab6b7b0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b22ad-185d-4251-8545-744f9e403c59",
   "metadata": {},
   "source": [
    "# ResNet18 with pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e46b01-7e1d-4215-b2ae-c0afa84c82ac",
   "metadata": {},
   "source": [
    "## Definition and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472cdf9a-2854-4bd0-8ef5-4e5d179f0019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: resnet with pretrained weights.\n",
    "del model\n",
    "model = torchvision.models.resnet18(\n",
    "    weights=torchvision.models.ResNet18_Weights.DEFAULT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9f8c6-dc14-4557-9ed2-b9d6084093b5",
   "metadata": {},
   "source": [
    "## Adjust final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63f366-3feb-40fa-bbcf-117fbca74817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check old final layer.\n",
    "print(model.fc)\n",
    "\n",
    "# Get the number of input features to the layer.\n",
    "num_ftrs = model.fc.in_features\n",
    "print(num_ftrs)\n",
    "\n",
    "# Adjust the final layer to the current number of classes.\n",
    "model.fc = torch.nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "# Parameters of newly constructed modules\n",
    "# have requires_grad=True by default.\n",
    "# Check new final layer.\n",
    "print(model.fc)\n",
    "\n",
    "# Freezing all the network except the final layer.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Model structure.\n",
    "summary(\n",
    "    model,\n",
    "    input_size=(exp.batch_size, 3, exp.input_size, exp.input_size),\n",
    "    device=exp.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e916e25-8b38-4e14-beb4-f311126e01f1",
   "metadata": {},
   "source": [
    "## Loss fcn and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaccb2dc-7957-4857-9bf7-b9eab949bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: cross-entropy loss.\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizers: specified in the torch.optim package\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ec5c5-ddc4-4938-9720-8e07068997d0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea3d48-0270-41dc-a48d-15953ffb23e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss_history = train_model(\n",
    "    model,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    exp.device,\n",
    "    epochs=exp.epochs,\n",
    "    save_best_model=True\n",
    ")\n",
    "\n",
    "plot_losses(loss_history, 'Model w/ pretrained weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a40815-9743-42c6-8643-a963ce8533e6",
   "metadata": {},
   "source": [
    "## Check performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9e847-5b2e-49fc-ad8e-92529b5410e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_on_test(model, exp.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ab91b-c609-44a1-bfd7-61a456c6a22b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790a3a2-2350-49a7-9461-dbcca50335e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "conf_mat, class_accuracy = utils.create_confusion_matrix(\n",
    "    model,\n",
    "    dataloader_test,\n",
    "    exp.device,\n",
    "    class_names\n",
    ")\n",
    "\n",
    "# Bar plot for accuracy values.\n",
    "utils.simple_bar_plot(range(len(class_names)),\n",
    "                      class_accuracy,\n",
    "                      'Classes',\n",
    "                      'Accuracy (%)',\n",
    "                      'class_accuracy',\n",
    "                      fig_size=(15, 5),\n",
    "                      save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bba927-e499-45b7-9d32-bd5e83b8d257",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a486bc-8b38-451a-9424-477585524559",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f806d186-1c7c-4423-8337-5d7f9433c746",
   "metadata": {},
   "source": [
    "# SSL model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839bbef-1ec9-4899-972b-9d6db8f2613c",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42234674-8005-4fcd-a76f-5d54f586fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State model class.\n",
    "resnet18 = torchvision.models.resnet18(weights=None)\n",
    "\n",
    "# Only backbone (w/o final fc layer).\n",
    "pt_backbone = torch.nn.Sequential(*list(resnet18.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147bd03f-ee69-4664-a0f3-57ea6aa78331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of trained models.\n",
    "model_list = []\n",
    "for root, dirs, files in os.walk('pytorch_models/history_log/'):\n",
    "    for i, filename in enumerate(sorted(files, reverse=True)):\n",
    "        model_list.append(root + filename)\n",
    "        print(f'{i:02}: {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad21f18d-da7d-4007-a8be-edd49331ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model.\n",
    "idx = 0\n",
    "print(model_list[idx])\n",
    "pt_backbone.load_state_dict(torch.load(model_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b953213-478a-41fc-8f77-c418c5cf9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model is loaded on GPU.\n",
    "next(pt_backbone.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebf71e-7453-442a-898a-1a38825e025b",
   "metadata": {},
   "source": [
    "## Checking the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d413d46-02cf-49a2-9a86-b39fbfaac08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convolutional layer weights.\n",
    "# print(backbone)\n",
    "print(pt_backbone[0])\n",
    "print(pt_backbone[0].weight[63])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd91df-abe1-42d3-ace9-8a8b45fbb0f8",
   "metadata": {},
   "source": [
    "## Adding a final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768a0a1-85c5-40a4-b843-4d50d44fd161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a linear layer on top of the model (linear classifier).\n",
    "model_ssl = torch.nn.Sequential(\n",
    "    pt_backbone,\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(in_features=512, out_features=len(class_names), bias=True),\n",
    "    # torch.nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Freezing all the network except the final layer.\n",
    "for param in model_ssl.parameters():\n",
    "    param.requires_grad = False\n",
    "# for param in model_ssl[0][7].parameters():\n",
    "#     param.requires_grad = True\n",
    "for param in model_ssl[2].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Model structure.\n",
    "summary(\n",
    "    model_ssl,\n",
    "    input_size=(exp.batch_size, 3, exp.input_size, exp.input_size),\n",
    "    device=exp.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c0096-fb2f-4183-bdab-dfc345be59c7",
   "metadata": {},
   "source": [
    "## Loss fcn and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f3ed4b-1948-4843-96ca-b4cced958a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: cross-entropy loss.\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizers: specified in the torch.optim package\n",
    "optimizer = torch.optim.Adam(model_ssl.parameters(),\n",
    "                             lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106ebc3-d159-45f8-b956-abd33add0552",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9d4f1-bc8a-45fe-88f5-dbcb71b0c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ssl, loss_history = train_model(\n",
    "    model_ssl,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    exp.device,\n",
    "    epochs=exp.epochs,\n",
    "    save_best_model=True\n",
    ")\n",
    "\n",
    "plot_losses(loss_history, 'Model w/ ssl pretrained weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815545a-4949-41fa-b208-774516170576",
   "metadata": {},
   "source": [
    "## Checking the weights after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea1ae5-4c44-4c2a-8efe-ac210a30ed79",
   "metadata": {},
   "source": [
    "They should have remained the same (frozen) except for the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f242c-38db-4d11-b84b-4bbab655297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convolutional layer weights.\n",
    "# print(backbone)\n",
    "print(pt_backbone[0])\n",
    "print(pt_backbone[0].weight[63])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9dff9a-1277-4301-b08e-8152110abe26",
   "metadata": {},
   "source": [
    "## Check performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636a4a8-8ee4-4861-9aa8-920f0df40b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_on_test(model_ssl, exp.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc6156-7506-449e-bc56-873c23aaf091",
   "metadata": {
    "tags": []
   },
   "source": [
    "## F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5024fea-751f-40a7-93ac-3a897285f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "conf_mat, class_accuracy = utils.create_confusion_matrix(\n",
    "    model_ssl,\n",
    "    dataloader_test,\n",
    "    exp.device,\n",
    "    class_names\n",
    ")\n",
    "\n",
    "# Bar plot for accuracy values.\n",
    "utils.simple_bar_plot(range(len(class_names)),\n",
    "                      class_accuracy,\n",
    "                      'Classes',\n",
    "                      'Accuracy (%)',\n",
    "                      'class_accuracy',\n",
    "                      fig_size=(15, 5),\n",
    "                      save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ad0aa-d143-42dd-96f9-63c9cf6ad81f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238c6eb-d2e8-419d-b24c-1c828f995abe",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lulc-conda",
   "language": "python",
   "name": "lulc-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "19b3d86a20f2229bb5b60544c2c1729b82d0d9e897d3203c1cf087eb7e47686c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
